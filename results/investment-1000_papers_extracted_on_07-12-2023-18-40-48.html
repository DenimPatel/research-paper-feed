<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    <html>
    <head>
    <title>Mathedemo</title>
    <style>
          body {
             margin-left: 400px;
             margin-right: 400px;
          }
       </style>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    </head>

     <br> <br> <br> <font size='5'> 1 </font> <div style="text-align: right"> 2023-07-09 18:47:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reinforcement Learning for Joint Design and Control of Battery-PV Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The decentralisation and unpredictability of new renewable energy sources
require rethinking our energy system. Data-driven approaches, such as
reinforcement learning (RL), have emerged as new control strategies for
operating these systems, but they have not yet been applied to system design.
This paper aims to bridge this gap by studying the use of an RL-based method
for joint design and control of a real-world PV and battery system. The design
problem is first formulated as a mixed-integer linear programming problem
(MILP). The optimal MILP solution is then used to evaluate the performance of
an RL agent trained in a surrogate environment designed for applying an
existing data-driven algorithm. The main difference between the two models lies
in their optimization approaches: while MILP finds a solution that minimizes
the total costs for a one-year operation given the deterministic historical
data, RL is a stochastic method that searches for an optimal strategy over one
week of data on expectation over all weeks in the historical dataset. Both
methods were applied on a toy example using one-week data and on a case study
using one-year data. In both cases, models were found to converge to similar
control solutions, but their investment decisions differed. Overall, these
outcomes are an initial step illustrating benefits and challenges of using RL
for the joint design and control of energy systems. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04244v1' target="_blank">http://arxiv.org/pdf/2307.04244v1</a><br> <br> <br> <font size='5'> 2 </font> <div style="text-align: right"> 2023-07-08 08:46:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Better Research Software Tools to Elevate the Rate of Scientific Discovery -- or why we need to invest in research software engineering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the past decade, enormous progress has been made in advancing the
state-of-the-art in bioimage analysis - a young computational field that works
in close collaboration with the life sciences on the quantitative analysis of
scientific image data. In many cases, tremendous effort has been spent to
package these new advances into usable software tools and, as a result, users
can nowadays routinely apply cutting-edge methods to their analysis problems
using software tools such as ilastik [1], cellprofiler [2], Fiji/ImageJ2 [3,4]
and its many modern plugins that build on the BigDataViewer ecosystem [5], and
many others. Such software tools have now become part of a critical
infrastructure for science [6]. Unfortunately, overshadowed by the few
exceptions that have had long-lasting impact, many other potentially useful
tools fail to find their way into the hands of users. While there are many
reasons for this, we believe that at least some of the underlying problems,
which we discuss in more detail below, can be mitigated. In this opinion piece,
we specifically argue that embedding teams of research software engineers
(RSEs) within imaging and image analysis core facilities would be a major step
towards sustainable bioimage analysis software. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03934v1' target="_blank">http://arxiv.org/pdf/2307.03934v1</a><br> <br> <br> <font size='5'> 3 </font> <div style="text-align: right"> 2023-07-06 18:49:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Numerically Unveiling Hidden Chaotic Dynamics in Nonlinear Differential Equations with Riemann-Liouville, Caputo-Fabrizio, and Atangana-Baleanu Fractional Derivatives</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, the use of variable-order differential operators has emerged
as a powerful tool in the analysis of nonlinear fractional differential
equations and chaotic systems. In finance, the accurate prediction of market
trends and the ability to make informed investment decisions is of great
importance, and the integration of artificial intelligence and mathematics has
greatly improved the accuracy of these predictions. In this study, we displayed
an analysis of adaptive equations produced by three fractional derivatives: the
Riemann-Lioville, Caputo-Fabrizio, and Atangana-Baleanu fractional derivatives.
These fractional derivatives were employed to analyze financial models in order
to gain a deeper understanding of the complex dynamics of financial markets.
The models studied were the Lorenz system, Rossler system, and Shilnikov
cashless model. The results showed that each fractional derivative produced
varying outcomes and computation times, highlighting the importance of
selecting the appropriate mathematical approach and software for financial
modeling. The findings of this study underscore the continued integration of
Artificial Intelligence and mathematics in financial analysis and
decision-making, driving the future of investment strategies and market
predictions.The application of variable-order differential operators in the
analysis of nonlinear fractional differential equations and chaotic systems is
an important and growing area of research that holds great promise for the
field of finance. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03251v1' target="_blank">http://arxiv.org/pdf/2307.03251v1</a><br> <br> <br> <font size='5'> 4 </font> <div style="text-align: right"> 2023-07-05 20:54:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The geography of innovation dynamics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cities and metropolitan areas are major drivers of creativity and innovation
in all possible sectors: scientific, technological, social, artistic, etc. The
critical concentration and proximity of diverse mindsets and opportunities,
supported by efficient infrastructures, enable new technologies and ideas to
emerge, thrive, and trigger further innovation. Though this pattern seems well
established, geography's role in the emergence and diffusion of new
technologies still needs to be clarified. An additional important question
concerns the identification of the innovation pathways of metropolitan areas.
Here, we explore the factors that influence the spread of technology among
metropolitan areas worldwide and how geography and political borders impact
this process. Our evidence suggests that political geography has been highly
important for the diffusion of innovation till around two decades ago, slowly
declining afterwards in favour of a more global innovation ecosystem. Further,
the visualisation of the evolution of countries and metropolitan areas in a 2d
space of competitiveness and diversification reveals the existence of two main
innovation pathways, discriminating between different strategies towards
progress. Our work provides insights for policymakers seeking to promote
economic growth and technological advancement through tailored investments in
prioritarian innovation areas. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02651v1' target="_blank">http://arxiv.org/pdf/2307.02651v1</a><br> <br> <br> <font size='5'> 5 </font> <div style="text-align: right"> 2023-07-05 14:13:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust Hedging GANs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The availability of deep hedging has opened new horizons for solving hedging
problems under a large variety of realistic market conditions. At the same
time, any model - be it a traditional stochastic model or a market generator -
is at best an approximation of market reality, prone to model-misspecification
and estimation errors. This raises the question, how to furnish a modelling
setup with tools that can address the risk of discrepancy between anticipated
distribution and market reality, in an automated way. Automated robustification
is currently attracting increased attention in numerous investment problems,
but it is a delicate task due to its imminent implications on risk management.
Hence, it is beyond doubt that more activity can be anticipated on this topic
to converge towards a consensus on best practices.
  This paper presents a natural extension of the original deep hedging
framework to address uncertainty in the data generating process via an
adversarial approach inspired by GANs to automate robustification in our
hedging objective. This is achieved through an interplay of three modular
components: (i) a (deep) hedging engine, (ii) a data-generating process (that
is model agnostic permitting a large variety of classical models as well as
machine learning-based market generators), and (iii) a notion of distance on
model space to measure deviations between our market prognosis and reality. We
do not restrict the ambiguity set to a region around a reference model, but
instead penalize deviations from the anticipated distribution. Our suggested
choice for each component is motivated by model agnosticism, allowing a
seamless transition between settings. Since all individual components are
already used in practice, we believe that our framework is easily adaptable to
existing functional settings. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02310v1' target="_blank">http://arxiv.org/pdf/2307.02310v1</a><br> <br> <br> <font size='5'> 6 </font> <div style="text-align: right"> 2023-07-04 13:45:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MOPO-LSI: A User Guide</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01719v1' target="_blank">http://arxiv.org/pdf/2307.01719v1</a><br> <br> <br> <font size='5'> 7 </font> <div style="text-align: right"> 2023-07-04 10:14:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Stackelberg viral marketing design for two competing players</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A Stackelberg duopoly model in which two firms compete to maximize their
market share is considered. The firms offer a service/product to customers that
are spread over several geographical regions (e.g., countries, provinces, or
states). Each region has its own characteristics (spreading and recovery rates)
of each service propagation. We consider that the spreading rate can be
controlled by each firm and is subject to some investment that the firm does in
each region. One of the main objectives of this work is to characterize the
advertising budget allocation strategy for each firm across regions to maximize
its market share when competing. To achieve this goal we propose a Stackelberg
game model that is relatively simple while capturing the main effects of the
competition for market share. {By characterizing the strong/weak Stackelberg
equilibria of the game, we provide the associated budget allocation strategy.}
In this setting, it is established under which conditions the solution of the
game is the so-called ``winner takes all". Numerical results expand upon our
theoretical findings and we provide the equilibrium characterization for an
example. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01618v1' target="_blank">http://arxiv.org/pdf/2307.01618v1</a><br> <br> <br> <font size='5'> 8 </font> <div style="text-align: right"> 2023-07-04 02:38:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Emissions and Energy Impacts of the Inflation Reduction Act</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: If goals set under the Paris Agreement are met, the world may hold warming
well below 2 C; however, parties are not on track to deliver these commitments,
increasing focus on policy implementation to close the gap between ambition and
action. Recently, the US government passed its most prominent piece of climate
legislation to date, the Inflation Reduction Act of 2022 (IRA), designed to
invest in a wide range of programs that, among other provisions, incentivize
clean energy and carbon management, encourage electrification and efficiency
measures, reduce methane emissions, promote domestic supply chains, and address
environmental justice concerns. IRA's scope and complexity make modeling
important to understand impacts on emissions and energy systems. We leverage
results from nine independent, state-of-the-art models to examine potential
implications of key IRA provisions, showing economy wide emissions reductions
between 43-48% below 2005 by 2035. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01443v1' target="_blank">http://arxiv.org/pdf/2307.01443v1</a><br> <br> <br> <font size='5'> 9 </font> <div style="text-align: right"> 2023-07-03 07:44:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Replication of financial derivatives under extreme market models given marginals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Black-Scholes-Merton model is a mathematical model for the dynamics of a
financial market that includes derivative investment instruments, and its
formula provides a theoretical price estimate of European-style options. The
model's fundamental idea is to eliminate risk by hedging the option by
purchasing and selling the underlying asset in a specific way, that is, to
replicate the payoff of the option with a portfolio (which continuously trades
the underlying) whose value at each time can be verified. One of the most
crucial, yet restrictive, assumptions for this task is that the market follows
a geometric Brownian motion, which has been relaxed and generalized in various
ways.
  The concept of robust finance revolves around developing models that account
for uncertainties and variations in financial markets. Martingale Optimal
Transport, which is an adaptation of the Optimal Transport theory to the robust
financial framework, is one of the most prominent directions. In this paper, we
consider market models with arbitrarily many underlying assets whose values are
observed over arbitrarily many time periods, and demonstrates the existence of
a portfolio sub- or super-hedging a general path-dependent derivative security
in terms of trading European options and underlyings, as well as the portfolio
replicating the derivative payoff when the market model yields the extremal
price of the derivative given marginal distributions of the underlyings. In
mathematical terms, this paper resolves the question of dual attainment for the
multi-period vectorial martingale optimal transport problem. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00807v1' target="_blank">http://arxiv.org/pdf/2307.00807v1</a><br> <br> <br> <font size='5'> 10 </font> <div style="text-align: right"> 2023-07-02 15:33:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A key challenge in robotic manipulation in open domains is how to acquire
diverse and generalizable skills for robots. Recent research in one-shot
imitation learning has shown promise in transferring trained policies to new
tasks based on demonstrations. This feature is attractive for enabling robots
to acquire new skills and improving task and motion planning. However, due to
limitations in the training dataset, the current focus of the community has
mainly been on simple cases, such as push or pick-place tasks, relying solely
on visual guidance. In reality, there are many complex skills, some of which
may even require both visual and tactile perception to solve. This paper aims
to unlock the potential for an agent to generalize to hundreds of real-world
skills with multi-modal perception. To achieve this, we have collected a
dataset comprising over 110,000 \emph{contact-rich} robot manipulation
sequences across diverse skills, contexts, robots, and camera viewpoints, all
collected \emph{in the real world}. Each sequence in the dataset includes
visual, force, audio, and action information, along with a corresponding human
demonstration video. We have invested significant efforts in calibrating all
the sensors and ensuring a high-quality dataset. The dataset is made publicly
available at rh20t.github.io </font><br> Link: <a href='http://arxiv.org/pdf/2307.00595v1' target="_blank">http://arxiv.org/pdf/2307.00595v1</a><br> <br> <br> <font size='5'> 11 </font> <div style="text-align: right"> 2023-07-01 14:52:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparing Mobile Testing Tools Using Documentary Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Due to the high demand for mobile applications, given the exponential growth
of users of this type of technology, testing professionals are frequently
required to invest time in studying testing tools, in particular, because
nowadays, several different tools are available. A variety of tools makes it
difficult for testing professionals to choose the one that best fits their
goals and supports them in their work. In this sense, we conducted a
comparative analysis among five open-source tools for mobile testing: Appium,
Robotium, Espresso, Frank, and EarGrey. We used the documentary analysis method
to explore the official documentation of each above-cited tool and developed
various comparisons based on technical criteria reported in the literature
about characteristics that mobile testing tools should have. Our findings are
expected to help practitioners understand several aspects of mobile testing
tools. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00355v1' target="_blank">http://arxiv.org/pdf/2307.00355v1</a><br> <br> <br> <font size='5'> 12 </font> <div style="text-align: right"> 2023-06-28 21:04:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Monte Carlo Methods for Retirement Simulations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Retirement prediction helps individuals and institutions make informed
finan-cial, lifestyle, and workforce decisions based on estimated retirement
portfolios. This paper attempts to predict retirement using Monte Carlo
simulations, allow-ing one to probabilistically account for a range of
possibilities. The authors propose a model to predict the values of the
investment accounts IRA and 401(k) through the simulation of inflation rates,
interest rates, and other perti-nent factors. They provide a user case study to
discuss the implications of the proposed model. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16563v1' target="_blank">http://arxiv.org/pdf/2306.16563v1</a><br> <br> <br> <font size='5'> 13 </font> <div style="text-align: right"> 2023-06-28 18:42:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessing Water Performance Indicators for Leakage Reduction and Asset Management in Water Supply Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Water Supply Systems are essential infrastructures for the socio-economic
life of urban cities. To improve their reliability, water utilities undertake
several short- and long-term operational tasks based on technical and economic
constraints. These activities are motivated by many factors, including
increasing leakage rates due to infrastructure aging, increased consumer
demands and need for sustainable use of water and energy. European and national
regulatory bodies have promoted investment programs for allowing water
utilities to reach common standards of reliability and quality of service among
countries. Targets of management and operational achievements are usually
measured using specific performance indicators. The Italian Regulatory
Authority for Energy, Networks and Environment (ARERA) recently introduced the
Regulation of the technical performances of water utilities. Performances on
leakage management and investment plans of the utilities are thus based on two
indicators named M1a (linear leakage index) and M1b (percentage leakage index).
This paper analyzes in details the inconsistencies of the percentage leakage
index (M1b), mainly due to its mathematical formulation and the ambiguity of
defining water consumption as part of the total system inflow. The discussion
is supported by a real case study, where both indicators have been calculated
to assess their impact on management decisions and investment plans. The
inconsistencies of the percentage leakage index are further demonstrated for
various layouts of water supply systems. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17195v1' target="_blank">http://arxiv.org/pdf/2306.17195v1</a><br> <br> <br> <font size='5'> 14 </font> <div style="text-align: right"> 2023-06-27 17:51:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent work CLIPA presents an inverse scaling law for CLIP training --
whereby the larger the image/text encoders used, the shorter the sequence
length of image/text tokens that can be applied in training. This finding
enables us to train high-performance CLIP models with significantly reduced
computations. Building upon this work, we hereby present CLIPA-v2 with two key
contributions. Technically, we find this inverse scaling law is also applicable
in the finetuning stage, enabling further reduction in computational needs.
Empirically, we explore CLIPA at scale, extending the experiments up to the
H/14 model with ~13B image-text pairs seen during training.
  Our results are exciting -- by only allocating a budget of \$10,000, our CLIP
model achieves an impressive zero-shot ImageNet accuracy of 81.1%, surpassing
the prior best CLIP model (from OpenCLIP, 80.1%) by 1.0% and meanwhile reducing
the computational cost by ~39X. Moreover, with an additional investment of
$4,000, we can further elevate the zero-shot ImageNet accuracy to 81.8%. Our
code and models are available at https://github.com/UCSC-VLAA/CLIPA. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15658v1' target="_blank">http://arxiv.org/pdf/2306.15658v1</a><br> <br> <br> <font size='5'> 15 </font> <div style="text-align: right"> 2023-06-26 19:32:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Valuation of Equity Linked Securities with Guaranteed Return</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Equity-linked securities with a guaranteed return become very popular in
financial markets ether as investment instruments or life insurance policies.
The contract pays off a guaranteed amount plus a payment linked to the
performance of a basket of equities averaged over a certain period. This paper
presents a new model for valuing equity-linked securities. Our study shows that
the security price can be replicated by the sum of the guaranteed amount plus
the price of an Asian style option on the basket. Analytical formulas are
derived for the security price and corresponding hedge ratios. The model
appears to be accurate over a wide range of underlying security parameters
according to numerical studies. Finally, we use our model to value a segregated
fund with a guarantee at maturity. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15026v1' target="_blank">http://arxiv.org/pdf/2306.15026v1</a><br> <br> <br> <font size='5'> 16 </font> <div style="text-align: right"> 2023-06-26 15:51:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Private Federated Learning in Gboard</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This white paper describes recent advances in Gboard(Google Keyboard)'s use
of federated learning, DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm,
and secure aggregation techniques to train machine learning (ML) models for
suggestion, prediction and correction intelligence from many users' typing
data. Gboard's investment in those privacy technologies allows users' typing
data to be processed locally on device, to be aggregated as early as possible,
and to have strong anonymization and differential privacy where possible.
Technical strategies and practices have been established to allow ML models to
be trained and deployed with meaningfully formal DP guarantees and high
utility. The paper also looks ahead to how technologies such as trusted
execution environments may be used to further improve the privacy and security
of Gboard's ML models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14793v1' target="_blank">http://arxiv.org/pdf/2306.14793v1</a><br> <br> <br> <font size='5'> 17 </font> <div style="text-align: right"> 2023-06-25 12:08:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancement of Large Language Models (LLMs) has led to extensive
discourse regarding their potential to boost the return of quantitative stock
trading strategies. This discourse primarily revolves around harnessing the
remarkable comprehension capabilities of LLMs to extract sentiment factors
which facilitate informed and high-frequency investment portfolio adjustments.
To ensure successful implementations of these LLMs into the analysis of Chinese
financial texts and the subsequent trading strategy development within the
Chinese stock market, we provide a rigorous and encompassing benchmark as well
as a standardized back-testing framework aiming at objectively assessing the
efficacy of various types of LLMs in the specialized domain of sentiment factor
extraction from Chinese news text data. To illustrate how our benchmark works,
we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the
Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) the
financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply
them directly to the task of sentiment factor extraction from large volumes of
Chinese news summary texts. We then proceed to building quantitative trading
strategies and running back-tests under realistic trading scenarios based on
the derived sentiment factors and evaluate their performances with our
benchmark. By constructing such a comparative analysis, we invoke the question
of what constitutes the most important element for improving a LLM's
performance on extracting sentiment factors. And by ensuring that the LLMs are
evaluated on the same benchmark, following the same standardized experimental
procedures that are designed with sufficient expertise in quantitative trading,
we make the first stride toward answering such a question. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14222v1' target="_blank">http://arxiv.org/pdf/2306.14222v1</a><br> <br> <br> <font size='5'> 18 </font> <div style="text-align: right"> 2023-06-24 03:57:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Unified Approach to Controlling Implicit Regularization via Mirror Descent</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Inspired by the remarkable success of deep neural networks, there has been
significant interest in understanding the generalization performance of
overparameterized models. Substantial efforts have been invested in
characterizing how optimization algorithms impact generalization through their
"preferred" solutions, a phenomenon commonly referred to as implicit
regularization. In particular, it has been argued that gradient descent (GD)
induces an implicit $\ell_2$-norm regularization in regression and
classification problems. However, the implicit regularization of different
algorithms are confined to either a specific geometry or a particular class of
learning problems, indicating a gap in a general approach for controlling the
implicit regularization. To address this, we present a unified approach using
mirror descent (MD), a notable generalization of GD, to control implicit
regularization in both regression and classification settings. More
specifically, we show that MD with the general class of homogeneous potential
functions converges in direction to a generalized maximum-margin solution for
linear classification problems, thereby answering a long-standing question in
the classification setting. Further, we show that MD can be implemented
efficiently and under suitable conditions, enjoys fast convergence. Through
comprehensive experiments, we demonstrate that MD is a versatile method to
produce learned models with different regularizers, which in turn have
different generalization performances. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13853v1' target="_blank">http://arxiv.org/pdf/2306.13853v1</a><br> <br> <br> <font size='5'> 19 </font> <div style="text-align: right"> 2023-06-24 01:28:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: In Pursuit of Unification of Conceptual Models: Sets as Machines</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conceptual models as representations of real-world systems are based on
diverse techniques in various disciplines but lack a framework that provides
multidisciplinary ontological understanding of real-world phenomena.
Concurrently, systems complexity has intensified, leading to a rise in
developing models using different formalisms and diverse representations even
within a single domain. Conceptual models have become larger; languages tend to
acquire more features, and it is not unusual to use different modeling
languages for different components. This diversity has caused problems with
consistency between models and incompatibly with designed systems. Two main
solutions have been adopted over the last few years: (1) A currently dominant
technology-based solution tries to harmonize or unify models, e.g., unifies EER
and UML. This solution would solidify modeling achievements, reaping benefits
from huge investments over the last thirty years. (2) A less prevalent solution
is to pursuit deeper roots that reveal unifying modeling principles and
apparatuses. An example of the second method is a category theory-based
approach that utilizes the strengths of the graph and set theory, along with
other topological tools. This manuscript is a sequel in a research venture that
belongs to the second approach and uses a model called thinging machines (TMs)
founded on Stoic ontology and Lupascian logic. TM modeling contests the thesis
that there is no universal approach that covers all aspects of an application,
and the paper demonstrates that pursuing such universality is anything but a
dead-end method. This paper continues in this direction, with emphasis on TM
foundation (e.g., existence and subsistence of things) and exemplifies this
pursuit by proposing an alternative representation of set theory. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13833v1' target="_blank">http://arxiv.org/pdf/2306.13833v1</a><br> <br> <br> <font size='5'> 20 </font> <div style="text-align: right"> 2023-06-23 19:05:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cosmic Explorer: A Submission to the NSF MPSAC ngGW Subcommittee</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Gravitational-wave astronomy has revolutionized humanity's view of the
universe, a revolution driven by observations that no other field can make.
This white paper describes an observatory that builds on decades of investment
by the National Science Foundation and that will drive discovery for decades to
come: Cosmic Explorer. Major discoveries in astronomy are driven by three
related improvements: better sensitivity, higher precision, and opening new
observational windows. Cosmic Explorer promises all three and will deliver an
order-of-magnitude greater sensitivity than LIGO. Cosmic Explorer will push the
gravitational-wave frontier to almost the edge of the observable universe using
technologies that have been proven by LIGO during its development.
  With the unprecedented sensitivity that only a new facility can deliver,
Cosmic Explorer will make discoveries that cannot yet be anticipated,
especially since gravitational waves are both synergistic with electromagnetic
observations and can reach into regions of the universe that electromagnetic
observations cannot explore. With Cosmic Explorer, scientists can use the
universe as a laboratory to test the laws of physics and study the nature of
matter. Cosmic Explorer allows the United States to continue its leading role
in gravitational-wave science and the international network of next-generation
observatories. With its extraordinary discovery potential, Cosmic Explorer will
deliver revolutionary observations across astronomy, physics, and cosmology
including: Black Holes and Neutron Stars Throughout Cosmic Time,
Multi-Messenger Astrophysics and Dynamics of Dense Matter, New Probes of
Extreme Astrophysics, Fundamental Physics and Precision Cosmology, Dark Matter
and the Early Universe. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13745v1' target="_blank">http://arxiv.org/pdf/2306.13745v1</a><br> <br> <br> <font size='5'> 21 </font> <div style="text-align: right"> 2023-06-23 15:41:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detector R&D needs for the next generation $e^+e^-$ collider</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The 2021 Snowmass Energy Frontier panel wrote in its final report "The
realization of a Higgs factory will require an immediate, vigorous and targeted
detector R&D program". Both linear and circular $e^+e^-$ collider efforts have
developed a conceptual design for their detectors and are aggressively pursuing
a path to formalize these detector concepts. The U.S. has world-class expertise
in particle detectors, and is eager to play a leading role in the next
generation $e^+e^-$ collider, currently slated to become operational in the
2040s. It is urgent that the U.S. organize its efforts to provide leadership
and make significant contributions in detector R&D. These investments are
necessary to build and retain the U.S. expertise in detector R&D and future
projects, enable significant contributions during the construction phase and
maintain its leadership in the Energy Frontier regardless of the choice of the
collider project. In this document, we discuss areas where the U.S. can and
must play a leading role in the conceptual design and R&D for detectors for
$e^+e^-$ colliders. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13567v2' target="_blank">http://arxiv.org/pdf/2306.13567v2</a><br> <br> <br> <font size='5'> 22 </font> <div style="text-align: right"> 2023-06-23 13:38:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving City Life via Legitimate and Participatory Policy-making: A Data-driven Approach in Switzerland</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduces a novel data-driven approach to address challenges
faced by city policymakers concerning the distribution of public funds.
Providing budgeting processes for improving quality of life based on objective
(data-driven) evidence has been so far a missing element in policy-making. This
paper focuses on a case study of 1,204 citizens in the city of Aarau,
Switzerland, and analyzes survey data containing insightful indicators that can
impact the legitimacy of decision-making. Our approach is twofold. On the one
hand, we aim to optimize the legitimacy of policymakers' decisions by
identifying the level of investment in neighborhoods and projects that offer
the greatest return in legitimacy. To do so, we introduce a new
context-independent legitimacy metric for policymakers. This metric allows us
to distinguish decisive vs. indecisive collective preferences for neighborhoods
or projects on which to invest, enabling policymakers to prioritize impactful
bottom-up consultations and participatory initiatives (e.g., participatory
budgeting). The metric also allows policymakers to identify the optimal number
of investments in various project sectors and neighborhoods (in terms of
legitimacy gain). On the other hand, we aim to offer guidance to policymakers
concerning which satisfaction and participation factors influence citizens'
quality of life through an accurate classification model and an evaluation of
relocations. By doing so, policymakers may be able to further refine their
strategy, making targeted investments with significant benefits to citizens'
quality of life. These findings are expected to provide transformative insights
for practicing direct democracy in Switzerland and a blueprint for
policy-making to adopt worldwide. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13696v1' target="_blank">http://arxiv.org/pdf/2306.13696v1</a><br> <br> <br> <font size='5'> 23 </font> <div style="text-align: right"> 2023-06-23 07:50:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment with Stochastic Interest Rates and Ambiguity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies dynamic asset allocation with interest rate risk and
several sources of ambiguity. The market consists of a risk-free asset, a
zero-coupon bond (both determined by a Vasicek model), and a stock. There is
ambiguity about the risk premia, the volatilities, and the correlation. The
investor's preferences display both risk aversion and ambiguity aversion. The
optimal investment problem can be solved in closed-form under typical market
conditions. The solution shows that the investor does not hedge ambiguity but
only risk, while the ambiguity only affects the speculative motives of the
investor. An implementation of the optimal investment strategy shows the impact
of the different sources of ambiguity. Ambiguity aversion helps to tame the
highly leveraged portfolios neglecting ambiguity and leads to strategies that
are more in line with popular investment advice. The solution method for the
optimal investment problem is based on an extension of the martingale
optimality principle. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13343v1' target="_blank">http://arxiv.org/pdf/2306.13343v1</a><br> <br> <br> <font size='5'> 24 </font> <div style="text-align: right"> 2023-06-22 02:46:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient Solution of Portfolio Optimization Problems via Dimension Reduction and Sparsification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Markowitz mean-variance portfolio optimization model aims to balance
expected return and risk when investing. However, there is a significant
limitation when solving large portfolio optimization problems efficiently: the
large and dense covariance matrix. Since portfolio performance can be
potentially improved by considering a wider range of investments, it is
imperative to be able to solve large portfolio optimization problems
efficiently, typically in microseconds. We propose dimension reduction and
increased sparsity as remedies for the covariance matrix. The size reduction is
based on predictions from machine learning techniques and the solution to a
linear programming problem. We find that using the efficient frontier from the
linear formulation is much better at predicting the assets on the Markowitz
efficient frontier, compared to the predictions from neural networks. Reducing
the covariance matrix based on these predictions decreases both runtime and
total iterations. We also present a technique to sparsify the covariance matrix
such that it preserves positive semi-definiteness, which improves runtime per
iteration. The methods we discuss all achieved similar portfolio expected risk
and return as we would obtain from a full dense covariance matrix but with
improved optimizer performance. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12639v1' target="_blank">http://arxiv.org/pdf/2306.12639v1</a><br> <br> <br> <font size='5'> 25 </font> <div style="text-align: right"> 2023-06-21 19:59:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Estimating the Value of Evidence-Based Decision Making</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Business/policy decisions are often based on evidence from randomized
experiments and observational studies. In this article we propose an empirical
framework to estimate the value of evidence-based decision making (EBDM) and
the return on the investment in statistical precision. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13681v1' target="_blank">http://arxiv.org/pdf/2306.13681v1</a><br> <br> <br> <font size='5'> 26 </font> <div style="text-align: right"> 2023-06-21 15:42:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Practical Overview of Quantum Computing: Is Exascale Possible?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite numerous advances in the field and a seemingly ever-increasing amount
of investment, we are still some years away from seeing a production quantum
computer in action. However, it is possible to make some educated guesses about
the operational difficulties and challenges that may be encountered in
practice. We can be reasonably confident that the early machines will be
hybrid, with the quantum devices used in an apparently similar way to current
accelerators such as FPGAs or GPUs. Compilers, libraries and the other tools
relied upon currently for development of software will have to evolve/be
reinvented to support the new technology, and training courses will have to be
rethought completely rather than ``just'' updated alongside them.
  The workloads we are likely to see making best use of these hybrid machines
will initially be few, before rapidly increasing in diversity as we saw with
the uptake of GPUs and other new technologies in the past. This will again be
helped by the increase in the number of supporting libraries and development
tools, and by the gradual re-development of existing software, to make use of
the new quantum devices.
  Unfortunately, at present the problem of error correction is still largely
unsolved, although there have been many advances. Quantum computation is very
sensitive to noise, leading to frequent errors during execution.
  Quantum calculations, although asymptotically faster than their equivalents
in ``traditional'' HPC, still take time, and while the profiling tools and
programming approaches will have to change drastically, many of the skills
honed in the current HPC industry will not suddenly become obsolete, but
continue to be useful in the quantum era. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12346v1' target="_blank">http://arxiv.org/pdf/2306.12346v1</a><br> <br> <br> <font size='5'> 27 </font> <div style="text-align: right"> 2023-06-20 15:21:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Collective Arbitrage and the Value of Cooperation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce the notions of Collective Arbitrage and of Collective
Super-replication in a setting where agents are investing in their markets and
are allowed to cooperate through exchanges. We accordingly establish versions
of the fundamental theorem of asset pricing and of the pricing-hedging duality.
Examples show the advantage of our approach. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11599v1' target="_blank">http://arxiv.org/pdf/2306.11599v1</a><br> <br> <br> <font size='5'> 28 </font> <div style="text-align: right"> 2023-06-20 14:30:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uniform taxation of electricity: incentives for flexibility and cost redistribution among household categories</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent years have shown a rapid adoption of residential solar PV with
increased self-consumption and self-sufficiency levels in Europe. A major
driver for their economic viability is the electricity tax exemption for the
consumption of self-produced electricity. This leads to large residential PV
capacities and partially overburdened distribution grids. Furthermore, the tax
exemption that benefits wealthy households that can afford capital-intense
investments in solar panels in particular has sparked discussions about energy
equity and the appropriate taxation level for self-consumption. This study
investigates the implementation of uniform electricity taxes on all
consumption, irrespective of the origin of the production, by means of a case
study of 155,000 hypothetical Danish prosumers. The results show that the new
taxation policy redistributes costs progressively across household sizes. As
more consumption is taxed, the tax level can be reduced by 38%, leading to 61%
of all households seeing net savings of up to 23% off their yearly tax bill.
High-occupancy houses save an average of 116 Euro per year at the expense of
single households living in large dwellings who pay 55 Euro per year more.
Implementing a uniform electricity tax in combination with a reduced overall
tax level can (a) maintain overall tax revenues and (b) increase the
interaction of batteries with the grid at the expense of behind-the-meter
operations. In the end, the implicit cross-subsidy is removed by taxing
self-consumption uniformly, leading to a cost redistribution supporting
occupant-dense households and encouraging the flexible behavior of prosumers.
This policy measure improves economic efficiency and greater use of technology
with positive system-wide impacts. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11566v1' target="_blank">http://arxiv.org/pdf/2306.11566v1</a><br> <br> <br> <font size='5'> 29 </font> <div style="text-align: right"> 2023-06-18 23:45:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the investment industry, it is often essential to carry out fine-grained
company similarity quantification for a range of purposes, including market
mapping, competitor analysis, and mergers and acquisitions. We propose and
publish a knowledge graph, named CompanyKG, to represent and learn diverse
company features and relations. Specifically, 1.17 million companies are
represented as nodes enriched with company description embeddings; and 15
different inter-company relations result in 51.06 million weighted edges. To
enable a comprehensive assessment of methods for company similarity
quantification, we have devised and compiled three evaluation tasks with
annotated test sets: similarity prediction, competitor retrieval and similarity
ranking. We present extensive benchmarking results for 11 reproducible
predictive methods categorized into three groups: node-only, edge-only, and
node+edge. To the best of our knowledge, CompanyKG is the first large-scale
heterogeneous graph dataset originating from a real-world investment platform,
tailored for quantifying inter-company similarity. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10649v1' target="_blank">http://arxiv.org/pdf/2306.10649v1</a><br> <br> <br> <font size='5'> 30 </font> <div style="text-align: right"> 2023-06-17 18:23:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A semi-parametric estimation method for quantile coherence with an application to bivariate financial time series clustering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In multivariate time series analysis, the coherence measures the linear
dependency between two-time series at different frequencies. However, real data
applications often exhibit nonlinear dependency in the frequency domain.
Conventional coherence analysis fails to capture such dependency. The quantile
coherence, on the other hand, characterizes nonlinear dependency by defining
the coherence at a set of quantile levels based on trigonometric quantile
regression. Although quantile coherence is a more powerful tool, its estimation
remains challenging due to the high level of noise. This paper introduces a new
estimation technique for quantile coherence. The proposed method is
semi-parametric, which uses the parametric form of the spectrum of the vector
autoregressive (VAR) model as an approximation to the quantile spectral matrix,
along with nonparametric smoothing across quantiles. For each fixed quantile
level, we obtain the VAR parameters from the quantile periodograms, then, using
the Durbin-Levinson algorithm, we calculate the preliminary estimate of
quantile coherence using the VAR parameters. Finally, we smooth the preliminary
estimate of quantile coherence across quantiles using a nonparametric smoother.
Numerical results show that the proposed estimation method outperforms
nonparametric methods. We show that quantile coherence-based bivariate time
series clustering has advantages over the ordinary VAR coherence. For
applications, the identified clusters of financial stocks by quantile coherence
with a market benchmark are shown to have an intriguing and more accurate
structure of diversified investment portfolios that may be used by investors to
make better decisions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10405v2' target="_blank">http://arxiv.org/pdf/2306.10405v2</a><br> <br> <br> <font size='5'> 31 </font> <div style="text-align: right"> 2023-06-16 14:18:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stock trend forecasting is a fundamental task of quantitative investment
where precise predictions of price trends are indispensable. As an online
service, stock data continuously arrive over time. It is practical and
efficient to incrementally update the forecast model with the latest data which
may reveal some new patterns recurring in the future stock market. However,
incremental learning for stock trend forecasting still remains under-explored
due to the challenge of distribution shifts (a.k.a. concept drifts). With the
stock market dynamically evolving, the distribution of future data can slightly
or significantly differ from incremental data, hindering the effectiveness of
incremental updates. To address this challenge, we propose DoubleAdapt, an
end-to-end framework with two adapters, which can effectively adapt the data
and the model to mitigate the effects of distribution shifts. Our key insight
is to automatically learn how to adapt stock data into a locally stationary
distribution in favor of profitable updates. Complemented by data adaptation,
we can confidently adapt the model parameters under mitigated distribution
shifts. We cast each incremental learning task as a meta-learning task and
automatically optimize the adapters for desirable data adaptation and parameter
initialization. Experiments on real-world stock datasets demonstrate that
DoubleAdapt achieves state-of-the-art predictive performance and shows
considerable efficiency. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09862v1' target="_blank">http://arxiv.org/pdf/2306.09862v1</a><br> <br> <br> <font size='5'> 32 </font> <div style="text-align: right"> 2023-06-16 02:58:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cybersecurity Career Requirements: A Literature Review</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study employs a systematic literature review approach to identify the
requirements of a career as a cybersecurity professional. It aims to raise
public awareness regarding opportunities in the Information Security (IS)
profession. A total of 1,520 articles were identified from four academic
databases by searching using the terms "cybersecurity" and "skills". After
rigorous screening according to various criteria, 31 papers remained. The
findings of these studies were thematically analyzed to describe the knowledge
and skills an IS professional should possess. The research found that a
considerable investment in time is necessary for cybersecurity professionals to
reach the required technical proficiency. It also identified female gender
barriers to cybersecurity careers due to the unique requirements of the field
and suggests that females may successfully enter at lower levels and progress
up the tiers as circumstances dictate. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09599v1' target="_blank">http://arxiv.org/pdf/2306.09599v1</a><br> <br> <br> <font size='5'> 33 </font> <div style="text-align: right"> 2023-06-14 07:38:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identification of Energy Management Configuration Concepts from a Set of Pareto-optimal Solutions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Optimizing building configurations for an efficient use of energy is
increasingly receiving attention by current research and several methods have
been developed to address this task. Selecting a suitable configuration based
on multiple conflicting objectives, such as initial investment cost, recurring
cost, robustness with respect to uncertainty of grid operation is, however, a
difficult multi-criteria decision making problem. Concept identification can
facilitate a decision maker by sorting configuration options into semantically
meaningful groups (concepts), further introducing constraints to meet trade-off
expectations for a selection of objectives. In this study, for a set of 20000
Pareto-optimal building energy management configurations, resulting from a
many-objective evolutionary optimization, multiple concept identification
iterations are conducted to provide a basis for making an informed investment
decision. In a series of subsequent analysis steps, it is shown how the choice
of description spaces, i.e., the partitioning of the features into sets for
which consistent and non-overlapping concepts are required, impacts the type of
information that can be extracted and that different setups of description
spaces illuminate several different aspects of the configuration data - an
important aspect that has not been addressed in previous work. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08318v1' target="_blank">http://arxiv.org/pdf/2306.08318v1</a><br> <br> <br> <font size='5'> 34 </font> <div style="text-align: right"> 2023-06-13 22:07:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cryptocurrencies have gained popularity across various sectors, especially in
finance and investment. The popularity is partly due to their unique
specifications originating from blockchain-related characteristics such as
privacy, decentralisation, and untraceability. Despite their growing
popularity, cryptocurrencies remain a high-risk investment due to their price
volatility and uncertainty. The inherent volatility in cryptocurrency prices,
coupled with internal cryptocurrency-related factors and external influential
global economic factors makes predicting their prices and price movement
directions challenging. Nevertheless, the knowledge obtained from predicting
the direction of cryptocurrency prices can provide valuable guidance for
investors in making informed investment decisions. To address this issue, this
paper proposes a dynamic Bayesian network (DBN) approach, which can model
complex systems in multivariate settings, to predict the price movement
direction of five popular altcoins (cryptocurrencies other than Bitcoin) in the
next trading day. The efficacy of the proposed model in predicting
cryptocurrency price directions is evaluated from two perspectives. Firstly,
our proposed approach is compared to two baseline models, namely an
auto-regressive integrated moving average and support vector regression.
Secondly, from a feature engineering point of view, the impact of twenty-three
different features, grouped into four categories, on the DBN's prediction
performance is investigated. The experimental results demonstrate that the DBN
significantly outperforms the baseline models. In addition, among the groups of
features, technical indicators are found to be the most effective predictors of
cryptocurrency price directions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08157v1' target="_blank">http://arxiv.org/pdf/2306.08157v1</a><br> <br> <br> <font size='5'> 35 </font> <div style="text-align: right"> 2023-06-13 10:56:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Advanced persistent threats (APTs) have novel features such as multi-stage
penetration, highly-tailored intention, and evasive tactics. APTs defense
requires fusing multi-dimensional Cyber threat intelligence data to identify
attack intentions and conducts efficient knowledge discovery strategies by
data-driven machine learning to recognize entity relationships. However,
data-driven machine learning lacks generalization ability on fresh or unknown
samples, reducing the accuracy and practicality of the defense model. Besides,
the private deployment of these APT defense models on heterogeneous
environments and various network devices requires significant investment in
context awareness (such as known attack entities, continuous network states,
and current security strategies). In this paper, we propose a few-shot
multi-domain knowledge rearming (FMKR) scheme for context-aware defense against
APTs. By completing multiple small tasks that are generated from different
network domains with meta-learning, the FMKR firstly trains a model with good
discrimination and generalization ability for fresh and unknown APT attacks. In
each FMKR task, both threat intelligence and local entities are fused into the
support/query sets in meta-learning to identify possible attack stages.
Secondly, to rearm current security strategies, an finetuning-based deployment
mechanism is proposed to transfer learned knowledge into the student model,
while minimizing the defense cost. Compared to multiple model replacement
strategies, the FMKR provides a faster response to attack behaviors while
consuming less scheduling cost. Based on the feedback from multiple real users
of the Industrial Internet of Things (IIoT) over 2 months, we demonstrate that
the proposed scheme can improve the defense satisfaction rate. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07685v2' target="_blank">http://arxiv.org/pdf/2306.07685v2</a><br> <br> <br> <font size='5'> 36 </font> <div style="text-align: right"> 2023-06-13 09:27:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economical Accommodations for Neurodivergent Students in Software Engineering Education: Experiences from an Intervention in Four Undergraduate Courses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Neurodiversity is an umbrella term that describes variation in brain function
among individuals, including conditions such as Attention deficit hyperactivity
disorder (ADHD), or dyslexia. Neurodiversity is common in the general
population, with an estimated 5.0% to 7.1% and 7% of the world population being
diagnosed with ADHD and dyslexia respectively. Neurodivergent (ND) individuals
often experience challenges in specific tasks, such as difficulties in
communication or a reduced attention span in comparison to neurotypical (NT)
individuals. However, they also exhibit specific strengths, such as high
creativity or attention to detail. Therefore, improving the inclusion of ND
individuals is desirable for economic, ethical, and for talent reasons.
  In higher education, struggles of ND students are well-documented. Common
issues in this area are a lack of awareness among other students and staff,
forms of assessment that are particularly challenging for some students, and a
lack of offered accommodations. These factors commonly lead to stress, anxiety,
and ultimately a risk of dropping out of the studies.
  Accommodations for ND students can require substantial effort. However,
smaller changes in course material can already have major impact. In this
chapter, we summarise the lessons learned from an intervention in four courses
in undergraduate computer science programmes at Reykjavik University, Iceland,
over a period of two terms. Following accessibility guidelines produced by
interest groups for different ND conditions, we created course material in the
form of slides and assignments specifically tailored to ND audiences. We
focused on small, economical changes that could be replicated by educators with
a minimal investment of time. We evaluated the success of our intervention
through two surveys, showing an overall positive response among ND students and
NT students. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07643v1' target="_blank">http://arxiv.org/pdf/2306.07643v1</a><br> <br> <br> <font size='5'> 37 </font> <div style="text-align: right"> 2023-06-13 00:55:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Incentivizing High-Quality Content in Online Recommender Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For content recommender systems such as TikTok and YouTube, the platform's
decision algorithm shapes the incentives of content producers, including how
much effort the content producers invest in the quality of their content. Many
platforms employ online learning, which creates intertemporal incentives, since
content produced today affects recommendations of future content. In this
paper, we study the incentives arising from online learning, analyzing the
quality of content produced at a Nash equilibrium. We show that classical
online learning algorithms, such as Hedge and EXP3, unfortunately incentivize
producers to create low-quality content. In particular, the quality of content
is upper bounded in terms of the learning rate and approaches zero for typical
learning rate schedules. Motivated by this negative result, we design a
different learning algorithm -- based on punishing producers who create
low-quality content -- that correctly incentivizes producers to create
high-quality content. At a conceptual level, our work illustrates the
unintended impact that a platform's learning algorithm can have on content
quality and opens the door towards designing platform learning algorithms that
incentivize the creation of high-quality content. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07479v2' target="_blank">http://arxiv.org/pdf/2306.07479v2</a><br> <br> <br> <font size='5'> 38 </font> <div style="text-align: right"> 2023-06-12 23:42:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unlocking Sales Growth: Account Prioritization Engine with Explainable AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: B2B sales requires effective prediction of customer growth, identification of
upsell potential, and mitigation of churn risks. LinkedIn sales representatives
traditionally relied on intuition and fragmented data signals to assess
customer performance. This resulted in significant time investment in data
understanding as well as strategy formulation and under-investment in active
selling. To overcome this challenge, we developed a data product called Account
Prioritizer, an intelligent sales account prioritization engine. It uses
machine learning recommendation models and integrated account-level explanation
algorithms within the sales CRM to automate the manual process of sales book
prioritization. A successful A/B test demonstrated that the Account Prioritizer
generated a substantial +8.08% increase in renewal bookings for the LinkedIn
Business. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07464v1' target="_blank">http://arxiv.org/pdf/2306.07464v1</a><br> <br> <br> <font size='5'> 39 </font> <div style="text-align: right"> 2023-06-12 15:08:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimizing Investment Strategies with Lazy Factor and Probability Weighting: A Price Portfolio Forecasting and Mean-Variance Model with Transaction Costs Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Market traders often engage in the frequent transaction of volatile assets to
optimize their total return. In this study, we introduce a novel investment
strategy model, anchored on the 'lazy factor.' Our approach bifurcates into a
Price Portfolio Forecasting Model and a Mean-Variance Model with Transaction
Costs, utilizing probability weights as the coefficients of laziness factors.
The Price Portfolio Forecasting Model, leveraging the EXPMA Mean Method, plots
the long-term price trend line and forecasts future price movements,
incorporating the tangent slope and rate of change. For short-term investments,
we apply the ARIMA Model to predict ensuing prices. The Mean-Variance Model
with Transaction Costs employs the Monte Carlo Method to formulate the feasible
region. To strike an optimal balance between risk and return, equal probability
weights are incorporated as coefficients of the laziness factor. To assess the
efficacy of this combined strategy, we executed extensive experiments on a
specified dataset. Our findings underscore the model's adaptability and
generalizability, indicating its potential to transform investment strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07928v1' target="_blank">http://arxiv.org/pdf/2306.07928v1</a><br> <br> <br> <font size='5'> 40 </font> <div style="text-align: right"> 2023-06-12 14:13:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Auctioning Corporate Bonds: A Uniform-Price under Investment Mandates</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper examines how risk and budget limits on investment mandates affect
the bidding strategy in a uniform-price auction for issuing corporate bonds. I
prove the existence of symmetric Bayesian Nash equilibrium and explore how the
risk limits imposed on the mandate may mitigate severe underpricing, as the
symmetric equilibrium's yield positively relates to the risk limit. Investment
mandates with low-risk acceptance inversely affect the equilibrium bid. The
equilibrium bid provides insights into the optimal mechanism for pricing
corporate bonds conveying information about the bond's valuation, market power,
and the number of bidders. These findings contribute to auction theory and have
implications for empirical research in the corporate bond market. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07134v1' target="_blank">http://arxiv.org/pdf/2306.07134v1</a><br> <br> <br> <font size='5'> 41 </font> <div style="text-align: right"> 2023-06-12 10:31:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Combining Reinforcement Learning and Barrier Functions for Adaptive Risk Management in Portfolio Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reinforcement learning (RL) based investment strategies have been widely
adopted in portfolio management (PM) in recent years. Nevertheless, most
RL-based approaches may often emphasize on pursuing returns while ignoring the
risks of the underlying trading strategies that may potentially lead to great
losses especially under high market volatility. Therefore, a risk-manageable PM
investment framework integrating both RL and barrier functions (BF) is proposed
to carefully balance the needs for high returns and acceptable risk exposure in
PM applications. Up to our understanding, this work represents the first
attempt to combine BF and RL for financial applications. While the involved RL
approach may aggressively search for more profitable trading strategies, the
BF-based risk controller will continuously monitor the market states to
dynamically adjust the investment portfolio as a controllable measure for
avoiding potential losses particularly in downtrend markets. Additionally, two
adaptive mechanisms are provided to dynamically adjust the impact of risk
controllers such that the proposed framework can be flexibly adapted to uptrend
and downtrend markets. The empirical results of our proposed framework clearly
reveal such advantages against most well-known RL-based approaches on
real-world data sets. More importantly, our proposed framework shed lights on
many possible directions for future investigation. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07013v1' target="_blank">http://arxiv.org/pdf/2306.07013v1</a><br> <br> <br> <font size='5'> 42 </font> <div style="text-align: right"> 2023-06-12 06:03:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey of Modern Compiler Fuzzing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Most software that runs on computers undergoes processing by compilers. Since
compilers constitute the fundamental infrastructure of software development,
their correctness is paramount. Over the years, researchers have invested in
analyzing, understanding, and characterizing the bug features over mainstream
compilers. These studies have demonstrated that compilers correctness requires
greater research attention, and they also pave the way for compiler fuzzing. To
improve compilers correctness, researchers have proposed numerous compiler
fuzzing techniques. These techniques were initially developed for testing
traditional compilers such as GCC/LLVM and have since been generalized to test
various newly developed, domain-specific compilers, such as graphics shader
compilers and deep learning (DL) compilers. In this survey, we provide a
comprehensive summary of the research efforts for understanding and addressing
compilers defects. Specifically, this survey mainly covers two aspects. First,
it covers researchers investigation and expertise on compilers bugs, such as
their symptoms and root causes. The compiler bug studies cover GCC/LLVM, JVM
compilers, and DL compilers. In addition, it covers researchers efforts in
designing fuzzing techniques, including constructing test programs and
designing test oracles. Besides discussing the existing work, this survey
outlines several open challenges and highlights research opportunities. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06884v3' target="_blank">http://arxiv.org/pdf/2306.06884v3</a><br> <br> <br> <font size='5'> 43 </font> <div style="text-align: right"> 2023-06-11 04:51:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mean-Variance Efficient Collaborative Filtering for Stock Recommendation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of FinTech has transformed financial services onto online platforms,
yet stock investment recommender systems have received limited attention
compared to other industries. Personalized stock recommendations can
significantly impact customer engagement and satisfaction within the industry.
However, traditional investment recommendations focus on high-return stocks or
highly diversified portfolios based on the modern portfolio theory, often
neglecting user preferences. On the other hand, collaborative filtering (CF)
methods also may not be directly applicable to stock recommendations, because
it is inappropriate to just recommend stocks that users like. The key is to
optimally blend users preference with the portfolio theory. However, research
on stock recommendations within the recommender system domain remains
comparatively limited, and no existing model considers both the preference of
users and the risk-return characteristics of stocks. In this regard, we propose
a mean-variance efficient collaborative filtering (MVECF) model for stock
recommendations that consider both aspects. Our model is specifically designed
to improve the pareto optimality (mean-variance efficiency) in a trade-off
between the risk (variance of return) and return (mean return) by systemically
handling uncertainties in stock prices. Such improvements are incorporated into
the MVECF model using regularization, and the model is restructured to fit into
the ordinary matrix factorization scheme to boost computational efficiency.
Experiments on real-world fund holdings data show that our model can increase
the mean-variance efficiency of suggested portfolios while sacrificing just a
small amount of mean average precision and recall. Finally, we further show
MVECF is easily applicable to the state-of-the-art graph-based ranking models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06590v1' target="_blank">http://arxiv.org/pdf/2306.06590v1</a><br> <br> <br> <font size='5'> 44 </font> <div style="text-align: right"> 2023-06-10 21:40:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards using utility data to quantify how investments would have increased the wind resilience of distribution systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We quantify resilience with metrics extracted from the historical outage data
that is routinely recorded by many distribution utilities. The outage data is
coordinated with wind data to relate average outage rates in an area to wind
speed measured at a nearby weather station. A past investment in wind hardening
would have reduced the outage rates, and the effect of this on metrics can be
calculated by sampling a reduced number of the historical outages and
recomputing the metrics. This quantifies the impact that the hardening would
have had on customers. This is a tangible way to relate an investment in wind
resilience to the benefits it would have had on the lived experience of
customers that could help make the case for the investment to the public and
regulators. We also quantify the impact of earlier or faster restoration on
customer metrics and compare this to the impact of investment in hardening.
Overall this is a new and straightforward approach to quantify resilience and
justify resilience investments to stakeholders that is directly driven by
utility data. The approach driven by data avoids complicated models or modeling
assumptions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06526v1' target="_blank">http://arxiv.org/pdf/2306.06526v1</a><br> <br> <br> <font size='5'> 45 </font> <div style="text-align: right"> 2023-06-10 20:24:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Practical Problems of Statistical Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Statistical models have seen a significant rise in popularity in recent
years. Despite their undeniable success in various industry use cases such as
sabermetrics, investment portfolio management, and artificial intelligence,
there has been immense debate about the value of results produced by
statistical methods. This paper focuses on presenting the common issues
practitioners have when implementing statistical learning models, and why these
issues make it difficult to interpret results produced by such methods. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06518v1' target="_blank">http://arxiv.org/pdf/2306.06518v1</a><br> <br> <br> <font size='5'> 46 </font> <div style="text-align: right"> 2023-06-10 09:40:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: New insights into the role of AGNs in forming the cluster red sequence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As a considerable investment of time from various telescope facilities were
dedicated toward studying the Spiderweb protocluster at $z=2.2$, it so far
remains one of the most extensively studied protocluster. We report here the
latest results in this field, adding a new dimension to previous research on
cluster formation at high redshift. Previous studies have reported a
significant overdensity ($\delta\sim10$) of massive H$\alpha$ (+ [Nii])
-emitting galaxies in 3700 comoving Mpc$^3$. Many of these were previously
considered to be dusty, active star-forming galaxies, given their rest-frame
optical and infrared features. However, this study argues that a third of them
are more likely to be "passively-evolving" galaxies with active galactic nuclei
(AGNs) rather than star-forming galaxies, given the multi-wavelength spectral
energy distribution (SED) fitting including an AGN component. For their
SED-based star formation rates to be valid, bulk of their H$\alpha$ + [Nii]
emission should come from the central AGNs. This difference in interpretation
between this work and past studies, including ours, is particularly supported
by the recent deep Chandra X-ray observation. Furthermore, we have
spectroscopically confirmed a quiescent nature for one of these AGNs, with its
multiple stellar absorption lines but also low ionisation emission lines. This
important update provides new insights into the role of AGNs in forming the
cluster red sequence observed in the present-day universe. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06392v1' target="_blank">http://arxiv.org/pdf/2306.06392v1</a><br> <br> <br> <font size='5'> 47 </font> <div style="text-align: right"> 2023-06-09 15:05:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating the Social Impact of Generative AI Systems in Systems and Society</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI systems across modalities, ranging from text, image, audio, and
video, have broad social impacts, but there exists no official standard for
means of evaluating those impacts and which impacts should be evaluated. We
move toward a standard approach in evaluating a generative AI system for any
modality, in two overarching categories: what is able to be evaluated in a base
system that has no predetermined application and what is able to be evaluated
in society. We describe specific social impact categories and how to approach
and conduct evaluations in the base technical system, then in people and
society. Our framework for a base system defines seven categories of social
impact: bias, stereotypes, and representational harms; cultural values and
sensitive content; disparate performance; privacy and data protection;
financial costs; environmental costs; and data and content moderation labor
costs. Suggested methods for evaluation apply to all modalities and analyses of
the limitations of existing evaluations serve as a starting point for necessary
investment in future evaluations. We offer five overarching categories for what
is able to be evaluated in society, each with their own subcategories:
trustworthiness and autonomy; inequality, marginalization, and violence;
concentration of authority; labor and creativity; and ecosystem and
environment. Each subcategory includes recommendations for mitigating harm. We
are concurrently crafting an evaluation repository for the AI research
community to contribute existing evaluations along the given categories. This
version will be updated following a CRAFT session at ACM FAccT 2023. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05949v2' target="_blank">http://arxiv.org/pdf/2306.05949v2</a><br> <br> <br> <font size='5'> 48 </font> <div style="text-align: right"> 2023-06-09 10:40:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Causality between Sentiment and Cryptocurrency Prices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study investigates the relationship between narratives conveyed through
microblogging platforms, namely Twitter, and the value of crypto assets. Our
study provides a unique technique to build narratives about cryptocurrency by
combining topic modelling of short texts with sentiment analysis. First, we
used an unsupervised machine learning algorithm to discover the latent topics
within the massive and noisy textual data from Twitter, and then we revealed
4-5 cryptocurrency-related narratives, including financial investment,
technological advancement related to crypto, financial and political
regulations, crypto assets, and media coverage. In a number of situations, we
noticed a strong link between our narratives and crypto prices. Our work
connects the most recent innovation in economics, Narrative Economics, to a new
area of study that combines topic modelling and sentiment analysis to relate
consumer behaviour to narratives. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05803v1' target="_blank">http://arxiv.org/pdf/2306.05803v1</a><br> <br> <br> <font size='5'> 49 </font> <div style="text-align: right"> 2023-06-09 04:45:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Random matrix theory and nested clustered portfolios on Mexican markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This work aims to deal with the optimal allocation instability problem of
Markowitz's modern portfolio theory in high dimensionality. We propose a
combined strategy that considers covariance matrix estimators from Random
Matrix Theory~(RMT) and the machine learning allocation methodology known as
Nested Clustered Optimization~(NCO). The latter methodology is modified and
reformulated in terms of the spectral clustering algorithm and Minimum Spanning
Tree~(MST) to solve internal problems inherent to the original proposal.
Markowitz's classical mean-variance allocation and the modified NCO machine
learning approach are tested on financial instruments listed on the Mexican
Stock Exchange~(BMV) in a moving window analysis from 2018 to 2022. The
modified NCO algorithm achieves stable allocations by incorporating RMT
covariance estimators. In particular, the allocation weights are positive, and
their absolute value adds up to the total capital without considering explicit
restrictions in the formulation. Our results suggest that can be avoided the
risky \emph{short position} investment strategy by means of RMT inference and
statistical learning techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05667v1' target="_blank">http://arxiv.org/pdf/2306.05667v1</a><br> <br> <br> <font size='5'> 50 </font> <div style="text-align: right"> 2023-06-08 18:05:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Small Platforms, High Return: The Need to Enhance Investment in Small Satellites for Focused Science, Career Development, and Improved Equity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the next decade, there is an opportunity for very high return on
investment of relatively small budgets by elevating the priority of smallsat
funding in heliophysics. We've learned in the past decade that these missions
perform exceptionally well by traditional metrics, e.g., papers/year/\$M
(Spence et al. 2022 -- arXiv:2206.02968). It is also well established that
there is a "leaky pipeline" resulting in too little diversity in leadership
positions (see the National Academies Report at
https://www.nationalacademies.org/our-work/increasing-diversity-in-the-leadership-of-competed-space-missions).
Prioritizing smallsat funding would significantly increase the number of
opportunities for new leaders to learn -- a crucial patch for the pipeline and
an essential phase of career development. At present, however, there are far
more proposers than the available funding can support, leading to selection
ratios that can be as low as 6% -- in the bottom 0.5th percentile of selection
ratios across the history of ROSES. Prioritizing SmallSat funding and
substantially increasing that selection ratio are the fundamental
recommendations being made by this white paper. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05481v1' target="_blank">http://arxiv.org/pdf/2306.05481v1</a><br> <br> <br> <font size='5'> 51 </font> <div style="text-align: right"> 2023-06-08 17:47:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The need for focused, hard X-ray investigations of the Sun</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Understanding the nature of energetic particles in the solar atmosphere is
one of the most important outstanding problems in heliophysics.
Flare-accelerated particles compose a huge fraction of the flare energy budget;
they have large influences on how events develop; they are an important source
of high-energy particles found in the heliosphere; and they are the single most
important corollary to other areas of high-energy astrophysics. Despite the
importance of this area of study, this topic has in the past decade received
only a small fraction of the resources necessary for a full investigation. For
example, NASA has selected no new Explorer-class instrument in the past two
decades that is capable of examining this topic. The advances that are
currently being made in understanding flare-accelerated electrons are largely
undertaken with data from EOVSA (NSF), STIX (ESA), and NuSTAR (NASA
Astrophysics). This is despite the inclusion in the previous Heliophysics
decadal survey of the FOXSI concept as part of the SEE2020 mission, and also
despite NASA's having invested heavily in readying the technology for such an
instrument via four flights of the FOXSI sounding rocket experiment. Due to
that investment, the instrumentation stands ready to implement a hard X-ray
mission to investigate flare-accelerated electrons. This white paper describes
the scientific motivation for why this venture should be undertaken soon. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05447v1' target="_blank">http://arxiv.org/pdf/2306.05447v1</a><br> <br> <br> <font size='5'> 52 </font> <div style="text-align: right"> 2023-06-08 08:35:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cash, Credibility, and Conversion: The Influence of Synthetic Media on Investment Behavior</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Prior to November of 2022, the topic of synthetic media was largely buried
within academic journals, constrained to conversations about national security,
and often fundamentally misunderstood. The release of ChatGPT, however, has
accelerated discourse on the societal impacts of synthetic media. This study
first highlights several gaps within existing literature on synthetic media,
structuring the impact potential and limitations of synthetic media threats
within a theoretical framework. Second, it identifies financial information
environments as prime candidates for future disruption via synthetic text
modalities, proposing an experimental survey for measuring the influential
power of synthetic financial text on global investment communities. Rather than
merely assessing the ability of survey participants to distinguish genuine from
synthetic text, the experiment contained within this study measures synthetic
media influence by observing its ability to manipulate belief via a series of
behavioral variables. The results indicate that synthetic text can
significantly shift investor sentiment away from what it might otherwise have
been under truthful information conditions. Furthermore, synthetic financial
text demonstrated a unique ability to "convert" investors, inspiring extreme
changes in outlook about a company compared to genuine financial texts. This
trend should inspire concern within the global financial community,
particularly given the historical vulnerability of equity markets to investor
sentiment shocks. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05033v1' target="_blank">http://arxiv.org/pdf/2306.05033v1</a><br> <br> <br> <font size='5'> 53 </font> <div style="text-align: right"> 2023-06-08 01:39:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Computational methods for fast Bayesian model assessment via calibrated posterior p-values</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Posterior predictive p-values (ppps) have become popular tools for Bayesian
model criticism, being general-purpose and easy to use. However, their
interpretation can be difficult because their distribution is not uniform under
the hypothesis that the model did generate the data. To address this issue,
procedures to obtain calibrated ppps (cppps) have been proposed although not
used in practice, because they require repeated simulation of new data and
model estimation via MCMC. Here we give methods to balance the computational
trade-off between the number of calibration replicates and the number of MCMC
samples per replicate. Our results suggest that investing in a large number of
calibration replicates while using short MCMC chains can save significant
computation time compared to naive implementations, without significant loss in
accuracy. We propose different estimators for the variance of the cppp that can
be used to confirm quickly when the model fits the data well. Variance
estimation requires the effective sample sizes of many short MCMC chains; we
show that these can be well approximated using the single long MCMC chain from
the real-data model. The procedure for cppp is implemented in NIMBLE, a
flexible framework for hierarchical modeling that supports many models and
discrepancy measures. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04866v1' target="_blank">http://arxiv.org/pdf/2306.04866v1</a><br> <br> <br> <font size='5'> 54 </font> <div style="text-align: right"> 2023-06-07 19:59:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work we explore recent advances in instruction-tuning language models
on a range of open instruction-following datasets. Despite recent claims that
open models can be on par with state-of-the-art proprietary models, these
claims are often accompanied by limited evaluation, making it difficult to
compare models across the board and determine the utility of various resources.
We provide a large set of instruction-tuned models from 6.7B to 65B parameters
in size, trained on 12 instruction datasets ranging from manually curated
(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and
systematically evaluate them on their factual knowledge, reasoning,
multilinguality, coding, and open-ended instruction following abilities through
a collection of automatic, model-based, and human-based metrics. We further
introduce T\"ulu, our best performing instruction-tuned model suite finetuned
on a combination of high-quality open resources.
  Our experiments show that different instruction-tuning datasets can uncover
or enhance specific skills, while no single dataset (or combination) provides
the best performance across all evaluations. Interestingly, we find that model
and human preference-based evaluations fail to reflect differences in model
capabilities exposed by benchmark-based evaluations, suggesting the need for
the type of systemic evaluation performed in this work. Our evaluations show
that the best model in any given evaluation reaches on average 83% of ChatGPT
performance, and 68% of GPT-4 performance, suggesting that further investment
in building better base models and instruction-tuning data is required to close
the gap. We release our instruction-tuned models, including a fully finetuned
65B T\"ulu, along with our code, data, and evaluation framework at
https://github.com/allenai/open-instruct to facilitate future research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04751v1' target="_blank">http://arxiv.org/pdf/2306.04751v1</a><br> <br> <br> <font size='5'> 55 </font> <div style="text-align: right"> 2023-06-07 19:37:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automatic graph representation algorithm for heterogeneous catalysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One of the most appealing aspects of machine learning for material design is
its high throughput exploration of chemical spaces, but to reach the ceiling of
ML-aided exploration, more than current model architectures and processing
algorithms are required. New architectures such as Graph Neural Networks (GNNs)
have seen significant research investments recently. For heterogeneous
catalysis, defining substrate intramolecular bonds and adsorbate/substrate
intermolecular bonds is a time-consuming and challenging process. Before
applying a model, dataset pre-processing, node/bond descriptor design, and
specific model constraints have to be considered. In this work, a framework
designed to solve these issues is presented in the form of an automatic graph
representation algorithm (AGRA) tool to extract the local chemical environment
of metallic surface adsorption sites is presented. This tool is able to gather
multiple adsorption geometry datasets composed of different systems and combine
them into a single model. To show AGRA's excellent transferability and reduced
computational cost compared to other graph representation methods, it was
applied to 5 different catalytic reaction datasets and benchmarked against the
Open Catalyst Projects (OCP) graph representation method. The two ORR datasets
with O/OH adsorbates obtained 0.053 eV RMSD when combined together, whereas the
three CO2RR datasets with CHO/CO/COOH obtained an average performance of 0.088
eV RMSD. To further display the algorithm's versatility and extrapolation
ability, a model was trained on a subset combination of all 5 datasets with an
RMSD of 0.105 eV. This universal model was then used to predict a wide range of
adsorption energies and an entirely new ORR catalyst system and then verified
through Density Functional Theory calculations </font><br> Link: <a href='http://arxiv.org/pdf/2306.04742v1' target="_blank">http://arxiv.org/pdf/2306.04742v1</a><br> <br> <br> <font size='5'> 56 </font> <div style="text-align: right"> 2023-06-07 05:14:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bachelier's Market Model for ESG Asset Pricing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Environmental, Social, and Governance (ESG) finance is a cornerstone of
modern finance and investment, as it changes the classical return-risk view of
investment by incorporating an additional dimension of investment performance:
the ESG score of the investment. We define the ESG price process and integrate
it into an extension of Bachelier's market model in both discrete and
continuous time, enabling option pricing valuation. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04158v1' target="_blank">http://arxiv.org/pdf/2306.04158v1</a><br> <br> <br> <font size='5'> 57 </font> <div style="text-align: right"> 2023-06-06 23:48:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sustainability criterion implied externality pricing for resource extraction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A dynamic model is constructed that generalises the Hartwick and Van Long
(2020) endogenous discounting setup by introducing externalities and asks what
implications this has for optimal natural resource extraction with constant
consumption. It is shown that a modified form of the Hotelling and Hartwick
rule holds in which the externality component of price is a specific function
of the instantaneous user costs and cross price elasticities. It is
demonstrated that the externality adjusted marginal user cost of remaining
natural reserves is equal to the marginal user cost of extracted resources
invested in human-made reproducible capital. This lends itself to a discrete
form with a readily intuitive economic interpretation that illuminates the
stepwise impact of externality pricing on optimal extraction schedules. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04065v1' target="_blank">http://arxiv.org/pdf/2306.04065v1</a><br> <br> <br> <font size='5'> 58 </font> <div style="text-align: right"> 2023-06-06 19:34:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Developing an Index of National Research Capacity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Public managers lack feedback on the effectiveness of public investments,
policies, and programs instituted to build and use research capacity. Numerous
reports rank countries on global performance on innovation and competitiveness,
but the highly globalized data does not distinguish country contributions from
global ones. We suggest improving upon global reports by removing globalized
measures and combining a reliable set of national indicators into an index. We
factor analyze 14 variables for 172 countries from 2013 to 2021. Two factors
emerge, one for raw or core research capacity and the other indicating the
wider context of governance. Analysis shows convergent validity within the two
factors and divergent validity between them. Nations rank differently between
capacity, governance context, and the product of the two. Ranks also vary as a
function of the chosen aggregation method. Finally, as a test of the predictive
validity of the capacity index, a regression analysis was implemented
predicting national citation strength. Policymakers and analysts may find
stronger feedback from this approach to quantifying national research strength. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03981v1' target="_blank">http://arxiv.org/pdf/2306.03981v1</a><br> <br> <br> <font size='5'> 59 </font> <div style="text-align: right"> 2023-06-06 05:20:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DEK-Forecaster: A Novel Deep Learning Model Integrated with EMD-KNN for Traffic Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Internet traffic volume estimation has a significant impact on the business
policies of the ISP (Internet Service Provider) industry and business
successions. Forecasting the internet traffic demand helps to shed light on the
future traffic trend, which is often helpful for ISPs decision-making in
network planning activities and investments. Besides, the capability to
understand future trend contributes to managing regular and long-term
operations. This study aims to predict the network traffic volume demand using
deep sequence methods that incorporate Empirical Mode Decomposition (EMD) based
noise reduction, Empirical rule based outlier detection, and $K$-Nearest
Neighbour (KNN) based outlier mitigation. In contrast to the former studies,
the proposed model does not rely on a particular EMD decomposed component
called Intrinsic Mode Function (IMF) for signal denoising. In our proposed
traffic prediction model, we used an average of all IMFs components for signal
denoising. Moreover, the abnormal data points are replaced by $K$ nearest data
points average, and the value for $K$ has been optimized based on the KNN
regressor prediction error measured in Root Mean Squared Error (RMSE). Finally,
we selected the best time-lagged feature subset for our prediction model based
on AutoRegressive Integrated Moving Average (ARIMA) and Akaike Information
Criterion (AIC) value. Our experiments are conducted on real-world internet
traffic datasets from industry, and the proposed method is compared with
various traditional deep sequence baseline models. Our results show that the
proposed EMD-KNN integrated prediction models outperform comparative models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03412v1' target="_blank">http://arxiv.org/pdf/2306.03412v1</a><br> <br> <br> <font size='5'> 60 </font> <div style="text-align: right"> 2023-06-05 23:55:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Private Equity (PE) firms operate investment funds by acquiring and
managing companies to achieve a high return upon selling. Many PE funds are
thematic, meaning investment professionals aim to identify trends by covering
as many industry sectors as possible, and picking promising companies within
these sectors. So, inferring sectors for companies is critical to the success
of thematic PE funds. In this work, we standardize the sector framework and
discuss the typical challenges; we then introduce our sector inference system
addressing these challenges. Specifically, our system is built on a
medium-sized generative language model, finetuned with a prompt + model tuning
procedure. The deployed model demonstrates a superior performance than the
common baselines. The system has been serving many PE professionals for over a
year, showing great scalability to data volume and adaptability to any change
in sector framework and/or annotation. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03313v1' target="_blank">http://arxiv.org/pdf/2306.03313v1</a><br> <br> <br> <font size='5'> 61 </font> <div style="text-align: right"> 2023-06-05 18:04:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many modern applications use computer vision to detect and count objects in
massive image collections. However, when the detection task is very difficult
or in the presence of domain shifts, the counts may be inaccurate even with
significant investments in training data and model development. We propose
DISCount -- a detector-based importance sampling framework for counting in
large image collections that integrates an imperfect detector with
human-in-the-loop screening to produce unbiased estimates of counts. We propose
techniques for solving counting problems over multiple spatial or temporal
regions using a small number of screened samples and estimate confidence
intervals. This enables end-users to stop screening when estimates are
sufficiently accurate, which is often the goal in a scientific study. On the
technical side we develop variance reduction techniques based on control
variates and prove the (conditional) unbiasedness of the estimators. DISCount
leads to a 9-12x reduction in the labeling costs over naive screening for tasks
we consider, such as counting birds in radar imagery or estimating damaged
buildings in satellite imagery, and also surpasses alternative covariate-based
screening approaches in efficiency. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03151v1' target="_blank">http://arxiv.org/pdf/2306.03151v1</a><br> <br> <br> <font size='5'> 62 </font> <div style="text-align: right"> 2023-06-05 13:47:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rebooting Internet Immunity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We do everything online. We shop, travel, invest, socialize, and even hold
garage sales. Even though we may not care whether a company operates online or
in the physical world, however, the question has dramatic consequences for the
companies themselves. Online and offline entities are governed by different
rules. Under Section 230 of the Communications Decency Act, online entities --
but not physical-world entities -- are immune from lawsuits related to content
authored by their users or customers. As a result, online entities have been
able to avoid claims for harms caused by their negligence and defective product
designs simply because they operate online.
  The reason for the disparate treatment is the internet's dramatic evolution
over the last two decades. The internet of 1996 served as an information
repository and communications channel and was well governed by Section 230,
which treats internet entities as another form of mass media: Because Facebook,
Twitter and other online companies could not possibly review the mass of
content that flows through their systems, Section 230 immunizes them from
claims related to user content. But content distribution is not the internet's
only function, and it is even less so now than it was in 1996. The internet
also operates as a platform for the delivery of real-world goods and services
and requires a correspondingly diverse immunity doctrine. This Article proposes
refining online immunity by limiting it to claims that threaten to impose a
content-moderation burden on internet defendants. Where a claim is preventable
other than by content moderation -- for example, by redesigning an app or
website -- a plaintiff could freely seek relief, just as in the physical world.
This approach empowers courts to identify culpable actors in the virtual world
and treat like conduct alike wherever it occurs. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02876v1' target="_blank">http://arxiv.org/pdf/2306.02876v1</a><br> <br> <br> <font size='5'> 63 </font> <div style="text-align: right"> 2023-06-05 12:58:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HireVAE: An Online and Adaptive Factor Model Based on Hierarchical and Regime-Switch VAE</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Factor model is a fundamental investment tool in quantitative investment,
which can be empowered by deep learning to become more flexible and efficient
in practical complicated investing situations. However, it is still an open
question to build a factor model that can conduct stock prediction in an online
and adaptive setting, where the model can adapt itself to match the current
market regime identified based on only point-in-time market information. To
tackle this problem, we propose the first deep learning based online and
adaptive factor model, HireVAE, at the core of which is a hierarchical latent
space that embeds the underlying relationship between the market situation and
stock-wise latent factors, so that HireVAE can effectively estimate useful
latent factors given only historical market information and subsequently
predict accurate stock returns. Across four commonly used real stock market
benchmarks, the proposed HireVAE demonstrate superior performance in terms of
active returns over previous methods, verifying the potential of such online
and adaptive factor model. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02848v1' target="_blank">http://arxiv.org/pdf/2306.02848v1</a><br> <br> <br> <font size='5'> 64 </font> <div style="text-align: right"> 2023-06-03 19:20:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Buying Time: Latency Racing vs. Bidding in Fair Transaction Ordering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We design a practical algorithm for transaction ordering that takes into
account both transaction timestamps and bids. The algorithm guarantees that
users get their transactions published with bounded delay against a bid, while
it extracts a fair value from sophisticated users that have an edge in latency,
by moving expenditure from investment in latency improvement technology to
bidding. The algorithm creates a score from timestamps and bids, and orders
transactions based on the score. We first show that a scoring rule is the only
type of rule that satisfies the independence of latency races. We provide an
economic analysis of the protocol in an environment of private information,
where investment in latency is made ex-ante or interim stages, while bidding
happens at the interim stage where private signals have been observed. The
algorithm is useful for transaction sequencing in rollups or in other
environments where the sequencer has privileged access to order flows. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02179v1' target="_blank">http://arxiv.org/pdf/2306.02179v1</a><br> <br> <br> <font size='5'> 65 </font> <div style="text-align: right"> 2023-06-02 16:31:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A systematic literature review on solution approaches for the index tracking problem in the last decade</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The passive management approach offers conservative investors a way to reduce
risk concerning the market. This investment strategy aims at replicating a
specific index, such as the NASDAQ Composite or the FTSE100 index. The problem
is that buying all the index's assets incurs high rebalancing costs, and this
harms future returns. The index tracking problem concerns building a portfolio
that follows a specific benchmark with fewer transaction costs. Since a subset
of assets is required to solve the index problem this class of problems is
NP-hard, and in the past years, researchers have been studying solution
approaches to obtain tracking portfolios more practically. This work brings an
analysis, spanning the last decade, of the advances in mathematical approaches
for index tracking. The systematic literature review covered important issues,
such as the most relevant research areas, solution methods, and model
structures. Special attention was given to the exploration and analysis of
metaheuristics applied to the index tracking problem. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01660v2' target="_blank">http://arxiv.org/pdf/2306.01660v2</a><br> <br> <br> <font size='5'> 66 </font> <div style="text-align: right"> 2023-06-01 14:20:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Diffusion-based Generative Models (DGMs) have achieved unparalleled
performance in synthesizing high-quality visual content, opening up the
opportunity to improve image super-resolution (SR) tasks. Recent solutions for
these tasks often train architecture-specific DGMs from scratch, or require
iterative fine-tuning and distillation on pre-trained DGMs, both of which take
considerable time and hardware investments. More seriously, since the DGMs are
established with a discrete pre-defined upsampling scale, they cannot well
match the emerging requirements of arbitrary-scale super-resolution (ASSR),
where a unified model adapts to arbitrary upsampling scales, instead of
preparing a series of distinct models for each case. These limitations beg an
intriguing question: can we identify the ASSR capability of existing
pre-trained DGMs without the need for distillation or fine-tuning? In this
paper, we take a step towards resolving this matter by proposing Diff-SR, a
first ASSR attempt based solely on pre-trained DGMs, without additional
training efforts. It is motivated by an exciting finding that a simple
methodology, which first injects a specific amount of noise into the
low-resolution images before invoking a DGM's backward diffusion process,
outperforms current leading solutions. The key insight is determining a
suitable amount of noise to inject, i.e., small amounts lead to poor low-level
fidelity, while over-large amounts degrade the high-level signature. Through a
finely-grained theoretical analysis, we propose the Perceptual Recoverable
Field (PRF), a metric that achieves the optimal trade-off between these two
factors. Extensive experiments verify the effectiveness, flexibility, and
adaptability of Diff-SR, demonstrating superior performance to state-of-the-art
solutions under diverse ASSR environments. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00714v1' target="_blank">http://arxiv.org/pdf/2306.00714v1</a><br> <br> <br> <font size='5'> 67 </font> <div style="text-align: right"> 2023-06-01 05:21:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Finding Performance Issues in Database Engines via Cardinality Estimation Testing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Database Management Systems (DBMSs) process a given query by creating an
execution plan, which is subsequently executed, to compute the query's result.
Deriving an efficient query plan is challenging, and both academia and industry
have invested decades into researching query optimization. Despite this, DBMSs
are prone to performance issues, where a DBMS produces an inefficient query
plan that might lead to the slow execution of a query. Finding such issues is a
longstanding problem and inherently difficult, because no ground truth
information on an expected execution time exists. In this work, we propose
Cardinality Estimation Restriction Testing (CERT), a novel technique that
detects performance issues through the lens of cardinality estimation. Given a
query on a database, CERT derives a more restrictive query (e.g., by replacing
a LEFT JOIN with an INNER JOIN), whose estimated number of rows should not
exceed the number of estimated rows for the original query. CERT tests
cardinality estimators specifically, because they were shown to be the most
important component for query optimization; thus, we expect that finding and
fixing such issues might result in the highest performance gains. In addition,
we found that some other kinds of query optimization issues are exposed by the
unexpected cardinality estimation, which can also be detected by CERT. CERT is
a black-box technique that does not require access to the source code; DBMSs
expose query plans via the EXPLAIN statement. CERT eschews executing queries,
which is costly and prone to performance fluctuations. We evaluated CERT on
three widely used and mature DBMSs, MySQL, TiDB, and CockroachDB. CERT found 13
unique issues, of which 2 issues were fixed and 9 confirmed by the developers.
We expect that this new angle on finding performance bugs will help DBMS
developers in improving DMBSs' performance. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00355v1' target="_blank">http://arxiv.org/pdf/2306.00355v1</a><br> <br> <br> <font size='5'> 68 </font> <div style="text-align: right"> 2023-05-31 14:35:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Gemtelligence: Accelerating Gemstone classification with Deep Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The value of luxury goods, particularly investment-grade gemstones, is
greatly influenced by their origin and authenticity, sometimes resulting in
differences worth millions of dollars. Traditionally, human experts have
determined the origin and detected treatments on gemstones through visual
inspections and a range of analytical methods. However, the interpretation of
the data can be subjective and time-consuming, resulting in inconsistencies. In
this study, we propose Gemtelligence, a novel approach based on deep learning
that enables accurate and consistent origin determination and treatment
detection. Gemtelligence comprises convolutional and attention-based neural
networks that process heterogeneous data types collected by multiple
instruments. Notably, the algorithm demonstrated comparable predictive
performance to expensive laser-ablation inductively-coupled-plasma
mass-spectrometry (ICP-MS) analysis and visual examination by human experts,
despite using input data from relatively inexpensive analytical methods. Our
innovative methodology represents a major breakthrough in the field of gemstone
analysis by significantly improving the automation and robustness of the entire
analytical process pipeline. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06069v1' target="_blank">http://arxiv.org/pdf/2306.06069v1</a><br> <br> <br> <font size='5'> 69 </font> <div style="text-align: right"> 2023-05-30 23:37:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Implementation of a framework for deploying AI inference engines in FPGAs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The LCLS2 Free Electron Laser FEL will generate xray pulses to beamline
experiments at up to 1Mhz These experimentals will require new ultrahigh rate
UHR detectors that can operate at rates above 100 kHz and generate data
throughputs upwards of 1 TBs a data velocity which requires prohibitively large
investments in storage infrastructure Machine Learning has demonstrated the
potential to digest large datasets to extract relevant insights however current
implementations show latencies that are too high for realtime data reduction
objectives SLAC has endeavored on the creation of a software framework which
translates MLs structures for deployment on Field Programmable Gate Arrays
FPGAs deployed at the Edge of the data chain close to the instrumentation This
framework leverages Xilinxs HLS framework presenting an API modeled after the
open source Keras interface to the TensorFlow library This SLAC Neural Network
Library SNL framework is designed with a streaming data approach optimizing the
data flow between layers while minimizing the buffer data buffering
requirements The goal is to ensure the highest possible framerate while keeping
the maximum latency constrained to the needs of the experiment Our framework is
designed to ensure the RTL implementation of the network layers supporting full
redeployment of weights and biases without requiring resynthesis after training
The ability to reduce the precision of the implemented networks through
quantization is necessary to optimize the use of both DSP and memory resources
in the FPGA We currently have a preliminary version of the toolset and are
experimenting with both general purpose example networks and networks being
designed for specific LCLS2 experiments. </font><br> Link: <a href='http://arxiv.org/pdf/2305.19455v1' target="_blank">http://arxiv.org/pdf/2305.19455v1</a><br> <br> <br> <font size='5'> 70 </font> <div style="text-align: right"> 2023-05-30 06:24:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: It begins with a boundary: A geometric view on probabilistically robust learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Although deep neural networks have achieved super-human performance on many
classification tasks, they often exhibit a worrying lack of robustness towards
adversarially generated examples. Thus, considerable effort has been invested
into reformulating Empirical Risk Minimization (ERM) into an adversarially
robust framework. Recently, attention has shifted towards approaches which
interpolate between the robustness offered by adversarial training and the
higher clean accuracy and faster training times of ERM. In this paper, we take
a fresh and geometric view on one such method -- Probabilistically Robust
Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for
understanding PRL, which allows us to identify a subtle flaw in its original
formulation and to introduce a family of probabilistic nonlocal perimeter
functionals to address this. We prove existence of solutions using novel
relaxation methods and study properties as well as local limits of the
introduced perimeters. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18779v1' target="_blank">http://arxiv.org/pdf/2305.18779v1</a><br> <br> <br> <font size='5'> 71 </font> <div style="text-align: right"> 2023-05-27 11:30:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis over vision-based models for pedestrian action anticipation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Anticipating human actions in front of autonomous vehicles is a challenging
task. Several papers have recently proposed model architectures to address this
problem by combining multiple input features to predict pedestrian crossing
actions. This paper focuses specifically on using images of the pedestrian's
context as an input feature. We present several spatio-temporal model
architectures that utilize standard CNN and Transformer modules to serve as a
backbone for pedestrian anticipation. However, the objective of this paper is
not to surpass state-of-the-art benchmarks but rather to analyze the positive
and negative predictions of these models. Therefore, we provide insights on the
explainability of vision-based Transformer models in the context of pedestrian
action prediction. We will highlight cases where the model can achieve correct
quantitative results but falls short in providing human-like explanations
qualitatively, emphasizing the importance of investing in explainability for
pedestrian action anticipation problems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17451v1' target="_blank">http://arxiv.org/pdf/2305.17451v1</a><br> <br> <br> <font size='5'> 72 </font> <div style="text-align: right"> 2023-05-26 11:04:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Transitioning towards fit-for-purpose Public Health Surveillance Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The COVID-19 pandemic has exposed several weaknesses in the public health
infrastructure, including supply chain mechanisms and public health ICT
systems. The expansion of testing and contact tracing has been key to
identifying and isolating infected individuals, as well as tracking and
containing the spread of the virus. Digital technologies, such as telemedicine
and virtual consultations, have experienced a surge in demand to provide
medical support while minimizing the risk of transmission and infection. The
pandemic has made it clear that cooperation, information sharing, and
communication among stakeholders are crucial in making the right decisions and
preventing future outbreaks. Redesigning public health systems for effective
management of outbreaks should include five key elements: disease surveillance
and early warning systems, contact tracing and case management, data analytics
and visualization, communication and education, and telemedicine. As the world
navigates the COVID-19 pandemic, healthcare ICT systems will play an
increasingly important role in the future of healthcare delivery. In a post
COVID-19 world, several ICT strategies should be implemented to improve the
quality, efficiency, and accessibility of healthcare services, including the
expansion of telemedicine, data analytics and population health management,
interoperability, and cybersecurity. Overall, this report summarises the
importance of early detection and rapid response, international cooperation and
coordination, clear and consistent communication, investing in public health
systems and emergency preparedness, digital technology and telemedicine, and
equity and social determinants of health. These lessons demonstrate the need
for better preparedness and planning for future crises and the importance of
addressing underlying issues to create a more resilient and accessible digital
infrastructure. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16821v1' target="_blank">http://arxiv.org/pdf/2305.16821v1</a><br> <br> <br> <font size='5'> 73 </font> <div style="text-align: right"> 2023-05-26 07:56:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Green portfolio optimization: A scenario analysis and stress testing based novel approach for sustainable investing in the paradigm Indian markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this article, we present a novel approach for the construction of an
environment-friendly green portfolio using the ESG ratings, and application of
the modern portfolio theory to present what we call as the ``green efficient
frontier'' (wherein the environmental score is included as a third dimension to
the traditional mean-variance framework). Based on the prevailing action levels
and policies, as well as additional market information, scenario analyses and
stress testing are conducted to anticipate the future performance of the green
portfolio in varying circumstances. The performance of the green portfolio is
evaluated against the market returns in order to highlight the importance of
sustainable investing and recognizing climate risk as a significant risk factor
in financial analysis. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16712v1' target="_blank">http://arxiv.org/pdf/2305.16712v1</a><br> <br> <br> <font size='5'> 74 </font> <div style="text-align: right"> 2023-05-25 23:34:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving Multi-Dimensional Data Formats, Access, and Assimilation Tools for the Twenty-First Century</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Heliophysics image data largely relies on a forty-year-old ecosystem built on
the venerable Flexible Image Transport System (FITS) data standard. While many
in situ measurements use newer standards, they are difficult to integrate with
multiple data streams required to develop global understanding. Additionally,
most data users still engage with data in much the same way as they did decades
ago. However, contemporary missions and models require much more complex
support for 3D multi-parameter data, robust data assimilation strategies, and
integration of multiple individual data streams required to derive complete
physical characterizations of the Sun and Heliospheric plasma environment. In
this white paper we highlight some of the 21$^\mathsf{st}$ century challenges
for data frameworks in heliophysics, consider an illustrative case study, and
make recommendations for important steps the field can take to modernize its
data products and data usage models. Our specific recommendations include: (1)
Investing in data assimilation capability to drive advanced data-constrained
models, (2) Investing in new strategies for integrating data across multiple
instruments to realize measurements that cannot be produced from single
observations, (3) Rethinking old data use paradigms to improve user access,
develop deep understanding, and decrease barrier to entry for new datasets, and
(4) Investing in research on data formats better suited for multi-dimensional
data and cloud-based computing. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16535v1' target="_blank">http://arxiv.org/pdf/2305.16535v1</a><br> <br> <br> <font size='5'> 75 </font> <div style="text-align: right"> 2023-05-25 23:31:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: COMPLETE: A flagship mission for complete understanding of 3D coronal magnetic energy release</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: COMPLETE is a flagship mission concept combining broadband spectroscopic
imaging and comprehensive magnetography from multiple viewpoints around the Sun
to enable tomographic reconstruction of 3D coronal magnetic fields and
associated dynamic plasma properties, which provide direct diagnostics of
energy release. COMPLETE re-imagines the paradigm for solar remote-sensing
observations through purposefully co-optimized detectors distributed on
multiple spacecraft that operate as a single observatory, linked by a
comprehensive data/model assimilation strategy to unify individual observations
into a single physical framework. We describe COMPLETE's science goals,
instruments, and mission implementation. With targeted investment by NASA,
COMPLETE is feasible for launch in 2032 to observe around the maximum of Solar
Cycle 26. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16533v1' target="_blank">http://arxiv.org/pdf/2305.16533v1</a><br> <br> <br> <font size='5'> 76 </font> <div style="text-align: right"> 2023-05-25 13:41:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the field of quantitative trading, it is common practice to transform raw
historical stock data into indicative signals for the market trend. Such
signals are called alpha factors. Alphas in formula forms are more
interpretable and thus favored by practitioners concerned with risk. In
practice, a set of formulaic alphas is often used together for better modeling
precision, so we need to find synergistic formulaic alpha sets that work well
together. However, most traditional alpha generators mine alphas one by one
separately, overlooking the fact that the alphas would be combined later. In
this paper, we propose a new alpha-mining framework that prioritizes mining a
synergistic set of alphas, i.e., it directly uses the performance of the
downstream combination model to optimize the alpha generator. Our framework
also leverages the strong exploratory capabilities of reinforcement
learning~(RL) to better explore the vast search space of formulaic alphas. The
contribution to the combination models' performance is assigned to be the
return used in the RL process, driving the alpha generator to find better
alphas that improve upon the current set. Experimental evaluations on
real-world stock market data demonstrate both the effectiveness and the
efficiency of our framework for stock trend forecasting. The investment
simulation results show that our framework is able to achieve higher returns
compared to previous approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12964v1' target="_blank">http://arxiv.org/pdf/2306.12964v1</a><br> <br> <br> <font size='5'> 77 </font> <div style="text-align: right"> 2023-05-25 12:43:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Overview of FPGA-inspired Obfuscation Techniques</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building and maintaining a silicon foundry is a costly endeavor that requires
substantial financial investment. From this scenario, the semiconductor
business has largely shifted to a fabless model where the Integrated Circuit
supply chain is globalized but potentially untrusted. In recent years, several
hardware obfuscation techniques have emerged to thwart hardware security
threats related to untrusted IC fabrication. Reconfigurable-based obfuscation
schemes have shown great promise of security against state-of-the-art attacks
-- these are techniques that rely on the transformation of static logic
configurable elements such as Look Up Tables (LUTs). This survey provides a
comprehensive analysis of reconfigurable-based obfuscation techniques,
evaluating their overheads and enumerating their effectiveness against all
known attacks. The techniques are also classified based on different factors,
including the technology used, element type, and IP type. Additionally, we
present a discussion on the advantages of reconfigurable-based obfuscation
techniques when compared to Logic Locking techniques and the challenges
associated with evaluating these techniques on hardware, primarily due to the
lack of tapeouts. The survey's findings are essential for researchers
interested in hardware obfuscation and future trends in this area. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15999v1' target="_blank">http://arxiv.org/pdf/2305.15999v1</a><br> <br> <br> <font size='5'> 78 </font> <div style="text-align: right"> 2023-05-25 11:57:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Universal Quantum Technology Education Program</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum technology is an emerging cutting-edge field which offers a new
paradigm for computation and research in the field of physics, mathematics and
other scientific disciplines. This technology is of strategic importance to
governments globally and heavy investments and budgets are being sanctioned to
gain competitive advantage in terms of military, space and education. Due to
this, it is important to understand the educational and research needs required
to implement this technology at a large scale. Here, we propose a novel
universal quantum technology master's curriculum which comprises a balance
between quantum hardware and software skills to enhance the employability of
professionals thereby reducing the skill shortage faced by the academic
institutions and organizations today. The proposed curriculum holds the
potential to revolutionize the quantum education ecosystem by reducing the
pressure of hiring PhDs faced by startups and promoting the growth of a
balanced scientific mindset in quantum research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15959v2' target="_blank">http://arxiv.org/pdf/2305.15959v2</a><br> <br> <br> <font size='5'> 79 </font> <div style="text-align: right"> 2023-05-25 10:27:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: E2EAI: End-to-End Deep Learning Framework for Active Investing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Active investing aims to construct a portfolio of assets that are believed to
be relatively profitable in the markets, with one popular method being to
construct a portfolio via factor-based strategies. In recent years, there have
been increasing efforts to apply deep learning to pursue "deep factors'' with
more active returns or promising pipelines for asset trends prediction.
However, the question of how to construct an active investment portfolio via an
end-to-end deep learning framework (E2E) is still open and rarely addressed in
existing works. In this paper, we are the first to propose an E2E that covers
almost the entire process of factor investing through factor selection, factor
combination, stock selection, and portfolio construction. Extensive experiments
on real stock market data demonstrate the effectiveness of our end-to-end deep
leaning framework in active investing. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16364v1' target="_blank">http://arxiv.org/pdf/2305.16364v1</a><br> <br> <br> <font size='5'> 80 </font> <div style="text-align: right"> 2023-05-25 06:27:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TLNets: Transformation Learning Networks for long-range time-series prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Time series prediction is a prevalent issue across various disciplines, such
as meteorology, traffic surveillance, investment, and energy production and
consumption. Many statistical and machine-learning strategies have been
developed to tackle this problem. However, these approaches either lack
explainability or exhibit less satisfactory performance when the prediction
horizon increases. To this end, we propose a novel plan for the designing of
networks' architecture based on transformations, possessing the potential to
achieve an enhanced receptive field in learning which brings benefits to fuse
features across scales. In this context, we introduce four different
transformation mechanisms as bases to construct the learning model including
Fourier Transform (FT), Singular Value Decomposition (SVD), matrix
multiplication and Conv block. Hence, we develop four learning models based on
the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.
Note that the FT and SVD blocks are capable of learning global information,
while the Conv blocks focus on learning local information. The matrix block is
sparsely designed to learn both global and local information simultaneously.
The above Transformation Learning Networks (TLNets) have been extensively
tested and compared with multiple baseline models based on several real-world
datasets and showed clear potential in long-range time-series forecasting. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15770v1' target="_blank">http://arxiv.org/pdf/2305.15770v1</a><br> <br> <br> <font size='5'> 81 </font> <div style="text-align: right"> 2023-05-25 05:52:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessing the Spatial Structure of the Association between Attendance at Preschool and Childrens Developmental Vulnerabilities in Queensland Australia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The research explores the influence of preschool attendance (one year before
full-time school) on the development of children during their first year of
school. Using data collected by the Australian Early Development Census, the
findings show that areas with high proportions of preschool attendance tended
to have lower proportions of children with at least one developmental
vulnerability. Developmental vulnerablities include not being able to cope with
the school day (tired, hungry, low energy), unable to get along with others or
aggressive behaviour, trouble with reading/writing or numbers. These findings,
of course, vary by region. Using Data Analysis and Machine Learning, the
researchers were able to identify three distinct clusters within Queensland,
each characterised by different socio-demographic variables influencing the
relationship between preschool attendance and developmental vulnerability.
These analyses contribute to understanding regions with high vulnerability and
the potential need for tailored policies or investments </font><br> Link: <a href='http://arxiv.org/pdf/2305.15746v1' target="_blank">http://arxiv.org/pdf/2305.15746v1</a><br> <br> <br> <font size='5'> 82 </font> <div style="text-align: right"> 2023-05-24 17:23:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The adoption of data science brings vast benefits to Small and Medium-sized
Enterprises (SMEs) including business productivity, economic growth, innovation
and jobs creation. Data Science can support SMEs to optimise production
processes, anticipate customers' needs, predict machinery failures and deliver
efficient smart services. Businesses can also harness the power of Artificial
Intelligence (AI) and Big Data and the smart use of digital technologies to
enhance productivity and performance, paving the way for innovation. However,
integrating data science decisions into an SME requires both skills and IT
investments. In most cases, such expenses are beyond the means of SMEs due to
limited resources and restricted access to financing. This paper presents
trends and challenges towards an effective data-driven decision making for
organisations based on a case study of 85 SMEs, mostly from the West Midlands
region of England. The work is supported as part of a 3 years ERDF (European
Regional Development Funded project) in the areas of big data management,
analytics and business intelligence. We present two case studies that
demonstrates the potential of Digitisation, AI and Machine Learning and use
these as examples to unveil challenges and showcase the wealth of current
available opportunities for SMEs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15454v1' target="_blank">http://arxiv.org/pdf/2305.15454v1</a><br> <br> <br> <font size='5'> 83 </font> <div style="text-align: right"> 2023-05-24 11:59:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Life cycle economic viability analysis of battery storage in electricity market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Battery storage is essential to enhance the flexibility and reliability of
electric power systems by providing auxiliary services and load shifting.
Storage owners typically gains incentives from quick responses to auxiliary
service prices, but frequent charging and discharging also reduce its lifetime.
Therefore, this paper embeds the battery degradation cost into the operation
simulation to avoid overestimated profits caused by an aggressive bidding
strategy. Based on an operation simulation model, this paper conducts the
economic viability analysis of whole life cycle using the internal rate of
return(IRR). A clustering method and a typical day method are developed to
reduce the huge computational burdens in the life-cycle simulation of battery
storage. Our models and algorithms are validated by the case study of two
mainstream technology routes currently: lithium nickel cobalt manganese oxide
(NCM) batteries and lithium iron phosphate (LFP) batteries. Then a sensitivity
analysis is presented to identify the critical factors that boost battery
storage in the future. We evaluate the IRR results of different types of
battery storage to provide guidance for investment portfolio. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15079v2' target="_blank">http://arxiv.org/pdf/2305.15079v2</a><br> <br> <br> <font size='5'> 84 </font> <div style="text-align: right"> 2023-05-24 10:18:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adversarial robustness of amortized Bayesian inference</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Bayesian inference usually requires running potentially costly inference
procedures separately for every new observation. In contrast, the idea of
amortized Bayesian inference is to initially invest computational cost in
training an inference network on simulated data, which can subsequently be used
to rapidly perform inference (i.e., to return estimates of posterior
distributions) for new observations. This approach has been applied to many
real-world models in the sciences and engineering, but it is unclear how robust
the approach is to adversarial perturbations in the observed data. Here, we
study the adversarial robustness of amortized Bayesian inference, focusing on
simulation-based estimation of multi-dimensional posterior distributions. We
show that almost unrecognizable, targeted perturbations of the observations can
lead to drastic changes in the predicted posterior and highly unrealistic
posterior predictive samples, across several benchmark tasks and a real-world
example from neuroscience. We propose a computationally efficient
regularization scheme based on penalizing the Fisher information of the
conditional density estimator, and show how it improves the adversarial
robustness of amortized Bayesian inference. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14984v1' target="_blank">http://arxiv.org/pdf/2305.14984v1</a><br> <br> <br> <font size='5'> 85 </font> <div style="text-align: right"> 2023-05-24 03:57:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis of Contagion Dynamics with Active Cyber Defenders</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we analyze the infection spreading dynamics of malware in a
population of cyber nodes (i.e., computers or devices). Unlike most prior
studies where nodes are reactive to infections, in our setting some nodes are
active defenders meaning that they are able to clean up malware infections of
their neighboring nodes, much like how spreading malware exploits the network
connectivity properties in order to propagate. We formulate these dynamics as
an Active Susceptible-Infected-Susceptible (A-SIS) compartmental model of
contagion. We completely characterize the system's asymptotic behavior by
establishing conditions for the global asymptotic stability of the
infection-free equilibrium and for an endemic equilibrium state. We show that
the presence of active defenders counter-acts infectious spreading, effectively
increasing the epidemic threshold on parameters for which an endemic state
prevails. Leveraging this characterization, we investigate a general class of
problems for finding optimal investments in active cyber defense capabilities
given limited resources. We show that this class of problems has unique
solutions under mild assumptions. We then analyze an Active
Susceptible-Infected-Recovered (A-SIR) compartmental model, where the peak
infection level of any trajectory is explicitly derived. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14694v1' target="_blank">http://arxiv.org/pdf/2305.14694v1</a><br> <br> <br> <font size='5'> 86 </font> <div style="text-align: right"> 2023-05-23 11:16:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This work introduces the first small-loss and gradual-variation regret bounds
for online portfolio selection, marking the first instances of data-dependent
bounds for online convex optimization with non-Lipschitz, non-smooth losses.
The algorithms we propose exhibit sublinear regret rates in the worst cases and
achieve logarithmic regrets when the data is "easy," with per-iteration time
almost linear in the number of investment alternatives. The regret bounds are
derived using novel smoothness characterizations of the logarithmic loss, a
local norm-based analysis of following the regularized leader (FTRL) with
self-concordant regularizers, which are not necessarily barriers, and an
implicit variant of optimistic FTRL with the log-barrier. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13946v1' target="_blank">http://arxiv.org/pdf/2305.13946v1</a><br> <br> <br> <font size='5'> 87 </font> <div style="text-align: right"> 2023-05-22 10:35:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There have been many recent investigations into prompt-based training of
transformer language models for new text genres in low-resource settings. The
prompt-based training approach has been found to be effective in generalizing
pre-trained or fine-tuned models for transfer to resource-scarce settings. This
work, for the first time, reports results on adopting prompt-based training of
transformers for \textit{scholarly knowledge graph object prediction}. The work
is unique in the following two main aspects. 1) It deviates from the other
works proposing entity and relation extraction pipelines for predicting objects
of a scholarly knowledge graph. 2) While other works have tested the method on
text genera relatively close to the general knowledge domain, we test the
method for a significantly different domain, i.e. scholarly knowledge, in turn
testing the linguistic, probabilistic, and factual generalizability of these
large-scale transformer models. We find that (i) per expectations, transformer
models when tested out-of-the-box underperform on a new domain of data, (ii)
prompt-based training of the models achieve performance boosts of up to 40\% in
a relaxed evaluation setting, and (iii) testing the models on a starkly
different domain even with a clever training objective in a low resource
setting makes evident the domain knowledge capture gap offering an
empirically-verified incentive for investing more attention and resources to
the scholarly domain in the context of transformer models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12900v2' target="_blank">http://arxiv.org/pdf/2305.12900v2</a><br> <br> <br> <font size='5'> 88 </font> <div style="text-align: right"> 2023-05-22 09:31:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ownership Chains in Multinational Enterprises</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this contribution, we investigate the role of ownership chains developed
by multinational enterprises across different national borders. First, we
document that parent companies control a majority (58%) of foreign subsidiaries
through indirect control relationships involving at least two countries along
an ownership chain. Therefore, we hypothesize that locations along ownership
chains are driven by the existence of communication costs to transmit
management decisions. In line with motivating evidence, we develop a
theoretical model for competition on corporate control that considers the
possibility that parent companies in the origin countries can delegate their
monitoring activities in final subsidiaries to middlemen subsidiaries that are
located in intermediate jurisdictions. Our model returns us a two-step
empirical strategy with two gravity equations: i) a triangular gravity for
establishing a middleman by the parent, conditional on final investments'
locations; ii) a classical gravity for the location of final investments. First
estimates confirm the predictions that ease of communication at the country
level shapes the heterogeneous locations of subsidiaries along global ownership
chains. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12857v1' target="_blank">http://arxiv.org/pdf/2305.12857v1</a><br> <br> <br> <font size='5'> 89 </font> <div style="text-align: right"> 2023-05-22 08:38:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Simulation Package in VBA to Support Finance Students for Constructing Optimal Portfolios</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduces a software component created in Visual Basic for
Applications (VBA) that can be applied for creating an optimal portfolio using
two different methods. The first method is the seminal approach of Markowitz
that is based on finding budget shares via the minimization of the variance of
the underlying portfolio. The second method is developed by El-Khatib and
Hatemi-J, which combines risk and return directly in the optimization problem
and yields budget shares that lead to maximizing the risk adjusted return of
the portfolio. This approach is consistent with the expectation of rational
investors since these investors consider both risk and return as the
fundamental basis for selection of the investment assets. Our package offers
another advantage that is usually neglected in the literature, which is the
number of assets that should be included in the portfolio. The common practice
is to assume that the number of assets is given exogenously when the portfolio
is constructed. However, the current software component constructs all possible
combinations and thus the investor can figure out empirically which portfolio
is the best one among all portfolios considered. The software is consumer
friendly via a graphical user interface. An application is also provided to
demonstrate how the software can be used using real-time series data for
several assets. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12826v1' target="_blank">http://arxiv.org/pdf/2305.12826v1</a><br> <br> <br> <font size='5'> 90 </font> <div style="text-align: right"> 2023-05-21 06:28:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Learning for Socially Responsible Portfolio Optimisation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Socially responsible investors build investment portfolios intending to
incite social and environmental advancement alongside a financial return.
Although Mean-Variance (MV) models successfully generate the highest possible
return based on an investor's risk tolerance, MV models do not make provisions
for additional constraints relevant to socially responsible (SR) investors. In
response to this problem, the MV model must consider Environmental, Social, and
Governance (ESG) scores in optimisation. Based on the prominent MV model, this
study implements portfolio optimisation for socially responsible investors. The
amended MV model allows SR investors to enter markets with competitive SR
portfolios despite facing a trade-off between their investment Sharpe Ratio and
the average ESG score of the portfolio. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12364v1' target="_blank">http://arxiv.org/pdf/2305.12364v1</a><br> <br> <br> <font size='5'> 91 </font> <div style="text-align: right"> 2023-05-20 19:57:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rebalance your portfolio without selling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: How do you bring your assets as close as possible to your target allocation
by only investing a fixed amount of additional funds, and not selling any
assets? We look at two versions of this problem which have simple, closed form
solutions revealed by basic calculus and algebra. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12274v1' target="_blank">http://arxiv.org/pdf/2305.12274v1</a><br> <br> <br> <font size='5'> 92 </font> <div style="text-align: right"> 2023-05-19 08:54:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generalizing to new geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generation of simulated detector response to collision products is crucial to
data analysis in particle physics, but computationally very expensive. One
subdetector, the calorimeter, dominates the computational time due to the high
granularity of its cells and complexity of the interactions. Generative models
can provide more rapid sample production, but currently require significant
effort to optimize performance for specific detector geometries, often
requiring many models to describe the varying cell sizes and arrangements,
without the ability to generalize to other geometries. We develop a
$\textit{geometry-aware}$ autoregressive model, which learns how the
calorimeter response varies with geometry, and is capable of generating
simulated responses to unseen geometries without additional training. The
geometry-aware model outperforms a baseline unaware model by over $50\%$ in
several metrics such as the Wasserstein distance between the generated and the
true distributions of key quantities which summarize the simulated response. A
single geometry-aware model could replace the hundreds of generative models
currently designed for calorimeter simulation by physicists analyzing data
collected at the Large Hadron Collider. For the study of future detectors, such
a foundational model will be a crucial tool, dramatically reducing the large
upfront investment usually needed to develop generative calorimeter models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11531v2' target="_blank">http://arxiv.org/pdf/2305.11531v2</a><br> <br> <br> <font size='5'> 93 </font> <div style="text-align: right"> 2023-05-18 07:06:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The evolution of k-shell in syndication networks reveals financial performance of venture capital institutions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Venture capital (VC) is a relatively newly emergent industry that is still
subject to large uncertainties in China. Therefore, building a robust social
network with other VC institutions is a good way to share information, various
resources, and benefit from skill and knowledge complementarity to against
risks. Strong evidences indicate that better networked VC institutions are of a
better financial performance, however, most of previous works overlook the
evolution of VC institutions and only focus on some simple topology indicators
of the static syndication network, which also neglects higher-order network
structure and cannot give a comprehensive evaluation. In this paper, based on
VC investment records in the Chinese market, we construct temporal syndication
networks between VC institutions year by year. As k-shell decomposition
considers higher-order connection patterns, we employ k-shell as an evaluation
of the influence of VC institutions in syndication networks. By clustering time
series of k-shell values, the VC institutions in China fall into five groups
that are quite different from each other on financial performances and
investment behaviors. This, in turn, proves the power of our method that only
based on proper sequential network properties, we can reveal their financial
investment performance. Compared to other network centrality measurements,
k-shell is a better indicator that is indicated by a smaller intra-group
distance and a larger inter-group distance. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10762v1' target="_blank">http://arxiv.org/pdf/2305.10762v1</a><br> <br> <br> <font size='5'> 94 </font> <div style="text-align: right"> 2023-05-18 04:07:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Gated Deeper Models are Effective Factor Learners</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Precisely forecasting the excess returns of an asset (e.g., Tesla stock) is
beneficial to all investors. However, the unpredictability of market dynamics,
influenced by human behaviors, makes this a challenging task. In prior
research, researcher have manually crafted among of factors as signals to guide
their investing process. In contrast, this paper view this problem in a
different perspective that we align deep learning model to combine those human
designed factors to predict the trend of excess returns. To this end, we
present a 5-layer deep neural network that generates more meaningful factors in
a 2048-dimensional space. Modern network design techniques are utilized to
enhance robustness training and reduce overfitting. Additionally, we propose a
gated network that dynamically filters out noise-learned features, resulting in
improved performance. We evaluate our model over 2,000 stocks from the China
market with their recent three years records. The experimental results show
that the proposed gated activation layer and the deep neural network could
effectively overcome the problem. Specifically, the proposed gated activation
layer and deep neural network contribute to the superior performance of our
model. In summary, the proposed model exhibits promising results and could
potentially benefit investors seeking to optimize their investment strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10693v1' target="_blank">http://arxiv.org/pdf/2305.10693v1</a><br> <br> <br> <font size='5'> 95 </font> <div style="text-align: right"> 2023-05-17 05:41:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockchain-enabled Parametric Solar Energy Insurance via Remote Sensing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite its popularity, the nature of solar energy is highly uncertain and
weather dependent, affecting the business viability and investment of solar
energy generation, especially for household users. To stabilize the income from
solar energy generation, there have been limited traditional options, such as
using energy storage to pool excessive solar energy in off-peak periods or
financial derivatives from future markets to hedge energy prices. In this
paper, we explore a novel idea of "parametric solar energy insurance", by which
solar panel owners can insure their solar energy generation based on a
verifiable geographically specific index (surface solar irradiation).
Parametric solar energy insurance offers opportunities of financial subsidies
for insufficient solar energy generation and amortizes the fluctuations of
renewable energy generation geographically. Furthermore, we propose to leverage
blockchain and remote sensing (satellite imagery) to provide a publicly
verifiable platform for solar energy insurance, which not only automates the
underwriting and claims of a solar energy insurance policy, but also improves
its accountability and transparency. We utilize the state-of-the-art succinct
zero-knowledge proofs (zk-SNARK) to realize privacy-preserving blockchain-based
solar energy insurance on real-world permissionless blockchain platform
Ethereum. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09961v2' target="_blank">http://arxiv.org/pdf/2305.09961v2</a><br> <br> <br> <font size='5'> 96 </font> <div style="text-align: right"> 2023-05-16 01:48:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multivariate range Value-at-Risk and covariance risk measures for elliptical and log-elliptical distributions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we propose the multivariate range Value-at-Risk (MRVaR) and
the multivariate range covariance (MRCov) as two risk measures and explore
their desirable properties in risk management. In particular, we explain that
such range-based risk measures are appropriate for risk management of
regulation and investment purposes. The multivariate range correlation matrix
(MRCorr) is introduced accordingly. To facilitate analytical analyses, we
derive explicit expressions of the MRVaR and the MRCov in the context of the
multivariate (log-)elliptical distribution family. Frequently-used cases in
industry, such as normal, student-$t$, logistic, Laplace, and Pearson type VII
distributions, are presented with numerical examples. As an application, we
propose a range-based mean-variance framework of optimal portfolio selection.
We calculate the range-based efficient frontiers of the optimal portfolios
based on real data of stocks' returns. Both the numerical examples and the
efficient frontiers demonstrate consistences with the desirable properties of
the range-based risk measures. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09097v1' target="_blank">http://arxiv.org/pdf/2305.09097v1</a><br> <br> <br> <font size='5'> 97 </font> <div style="text-align: right"> 2023-05-15 18:57:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Kites and Quails: Monetary Policy and Communication with Strategic Financial Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a simple game-theoretic model to determine the consequences of
explicitly including financial market stability in the central bank objective
function, when policymakers and the financial market are strategic players, and
market stability is negatively affected by policy surprises. We find that the
inclusion of financial sector stability among the policy objectives can induce
an inefficiency, whereby market anticipation of policymakers' goals biases
investment choices. When the central bank has private information about its
policy intentions, the equilibrium communication is vague, because fully
informative communication is not credible. The appointment of a ``kitish''
central banker, who puts little weight on market stability, reduces these
inefficiencies. If interactions are repeated, communication transparency and
overall efficiency can be improved if the central bank punishes any abuse of
market power by withholding forward guidance. At the same time, repeated
interaction also opens the doors to collusion between large investors, with
uncertain welfare consequences. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08958v1' target="_blank">http://arxiv.org/pdf/2305.08958v1</a><br> <br> <br> <font size='5'> 98 </font> <div style="text-align: right"> 2023-05-15 17:33:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Puzzling Structure of Solar Convection: Window into the Dynamo</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The operation of the solar dynamo, with all of its remarkable spatio-temporal
ordering, remains an outstanding problem of modern solar physics. A number of
mechanisms that might plausibly contribute to its operation have been proposed,
but the relative role played by each remains unclear. This uncertainty stems
from continuing questions concerning the speed and structure of deep-seated
convective flows. Those flows are in-turn thought to sustain both the Sun's
turbulent EMF and the large-scale flows of differential rotation and meridional
circulation suspected of influencing the dynamo's organization and timing.
Continued progress in this area is complicated by (i) inconsistencies between
helioseismic measurements of convective and meridional flow made with different
techniques and instruments, and (ii) a lack of high-latitude data for
convection, differential rotation, and meridional flow. We suggest that the
path forward to resolving these difficulties is twofold. First, the acquisition
of long-term helioseismic and emissivity measurements obtained from a polar
vantage point is vital to complete our picture of the Sun's outer convection
zone. Second, sustained and expanded investment in theory-oriented and combined
theory/observational research initiatives will be crucial to fully exploit
these new observations and to resolve inconsistencies between existing
measurements. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08823v1' target="_blank">http://arxiv.org/pdf/2305.08823v1</a><br> <br> <br> <font size='5'> 99 </font> <div style="text-align: right"> 2023-05-15 14:13:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MADDM: Multi-Advisor Dynamic Binary Decision-Making by Maximizing the Utility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Being able to infer ground truth from the responses of multiple imperfect
advisors is a problem of crucial importance in many decision-making
applications, such as lending, trading, investment, and crowd-sourcing. In
practice, however, gathering answers from a set of advisors has a cost.
Therefore, finding an advisor selection strategy that retrieves a reliable
answer and maximizes the overall utility is a challenging problem. To address
this problem, we propose a novel strategy for optimally selecting a set of
advisers in a sequential binary decision-making setting, where multiple
decisions need to be made over time. Crucially, we assume no access to ground
truth and no prior knowledge about the reliability of advisers. Specifically,
our approach considers how to simultaneously (1) select advisors by balancing
the advisors' costs and the value of making correct decisions, (2) learn the
trustworthiness of advisers dynamically without prior information by asking
multiple advisers, and (3) make optimal decisions without access to the ground
truth, improving this over time. We evaluate our algorithm through several
numerical experiments. The results show that our approach outperforms two other
methods that combine state-of-the-art models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08664v1' target="_blank">http://arxiv.org/pdf/2305.08664v1</a><br> <br> <br> <font size='5'> 100 </font> <div style="text-align: right"> 2023-05-15 02:00:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Systematic Mapping Study and Practitioner Insights on the Use of Software Engineering Practices to Develop MVPs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: [Background] The MVP concept has influenced the way in which development
teams apply Software Engineering practices. However, the overall understanding
of this influence of MVPs on SE practices is still poor. [Objective] Our goal
is to characterize the publication landscape on practices that have been used
in the context of software MVPs and to gather practitioner insights on the
identified practices. [Method] We conducted a systematic mapping study and
discussed its results in two focus groups sessions involving twelve industry
practitioners that extensively use MVPs in their projects to capture their
perceptions on the findings of the mapping study. [Results] We identified 33
papers published between 2013 and 2020 and observed some trends related to MVP
ideation and evaluation practices. For instance, regarding ideation, we found
six different approaches and mainly informal end-user involvement practices.
Regarding evaluation, there is an emphasis on end-user validations based on
practices such as usability tests, A/B testing, and usage data analysis.
However, there is still limited research related to MVP technical feasibility
assessment and effort estimation. Practitioners of the focus group sessions
reinforced the confidence in our results regarding ideation and evaluation
practices, being aware of most of the identified practices. They also reported
how they deal with the technical feasibility assessments and effort estimation
in practice. [Conclusion] Our analysis suggests that there are opportunities
for solution proposals and evaluation studies to address literature gaps
concerning technical feasibility assessment and effort estimation. Overall,
more effort needs to be invested into empirically evaluating the existing
MVP-related practices. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08299v1' target="_blank">http://arxiv.org/pdf/2305.08299v1</a><br> <br> <br> <font size='5'> 101 </font> <div style="text-align: right"> 2023-05-11 18:00:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stochastic Approximation of Variational Quantum Imaginary Time Evolution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The imaginary-time evolution of quantum states is integral to various fields,
ranging from natural sciences to classical optimization or machine learning.
Since simulating quantum imaginary-time evolution generally requires storing an
exponentially large wave function, quantum computers are emerging as a
promising platform for this task. However, variational approaches, suitable for
near-term quantum computers, struggle with a prohibitive number of measurements
and impractical runtimes for relevant system sizes. Here, we suggest a
stochastic approach to variational quantum imaginary-time evolution, which
allows a significant reduction in runtimes. Our approach allows trading off
invested resources and accuracy, which makes it also suitable for ground state
preparation, where simulating the exact dynamics is not required. We
demonstrate the efficiency of our algorithm in simulations and show a hardware
experiment performing the imaginary-time evolution of the transverse field
Ising model on 27 qubits. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07059v1' target="_blank">http://arxiv.org/pdf/2305.07059v1</a><br> <br> <br> <font size='5'> 102 </font> <div style="text-align: right"> 2023-05-10 12:10:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Glimpse in ChatGPT Capabilities and its impact for AI research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have recently become a popular topic in the
field of Artificial Intelligence (AI) research, with companies such as Google,
Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their
development. These models are trained on massive amounts of data and can be
used for a wide range of tasks, including language translation, text
generation, and question answering. However, the computational resources
required to train and run these models are substantial, and the cost of
hardware and electricity can be prohibitive for research labs that do not have
the funding and resources of the GAFA. In this paper, we will examine the
impact of LLMs on AI research. The pace at which such models are generated as
well as the range of domains covered is an indication of the trend which not
only the public but also the scientific community is currently experiencing. We
give some examples on how to use such models in research by focusing on
GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range
of capabilities in a single system is a strong sign of approaching general
intelligence. Innovations integrating such models will also expand along the
maturation of such AI systems and exhibit unforeseeable applications that will
have important impacts on several aspects of our societies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06087v1' target="_blank">http://arxiv.org/pdf/2305.06087v1</a><br> <br> <br> <font size='5'> 103 </font> <div style="text-align: right"> 2023-05-10 03:20:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optical Aberration Correction in Postprocessing using Imaging Simulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As the popularity of mobile photography continues to grow, considerable
effort is being invested in the reconstruction of degraded images. Due to the
spatial variation in optical aberrations, which cannot be avoided during the
lens design process, recent commercial cameras have shifted some of these
correction tasks from optical design to postprocessing systems. However,
without engaging with the optical parameters, these systems only achieve
limited correction for aberrations.In this work, we propose a practical method
for recovering the degradation caused by optical aberrations. Specifically, we
establish an imaging simulation system based on our proposed optical point
spread function model. Given the optical parameters of the camera, it generates
the imaging results of these specific devices. To perform the restoration, we
design a spatial-adaptive network model on synthetic data pairs generated by
the imaging simulation system, eliminating the overhead of capturing training
data by a large amount of shooting and registration. Moreover, we
comprehensively evaluate the proposed method in simulations and experimentally
with a customized digital-single-lens-reflex (DSLR) camera lens and HUAWEI
HONOR 20, respectively. The experiments demonstrate that our solution
successfully removes spatially variant blur and color dispersion. When compared
with the state-of-the-art deblur methods, the proposed approach achieves better
results with a lower computational overhead. Moreover, the reconstruction
technique does not introduce artificial texture and is convenient to transfer
to current commercial cameras. Project Page:
\url{https://github.com/TanGeeGo/ImagingSimulation}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05867v1' target="_blank">http://arxiv.org/pdf/2305.05867v1</a><br> <br> <br> <font size='5'> 104 </font> <div style="text-align: right"> 2023-05-09 20:42:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A spectral approach to stock market performance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We pose the estimation and predictability of stock market performance. Three
cases are taken: US, Japan, Germany, the monthly index of the value of realized
investment in stocks, prices plus the value of dividend payments (OECD data).
Once deflated and trend removed, harmonic analysis is applied. The series are
taken with and without the periods with evidence of exogenous shocks. The
series are erratic and the random walk hypothesis is reasonably falsified. The
estimation reveals relevant hidden periodicities, which approximate stock value
movements. From July 2008 onwards, it is successfully analyzed whether the
subsequent fall in share value would have been predictable. Again, the data are
irregular and scattered, but the sum of the first five harmonics in relevance
anticipates the fall in stock market values that followed. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05762v1' target="_blank">http://arxiv.org/pdf/2305.05762v1</a><br> <br> <br> <font size='5'> 105 </font> <div style="text-align: right"> 2023-05-09 12:02:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring assessment method of technological advancement based on literature cross-citation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Assessing advancements of technology is essential for creating science and
technology policies and making informed investments in the technology market.
However, current methods primarily focus on the characteristics of the
technologies themselves, making it difficult to accurately assess technologies
across various fields and generations. To address this challenge, we propose a
novel approach that uses bibliometrics, specifically literature citation
networks, to measure changes in knowledge flow throughout the evolution of
technology. This method can identify diverse trends in technology development
and is an effective tool for evaluating technological advancements. We
demonstrate its accuracy and applicability by applying it to mobile
communication technology and comparing its quantitative results with other
assessment methods. Our work provides critical support for assessing different
technical routes and formulating technology policy. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05367v1' target="_blank">http://arxiv.org/pdf/2305.05367v1</a><br> <br> <br> <font size='5'> 106 </font> <div style="text-align: right"> 2023-05-08 15:36:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Studying X-ray instruments with galaxy clusters</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We applied a scientific approach to the problem of the effective area
cross-calibration of the XMM-Newton EPIC instruments. Using a sample of galaxy
clusters observed with the XMM-Newton EPIC, we quantified the effective area
cross-calibration bias between the EPIC instruments as implemented in the
public calibration data base on November 2021 in the 0.5-6.1 keV energy band.
We invested significant efforts in controlling and minimising the systematic
uncertainties of the cross-calibration bias below 1%. The statistical
uncertainties are similar and thus we can reliably measure effects at 1% level.
The effective area cross-calibration in the 0.5-6.1 keV band between MOS and pn
is biased at a substantial level. The MOS/pn bias is systematic suggesting that
MOS (pn) effective area may be calibrated too low (high), by $\sim$3-27% on
average depending on the instrument and energy band. The excellent agreement of
the energy dependencies (i.e. shapes) of the effective area of MOS2 and pn
suggest that they are correctly calibrated within $\sim$1% in the 0.5-4.5 keV
band. Comparison with an independent data set of point sources (3XMM) confirms
this. The cluster sample indicates that the MOS1/pn effective area shape
cross-calibration has an approximately linear bias amounting to $\sim$10% in
maximum in the 0.5-4.5 keV band. The effective area cross-calibration of
XMM-Newton/EPIC instruments on November 2021 in the 0.5-4.5 keV band is in a
relatively good shape. However, the cluster-to-cluster rms scatter of the bias
is substantial compared to the median bias itself. Thus, a statistically robust
implementation of the cross-calibration uncertainties to a scientific analysis
of XMM-Newton/EPIC data should include the propagation of the scatter to the
best-fit parameters, instead of a simple average bias correction of the
effective area. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04785v1' target="_blank">http://arxiv.org/pdf/2305.04785v1</a><br> <br> <br> <font size='5'> 107 </font> <div style="text-align: right"> 2023-05-08 13:35:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why Students Trade? The Analysis of Young Investors behavior</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Interestingly the numbers of young traders in Jakarta Stock Exchange had been
increasing in recent years. Even in the middle of the global crisis caused by
covid19 pandemic, in December 2021 according to KSEI, Individual investors were
dominated by young investors. Data presented by KSEI showed that 60 percent of
the investors listed in Indonesian Stock Exchange were young investors. Other
data shows that 28 percent of the investors listed were shockingly students. It
was interesting to study the behavior of young and Rookie investors at the
branch of stock market in Manado State University. Basically, they varied in
how to make decision to trade on the stock exchange. The problems were
discussed by qualitative approach. Descriptive analysis was conducted prior to
interviews. Data will be collected through data observation techniques and
interviews. The study succeeded in investigating the investment behavior of
young or Rookie investors at Manado State University in accordance with
investment decision making and the perception of behavioral control. The
perception of behavioral control greatly influenced investors decision making.
Students were greatly influenced by lecturer, friends and more experienced
investors. The results of the interview provide information that before they
determine their behavior, first they do stock analysis, both technical and
fundamental analysis. These facts shows that students investors were well
literate. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04703v1' target="_blank">http://arxiv.org/pdf/2305.04703v1</a><br> <br> <br> <font size='5'> 108 </font> <div style="text-align: right"> 2023-05-06 19:33:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Novel Decentralized Algorithm for Coordinating the Optimal Power and Traffic Flows with EVs based on Variable Inner Loop Selection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The electric power distribution network (PDN) and the transportation network
(TN) are generally operated/coordinated by different entities. However, they
are coupled with each other due to electric vehicle charging stations (EVCSs).
This paper proposes to coordinate the operation of the two systems via a fully
decentralized framework where the PDN and TN operators solve their own
operation problems by sharing only limited information. Nevertheless, the
operation problems generally are in mixed-integer programming (MIP) form. To
the best of our knowledge, the most existing decentralized/distributed
optimization algorithms, such as the alternating direction method of
multipliers (ADMM), are not always guaranteed to converge for such MIP
problems. Therefore, a novel fully decentralized optimization algorithm is
proposed, whose contributions include: 1) it is applicable to MIP problems with
convergence and optimality guaranteed for mild assumptions, 2) it only requires
limited information exchange between PDN and TN operators, which will help
preserve the privacy of the two systems and reduce the investment in building
communication channels, and 3) it is fully decentralized so that all the
computations are carried out by PDN operator and TN coordinator only.
Simulations on the test cases show the efficiency and efficacy of the proposed
framework and algorithm. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04124v1' target="_blank">http://arxiv.org/pdf/2305.04124v1</a><br> <br> <br> <font size='5'> 109 </font> <div style="text-align: right"> 2023-05-05 21:42:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of Communication Issues in Primal-Dual-Based Distributed Energy Resource Management Systems (DERMS)</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the increasing adoption of distributed energy resources (DERs) in
distribution networks, distributed energy resource management systems (DERMS)
are becoming an attractive option to coordinate the control of DERs, especially
primal-dual-based DERMS, which is a well-developed class. To help reduce the
uncertainty in commercializing DERMS, we evaluate and find the upper limits of
severity for some common issues that can occur within the communication systems
upon which a primal-dual-based DERMS relies; these issues include packet
losses, link failures, and delays. This information can help a DERMS operator
decide on the specifications for the communication infrastructure it will
invest in for the DERMS. Our evaluation is based on numerical simulations of a
real-world feeder with approximately 2,000 nodes and a DERMS that controls 163
photovoltaic generators and 140 energy storage batteries. In general, we find
that the DERMS becomes less resistant to communication issues as the
information flows closer to the DERs implementing the controllable power
injections. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03854v1' target="_blank">http://arxiv.org/pdf/2305.03854v1</a><br> <br> <br> <font size='5'> 110 </font> <div style="text-align: right"> 2023-05-05 12:16:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Empirical Evidence for the New Definitions in Financial Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study presents empirical evidence to support the validity of new
definitions in financial markets. The risk attitudes of investors in US
financial markets from 1889-1978 are analyzed and the results indicate that
equity investors who invested in the composite S&P 500 index were risk-averse
in 1977. Conversely, risk-free asset investors who invested in US Treasury
bills were found to exhibit not enough risk-loving behavior, which can be
considered a type of risk-averse behavior. These findings suggest that the new
definitions in financial markets accurately reflect the behavior of investors
and should be considered in investment strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03468v1' target="_blank">http://arxiv.org/pdf/2305.03468v1</a><br> <br> <br> <font size='5'> 111 </font> <div style="text-align: right"> 2023-05-04 21:55:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Range Summary Queries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the query version of the approximate heavy hitter and quantile
problems. In the former problem, the input is a parameter $\varepsilon$ and a
set $P$ of $n$ points in $\mathbb{R}^d$ where each point is assigned a color
from a set $C$, and we want to build a structure s.t. given any geometric range
$\gamma$, we can efficiently find a list of approximate heavy hitters in
$\gamma\cap P$, i.e., colors that appear at least $\varepsilon |\gamma \cap P|$
times in $\gamma \cap P$, as well as their frequencies with an additive error
of $\varepsilon |\gamma \cap P|$. In the latter problem, each point is assigned
a weight from a totally ordered universe and the query must output a sequence
$S$ of $1+1/\varepsilon$ weights s.t. the $i$-th weight in $S$ has approximate
rank $i\varepsilon|\gamma\cap P|$, meaning, rank $i\varepsilon|\gamma\cap P|$
up to an additive error of $\varepsilon|\gamma\cap P|$. Previously, optimal
results were only known in 1D [WY11] but a few sub-optimal methods were
available in higher dimensions [AW17, ACH+12].
  We study the problems for 3D halfspace and dominance queries. We consider the
real RAM model with integer registers of size $w=\Theta(\log n)$ bits. For
dominance queries, we show optimal solutions for both heavy hitter and quantile
problems: using linear space, we can answer both queries in time $O(\log n +
1/\varepsilon)$. Note that as the output size is $\frac{1}{\varepsilon}$, after
investing the initial $O(\log n)$ searching time, our structure takes on
average $O(1)$ time to find a heavy hitter or a quantile! For more general
halfspace heavy hitter queries, the same optimal query time can be achieved by
increasing the space by an extra $\log_w\frac{1}{\varepsilon}$ (resp.
$\log\log_w\frac{1}{\varepsilon}$) factor in 3D (resp. 2D). By spending extra
$\log^{O(1)}\frac{1}{\varepsilon}$ factors in time and space, we can also
support quantile queries. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03180v1' target="_blank">http://arxiv.org/pdf/2305.03180v1</a><br> <br> <br> <font size='5'> 112 </font> <div style="text-align: right"> 2023-05-04 04:45:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why Not Borrow, Invest, and Escape Poverty?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Take up of microcredit by the poor for investment in businesses or human
capital turned out to be very low. We show that this could be explained by risk
aversion, without relying on fixed costs or other forms of non-convexity in the
technology, if the investment is aimed at increasing the probability of
success. Under this framework, rational risk-averse agents choose corner
solutions, unlike in the case of a risky investment with an exogenous
probability of success. Our online experiment confirms our theoretical
predictions about how agents' choices differ when facing the two types of
investments. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02546v1' target="_blank">http://arxiv.org/pdf/2305.02546v1</a><br> <br> <br> <font size='5'> 113 </font> <div style="text-align: right"> 2023-05-03 15:11:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite the extensive investment and impressive recent progress at reasoning
by similarity, deep learning continues to struggle with more complex forms of
reasoning such as non-monotonic and commonsense reasoning. Non-monotonicity is
a property of non-classical reasoning typically seen in commonsense reasoning,
whereby a reasoning system is allowed (differently from classical logic) to
jump to conclusions which may be retracted later, when new information becomes
available. Neural-symbolic systems such as Logic Tensor Networks (LTN) have
been shown to be effective at enabling deep neural networks to achieve
reasoning capabilities. In this paper, we show that by combining a
neural-symbolic system with methods from continual learning, LTN can obtain a
higher level of accuracy when addressing non-monotonic reasoning tasks.
Continual learning is added to LTNs by adopting a curriculum of learning from
knowledge and data with recall. We call this process Continual Reasoning, a new
methodology for the application of neural-symbolic systems to reasoning tasks.
Continual Reasoning is applied to a prototypical non-monotonic reasoning
problem as well as other reasoning examples. Experimentation is conducted to
compare and analyze the effects that different curriculum choices may have on
overall learning and reasoning results. Results indicate significant
improvement on the prototypical non-monotonic reasoning problem and a promising
outlook for the proposed approach on statistical relational learning examples. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02171v1' target="_blank">http://arxiv.org/pdf/2305.02171v1</a><br> <br> <br> <font size='5'> 114 </font> <div style="text-align: right"> 2023-05-02 22:02:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Attempt to Salvage Multi-million Dollars of Ill-conceived HPC System Investment by Creating Academic Cloud Computing Infrastructure. A Tale of Errors and Belated Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In 2015 the Interdisciplinary Centre for Mathematical and Computational
Modelling (ICM), University of Warsaw built a modern datacenter and installed
three substantial HPC systems as part of a 168 M PLN (36 M Euro) OCEAN project.
Some of the systems were ill-conceived, badly architected and for the five
years of their life span have brought minimal ROI. This paper reports on a
two-year intensive effort to reengineer two of these HPC systems into a hybrid,
multi-cloud solution called A-CHOICeM (Akademicka CHmura Obliczeniowa ICM). The
intention was to expand the user base of ICM typical HPC system from around 200
to 500 to about 100,000 potential general academic users from all institutes of
higher learning in the Warsaw area. The main characteristics of this solution
are integration of on-premises ICM Cloud with several public cloud providers,
building solution tailored to particular groups of academic users,
containerization, integration of special computational paradigms like AI and
Quantum Computing. Full process of designing the solution, competitive dialogue
with suppliers, and full final specifications for the solution are presented.
Several roadblocks, pitfalls and difficulties encountered along the way,
including the conservative attitude of "the old school" HPC admins, University
bureaucracy, national funding policies and others are presented. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01800v1' target="_blank">http://arxiv.org/pdf/2305.01800v1</a><br> <br> <br> <font size='5'> 115 </font> <div style="text-align: right"> 2023-05-02 20:34:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Data Valuation from Data-Driven Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the ongoing investment in data collection and communication technology
in power systems, data-driven optimization has been established as a powerful
tool for system operators to handle stochastic system states caused by weather-
and behavior-dependent resources. However, most methods are ignorant to data
quality, which may differ based on measurement and underlying
privacy-protection mechanisms. This paper addresses this shortcoming by (i)
proposing a practical data quality metric based on Wasserstein distance, (ii)
leveraging a novel modification of distributionally robust optimization using
information from multiple data sets with heterogeneous quality to valuate data,
(iii) applying the proposed optimization framework to an optimal power flow
problem, and (iv) showing a direct method to valuate data from the optimal
solution. We conduct numerical experiments to analyze and illustrate the
proposed model and publish the implementation open-source. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01775v1' target="_blank">http://arxiv.org/pdf/2305.01775v1</a><br> <br> <br> <font size='5'> 116 </font> <div style="text-align: right"> 2023-05-02 15:02:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the synergistic potential of quantum annealing and gate model computing for portfolio optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Portfolio optimization is one of the most studied problems for demonstrating
the near-term applications of quantum computing. However, large-scale problems
cannot be solved on today's quantum hardware. In this work, we extend upon a
study to use the best of both quantum annealing and gate-based quantum
computing systems to enable solving large-scale optimization problems
efficiently on the available hardware. The existing work uses a method called
Large System Sampling Approximation (LSSA) that involves dividing the large
problem into several smaller problems and then combining the multiple solutions
to approximate the solution to the original problem. This paper introduces a
novel technique to modify the sampling step of LSSA. We divide the portfolio
optimization problem into sub-systems of smaller sizes by selecting a diverse
set of assets that act as representatives of the entire market and capture the
highest correlations among assets. We conduct tests on real-world stock data
from the Indian stock market on up to 64 assets. Our experimentation shows that
the hybrid approach performs at par with the traditional classical optimization
methods with a good approximation ratio. We also demonstrate the effectiveness
of our approach on a range of portfolio optimization problems of different
sizes. We present the effects of different parameters on the proposed method
and compare its performance with the earlier work. Our findings suggest that
hybrid annealer-gate quantum computing can be a valuable tool for portfolio
managers seeking to optimize their investment portfolios in the near future. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01480v1' target="_blank">http://arxiv.org/pdf/2305.01480v1</a><br> <br> <br> <font size='5'> 117 </font> <div style="text-align: right"> 2023-05-01 14:08:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cascading failures: dynamics, stability and control</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a dynamic model of cascading failures in a financial network
whereby cross-holdings are viewed as feedback, external assets investments as
inputs and failure penalties as static nonlinearities. We provide sufficient
milder and stronger conditions for the system to be a positive one, and study
equilibrium points and stability. Stability implies absence of cascades and
convergence of market values to constant values. We provide a constructive
method for control design to obtain stabilizing market investments in the form
of feedback-feedforward control inputs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00838v2' target="_blank">http://arxiv.org/pdf/2305.00838v2</a><br> <br> <br> <font size='5'> 118 </font> <div style="text-align: right"> 2023-05-01 13:33:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On-demand Mobility-as-a-Service platform assignment games with guaranteed stable outcomes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Mobility-as-a-Service (MaaS) systems are two-sided markets, with two mutually
exclusive sets of agents, i.e., travelers/users and operators, forming a
mobility ecosystem in which multiple operators compete or cooperate to serve
customers under a governing platform provider. This study proposes a MaaS
platform equilibrium model based on many-to-many assignment games incorporating
both fixed-route transit services and mobility-on-demand (MOD) services. The
matching problem is formulated as a multicommodity flow network design problem
under congestion. The local stability conditions reflect a generalization of
Wardrop's principles that include operator decisions. A subsidy mechanism from
the platform is proposed to guarantee local stability. An exact solution
algorithm is proposed based on a branch and bound framework with a Frank-Wolfe
algorithm integrated with Lagrangian relaxation and subgradient optimization,
which guarantees the optimality of the matching problem but not stability. A
heuristic which integrates stability conditions and subsidy design is proposed,
which reaches either the optimal MaaS platform equilibrium solution with global
stability, or a feasible locally stable solution that may require subsidy. A
worst-case bound and condition for obtaining an exact solution are both
identified. Two sets of reproducible numerical experiments are conducted. The
first, on a toy network, verifies the model and algorithm, and illustrates the
differences between local and global stability. The second, on an expanded
Sioux Falls network with 82 nodes and 748 links, derives generalizable insights
about the model for coopetitive interdependencies between operators sharing the
platform, handling congestion effects in MOD services, effects of local
stability on investment impacts, and illustrating inequities that may arise
under heterogeneous populations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00818v1' target="_blank">http://arxiv.org/pdf/2305.00818v1</a><br> <br> <br> <font size='5'> 119 </font> <div style="text-align: right"> 2023-04-30 17:55:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Stationary Mean-Field Equilibrium Model of Irreversible Investment in a Two-Regime Economy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider a mean-field model of firms competing \`a la Cournot on a
commodity market, where the commodity price is given in terms of a power
inverse demand function of the industry-aggregate production. Investment is
irreversible and production capacity depreciates at a constant rate. Production
is subject to Gaussian productivity shocks, while large non-anticipated
macroeconomic events driven by a two-state continuous-time Markov chain can
change the volatility of the shocks, as well as the price function. Firms wish
to maximize expected discounted revenues of production, net of investment and
operational costs. Investment decisions are based on the long-run stationary
price of the commodity. We prove existence, uniqueness and characterization of
the stationary mean-field equilibrium of the model. The equilibrium investment
strategy is of barrier-type and it is triggered by a couple of endogenously
determined investment thresholds, one per state of the economy. We provide a
quasi-closed form expression of the stationary density of the state and we show
that our model can produce Pareto distribution of firms' size. This is a
feature that is consistent both with observations at the aggregate level of
industries and at the level of a particular industry. We establish a relation
between economic instability and market concentration and we show how
macroeconomic instability can harm firms' profitability more than productivity
fluctuations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00541v1' target="_blank">http://arxiv.org/pdf/2305.00541v1</a><br> <br> <br> <font size='5'> 120 </font> <div style="text-align: right"> 2023-04-30 17:31:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interpretability of Machine Learning: Recent Advances and Future Prospects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The proliferation of machine learning (ML) has drawn unprecedented interest
in the study of various multimedia contents such as text, image, audio and
video, among others. Consequently, understanding and learning ML-based
representations have taken center stage in knowledge discovery in intelligent
multimedia research and applications. Nevertheless, the black-box nature of
contemporary ML, especially in deep neural networks (DNNs), has posed a primary
challenge for ML-based representation learning. To address this black-box
problem, the studies on interpretability of ML have attracted tremendous
interests in recent years. This paper presents a survey on recent advances and
future prospects on interpretability of ML, with several application examples
pertinent to multimedia computing, including text-image cross-modal
representation learning, face recognition, and the recognition of objects. It
is evidently shown that the study of interpretability of ML promises an
important research direction, one which is worth further investment in. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00537v1' target="_blank">http://arxiv.org/pdf/2305.00537v1</a><br> <br> <br> <font size='5'> 121 </font> <div style="text-align: right"> 2023-04-30 04:36:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Beyond Classification: Financial Reasoning in State-of-the-Art Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), consisting of 100 billion or more parameters,
have demonstrated remarkable ability in complex multi-step reasoning tasks.
However, the application of such generic advancements has been limited to a few
fields, such as clinical or legal, with the field of financial reasoning
remaining largely unexplored. To the best of our knowledge, the ability of LLMs
to solve financial reasoning problems has never been dealt with, and whether it
can be performed at any scale remains unknown. To address this knowledge gap,
this research presents a comprehensive investigation into the potential
application of LLMs in the financial domain. The investigation includes a
detailed exploration of a range of subjects, including task formulation,
synthetic data generation, prompting methods, and evaluation capability.
Furthermore, the study benchmarks various GPT variants with parameter scales
ranging from 2.8B to 13B, with and without instruction tuning, on diverse
dataset sizes. By analyzing the results, we reveal that the ability to generate
coherent financial reasoning first emerges at 6B parameters, and continues to
improve with better instruction-tuning or larger datasets. Additionally, the
study provides a publicly accessible dataset named sFIOG (Synthetic-Financial
Investment Opinion Generation), consisting of 11,802 synthetic investment
thesis samples, to support further research in the field of financial
reasoning. Overall, this research seeks to contribute to the understanding of
the efficacy of language models in the field of finance, with a particular
emphasis on their ability to engage in sophisticated reasoning and analysis
within the context of investment decision-making. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01505v2' target="_blank">http://arxiv.org/pdf/2305.01505v2</a><br> <br> <br> <font size='5'> 122 </font> <div style="text-align: right"> 2023-04-29 07:48:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Systematic Review on Reinforcement Learning in the Field of Fintech</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Applications of Reinforcement Learning in the Finance Technology (Fintech)
have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning,
through its vast competence and proficiency, has aided remarkable results in
the field of Fintech. The objective of this systematic survey is to perform an
exploratory study on a correlation between reinforcement learning and Fintech
to highlight the prediction accuracy, complexity, scalability, risks,
profitability and performance. Major uses of reinforcement learning in finance
or Fintech include portfolio optimization, credit risk reduction, investment
capital management, profit maximization, effective recommendation systems, and
better price setting strategies. Several studies have addressed the actual
contribution of reinforcement learning to the performance of financial
institutions. The latest studies included in this survey are publications from
2018 onward. The survey is conducted using PRISMA technique which focuses on
the reporting of reviews and is based on a checklist and four-phase flow
diagram. The conducted survey indicates that the performance of RL-based
strategies in Fintech fields proves to perform considerably better than other
state-of-the-art algorithms. The present work discusses the use of
reinforcement learning algorithms in diverse decision-making challenges in
Fintech and concludes that the organizations dealing with finance can benefit
greatly from Robo-advising, smart order channelling, market making, hedging and
options pricing, portfolio optimization, and optimal execution. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07466v1' target="_blank">http://arxiv.org/pdf/2305.07466v1</a><br> <br> <br> <font size='5'> 123 </font> <div style="text-align: right"> 2023-04-28 12:48:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Carbon Emission Reduction Effect of RMB Appreciation: Empirical Evidence from 283 Prefecture-Level Cities of China</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Based on the panel data of 283 prefecture-level cities in China from 2006 to
2019, this paper measures the extent and mechanism of the impact of RMB real
effective exchange rate fluctuations on carbon emission intensity. The results
show that: (1) For every 1% appreciation of the real effective exchange rate of
RMB, the carbon emission intensity decreases by an average of 0.463 tons/10000
yuan; (2) The "carbon emission reduction effect" of RMB real effective exchange
rate appreciation is more obvious in the eastern regions, coastal areas,
regions with high urbanization levels, and areas with open information; (3) The
appreciation of RMB real effective exchange rate can reduce carbon dioxide
emission intensity by improving regional R&D and innovation ability,
restraining foreign trade and foreign investment, promoting industrial
structure optimization and upgrading, and improving income inequality. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01558v1' target="_blank">http://arxiv.org/pdf/2305.01558v1</a><br> <br> <br> <font size='5'> 124 </font> <div style="text-align: right"> 2023-04-27 07:41:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: STraM: a framework for strategic national freight transport modeling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: To achieve carbon emission targets worldwide, decarbonization of the freight
transport sector will be an important factor. To this end, national governments
must make plans that facilitate this transition. National freight transport
models are a useful tool to assess what the effects of various policies and
investments may be. The state of the art consists of very detailed, static
models. While useful for short-term policy assessment, these models are less
suitable for the long-term planning necessary to facilitate the transition to
low-carbon transportation in the upcoming decades.
  In this paper, we fill this gap by developing a framework for strategic
national freight transport modeling, which we call STraM, and which can be
characterized as a multi-period stochastic network design model, based on a
multimodal freight transport formulation. In STraM, we explicitly include
several aspects that are lacking in state-of-the art national freight transport
models: the dynamic nature of long-term planning, as well as new, low-carbon
fuel technologies and long-term uncertainties in the development of these
technologies. We illustrate our model using a case study of Norway and discuss
the resulting insights. In particular, we demonstrate the relevance of modeling
multiple time periods, the importance of including long-term uncertainty in
technology development, and the efficacy of carbon pricing. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14001v1' target="_blank">http://arxiv.org/pdf/2304.14001v1</a><br> <br> <br> <font size='5'> 125 </font> <div style="text-align: right"> 2023-04-27 07:20:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding the Impact of Culture in Assessing Helpfulness of Online Reviews</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Online reviews have become essential for users to make informed decisions in
everyday tasks ranging from planning summer vacations to purchasing groceries
and making financial investments. A key problem in using online reviews is the
overabundance of online that overwhelms the users. As a result, recommendation
systems for providing helpfulness of reviews are being developed. This paper
argues that cultural background is an important feature that impacts the nature
of a review written by the user, and must be considered as a feature in
assessing the helpfulness of online reviews. The paper provides an in-depth
study of differences in online reviews written by users from different cultural
backgrounds and how incorporating culture as a feature can lead to better
review helpfulness recommendations. In particular, we analyze online reviews
originating from two distinct cultural spheres, namely Arabic and Western
cultures, for two different products, hotels and books. Our analysis
demonstrates that the nature of reviews written by users differs based on their
cultural backgrounds and that this difference varies based on the specific
product being reviewed. Finally, we have developed six different review
helpfulness recommendation models that demonstrate that taking culture into
account leads to better recommendations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04836v1' target="_blank">http://arxiv.org/pdf/2305.04836v1</a><br> <br> <br> <font size='5'> 126 </font> <div style="text-align: right"> 2023-04-26 20:39:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Selecting Sustainable Optimal Stock by Using Multi-Criteria Fuzzy Decision-Making Approaches Based on the Development of the Gordon Model: A case study of the Toronto Stock Exchange</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Choosing the right stock portfolio with the highest efficiencies has always
concerned accurate and legal investors. Investors have always been concerned
about the accuracy and legitimacy of choosing the right stock portfolio with
high efficiency. Therefore, this paper aims to determine the criteria for
selecting an optimal stock portfolio with a high-efficiency ratio in the
Toronto Stock Exchange using the integrated evaluation and decision-making
trial laboratory (DEMATEL) model and Multi-Criteria Fuzzy decision-making
approaches regarding the development of the Gordon model. In the current study,
results obtained using combined multi-criteria fuzzy decision-making
approaches, the practical factors, the relative weight of dividends, discount
rate, and dividend growth rate have been comprehensively illustrated using
combined multi-criteria fuzzy decision-making approaches. A group of 10 experts
with at least a ten-year of experience in the stock exchange field was formed
to review the different and new aspects of the subject (portfolio selection) to
decide the interaction between the group members and the exchange of attitudes
and ideas regarding the criteria. The sequence of influence and effectiveness
of the main criteria with DEMATEL has shown that the profitability criterion
interacts most with other criteria. The criteria of managing methods and
operations (MPO), market, risk, and growth criteria are ranked next in terms of
interaction with other criteria. This study concludes that regarding the
model's appropriate and reliable validity in choosing the optimal stock
portfolio, it is recommended that portfolio managers in companies, investment
funds, and capital owners use the model to select stocks in the Toronto Stock
Exchange optimally. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13818v1' target="_blank">http://arxiv.org/pdf/2304.13818v1</a><br> <br> <br> <font size='5'> 127 </font> <div style="text-align: right"> 2023-04-26 15:51:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AutoCure: Automated Tabular Data Curation Technique for ML Pipelines</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine learning algorithms have become increasingly prevalent in multiple
domains, such as autonomous driving, healthcare, and finance. In such domains,
data preparation remains a significant challenge in developing accurate models,
requiring significant expertise and time investment to search the huge search
space of well-suited data curation and transformation tools. To address this
challenge, we present AutoCure, a novel and configuration-free data curation
pipeline that improves the quality of tabular data. Unlike traditional data
curation methods, AutoCure synthetically enhances the density of the clean data
fraction through an adaptive ensemble-based error detection method and a data
augmentation module. In practice, AutoCure can be integrated with open source
tools, e.g., Auto-sklearn, H2O, and TPOT, to promote the democratization of
machine learning. As a proof of concept, we provide a comparative evaluation of
AutoCure against 28 combinations of traditional data curation tools,
demonstrating superior performance and predictive accuracy without user
intervention. Our evaluation shows that AutoCure is an effective approach to
automating data preparation and improving the accuracy of machine learning
models. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13636v1' target="_blank">http://arxiv.org/pdf/2304.13636v1</a><br> <br> <br> <font size='5'> 128 </font> <div style="text-align: right"> 2023-04-25 22:40:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cell behavior in the face of uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Organisms that grow and survive in uncertain environments may need to change
their physiological state as the environment changes. When the environment is
uncertain, one strategy known as bet-hedging is to make these changes randomly
and independently of the environment, to ensure that at least part of the
population is well adapted. Organisms that collect information from their
environment may also use this information to modulate their changes of
physiological states. We review these different strategies and point out
parallels with the theory of optimal financial investments. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13733v1' target="_blank">http://arxiv.org/pdf/2304.13733v1</a><br> <br> <br> <font size='5'> 129 </font> <div style="text-align: right"> 2023-04-25 10:32:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economical-Epidemiological Analysis of the Coffee Trees Rust Pandemic</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Coffee tree leaf rust is a prevalent botanical disease that causes a
worldwide reduction in coffee supply and its quality, leading to immense
economic losses. While several pandemic intervention policies (PIPs) for
tackling this pandemic are commercially available, they seem to provide only
partial epidemiological relief for farmers. In this work, we develop a
high-resolution economical-epidemiological model that captures the pandemic's
spread in coffee tree farms and its associated economic impact. Through
extensive simulations for the case of Colombia, a country that consists mostly
of small-size coffee farms and is the second-largest coffee producer in the
world, our results show that it is economically impractical to sustain any
profit without directly tackling the pandemic. Furthermore, even in the
hypothetical case where farmers perfectly know their farm's epidemiological
state and the weather in advance, any pandemic-related efforts can only amount
to a limited profit of roughly 4% on investment. In the more realistic case,
any pandemic-related efforts are expected to result in economic losses,
indicating that major disturbances in the coffee market are anticipated. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14515v1' target="_blank">http://arxiv.org/pdf/2304.14515v1</a><br> <br> <br> <font size='5'> 130 </font> <div style="text-align: right"> 2023-04-25 10:17:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The biodiversity of our planet is under threat, with approximately one
million species expected to become extinct within decades. The reason; negative
human actions, which include hunting, overfishing, pollution, and the
conversion of land for urbanisation and agricultural purposes. Despite
significant investment from charities and governments for activities that
benefit nature, global wildlife populations continue to decline. Local wildlife
guardians have historically played a critical role in global conservation
efforts and have shown their ability to achieve sustainability at various
levels. In 2021, COP26 recognised their contributions and pledged US$1.7
billion per year; however, this is a fraction of the global biodiversity budget
available (between US$124 billion and US$143 billion annually) given they
protect 80% of the planets biodiversity. This paper proposes a radical new
solution based on "Interspecies Money," where animals own their own money.
Creating a digital twin for each species allows animals to dispense funds to
their guardians for the services they provide. For example, a rhinoceros may
release a payment to its guardian each time it is detected in a camera trap as
long as it remains alive and well. To test the efficacy of this approach 27
camera traps were deployed over a 400km2 area in Welgevonden Game Reserve in
Limpopo Province in South Africa. The motion-triggered camera traps were
operational for ten months and, using deep learning, we managed to capture
images of 12 distinct animal species. For each species, a makeshift bank
account was set up and credited with {\pounds}100. Each time an animal was
captured in a camera and successfully classified, 1 penny (an arbitrary amount
- mechanisms still need to be developed to determine the real value of species)
was transferred from the animal account to its associated guardian. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12703v1' target="_blank">http://arxiv.org/pdf/2304.12703v1</a><br> <br> <br> <font size='5'> 131 </font> <div style="text-align: right"> 2023-04-25 00:05:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The cross-sectional stock return predictions via quantum neural network and tensor network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper we investigate the application of quantum and quantum-inspired
machine learning algorithms to stock return predictions. Specifically, we
evaluate performance of quantum neural network, an algorithm suited for noisy
intermediate-scale quantum computers, and tensor network, a quantum-inspired
machine learning algorithm, against classical models such as linear regression
and neural networks. To evaluate their abilities, we construct portfolios based
on their predictions and measure investment performances. The empirical study
on the Japanese stock market shows the tensor network model achieves superior
performance compared to classical benchmark models, including linear and neural
network models. Though the quantum neural network model attains the lowered
risk-adjusted excess return than the classical neural network models over the
whole period, both the quantum neural network and tensor network models have
superior performances in the latest market environment, which suggests
capability of model's capturing non-linearity between input features. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12501v1' target="_blank">http://arxiv.org/pdf/2304.12501v1</a><br> <br> <br> <font size='5'> 132 </font> <div style="text-align: right"> 2023-04-24 07:09:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Portfolio Optimization using Predictive Auxiliary Classifier Generative Adversarial Networks with Measuring Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In financial engineering, portfolio optimization has been of consistent
interest. Portfolio optimization is a process of modulating asset distributions
to maximize expected returns and minimize risks. To obtain the expected
returns, deep learning models have been explored in recent years. However, due
to the deterministic nature of the models, it is difficult to consider the risk
of portfolios because conventional deep learning models do not know how
reliable their predictions can be. To address this limitation, this paper
proposes a probabilistic model, namely predictive auxiliary classifier
generative adversarial networks (PredACGAN). The proposed PredACGAN utilizes
the characteristic of the ACGAN framework in which the output of the generator
forms a distribution. While ACGAN has not been employed for predictive models
and is generally utilized for image sample generation, this paper proposes a
method to use the ACGAN structure for a probabilistic and predictive model.
Additionally, an algorithm to use the risk measurement obtained by PredACGAN is
proposed. In the algorithm, the assets that are predicted to be at high risk
are eliminated from the investment universe at the rebalancing moment.
Therefore, PredACGAN considers both return and risk to optimize portfolios. The
proposed algorithm and PredACGAN have been evaluated with daily close price
data of S&P 500 from 1990 to 2020. Experimental scenarios are assumed to
rebalance the portfolios monthly according to predictions and risk measures
with PredACGAN. As a result, a portfolio using PredACGAN exhibits 9.123% yearly
returns and a Sharpe ratio of 1.054, while a portfolio without considering risk
measures shows 1.024% yearly returns and a Sharpe ratio of 0.236 in the same
scenario. Also, the maximum drawdown of the proposed portfolio is lower than
the portfolio without PredACGAN. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11856v1' target="_blank">http://arxiv.org/pdf/2304.11856v1</a><br> <br> <br> <font size='5'> 133 </font> <div style="text-align: right"> 2023-04-24 05:18:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment-Consumption-Insurance with Partial Information and Correlation Between Assets Price and Factor Process</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this research, we present an analysis of the optimal investment,
consumption, and life insurance acquisition problem for a wage earner with
partial information. Our study considers the non-linear filter case where risky
asset prices are correlated to the factor processes under constant relative
risk aversion (CRRA) preferences. We introduce a more general framework with an
incomplete market, random parameters adapted to the Brownian motion filtration,
and a general factor process with a non-linear state estimation and a
correlation between the state process (risky asset prices) and the factor
process. To address the wage earner's problem, we formulate it as a stochastic
control problem with partial information where the risky assets prices are
correlated to the factor processes. Our framework is extensive since the
non-linear filter applied to the linear case gives a more robust result than
the Kalman filter. We obtain the non-linear filter through the Zakai equation
and derive a system of the Hamilton-Jacobi-Bellman (HJB) equation and two
backward stochastic differential equations (BSDE). We establish the existence
and uniqueness of the solution, prove the verification theorem, and construct
the optimal strategy. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11825v1' target="_blank">http://arxiv.org/pdf/2304.11825v1</a><br> <br> <br> <font size='5'> 134 </font> <div style="text-align: right"> 2023-04-23 21:56:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Introducing the Perturbative Solution of the Inter-Channel Stimulated Raman Scattering in Single-Mode Optical Fibers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The continuously increasing IP data traffic demand, with geometrical growth
rate exceeding 26%, requires a large transmission capacity increment from the
fiber optical infrastructure. As the deploy of new fiber cables requires
extensive investments, the development of multi-band amplifiers and
transceivers, already available as prototypes, is progressively considered
towards the entire low-loss single-mode bandwidth beyond the 5 THz C-band. In
this perspective, an adequate handling of the variations along the frequency of
the fiber physical features becomes crucial for the fiber propagation modeling
in multi-band wavelength division multiplexing (WDM) channel comb transmission
scenarios. In particular, the inter-channel stimulated Raman scattering (SRS)
is the fundamental inter-band effect in this context. The SRS effect on the WDM
comb propagated through a single-mode optical fiber is described by a set of
ordinary differential equations (ODEs). To date, an exact solution of the SRS
ODEs has not been proposed, and in the literature numerical solutions or
approximations have been considered in order to take into account this effect.
In this work, a perturbative solution of the SRS ODEs is presented enabling an
efficient trade-off between the target accuracy and the computational time.
Considering a C+L+S transmission scenario, the perturbative expansion up to the
2nd order ensures an excellent accuracy. Whereas, in an U-to-E transmission
scenario, the 3rd order is required in order to reach an equivalent accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11756v1' target="_blank">http://arxiv.org/pdf/2304.11756v1</a><br> <br> <br> <font size='5'> 135 </font> <div style="text-align: right"> 2023-04-23 08:22:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Time-Inconsistency in Linear Quadratic Stochastic Differential Games</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate a linear quadratic stochastic zero-sum game where two players
lobby a political representative to invest in a wind turbine farm. Players are
time-inconsistent because they discount performance with a non-constant rate.
Our objective is to identify a consistent planning equilibrium in which the
players are aware of their inconsistency and cannot commit to a lobbying
policy. We analyze the equilibrium behavior in both single-player and
two-player cases and compare the behavior of the game under constant and
non-constant discount rates. The equilibrium behavior is provided in
closed-loop form, either analytically or via numerical approximation. Our
numerical analysis of the equilibrium reveals that strategic behavior leads to
more intense lobbying without resulting in overshooting. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11577v1' target="_blank">http://arxiv.org/pdf/2304.11577v1</a><br> <br> <br> <font size='5'> 136 </font> <div style="text-align: right"> 2023-04-23 00:45:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improved Churn Causal Analysis Through Restrained High-Dimensional Feature Space Effects in Financial Institutions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Customer churn describes terminating a relationship with a business or
reducing customer engagement over a specific period. Customer acquisition cost
can be five to six times that of customer retention, hence investing in
customers with churn risk is wise. Causal analysis of the churn model can
predict whether a customer will churn in the foreseeable future and identify
effects and possible causes for churn. In general, this study presents a
conceptual framework to discover the confounding features that correlate with
independent variables and are causally related to those dependent variables
that impact churn. We combine different algorithms including the SMOTE,
ensemble ANN, and Bayesian networks to address churn prediction problems on a
massive and high-dimensional finance data that is usually generated in
financial institutions due to employing interval-based features used in
Customer Relationship Management systems. The effects of the curse and blessing
of dimensionality assessed by utilising the Recursive Feature Elimination
method to overcome the high dimension feature space problem. Moreover, a causal
discovery performed to find possible interpretation methods to describe cause
probabilities that lead to customer churn. Evaluation metrics on validation
data confirm the random forest and our ensemble ANN model, with %86 accuracy,
outperformed other approaches. Causal analysis results confirm that some
independent causal variables representing the level of super guarantee
contribution, account growth, and account balance amount were identified as
confounding variables that cause customer churn with a high degree of belief.
This article provides a real-world customer churn analysis from current status
inference to future directions in local superannuation funds. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11503v1' target="_blank">http://arxiv.org/pdf/2304.11503v1</a><br> <br> <br> <font size='5'> 137 </font> <div style="text-align: right"> 2023-04-22 20:00:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: (Vector) Space is Not the Final Frontier: Product Search as Program Synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As ecommerce continues growing, huge investments in ML and NLP for
Information Retrieval are following. While the vector space model dominated
retrieval modelling in product search - even as vectorization itself greatly
changed with the advent of deep learning -, our position paper argues in a
contrarian fashion that program synthesis provides significant advantages for
many queries and a significant number of players in the market. We detail the
industry significance of the proposed approach, sketch implementation details,
and address common objections drawing from our experience building a similar
system at Tooso. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11473v2' target="_blank">http://arxiv.org/pdf/2304.11473v2</a><br> <br> <br> <font size='5'> 138 </font> <div style="text-align: right"> 2023-04-21 22:53:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The allure of emerging AI technologies is undoubtedly thrilling. However, the
promise that AI technologies will benefit all of humanity is empty so long as
we lack a nuanced understanding of what humanity is supposed to be in the face
of widening global inequality and pressing existential threats. Going forward,
it is crucial to invest in rigorous and collaborative AI safety and ethics
research. We also need to develop standards in a sustainable and equitable way
that differentiate between merely speculative and well-researched questions.
Only the latter enable us to co-construct and deploy the values that are
necessary for creating beneficial AI. Failure to do so could result in a future
in which our AI technological advancements outstrip our ability to navigate
their ethical and social implications. This path we do not want to go down. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11163v1' target="_blank">http://arxiv.org/pdf/2304.11163v1</a><br> <br> <br> <font size='5'> 139 </font> <div style="text-align: right"> 2023-04-21 18:52:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding the householder solar panel consumer: a Markovian Model and its Societal implications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a Markovian model to understand how Italy's public sphere behaves
on the green energy transition theme. The paper uses the example of solar
photovoltaics as a point of reference. The adoption decision is assumed to be
sequentially influenced by the network communication of each person or family
and the payback period of the investment. We apply the model for a case study
based on the evolution of residential PV systems in Italy over the 2006-2026
period. The baseline configuration of the model is calibrated based on the
actual diffusion of residential PV in Italy from 2006 to 2020. The
comprehensive analysis leads to a discussion of two interesting societal
implications. (1) Cognitive biases such as hyperbolic discounting of individual
future utility crucially influence inter-temporal green choices. (2) Individual
green choices count for the effect that it has on the chances that other
individuals will also make a choice, in turn abating other greenhouse gas
emissions. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11213v1' target="_blank">http://arxiv.org/pdf/2304.11213v1</a><br> <br> <br> <font size='5'> 140 </font> <div style="text-align: right"> 2023-04-21 06:53:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How 'one-size-fits-all' public works contract does it better? An assessment of infrastructure provision in Italy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Public infrastructure procurement is crucial as a prerequisite for public and
private investments and for economic and social capital growth. However, low
performance in execution severely hinders infrastructure provision and benefits
delivery. One of the most sensitive phases in public infrastructure procurement
is the design because of the strategic relationship that it potentially creates
between procurers and contractors in the execution stage, affecting the costs
and the duration of the contract. In this paper, using recent developments in
non-parametric frontiers and propensity score matching, we evaluate the
performance in the execution of public works in Italy. The analysis provides
robust evidence of significant improvement of performance where procurers opt
for a design and build contracts, which lead to lower transaction costs,
allowing contractors to better accommodate the project in the execution. Our
findings bear considerable policy implications. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10776v1' target="_blank">http://arxiv.org/pdf/2304.10776v1</a><br> <br> <br> <font size='5'> 141 </font> <div style="text-align: right"> 2023-04-20 21:42:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: US National Gemini Office in the NOIRLab era</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article presents an overview of the US National Gemini Office (US NGO)
and its role within the International Gemini Observatory user community.
Throughout the years, the US NGO charter changed considerably to accommodate
the evolving needs of astronomers and the observatory. The current landscape of
observational astronomy requires effective communication between stakeholders
and reliable/accessible data reduction tools and products, which minimize the
time between data gathering and publication of scientific results. Because of
that, the US NGO heavily invests in producing data reduction tutorials and
cookbooks. Recently, the US NGO started engaging with the Gemini user community
through social media, and the results have been encouraging, increasing the
observatory's visibility. The US NGO staff developed tools to assess whether
the support provided to the user community is sufficient and effective, through
website analytics and social media engagement numbers. These quantitative
metrics serve as the baseline for internal reporting and directing efforts to
new or current products. In the era of the NSF's National Optical-Infrared
Astronomy Research Laboratory (NOIRLab), the US NGO is well-positioned to be
the liaison between the US user base and the Gemini Observatory. Furthermore,
collaborations within NOIRLab programs, such as the Astro Data Lab and the Time
Allocation Committee, enhance the US NGO outreach to attract users and develop
new products. The future landscape laid out by the Astro 2020 report confirms
the need to establish such synergies and provide more integrated user support
services to the astronomical community at large. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10657v1' target="_blank">http://arxiv.org/pdf/2304.10657v1</a><br> <br> <br> <font size='5'> 142 </font> <div style="text-align: right"> 2023-04-20 18:56:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Causal Analysis of Customer Churn Using Deep Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Customer churn describes terminating a relationship with a business or
reducing customer engagement over a specific period. Two main business
marketing strategies play vital roles to increase market share dollar-value:
gaining new and preserving existing customers. Customer acquisition cost can be
five to six times that for customer retention, hence investing in customers
with churn risk is smart. Causal analysis of the churn model can predict
whether a customer will churn in the foreseeable future and assist enterprises
to identify effects and possible causes for churn and subsequently use that
knowledge to apply tailored incentives. This paper proposes a framework using a
deep feedforward neural network for classification accompanied by a sequential
pattern mining method on high-dimensional sparse data. We also propose a causal
Bayesian network to predict cause probabilities that lead to customer churn.
Evaluation metrics on test data confirm the XGBoost and our deep learning model
outperformed previous techniques. Experimental analysis confirms that some
independent causal variables representing the level of super guarantee
contribution, account growth, and customer tenure were identified as
confounding factors for customer churn with a high degree of belief. This paper
provides a real-world customer churn analysis from current status inference to
future directions in local superannuation funds. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10604v1' target="_blank">http://arxiv.org/pdf/2304.10604v1</a><br> <br> <br> <font size='5'> 143 </font> <div style="text-align: right"> 2023-04-20 13:42:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investments in movie production are associated with a high level of risk as
movie revenues have long-tailed and bimodal distributions. Accurate prediction
of box-office revenue may mitigate the uncertainty and encourage investment.
However, learning effective representations for actors, directors, and
user-generated content-related keywords remains a challenging open problem. In
this work, we investigate the effects of self-supervised pretraining and
propose visual grounding of content keywords in objects from movie posters as a
pertaining objective. Experiments on a large dataset of 35,794 movies
demonstrate significant benefits of self-supervised training and visual
grounding. In particular, visual grounding pretraining substantially improves
learning on movies with content keywords and achieves 14.5% relative
performance gains compared to a finetuned BERT model with identical
architecture. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10311v1' target="_blank">http://arxiv.org/pdf/2304.10311v1</a><br> <br> <br> <font size='5'> 144 </font> <div style="text-align: right"> 2023-04-20 12:10:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the stock market, a successful investment requires a good balance between
profits and risks. Recently, stock recommendation has been widely studied in
quantitative investment to select stocks with higher return ratios for
investors. Despite the success in making profits, most existing recommendation
approaches are still weak in risk control, which may lead to intolerable paper
losses in practical stock investing. To effectively reduce risks, we draw
inspiration from adversarial perturbations and propose a novel Split
Variational Adversarial Training (SVAT) framework for risk-aware stock
recommendation. Essentially, SVAT encourages the model to be sensitive to
adversarial perturbations of risky stock examples and enhances the model's risk
awareness by learning from perturbations. To generate representative
adversarial examples as risk indicators, we devise a variational perturbation
generator to model diverse risk factors. Particularly, the variational
architecture enables our method to provide a rough risk quantification for
investors, showing an additional advantage of interpretability. Experiments on
three real-world stock market datasets show that SVAT effectively reduces the
volatility of the stock recommendation model and outperforms state-of-the-art
baseline methods by more than 30% in terms of risk-adjusted profits. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11043v1' target="_blank">http://arxiv.org/pdf/2304.11043v1</a><br> <br> <br> <font size='5'> 145 </font> <div style="text-align: right"> 2023-04-19 20:06:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Tunable and Portable Extreme-Scale Drug Discovery Platform at Exascale: the LIGATE Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Today digital revolution is having a dramatic impact on the pharmaceutical
industry and the entire healthcare system. The implementation of machine
learning, extreme-scale computer simulations, and big data analytics in the
drug design and development process offers an excellent opportunity to lower
the risk of investment and reduce the time to the patient.
  Within the LIGATE project, we aim to integrate, extend, and co-design
best-in-class European components to design Computer-Aided Drug Design (CADD)
solutions exploiting today's high-end supercomputers and tomorrow's Exascale
resources, fostering European competitiveness in the field.
  The proposed LIGATE solution is a fully integrated workflow that enables to
deliver the result of a virtual screening campaign for drug discovery with the
highest speed along with the highest accuracy. The full automation of the
solution and the possibility to run it on multiple supercomputing centers at
once permit to run an extreme scale in silico drug discovery campaign in few
days to respond promptly for example to a worldwide pandemic crisis. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09953v1' target="_blank">http://arxiv.org/pdf/2304.09953v1</a><br> <br> <br> <font size='5'> 146 </font> <div style="text-align: right"> 2023-04-18 19:47:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Analysis of How Many Undiscovered Vulnerabilities Remain in Information Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Vulnerability management strategy, from both organizational and public policy
perspectives, hinges on an understanding of the supply of undiscovered
vulnerabilities. If the number of undiscovered vulnerabilities is small enough,
then a reasonable investment strategy would be to focus on finding and removing
the remaining undiscovered vulnerabilities. If the number of undiscovered
vulnerabilities is and will continue to be large, then a better investment
strategy would be to focus on quick patch dissemination and engineering
resilient systems. This paper examines a paradigm, namely that the number of
undiscovered vulnerabilities is manageably small, through the lens of
mathematical concepts from the theory of computing. From this perspective, we
find little support for the paradigm of limited undiscovered vulnerabilities.
We then briefly support the notion that these theory-based conclusions are
relevant to practical computers in use today. We find no reason to believe
undiscovered vulnerabilities are not essentially unlimited in practice and we
examine the possible economic impacts should this be the case. Based on our
analysis, we recommend vulnerability management strategy adopts an approach
favoring quick patch dissemination and engineering resilient systems, while
continuing good software engineering practices to reduce (but never eliminate)
vulnerabilities in information systems. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09259v1' target="_blank">http://arxiv.org/pdf/2304.09259v1</a><br> <br> <br> <font size='5'> 147 </font> <div style="text-align: right"> 2023-04-18 16:17:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: IMoG -- a methodology for modeling future microelectronic innovations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: [Context and motivation] The automotive industry is currently undergoing a
fundamental transformation towards software defined vehicles. The automotive
market of the future demands a higher level of automation, electrification of
the power train, and individually configurable comfort functions.
[Question/problem] These demands pose a challenge to the automotive development
cycle, because they introduce complexity by larger and not yet well explored
design spaces that are difficult to manage. [Principal ideas/results] To cope
with these challenges, the main players along the value chain have an increased
interest in collaborating and aligning their development efforts along joint
roadmaps. Roadmap development can be viewed as a field of requirements
engineering with the goal to capture product aspects on an appropriate level of
abstraction to speed up investment decisions, reduce communication overhead and
parallelize development activities, while complying with competition laws.
[Contribution] In this paper, we present a refinement of the "Innovation
Modeling Grid" (IMoG), which encompasses a methodology, a process and a
proposed notation to support joint analysis of development roadmaps. IMoG is
focused on the automotive domain, yet there are clear potentials for other
applications. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09110v1' target="_blank">http://arxiv.org/pdf/2304.09110v1</a><br> <br> <br> <font size='5'> 148 </font> <div style="text-align: right"> 2023-04-18 11:30:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Separation of Estimation and Control in Risk-Sensitive Investment Problems under Incomplete Observation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A typical approach to tackle stochastic control problems with partial
observation is to separate the control and estimation tasks. However, it is
well known that this separation generally fails to deliver an actual optimal
solution for risk-sensitive control problems. This paper investigates the
separability of a general class of risk-sensitive investment management
problems when a finite-dimensional filter exists. We show that the
corresponding separated problem, where instead of the unobserved quantities,
one considers their conditional filter distribution given the observations, is
strictly equivalent to the original control problem. We widen the applicability
of the so-called Modified Zakai Equation (MZE) for the study of the separated
problem and prove that the MZE simplifies to a PDE in our approach.
Furthermore, we derive criteria for separability. We do not solve the separated
control problem but note that the existence of a finite-dimensional filter
leads to a finite state space for the separated problem. Hence, the difficulty
is equivalent to solving a complete observation risk-sensitive problem. Our
results have implications for existing risk-sensitive investment management
models with partial observations in that they establish their separability.
Their implications for future research on new applications is mainly to provide
conditions to ensure separability. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08910v1' target="_blank">http://arxiv.org/pdf/2304.08910v1</a><br> <br> <br> <font size='5'> 149 </font> <div style="text-align: right"> 2023-04-18 11:11:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Collective dynamics, diversification and optimal portfolio construction for cryptocurrencies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since its conception, the cryptocurrency market has been frequently described
as an immature market, characterized by significant swings in volatility and
occasionally described as lacking rhyme or reason. There has been great
speculation as to what role it plays in a diversified portfolio. For instance,
is cryptocurrency exposure an inflationary hedge or a speculative investment
that follows broad market sentiment with amplified beta? We have recently
explored similar questions with a clear focus on the equity market. There, our
research revealed several noteworthy dynamics such as: an increase in the
market's collective strength and uniformity during crises, greater
diversification benefits across equity sectors (rather than within them), and
the existence of a "best value" portfolio of equities. In essence, we can now
contrast any potential signatures of maturity we identify in the cryptocurrency
market and contrast these with the substantially larger, older and better
established equity market. This paper aims to investigate whether the
cryptocurrency market has recently exhibited similar mathematical properties as
the equity market. Instead of relying on traditional portfolio theory, which is
grounded in the financial dynamics of equity securities, we adjust our
experimental focus to capture the presumed behavioral purchasing patterns of
retail cryptocurrency investors. Our focus is on collective dynamics and
portfolio diversification in the cryptocurrency market, and examining whether
previously established results in the equity market hold in the cryptocurrency
market, and to what extent. Results reveal nuanced signatures of maturity
related to the equity market, including the fact that correlations collectively
spike around exchange collapses, and identify an ideal portfolio size and
spread across different groups of cryptocurrencies. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08902v2' target="_blank">http://arxiv.org/pdf/2304.08902v2</a><br> <br> <br> <font size='5'> 150 </font> <div style="text-align: right"> 2023-04-17 20:08:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Designing Policies for Truth: Combating Misinformation with Transparency and Information Design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Misinformation has become a growing issue on online social platforms (OSPs),
especially during elections or pandemics. To combat this, OSPs have implemented
various policies, such as tagging, to notify users about potentially misleading
information. However, these policies are often transparent and therefore
susceptible to being exploited by content creators, who may not be willing to
invest effort into producing authentic content, causing the viral spread of
misinformation. Instead of mitigating the reach of existing misinformation,
this work focuses on a solution of prevention, aiming to stop the spread of
misinformation before it has a chance to gain momentum. We propose a Bayesian
persuaded branching process ($\operatorname{BP}^2$) to model the strategic
interactions among the OSP, the content creator, and the user. The
misinformation spread on OSP is modeled by a multi-type branching process,
where users' positive and negative comments influence the misinformation
spreading. Using a Lagrangian induced by Bayesian plausibility, we characterize
the OSP's optimal policy under the perfect Bayesian equilibrium. The convexity
of the Lagrangian implies that the OSP's optimal policy is simply the fully
informative tagging policy: revealing the content's accuracy to the user. Such
a tagging policy solicits the best effort from the content creator in reducing
misinformation, even though the OSP exerts no direct control over the content
creator. We corroborate our findings using numerical simulations. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08588v1' target="_blank">http://arxiv.org/pdf/2304.08588v1</a><br> <br> <br> <font size='5'> 151 </font> <div style="text-align: right"> 2023-04-17 11:05:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trading green bonds using distributed ledger technology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The promising markets for voluntary carbon credits are faced with crippling
challenges to the certification of carbon sequestration and the lack of
scalable market infrastructure in which companies and institutions can invest
in carbon offsetting. This amounts to a funding problem for green transition
projects, such as in the agricultural sector, since farmers need access to the
liquidity needed to fund the transition to sustainable practices. We explore
the feasibility of mitigating infrastructural challenges based on a DLT Trading
and Settlement System for green bonds. The artefact employs a multi-sharded
architecture in which the nodes retain carefully orchestrated responsibilities
in the functioning of the network. We evaluate the artefact in a supranational
context with an EU-based regulator as part of a regulatory sandbox program
targeting the new EU DLT Pilot regime. By conducting design-driven research
with stakeholders from industrial and governmental bodies, we contribute to the
IS literature on the practical implications of DLT. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08154v1' target="_blank">http://arxiv.org/pdf/2304.08154v1</a><br> <br> <br> <font size='5'> 152 </font> <div style="text-align: right"> 2023-04-17 10:01:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Oriented right-angled Artin pro-$\ell$ groups and maximal pro-$\ell$ Galois groups</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For a prime number $\ell$ we introduce and study oriented right-angled Artin
pro-$\ell$ groups $G_{\Gamma,\lambda}$(oriented pro-$\ell$ RAAGs for short)
associated to a finite oriented graph $\Gamma$ and a continuous group
homomorphism $\lambda\colon\mathbb Z_\ell\to\mathbb Z_\ell^\times$. We show
that an oriented pro-$\ell$ RAAG $G_{\Gamma,\lambda}$ is a Bloch-Kato
pro-$\ell$ group if, and only if,
$(G_{\Gamma,\lambda},\theta_{\Gamma,\lambda})$ is an oriented pro-$\ell$ group
of elementary type generalizing a recent result of I. Snopche and P. Zalesskii.
Here $\theta_{\Gamma,\lambda}\colon G_{\Gamma,\lambda}\to\mathbb Z_p^\times$
denotes the canonical $\ell$-orientation on $G_{\Gamma,\lambda}$. We invest
some effort in order to show that oriented right-angled Artin pro-$\ell$ groups
share many properties with right-angled Artin pro-$\ell$-groups or even
discrete RAAG's, e.g., if $\Gamma$ is a specially oriented chordal graph, then
$G_{\Gamma,\lambda}$ is coherent, generalizing a result of C. Droms. Moreover,
in this case $(G_{\Gamma,\lambda},\theta_{\Gamma,\lambda})$ has the
Positselski-Bogomolov property generalizing a result of H. Servatius, C. Droms
and B. Servatius for discrete RAAG's. If $\Gamma$ is a specially oriented
chordal graph and ${\rm Im}(\lambda)\subseteq 1+4\mathbb Z_2$ in case that
$\ell=2$, then ${\rm H}^\bullet(G_{\Gamma,\lambda},\mathbb F_\ell) \simeq
\Lambda^\bullet(\ddot{\Gamma}^{\rm op})$ generalizing a well known result of M.
Salvetti. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08123v2' target="_blank">http://arxiv.org/pdf/2304.08123v2</a><br> <br> <br> <font size='5'> 153 </font> <div style="text-align: right"> 2023-04-17 01:30:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unpacking How Decentralized Autonomous Organizations (DAOs) Work in Practice</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Decentralized Autonomous Organizations (DAOs) have emerged as a novel way to
coordinate a group of (pseudonymous) entities towards a shared vision (e.g.,
promoting sustainability), utilizing self-executing smart contracts on
blockchains to support decentralized governance and decision-making. In just a
few years, over 4,000 DAOs have been launched in various domains, such as
investment, education, health, and research. Despite such rapid growth and
diversity, it is unclear how these DAOs actually work in practice and to what
extent they are effective in achieving their goals. Given this, we aim to
unpack how (well) DAOs work in practice. We conducted an in-depth analysis of a
diverse set of 10 DAOs of various categories and smart contracts, leveraging
on-chain (e.g., voting results) and off-chain data (e.g., community
discussions) as well as our interviews with DAO organizers/members.
Specifically, we defined metrics to characterize key aspects of DAOs, such as
the degrees of decentralization and autonomy. We observed CompoundDAO,
AssangeDAO, Bankless, and Krausehouse having poor decentralization in voting,
while decentralization has improved over time for one-person-one-vote DAOs
(e.g., Proof of Humanity). Moreover, the degree of autonomy varies among DAOs,
with some (e.g., Compound and Krausehouse) relying more on third parties than
others. Lastly, we offer a set of design implications for future DAO systems
based on our findings. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09822v1' target="_blank">http://arxiv.org/pdf/2304.09822v1</a><br> <br> <br> <font size='5'> 154 </font> <div style="text-align: right"> 2023-04-16 22:31:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SECAdvisor: a Tool for Cybersecurity Planning using Economic Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cybersecurity planning is challenging for digitized companies that want
adequate protection without overspending money. Currently, the lack of
investments and perverse economic incentives are the root cause of
cyberattacks, which results in several economic impacts on companies worldwide.
Therefore, cybersecurity planning has to consider technical and economic
dimensions to help companies achieve a better cybersecurity strategy. This
article introduces SECAdvisor, a tool to support cybersecurity planning using
economic models. SECAdvisor allows to (a) understand the risks and valuation of
different businesses' information, (b) calculate the optimal investment in
cybersecurity for a company, (c) receive a recommendation of protections based
on the budget available and demands, and (d) compare protection solutions in
terms of cost-efficiency. Furthermore, evaluations on usability and real-world
training activities performed using SECAdvisor are discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07909v1' target="_blank">http://arxiv.org/pdf/2304.07909v1</a><br> <br> <br> <font size='5'> 155 </font> <div style="text-align: right"> 2023-04-16 18:33:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI driven shadow model detection in agropv farms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Agro-photovoltaic (APV) is a growing farming practice that combines
agriculture and solar photovoltaic projects within the same area. This emerging
market is expected to experience significant growth in the next few years, with
a projected investment of $9 billion in 2030. Identifying shadows is crucial to
understanding the APV environment, as they impact plant growth, microclimate,
and evapotranspiration. In this study, we use state-of-the-art CNN and
GAN-based neural networks to detect shadows in agro-PV farms, demonstrating
their effectiveness. However, challenges remain, including partial shadowing
from moving objects and real-time monitoring. Future research should focus on
developing more sophisticated neural network-based shadow detection algorithms
and integrating them with control systems for APV farms. Overall, shadow
detection is crucial to increase productivity and profitability while
supporting the environment, soil, and farmers. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07853v1' target="_blank">http://arxiv.org/pdf/2304.07853v1</a><br> <br> <br> <font size='5'> 156 </font> <div style="text-align: right"> 2023-04-16 15:18:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Importance of Technical Distribution Network Limits in Dynamic Operating Envelopes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: End-users more often decide to invest in distributed generation (DG) units
that help them in decreasing electricity bills and allow them to become a
market player by selling the excess produced electricity. However, the
installation of DG is often limited by technical constraints of the network,
standards, and national grid codes. As a method for removing the mentioned
obstacles, the potential of dynamic operating envelopes (DOEs) is recently
becoming recognized as a way for maximizing the benefits of installing DG. In
this paper, we present an improvement of the already developed models that
often neglect voltage unbalance constraints or are not based on an optimization
approach. To test the model, two realistic case studies are defined. The
results show that not all technical constraints are equally important, that the
voltage unbalance constraint impacts the calculated DOEs for single-phase
installed DG units, and that neglecting the temporal and spatial component in
determining the limitation power is inadequate. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07806v1' target="_blank">http://arxiv.org/pdf/2304.07806v1</a><br> <br> <br> <font size='5'> 157 </font> <div style="text-align: right"> 2023-04-16 02:17:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment and Consumption Strategies with General and Linear Transaction Costs under CRRA Utility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transaction costs play a critical role in asset allocation and consumption
strategies in portfolio management. We apply the methods of dynamic programming
and singular perturbation expansion to derive the closed-form leading solutions
to this problem for small transaction costs with arbitrary transaction cost
structure by maximizing the expected CRRA (constant relative risk aversion)
utility function for this problem. We also discuss in detail the case which
consists of both fixed and proportional transaction costs. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07672v1' target="_blank">http://arxiv.org/pdf/2304.07672v1</a><br> <br> <br> <font size='5'> 158 </font> <div style="text-align: right"> 2023-04-15 19:22:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We examine the potential of ChatGPT, and other large language models, in
predicting stock market returns using sentiment analysis of news headlines. We
use ChatGPT to indicate whether a given headline is good, bad, or irrelevant
news for firms' stock prices. We then compute a numerical score and document a
positive correlation between these ``ChatGPT scores'' and subsequent daily
stock market returns. Further, ChatGPT outperforms traditional sentiment
analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT
cannot accurately forecast returns, indicating return predictability is an
emerging capacity of complex models. ChatGPT-4's implied Sharpe ratios are
larger than ChatGPT-3's; however, the latter model has larger total returns.
Our results suggest that incorporating advanced language models into the
investment decision-making process can yield more accurate predictions and
enhance the performance of quantitative trading strategies. Predictability is
concentrated on smaller stocks and more prominent on firms with bad news,
consistent with limits-to-arbitrage arguments rather than market
inefficiencies. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07619v3' target="_blank">http://arxiv.org/pdf/2304.07619v3</a><br> <br> <br> <font size='5'> 159 </font> <div style="text-align: right"> 2023-04-15 11:36:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Machine Learning-Enhanced Benders Decomposition Approach to Solve the Transmission Expansion Planning Problem under Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The necessary decarbonization efforts in energy sectors entail the
integration of flexibility assets, as well as increased levels of uncertainty
for the planning and operation of power systems. To cope with this in a
cost-effective manner, transmission expansion planning (TEP) models need to
incorporate progressively more details to represent potential long-term system
developments and the operation of power grids with intermittent renewable
generation. However, the increased modeling complexities of TEP exercises can
easily lead to computationally intractable optimization problems. Currently,
most techniques that address computational intractability alter the original
problem, thus neglecting critical modeling aspects or affecting the structure
of the optimal solution. In this paper, we propose an alternative approach to
significantly alleviate the computational burden of large-scale TEP problems.
Our approach integrates machine learning (ML) with the well-established Benders
decomposition to manage the problem size while preserving solution quality. The
proposed ML-enhanced Multicut Benders Decomposition algorithm improves
computational efficiency by identifying effective and ineffective optimality
cuts via supervised learning techniques. We illustrate the benefits of the
proposed methodology by solving a number of multi-stage TEP problems of
different sizes, based on the IEEE24 and IEEE118 test systems, while also
considering energy storage investment options. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07534v1' target="_blank">http://arxiv.org/pdf/2304.07534v1</a><br> <br> <br> <font size='5'> 160 </font> <div style="text-align: right"> 2023-04-15 08:01:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust Educational Dialogue Act Classifiers with Low-Resource and Imbalanced Datasets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Dialogue acts (DAs) can represent conversational actions of tutors or
students that take place during tutoring dialogues. Automating the
identification of DAs in tutoring dialogues is significant to the design of
dialogue-based intelligent tutoring systems. Many prior studies employ machine
learning models to classify DAs in tutoring dialogues and invest much effort to
optimize the classification accuracy by using limited amounts of training data
(i.e., low-resource data scenario). However, beyond the classification
accuracy, the robustness of the classifier is also important, which can reflect
the capability of the classifier on learning the patterns from different class
distributions. We note that many prior studies on classifying educational DAs
employ cross entropy (CE) loss to optimize DA classifiers on low-resource data
with imbalanced DA distribution. The DA classifiers in these studies tend to
prioritize accuracy on the majority class at the expense of the minority class
which might not be robust to the data with imbalanced ratios of different DA
classes. To optimize the robustness of classifiers on imbalanced class
distributions, we propose to optimize the performance of the DA classifier by
maximizing the area under the ROC curve (AUC) score (i.e., AUC maximization).
Through extensive experiments, our study provides evidence that (i) by
maximizing AUC in the training process, the DA classifier achieves significant
performance improvement compared to the CE approach under low-resource data,
and (ii) AUC maximization approaches can improve the robustness of the DA
classifier under different class imbalance ratios. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07499v1' target="_blank">http://arxiv.org/pdf/2304.07499v1</a><br> <br> <br> <font size='5'> 161 </font> <div style="text-align: right"> 2023-04-14 09:39:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stochastic maximum principle for recursive optimal control problems with varying terminal time</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduces a new recursive stochastic optimal control problem
driven by a forward-backward stochastic differential equations (FBSDEs), where
the ter?minal time varies according to the constraints of the state of the
forward equation. This new optimal control problem can be used to describe the
investment portfolio problems with the varying investment period. Based on
novel \r{ho}-moving variational and adjoint equations, we establish the
stochastic maximum principle for this optimal control problem including the
classical optimal control problem as a particular case. Furthermore, we propose
an example to verify our main results. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07026v1' target="_blank">http://arxiv.org/pdf/2304.07026v1</a><br> <br> <br> <font size='5'> 162 </font> <div style="text-align: right"> 2023-04-14 06:33:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient Estimation in Extreme Value Regression Models of Hedge Fund Tail Risks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce a method to estimate simultaneously the tail and the threshold
parameters of an extreme value regression model. This standard model finds its
use in finance to assess the effect of market variables on extreme loss
distributions of investment vehicles such as hedge funds. However, a major
limitation is the need to select ex ante a threshold below which data are
discarded, leading to estimation inefficiencies. To solve these issues, we
extend the tail regression model to non-tail observations with an auxiliary
splicing density, enabling the threshold to be selected automatically. We then
apply an artificial censoring mechanism of the likelihood contributions in the
bulk of the data to decrease specification issues at the estimation stage. We
illustrate the superiority of our approach for inference over classical
peaks-over-threshold methods in a simulation study. Empirically, we investigate
the determinants of hedge fund tail risks over time, using pooled returns of
1,484 hedge funds. We find a significant link between tail risks and factors
such as equity momentum, financial stability index, and credit spreads.
Moreover, sorting funds along exposure to our tail risk measure discriminates
between high and low alpha funds, supporting the existence of a fear premium. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06950v1' target="_blank">http://arxiv.org/pdf/2304.06950v1</a><br> <br> <br> <font size='5'> 163 </font> <div style="text-align: right"> 2023-04-14 06:12:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust utility maximization with intractable claims</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study a continuous-time expected utility maximization problem in which the
investor at maturity receives the value of a contingent claim in addition to
the investment payoff from the financial market. The investor knows nothing
about the claim other than its probability distribution, hence an ``intractable
claim''. In view of the lack of necessary information about the claim, we
consider a robust formulation to maximize her utility in the worst scenario. We
apply the quantile formulation to solve the problem, expressing the quantile
function of the optimal terminal investment income as the solution of certain
variational inequalities of ordinary differential equations. In the case of an
exponential utility, the problem reduces to a (non-robust) rank--dependent
utility maximization with probability distortion whose solution is available in
the literature. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06938v1' target="_blank">http://arxiv.org/pdf/2304.06938v1</a><br> <br> <br> <font size='5'> 164 </font> <div style="text-align: right"> 2023-04-13 17:01:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Inertia-Aware Microgrid Investment Planning Using Tractable Decomposition Algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The integration of the frequency dynamics into Micro-Grid (MG) investment and
operational planning problems is vital in ensuring the security of the system
in the post-contingency states. However, the task of including transient
security constraints in MG planning problems is non-trivial. This is due to the
highly non-linear and non-convex nature of the analytical closed form of the
frequency metrics (e.g., frequency nadir) and power flow constraints. To handle
this issue, this paper presents two algorithms for decomposing the MG
investment planning problem into multiple levels to enhance computational
tractability and optimality. Furthermore, the sensitivity of the decisions made
at each level is captured by corresponding dual cutting planes to model
feasible secure regions. This, in turn, ensures both the optimal determination
and placement of inertia services and accelerates the convergence of the
proposed decomposition algorithms. The efficient and effective performance of
the proposed algorithms is tested and verified on an 18-bus Low Voltage (LV)
network and a 30-bus Medium Voltage (MV) network under various operating
scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06674v1' target="_blank">http://arxiv.org/pdf/2304.06674v1</a><br> <br> <br> <font size='5'> 165 </font> <div style="text-align: right"> 2023-04-13 00:59:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Difficult Lessons on Social Prediction from Wisconsin Public Schools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Early warning systems (EWS) are prediction algorithms that have recently
taken a central role in efforts to improve graduation rates in public schools
across the US. These systems assist in targeting interventions at individual
students by predicting which students are at risk of dropping out. Despite
significant investments and adoption, there remain significant gaps in our
understanding of the efficacy of EWS. In this work, we draw on nearly a
decade's worth of data from a system used throughout Wisconsin to provide the
first large-scale evaluation of the long-term impact of EWS on graduation
outcomes.
  We present evidence that risk assessments made by the prediction system are
highly accurate, including for students from marginalized backgrounds. Despite
the system's accuracy and widespread use, we find no evidence that it has led
to improved graduation rates. We surface a robust statistical pattern that can
explain why these seemingly contradictory insights hold. Namely, environmental
features, measured at the level of schools, contain significant signal about
dropout risk. Within each school, however, academic outcomes are essentially
independent of individual student performance. This empirical observation
indicates that assigning all students within the same school the same
probability of graduation is a nearly optimal prediction.
  Our work provides an empirical backbone for the robust, qualitative
understanding among education researchers and policy-makers that dropout is
structurally determined. The primary barrier to improving outcomes lies not in
identifying students at risk of dropping out within specific schools, but
rather in overcoming structural differences across different school districts.
Our findings indicate that we should carefully evaluate the decision to fund
early warning systems without also devoting resources to interventions tackling
structural barriers. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06205v1' target="_blank">http://arxiv.org/pdf/2304.06205v1</a><br> <br> <br> <font size='5'> 166 </font> <div style="text-align: right"> 2023-04-12 18:46:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identifying Trades Using Technical Analysis and ML/DL Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The importance of predicting stock market prices cannot be overstated. It is
a pivotal task for investors and financial institutions as it enables them to
make informed investment decisions, manage risks, and ensure the stability of
the financial system. Accurate stock market predictions can help investors
maximize their returns and minimize their losses, while financial institutions
can use this information to develop effective risk management policies.
However, stock market prediction is a challenging task due to the complex
nature of the stock market and the multitude of factors that can affect stock
prices. As a result, advanced technologies such as deep learning are being
increasingly utilized to analyze vast amounts of data and provide valuable
insights into the behavior of the stock market. While deep learning has shown
promise in accurately predicting stock prices, there is still much research to
be done in this area. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09936v1' target="_blank">http://arxiv.org/pdf/2304.09936v1</a><br> <br> <br> <font size='5'> 167 </font> <div style="text-align: right"> 2023-04-11 21:51:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Time-frequency co-movements between commodities and economic policy uncertainty across different crises</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Commodity futures constitute an attractive asset class for portfolio
managers. Propelled by their low correlation with other assets, commodities
begin gaining popularity among investors, as they allow to capture
diversification benefits. After more than two decades of active investing
experience, this paper examines the time and frequency of spillovers between
Economic Policy Uncertainty (Davis, 2016) and a broad set of commodities. The
period under examination goes from December 1997 until April 2022, covering
political, economic, and even health crises. We apply a wavelet coherence
analysis between time series, in order to shed light on the time-frequency
comovements and lead-lag relationships. This research finds a distinct impact
on the commodities, depending on the nature of the crisis. In particular,
during the global financial crisis and the Covid-19 crisis, comovements are
stronger in most commodities. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05517v1' target="_blank">http://arxiv.org/pdf/2304.05517v1</a><br> <br> <br> <font size='5'> 168 </font> <div style="text-align: right"> 2023-04-11 19:07:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Searching for the "Holy Grail" of sponsorship-linked marketing: A generalizable sponsorship ROI model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Marketers routinely allocate a significant portion of their budget to
sponsorship. However, isolating the return on investment from such efforts has
remained a challenge. Thus, a dataset of more than 5,800 sponsorships is
analyzed using survival analysis approaches that utilizes the sponsor's renewal
of the sponsorship as a proxy for positive ROI. In addition to its contribution
to the sponsorship-linked marketing literature, the resulting model can be
utilized by managers to generate predicted values relative to the sponsor's
probability of renewal and the ultimate duration of time the sponsor will
remain with the property, representing a novel managerial contribution. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09473v1' target="_blank">http://arxiv.org/pdf/2305.09473v1</a><br> <br> <br> <font size='5'> 169 </font> <div style="text-align: right"> 2023-04-11 15:48:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Neural Network Approach to Portfolio Optimization with Leverage Constraints:a Case Study on High Inflation Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Motivated by the current global high inflation scenario, we aim to discover a
dynamic multi-period allocation strategy to optimally outperform a passive
benchmark while adhering to a bounded leverage limit. To this end, we formulate
an optimal control problem to outperform a benchmark portfolio throughout the
investment horizon. Assuming the asset prices follow the jump-diffusion model
during high inflation periods, we first establish a closed-form solution for
the optimal strategy that outperforms a passive strategy under the cumulative
quadratic tracking difference (CD) objective, assuming continuous trading and
no bankruptcy. To obtain strategies under the bounded leverage constraint among
other realistic constraints, we then propose a novel leverage-feasible neural
network (LFNN) to represent control, which converts the original constrained
optimization problem into an unconstrained optimization problem that is
computationally feasible with standard optimization methods. We establish
mathematically that the LFNN approximation can yield a solution that is
arbitrarily close to the solution of the original optimal control problem with
bounded leverage. We further apply the LFNN approach to a four-asset investment
scenario with bootstrap resampled asset returns from the filtered high
inflation regime data. The LFNN strategy is shown to consistently outperform
the passive benchmark strategy by about 200 bps (median annualized return),
with a greater than 90% probability of outperforming the benchmark at the end
of the investment horizon. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05297v2' target="_blank">http://arxiv.org/pdf/2304.05297v2</a><br> <br> <br> <font size='5'> 170 </font> <div style="text-align: right"> 2023-04-11 12:15:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Electricity Demand Forecasting with Hybrid Statistical and Machine Learning Algorithms: Case Study of Ukraine</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article presents a novel hybrid approach using statistics and machine
learning to forecast the national demand of electricity. As investment and
operation of future energy systems require long-term electricity demand
forecasts with hourly resolution, our mathematical model fills a gap in energy
forecasting. The proposed methodology was constructed using hourly data from
Ukraine's electricity consumption ranging from 2013 to 2020. To this end, we
analysed the underlying structure of the hourly, daily and yearly time series
of electricity consumption. The long-term yearly trend is evaluated using
macroeconomic regression analysis. The mid-term model integrates temperature
and calendar regressors to describe the underlying structure, and combines
ARIMA and LSTM ``black-box'' pattern-based approaches to describe the error
term. The short-term model captures the hourly seasonality through calendar
regressors and multiple ARMA models for the residual. Results show that the
best forecasting model is composed by combining multiple regression models and
a LSTM hybrid model for residual prediction. Our hybrid model is very effective
at forecasting long-term electricity consumption on an hourly resolution. In
two years of out-of-sample forecasts with 17520 timesteps, it is shown to be
within 96.83 \% accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05174v1' target="_blank">http://arxiv.org/pdf/2304.05174v1</a><br> <br> <br> <font size='5'> 171 </font> <div style="text-align: right"> 2023-04-11 03:32:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Intelligent humanoids in manufacturing to address worker shortage and skill gaps: Case of Tesla Optimus</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Technological evolution in the field of robotics is emerging with major
breakthroughs in recent years. This was especially fostered by revolutionary
new software applications leading to humanoid robots. Humanoids are being
envisioned for manufacturing applications to form human-robot teams. But their
implication in manufacturing practices especially for industrial safety
standards and lean manufacturing practices have been minimally addressed.
Humanoids will also be competing with conventional robotic arms and effective
methods to assess their return on investment are needed. To study the next
generation of industrial automation, we used the case context of the Tesla
humanoid robot. The company has recently unveiled its project on an intelligent
humanoid robot named Optimus to achieve an increased level of manufacturing
automation. This article proposes a framework to integrate humanoids for
manufacturing automation and also presents the significance of safety standards
of human-robot collaboration. A case of lean assembly cell for the
manufacturing of an open-source medical ventilator was used for human-humanoid
automation. Simulation results indicate that humanoids can increase the level
of manufacturing automation. Managerial and research implications are
presented. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04949v1' target="_blank">http://arxiv.org/pdf/2304.04949v1</a><br> <br> <br> <font size='5'> 172 </font> <div style="text-align: right"> 2023-04-10 15:38:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial Intelligence for IT operations (AIOps) aims to combine the power
of AI with the big data generated by IT Operations processes, particularly in
cloud infrastructures, to provide actionable insights with the primary goal of
maximizing availability. There are a wide variety of problems to address, and
multiple use-cases, where AI capabilities can be leveraged to enhance
operational efficiency. Here we provide a review of the AIOps vision, trends
challenges and opportunities, specifically focusing on the underlying AI
techniques. We discuss in depth the key types of data emitted by IT Operations
activities, the scale and challenges in analyzing them, and where they can be
helpful. We categorize the key AIOps tasks as - incident detection, failure
prediction, root cause analysis and automated actions. We discuss the problem
formulation for each task, and then present a taxonomy of techniques to solve
these problems. We also identify relatively under explored topics, especially
those that could significantly benefit from advances in AI literature. We also
provide insights into the trends in this field, and what are the key investment
opportunities. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04661v1' target="_blank">http://arxiv.org/pdf/2304.04661v1</a><br> <br> <br> <font size='5'> 173 </font> <div style="text-align: right"> 2023-04-07 23:04:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Clinical trials are essential to drug development but time-consuming, costly,
and prone to failure. Accurate trial outcome prediction based on historical
trial data promises better trial investment decisions and more trial success.
Existing trial outcome prediction models were not designed to model the
relations among similar trials, capture the progression of features and designs
of similar trials, or address the skewness of trial data which causes inferior
performance for less common trials.
  To fill the gap and provide accurate trial outcome prediction, we propose
Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first
identifies trial topics to cluster the multi-sourced trial data into relevant
trial topics. It then generates trial embeddings and organizes them by topic
and time to create clinical trial sequences. With the consideration of each
trial sequence as a task, it uses a meta-learning strategy to achieve a point
where the model can rapidly adapt to new tasks with minimal updates. In
particular, the topic discovery module enables a deeper understanding of the
underlying structure of the data, while sequential learning captures the
evolution of trial designs and outcomes. This results in predictions that are
not only more accurate but also more interpretable, taking into account the
temporal patterns and unique characteristics of each trial topic. We
demonstrate that SPOT wins over the prior methods by a significant margin on
trial outcome benchmark data: with a 21.5\% lift on phase I, an 8.9\% lift on
phase II, and a 5.5\% lift on phase III trials in the metric of the area under
precision-recall curve (PR-AUC). </font><br> Link: <a href='http://arxiv.org/pdf/2304.05352v1' target="_blank">http://arxiv.org/pdf/2304.05352v1</a><br> <br> <br> <font size='5'> 174 </font> <div style="text-align: right"> 2023-04-06 14:00:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Most machine learning and data analytics applications, including performance
engineering in software systems, require a large number of annotations and
labelled data, which might not be available in advance. Acquiring annotations
often requires significant time, effort, and computational resources, making it
challenging. We develop a unified active learning framework, specializing in
software performance prediction, to address this task. We begin by parsing the
source code to an Abstract Syntax Tree (AST) and augmenting it with data and
control flow edges. Then, we convert the tree representation of the source code
to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph
representation, we construct various graph embeddings (unsupervised and
supervised) into a latent space. Given such an embedding, the framework becomes
task agnostic since active learning can be performed using any regression
method and query strategy suited for regression. Within this framework, we
investigate the impact of using different levels of information for active and
passive learning, e.g., partially available labels and unlabeled test data. Our
approach aims to improve the investment in AI models for different software
performance predictions (execution time) based on the structure of the source
code. Our real-world experiments reveal that respectable performance can be
achieved by querying labels for only a small subset of all the data. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13032v1' target="_blank">http://arxiv.org/pdf/2304.13032v1</a><br> <br> <br> <font size='5'> 175 </font> <div style="text-align: right"> 2023-04-06 12:54:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modelling customer lifetime-value in the retail banking industry</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Understanding customer lifetime value is key to nurturing long-term customer
relationships, however, estimating it is far from straightforward. In the
retail banking industry, commonly used approaches rely on simple heuristics and
do not take advantage of the high predictive ability of modern machine learning
techniques. We present a general framework for modelling customer lifetime
value which may be applied to industries with long-lasting contractual and
product-centric customer relationships, of which retail banking is an example.
This framework is novel in facilitating CLV predictions over arbitrary time
horizons and product-based propensity models. We also detail an implementation
of this model which is currently in production at a large UK lender. In
testing, we estimate an 43% improvement in out-of-time CLV prediction error
relative to a popular baseline approach. Propensity models derived from our CLV
model have been used to support customer contact marketing campaigns. In
testing, we saw that the top 10% of customers ranked by their propensity to
take up investment products were 3.2 times more likely to take up an investment
product in the next year than a customer chosen at random. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03038v1' target="_blank">http://arxiv.org/pdf/2304.03038v1</a><br> <br> <br> <font size='5'> 176 </font> <div style="text-align: right"> 2023-04-06 06:28:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Connected and Automated Vehicles Investment and Smart Infrastructure in Tennessee Part 3: Infrastructure and Vehicular communications: From Dedicated Short-Range Communications to Cellular Vehicle-to-Everything</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This report aims to support the Tennessee Department of Transportation's
decisions about vehicle and infrastructure communication technologies. The
transition from Dedicated Short-Range communication (DSRC) V2X to Cellular
Vehicle to Everything (C-V2X) is explored using USDOT guidance on relevant
issues and presenting the results of experimentation in Tennessee and the
potential pros and cons. DSRC V2X technology has been planned at traffic signal
in Tennessee, e.g., 152 Roadside Units (RSUs) were planned by TDOT using DSRC
V2X and Bluetooth combination units in the I-24 smart corridor. Similarly, many
pilot programs and testbeds around the nation have deployed DSRC V2X technology
and are now impacted by the Federal Communication Commission's (FCC) ruling on
opening safety band. The implication is that DSRC V2X deployments (and future
deployments) should migrate to C-V2X. Notably, dual-mode RSUs are available
along with LTE C-V2X. The transition can be done by working with vendors, but
surely this involves more than swapping DSRC V2X devices with LTE C-V2X
devices. Complicating the migration to C-V2X is TDOT's role in traffic signal
operations and maintenance, which is limited to funding and
designing/construction of traffic signals, but local agencies operate and
maintain signals. Hence, local agencies will work with TDOT to operate and
maintain C-V2X technology. Moreover, C-V2X technologies are not widely
tested-interference by unlicensed devices and channel congestion can adversely
affect safety-critical applications. Given the substantial uncertainties in
transitioning to these technologies, TDOT's discussion with IOOs about the
operation and maintenance of C-V2X may have to wait for the resolution issues,
while TDOT can invest in experimentation with dual-mode devices.
Recommendations are provided about dual-mode devices, CAV data, and needed
research and testing. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02885v1' target="_blank">http://arxiv.org/pdf/2304.02885v1</a><br> <br> <br> <font size='5'> 177 </font> <div style="text-align: right"> 2023-04-05 15:11:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02496v1' target="_blank">http://arxiv.org/pdf/2304.02496v1</a><br> <br> <br> <font size='5'> 178 </font> <div style="text-align: right"> 2023-04-05 10:56:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A network-based strategy of price correlations for optimal cryptocurrency portfolios</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A cryptocurrency is a digital asset maintained by a decentralised system
using cryptography. Investors in this emerging digital market are exploring the
profitability potential of portfolios in place of single coins. Portfolios are
particularly useful given that price forecasting in such a volatile market is
challenging. The crypto market is a self-organised complex system where the
complex inter-dependencies between the cryptocurrencies may be exploited to
understand the market dynamics and build efficient portfolios. In this letter,
we use network methods to identify highly decorrelated cryptocurrencies to
create diversified portfolios using the Markowitz Portfolio Theory agnostic to
future market behaviour. The performance of our network-based portfolios is
optimal with 46 coins and superior to benchmarks up to an investment horizon of
14 days, reaching up to 1,066% average expected return within 1 day, with
reasonable associated risks. We also show that popular cryptocurrencies are
typically not included in the optimal portfolios. Past price correlations
reduce risk and may improve the performance of crypto portfolios in comparison
to methodologies based exclusively on price auto-correlations. Short-term
crypto investments may be competitive to traditional high-risk investments such
as the stock market or commodity market but call for caution given the high
variability of prices. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02362v1' target="_blank">http://arxiv.org/pdf/2304.02362v1</a><br> <br> <br> <font size='5'> 179 </font> <div style="text-align: right"> 2023-04-04 13:00:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pulse Profile Modeling of Thermonuclear Burst Oscillations I: The Effect of Neglecting Variability</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the effects of the time-variable properties of thermonuclear X-ray
bursts on modeling their millisecond-period burst oscillations. We apply the
pulse profile modeling technique that is being used in the analysis of
rotation-powered millisecond pulsars by the Neutron Star Interior Composition
Explorer (NICER) to infer masses, radii, and geometric parameters of neutron
stars. By simulating and analyzing a large set of models, we show that
overlooking burst time-scale variability in temperatures and sizes of the hot
emitting regions can result in substantial bias in the inferred mass and
radius. To adequately infer neutron star properties, it is essential to develop
a model for the time variable properties or invest a substantial amount of
computational time in segmenting the data into non-varying pieces. We discuss
prospects for constraints from proposed future X-ray telescopes. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01770v1' target="_blank">http://arxiv.org/pdf/2304.01770v1</a><br> <br> <br> <font size='5'> 180 </font> <div style="text-align: right"> 2023-04-03 11:51:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ScandEval: A Benchmark for Scandinavian Natural Language Processing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduces a Scandinavian benchmarking platform, ScandEval, which
can benchmark any pretrained model on four different tasks in the Scandinavian
languages. The datasets used in two of the tasks, linguistic acceptability and
question answering, are new. We develop and release a Python package and
command-line interface, scandeval, which can benchmark any model that has been
uploaded to the Hugging Face Hub, with reproducible results. Using this
package, we benchmark more than 100 Scandinavian or multilingual models and
present the results of these in an interactive online leaderboard, as well as
provide an analysis of the results. The analysis shows that there is
substantial cross-lingual transfer among the Mainland Scandinavian languages
(Danish, Swedish and Norwegian), with limited cross-lingual transfer between
the group of Mainland Scandinavian languages and the group of Insular
Scandinavian languages (Icelandic and Faroese). The benchmarking results also
show that the investment in language technology in Norway, Sweden and Denmark
has led to language models that outperform massively multilingual models such
as XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package
and leaderboard. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00906v1' target="_blank">http://arxiv.org/pdf/2304.00906v1</a><br> <br> <br> <font size='5'> 181 </font> <div style="text-align: right"> 2023-04-03 01:35:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CV2X-LOCA: Roadside Unit-Enabled Cooperative Localization Framework for Autonomous Vehicles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: An accurate and robust localization system is crucial for autonomous vehicles
(AVs) to enable safe driving in urban scenes. While existing global navigation
satellite system (GNSS)-based methods are effective at locating vehicles in
open-sky regions, achieving high-accuracy positioning in urban canyons such as
lower layers of multi-layer bridges, streets beside tall buildings, tunnels,
etc., remains a challenge. In this paper, we investigate the potential of
cellular-vehicle-to-everything (C-V2X) wireless communications in improving the
localization performance of AVs under GNSS-denied environments. Specifically,
we propose the first roadside unit (RSU)-enabled cooperative localization
framework, namely CV2X-LOCA, that only uses C-V2X channel state information to
achieve lane-level positioning accuracy. CV2X-LOCA consists of four key parts:
data processing module, coarse positioning module, environment parameter
correcting module, and vehicle trajectory filtering module. These modules
jointly handle challenges present in dynamic C-V2X networks. Extensive
simulation and field experiments show that CV2X-LOCA achieves state-of-the-art
performance for vehicle localization even under noisy conditions with
high-speed movement and sparse RSUs coverage environments. The study results
also provide insights into future investment decisions for transportation
agencies regarding deploying RSUs cost-effectively. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00676v1' target="_blank">http://arxiv.org/pdf/2304.00676v1</a><br> <br> <br> <font size='5'> 182 </font> <div style="text-align: right"> 2023-04-02 22:02:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automatic Design of Telecom Networks with Genetic Algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the increasing demand for high-quality internet services, deploying
GPON/Fiber-to-the-Home networks is one of the biggest challenges that internet
providers have to deal with due to the significant investments involved.
Automated network design usage becomes more critical to aid with planning the
network by minimising the costs of planning and deployment. The main objective
is to tackle this problem of optimisation of networks that requires taking into
account multiple factors such as the equipment placement and their
configuration, the optimisation of the cable routes, the optimisation of the
clients' allocation and other constraints involved in the minimisation problem.
An AI-based solution is proposed to automate network design, which is a task
typically done manually by teams of engineers. It is a difficult task requiring
significant time to complete manually. To alleviate this tiresome task, we
proposed a Genetic Algorithm using a two-level representation to design the
networks automatically. To validate the approach, we compare the quality of the
generated solutions with the handmade design ones that are deployed in the real
world. The results show that our method can save costs and time in finding
suitable and better solutions than existing ones, indicating its potential as a
support design tool of solutions for GPON/Fiber-to-the-Home networks. In
concrete, in the two scenarios where we validate our proposal, our approach can
cut costs by 31% and by 52.2%, respectively, when compared with existing
handmade ones, showcasing and validating the potential of the proposed
approach. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00637v1' target="_blank">http://arxiv.org/pdf/2304.00637v1</a><br> <br> <br> <font size='5'> 183 </font> <div style="text-align: right"> 2023-04-02 20:03:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Eight Things to Know about Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The widespread public deployment of large language models (LLMs) in recent
months has prompted a wave of new attention and engagement from advocates,
policymakers, and scholars from many fields. This attention is a timely
response to the many urgent questions that this technology raises, but it can
sometimes miss important considerations. This paper surveys the evidence for
eight potentially surprising such points:
  1. LLMs predictably get more capable with increasing investment, even without
targeted innovation.
  2. Many important LLM behaviors emerge unpredictably as a byproduct of
increasing investment.
  3. LLMs often appear to learn and use representations of the outside world.
  4. There are no reliable techniques for steering the behavior of LLMs.
  5. Experts are not yet able to interpret the inner workings of LLMs.
  6. Human performance on a task isn't an upper bound on LLM performance.
  7. LLMs need not express the values of their creators nor the values encoded
in web text.
  8. Brief interactions with LLMs are often misleading. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00612v1' target="_blank">http://arxiv.org/pdf/2304.00612v1</a><br> <br> <br> <font size='5'> 184 </font> <div style="text-align: right"> 2023-04-02 10:55:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Tech Decoupling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Financial market volatility is a crucial factor for investment planning,
option pricing, and financial market regulation, and technology is widely
recognized as a key driver of economic growth. In this project, we investigate
the co-movement of technology and non-technology sectors over the last two
decades. We identify a decoupling phenomenon in the levels and volatility of
the two sectors after 2015 and argue that this cannot be attributed to the
COVID-19 shock. Furthermore, we demonstrate that the technology sector serves
as a leading indicator of growth for the rest of the economy. Using ARIMA
modeling and stationarity tests, we process time series data to test our
hypotheses, finding that the technology sector follows an ARIMA(3,1,3) model,
while the non-technology sector follows an ARIMA(2,1,4) model. Our analysis
encompasses data wrangling, pre-processing, missing value treatment, and
exploratory data analysis, and we discuss the merits and shortcomings of our
work to aid in the interpretation of our results. Overall, our findings shed
new light on the relationship between technology and economic growth. Our
results can be used for understanding labor market fluctuations in the
technology sector and investment planning assessments. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00510v1' target="_blank">http://arxiv.org/pdf/2304.00510v1</a><br> <br> <br> <font size='5'> 185 </font> <div style="text-align: right"> 2023-04-02 08:56:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reduction of Excess Capacity with Response of Capital Intensity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Purpose: The objective of this research was to show the response of the
potential reduction of excess capacity in terms of capital intensity to the
growth rate of labor productivity in the manufacturing industrial sector.
Design/Methodology/Approach: The research was carried out in 2019 in 55 groups
of Indian manufacturing industry within six major Indian industrial states.
Mainly, the research used the modified VES (Variable Elasticity Substitution)
estimation model. The research focused on the value of the additional
substitution parameter of capital intensity (mu > 0). Findings: Almost all
selected industry groups with in six states need capital-intensive production.
The results found additional parameter of capital intensity (mu) is greater
than zero for all industry groups. It means that a higher product per man can
be obtained by increasing the capital per worker. Practical Implications:
Research shows that an increasingly need for capital investment in need for
higher labor productivity is likely to induce the manufacturing unit to use
more capacity in existence. It reveals that investors in these selected six
states can increase their capital investment. Originality/Value: The analysis
of the result allowed to determine the fact that capital intensity is an
essential variable for reduction of excess capacity which cannot be ignored in
explaining productivity. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00489v1' target="_blank">http://arxiv.org/pdf/2304.00489v1</a><br> <br> <br> <font size='5'> 186 </font> <div style="text-align: right"> 2023-04-02 05:31:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Life cycle costing analysis of deep energy retrofits of a mid-rise building to understand the impact of energy conservation measures</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building energy retrofits have been identified as key to realizing climate
mitigation goals in Canada. This study aims to provide a roadmap for existing
mid-rise building retrofits in order to understand the required capital
investment, energy savings, energy cost savings, and carbon footprint for
mid-rise residential buildings in Canada. This study employed EnergyPlus to
examine the energy performance of 11 energy retrofit measures for a typical
multi-unit residential building (MURB) in Metro Vancouver, British Columbia,
Canada. The author employed the energy simulation software (EnergyPlus) to
evaluate the pre-and post-retrofit operational energy performance of the
selected MURB. Two base building models powered by natural gas (NG-building)
and electricity (E-building) were created by SketchUP. The energy simulation
results were combined with cost and emission impact data to evaluate the
economic and environmental performance of the selected energy retrofit
measures. The results indicated that the NG-building can produce significant
GHG emission reductions (from 27.64 tCO2e to 3.77 tCO2e) by implementing these
energy retrofit measures. In terms of energy savings, solar PV, ASHP, water
heater HP, and HRV enhancement have great energy saving potential compared to
other energy retrofit measures. In addition, temperature setback, lighting, and
airtightness enhancement present the best economic performance from a life
cycle perspective. However, windows, ASHP, and solar PV, are not economical
choices because of higher life cycle costs. While ASHP can increase life cycle
costs for the NG-building, with the financial incentives provided by the
governments, ASHP could be the best choice to reduce GHG emissions when
stakeholders make decisions on implementing energy retrofits. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00456v1' target="_blank">http://arxiv.org/pdf/2304.00456v1</a><br> <br> <br> <font size='5'> 187 </font> <div style="text-align: right"> 2023-04-01 14:25:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Company Competition Graph</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Financial market participants frequently rely on numerous business
relationships to make investment decisions. Investors can learn about potential
risks and opportunities associated with other connected entities through these
corporate connections. Nonetheless, human annotation of a large corpus to
extract such relationships is highly time-consuming, not to mention that it
requires a considerable amount of industry expertise and professional training.
Meanwhile, we have yet to observe means to generate reliable knowledge graphs
of corporate relationships due to the lack of impartial and granular data
sources. This study proposes a system to process financial reports and
construct the public competitor graph to fill the void. Our method can retrieve
more than 83\% competition relationship of the S\&P 500 index companies. Based
on the output from our system, we construct a knowledge graph with more than
700 nodes and 1200 edges. A demo interactive graph interface is available. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00323v1' target="_blank">http://arxiv.org/pdf/2304.00323v1</a><br> <br> <br> <font size='5'> 188 </font> <div style="text-align: right"> 2023-04-01 06:54:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Managing Portfolio for Maximizing Alpha and Minimizing Beta</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Portfolio management is an essential component of investment strategy that
aims to maximize returns while minimizing risk. This paper explores several
portfolio management strategies, including asset allocation, diversification,
active management, and risk management, and their importance in optimizing
portfolio performance. These strategies are examined individually and in
combination to demonstrate how they can help investors maximize alpha and
minimize beta. Asset allocation is the process of dividing a portfolio among
different asset classes to achieve the desired level of risk and return.
Diversification involves spreading investments across different securities and
sectors to minimize the impact of individual security or sector-specific risks.
Active management involves security selection and risk management techniques to
generate excess returns while minimizing losses. Risk management strategies,
such as stop-loss orders and options strategies, aim to minimize losses in
adverse market conditions. The importance of combining these strategies for
optimizing portfolio performance is emphasized in this paper. The proper
implementation of these strategies can help investors achieve their investment
goals over the long-term, while minimizing exposure to risks. A call to action
for investors to utilize portfolio management strategies to maximize alpha and
minimize beta is also provided. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05900v1' target="_blank">http://arxiv.org/pdf/2304.05900v1</a><br> <br> <br> <font size='5'> 189 </font> <div style="text-align: right"> 2023-03-31 19:00:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Meta-Summary of Challenges in Building Products with ML Components -- Collecting Experiences from 4758+ Practitioners</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Incorporating machine learning (ML) components into software products raises
new software-engineering challenges and exacerbates existing challenges. Many
researchers have invested significant effort in understanding the challenges of
industry practitioners working on building products with ML components, through
interviews and surveys with practitioners. With the intention to aggregate and
present their collective findings, we conduct a meta-summary study: We collect
50 relevant papers that together interacted with over 4758 practitioners using
guidelines for systematic literature reviews. We then collected, grouped, and
organized the over 500 mentions of challenges within those papers. We highlight
the most commonly reported challenges and hope this meta-summary will be a
useful resource for the research community to prioritize research and education
in this field. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00078v1' target="_blank">http://arxiv.org/pdf/2304.00078v1</a><br> <br> <br> <font size='5'> 190 </font> <div style="text-align: right"> 2023-03-31 17:33:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Choose Your Weapon: Survival Strategies for Depressed AI Academics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Are you an AI researcher at an academic institution? Are you anxious you are
not coping with the current pace of AI advancements? Do you feel you have no
(or very limited) access to the computational and human resources required for
an AI research breakthrough? You are not alone; we feel the same way. A growing
number of AI academics can no longer find the means and resources to compete at
a global scale. This is a somewhat recent phenomenon, but an accelerating one,
with private actors investing enormous compute resources into cutting edge AI
research. Here, we discuss what you can do to stay competitive while remaining
an academic. We also briefly discuss what universities and the private sector
could do improve the situation, if they are so inclined. This is not an
exhaustive list of strategies, and you may not agree with all of them, but it
serves to start a discussion. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06035v1' target="_blank">http://arxiv.org/pdf/2304.06035v1</a><br> <br> <br> <font size='5'> 191 </font> <div style="text-align: right"> 2023-03-31 15:54:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Nash equilibria for relative investors with (non)linear price impact</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the strategic interaction of $n$ investors who are able to
influence a stock price process and at the same time measure their utilities
relative to the other investors. Our main aim is to find Nash equilibrium
investment strategies in this setting in a financial market driven by a
Brownian motion and investigate the influence the price impact has on the
equilibrium. We consider both CRRA and CARA utility functions. Our findings
show that the problem is well-posed as long as the price impact is at most
linear. Moreover, numerical results reveal that the investors behave very
aggressively when the price impact is beyond a critical parameter. </font><br> Link: <a href='http://arxiv.org/pdf/2303.18161v1' target="_blank">http://arxiv.org/pdf/2303.18161v1</a><br> <br> <br> <font size='5'> 192 </font> <div style="text-align: right"> 2023-03-30 09:34:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hot QCD White Paper</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Hot QCD physics studies the nuclear strong force under extreme temperature
and densities. Experimentally these conditions are achieved via high-energy
collisions of heavy ions at the Relativistic Heavy Ion Collider (RHIC) and the
Large Hadron Collider (LHC). In the past decade, a unique and substantial suite
of data was collected at RHIC and the LHC, probing hydrodynamics at the nucleon
scale, the temperature dependence of the transport properties of quark-gluon
plasma, the phase diagram of nuclear matter, the interaction of quarks and
gluons at different scales and much more. This document, as part of the 2023
nuclear science long range planning process, was written to review the progress
in hot QCD since the 2015 Long Range Plan for Nuclear Science, as well as
highlight the realization of previous recommendations, and present
opportunities for the next decade, building on the accomplishments and
investments made in theoretical developments and the construction of new
detectors. Furthermore, this document provides additional context to support
the recommendations voted on at the Joint Hot and Cold QCD Town Hall Meeting,
which are reported in a separate document. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17254v1' target="_blank">http://arxiv.org/pdf/2303.17254v1</a><br> <br> <br> <font size='5'> 193 </font> <div style="text-align: right"> 2023-03-29 09:46:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adjust factor with volatility model using MAXFLAT low-pass filter and construct portfolio in China A share market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the field of quantitative finance, volatility models, such as ARCH, GARCH,
FIGARCH, SV, EWMA, play the key role in risk and portfolio management.
Meanwhile, factor investing is more and more famous since mid of 20 century.
CAPM, Fama French three factor model, Fama French five-factor model, MSCI Barra
factor model are mentioned and developed during this period. In this paper, we
will show why we need adjust group of factors by our MAXFLAT low-pass
volatility model. All of our experiments are under China's CSI 300 and CSI 500
universe which represent China's large cap stocks and mid-small cap stocks. Our
result shows adjust factors by MAXFLAT volatility model have better performance
in both large cap and small cap universe than original factors or other risk
adjust factors in China A share. Also the portfolio constructed by MAXFLAT risk
adjust factors have continuous excess return and lower beta compare with
benchmark index. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04676v2' target="_blank">http://arxiv.org/pdf/2304.04676v2</a><br> <br> <br> <font size='5'> 194 </font> <div style="text-align: right"> 2023-03-29 08:39:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It is a challenging problem to predict trends of futures prices with
traditional econometric models as one needs to consider not only futures'
historical data but also correlations among different futures. Spatial-temporal
graph neural networks (STGNNs) have great advantages in dealing with such kind
of spatial-temporal data. However, we cannot directly apply STGNNs to
high-frequency future data because future investors have to consider both the
long-term and short-term characteristics when doing decision-making. To capture
both the long-term and short-term features, we exploit more label information
by designing four heterogeneous tasks: price regression, price moving average
regression, price gap regression (within a short interval), and change-point
detection, which involve both long-term and short-term scenes. To make full use
of these labels, we train our model in a continual manner. Traditional
continual GNNs define the gradient of prices as the parameter important to
overcome catastrophic forgetting (CF). Unfortunately, the losses of the four
heterogeneous tasks lie in different spaces. Hence it is improper to calculate
the parameter importance with their losses. We propose to calculate parameter
importance with mutual information between original observations and the
extracted features. The empirical results based on 49 commodity futures
demonstrate that our model has higher prediction performance on capturing
long-term or short-term dynamic change. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16532v1' target="_blank">http://arxiv.org/pdf/2303.16532v1</a><br> <br> <br> <font size='5'> 195 </font> <div style="text-align: right"> 2023-03-28 03:47:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Habitat fragmentation affects climate adaptation in a forest herb</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Climate change and the resulting increased drought frequencies pose
considerable threats to forest herb populations, especially when compounded by
additional environmental challenges. Specifically, habitat fragmentation may
disrupt climate adaptation and cause shifts in mating systems. To examine this,
we conducted a garden experiment with Primula elatior offspring from 24
populations across a climate and landscape fragmentation gradient. We evaluated
vegetative, regulatory, and reproductive traits under different soil moisture
regimes, assessing local adaptation and phenotypic plasticity. We also
conducted a field study in 60 populations along the same gradient to examine
potential breakdown of reciprocal herkogamy. Our results showed an evolutionary
shift from drought avoidance in southern populations to drought tolerance in
northern populations for large, connected populations. However, fragmentation
disrupted climate clines and adaptive responses to drought in key traits
related to growth, biomass allocation and water regulation. Our findings also
indicate the beginning of an evolutionary breakdown in reciprocal herkogamy.
These disruptions resulted in significantly reduced flowering investment,
especially in southern fragmented populations. These findings provide new
evidence of how habitat fragmentation disrupts climate adaptation and drought
tolerance in Primula elatior, emphasizing the need to account for habitat
fragmentation in conservation strategies to preserve resilient forest herb
populations amidst global changes. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15712v3' target="_blank">http://arxiv.org/pdf/2303.15712v3</a><br> <br> <br> <font size='5'> 196 </font> <div style="text-align: right"> 2023-03-27 18:55:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is there a Moore's law for quantum computing?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is a common wisdom according to which many technologies can progress
according to some exponential law like the empirical Moore's law that was
validated for over half a century with the growth of transistors number in
chipsets. As a still in the making technology with a lot of potential promises,
quantum computing is supposed to follow the pack and grow inexorably to
maturity. The Holy Grail in that domain is a large quantum computer with
thousands of errors corrected logical qubits made themselves of thousands, if
not more, of physical qubits. These would enable molecular simulations as well
as factoring 2048 RSA bit keys among other use cases taken from the intractable
classical computing problems book. How far are we from this? Less than 15 years
according to many predictions. We will see in this paper that Moore's empirical
law cannot easily be translated to an equivalent in quantum computing. Qubits
have various figures of merit that won't progress magically thanks to some new
manufacturing technique capacity. However, some equivalents of Moore's law may
be at play inside and outside the quantum realm like with quantum computers
enabling technologies, cryogeny and control electronics. Algorithms, software
tools and engineering also play a key role as enablers of quantum computing
progress. While much of quantum computing future outcomes depends on qubit
fidelities, it is progressing rather slowly, particularly at scale. We will
finally see that other figures of merit will come into play and potentially
change the landscape like the quality of computed results and the energetics of
quantum computing. Although scientific and technological in nature, this
inventory has broad business implications, on investment, education and
cybersecurity related decision-making processes. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15547v1' target="_blank">http://arxiv.org/pdf/2303.15547v1</a><br> <br> <br> <font size='5'> 197 </font> <div style="text-align: right"> 2023-03-26 21:54:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The growth of market capitalisation and the number of altcoins
(cryptocurrencies other than Bitcoin) provide investment opportunities and
complicate the prediction of their price movements. A significant challenge in
this volatile and relatively immature market is the problem of predicting
cryptocurrency prices which needs to identify the factors influencing these
prices. The focus of this study is to investigate the factors influencing
altcoin prices, and these factors have been investigated from a causal analysis
perspective using Bayesian networks. In particular, studying the nature of
interactions between five leading altcoins, traditional financial assets
including gold, oil, and S\&P 500, and social media is the research question.
To provide an answer to the question, we create causal networks which are built
from the historic price data of five traditional financial assets, social media
data, and price data of altcoins. The ensuing networks are used for causal
reasoning and diagnosis, and the results indicate that social media (in
particular Twitter data in this study) is the most significant influencing
factor of the prices of altcoins. Furthermore, it is not possible to generalise
the coins' reactions against the changes in the factors. Consequently, the
coins need to be studied separately for a particular price movement
investigation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16148v1' target="_blank">http://arxiv.org/pdf/2303.16148v1</a><br> <br> <br> <font size='5'> 198 </font> <div style="text-align: right"> 2023-03-26 14:27:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interdisciplinary Papers Supported by Disciplinary Grants Garner Deep and Broad Scientific Impact</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Interdisciplinary research has emerged as a hotbed for innovation and
discoveries. The increasing dominance of grant-supported research, combined
with growing interest in funding interdisciplinary work, raises fundamental
questions on the role of interdisciplinary grants in supporting high-impact
interdisciplinary advances. Here we develop a measurement framework to quantify
the interdisciplinarity of both research grants and publications and apply it
to 350K grants from 164 funding agencies over 26 countries and 1.3M papers that
acknowledged the support of these grants from 1985 to 2009. Our analysis
uncovers two contradictory patterns. On the one hand, interdisciplinary grants
tend to produce interdisciplinary papers and interdisciplinary papers are
associated with high impact. On the other hand, compared to their disciplinary
counterparts, interdisciplinary grants produce much fewer papers and
interdisciplinary papers that they support have substantially reduced impact.
We show that the key to resolving this discrepancy lies in the power of
disciplinary grants: Highly interdisciplinary papers supported by deeply
disciplinary grants garner disproportionately high impacts from both core
disciplines and broader fields. Further, the broad and deep impacts of
disciplinary grants are not simply due to funding size, reception of ideas
within disciplinary boundaries, or collaborative formats. When it comes to
producing key interdisciplinary advances, disciplinary grants appear to do more
with less and seem especially powerful when paired with other similar
disciplinary grants. Amidst the rapid rise of support for interdisciplinary
work across the sciences, these results highlight the underexplored role of
disciplinary grants in driving crucial interdisciplinary advances, suggesting
that interdisciplinary research is a risky endeavor and requires deep
disciplinary expertise and investments. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14732v1' target="_blank">http://arxiv.org/pdf/2303.14732v1</a><br> <br> <br> <font size='5'> 199 </font> <div style="text-align: right"> 2023-03-25 18:42:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Elasticity of Quantitative Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: What is the price elasticity of demand for canonical portfolio choice methods
in financial economics? Twelve models from the literature exhibit strikingly
inelastic demand, in contrast to classical models. This is due to the
difficulty of trading against price changes in practice, and is consistent with
demand elasticity estimates. This provides a novel answer to the inelastic
markets hypothesis, raises important concerns for the use of strongly elastic
investors in theory models, and quantifies the difficulty of trading against
potential mispricing aside from the standard limits to arbitrage frictions.
Counterfactual experiments with these demand functions exhibit large and
persistent alpha. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14533v1' target="_blank">http://arxiv.org/pdf/2303.14533v1</a><br> <br> <br> <font size='5'> 200 </font> <div style="text-align: right"> 2023-03-25 16:45:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Specific investments under negotiated transfer pricing: effects of different surplus sharing parameters on managerial performance: An agent-based simulation with fuzzy Q-learning agents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper focuses on a decentralized profit-center firm that uses negotiated
transfer pricing as an instrument to coordinate the production process.
Moreover, the firm's headquarters gives its divisions full authority over
operating decisions and it is assumed that each division can additionally make
an upfront investment decision that enhances the value of internal trade. On
early works, the paper expands the number of divisions by one downstream
division and relaxes basic assumptions, such as the assumption of common
knowledge of rationality. Based on an agent-based simulation, it is examined
whether cognitively bounded individuals modeled by fuzzy Q-learning achieve the
same results as fully rational utility maximizers. In addition, the paper
investigates different constellations of bargaining power to see whether a
deviation from the recommended optimal bargaining power leads to a higher
managerial performance. The simulation results show that fuzzy Q-learning
agents perform at least as well or better than fully individual rational
utility maximizers. The study also indicates that, in scenarios with different
marginal costs of divisions, a deviation from the recommended optimal
distribution ratio of the bargaining power of divisions can lead to higher
investment levels and, thus, to an increase in the headquarters' profit. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14515v1' target="_blank">http://arxiv.org/pdf/2303.14515v1</a><br> <br> <br> <font size='5'> 201 </font> <div style="text-align: right"> 2023-03-25 05:22:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Integer linear programming supporting portfolio design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In large organisations and companies, making investment decisions is a
complex and challenging task. In the Australian Department of Defence
(Defence), the complexity is even higher because defence capabilities are
public goods and do not have a financial return \textit{per se}. In this work
we mathematically define Defence's investment portfolio problem as a Set-Union
Knapsack Problem (SUKP). We present a practical way to linearise the model as
an Integer Linear Programming (ILP) problem. This linear model was developed as
the optimisation engine of the New Investments to Risked Options (NITRO)
portfolio selection tool developed by the Defence Science \& Technology Group
(DSTG) for Defence force design activities in 2021. The model is implemented in
the Python package called PuLP which can call several linear solver's
Application Programming Interface (API), such as GLPK, COIN CLP/CBC, IBM CPLEX,
and Gurobi. After comparing the performance of several solvers, we chose Gurobi
in the production server. The implementation of the new model and solver
enables the rapid execution of exact solutions to the Defence investment
portfolio problem. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14364v1' target="_blank">http://arxiv.org/pdf/2303.14364v1</a><br> <br> <br> <font size='5'> 202 </font> <div style="text-align: right"> 2023-03-24 20:19:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Dark Side of Algorithms? The Effect of Recommender Systems on Online Investor Behaviors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite the widespread adoption of recommender systems by online investment
platforms, empirical research into their impact on online investors' behaviors
is scarce. Using data from a global e-commerce platform, the authors of this
study adopt a regression discontinuity design to causally examine the effects
of recommender systems on online investor behaviors, specifically in a mutual
fund investment context. The results show that funds featured by recommender
systems prompt significantly more purchases. This effect is especially salient
among unsophisticated investors, who appear more likely to follow
system-provided recommendations. Further analysis also reveals that these
investors tend to suffer significantly worse investment performance after
purchasing the recommended funds. Thus, recommender systems threaten to amplify
wealth inequality among investors in financial markets. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14263v1' target="_blank">http://arxiv.org/pdf/2303.14263v1</a><br> <br> <br> <font size='5'> 203 </font> <div style="text-align: right"> 2023-03-23 16:25:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Return on Investment Driven Observability</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Observability, in cloud native systems, is the capability to continuously
generate and discover actionable insights, based on signals from the system
under observation. How do you know what insights are the most useful ones? What
signals should you be using to generate insights? This article discusses
challenges arising when rolling out observability in organizations and how you
can, based on Return on Investment (RoI) analysis, address said challenges. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13402v1' target="_blank">http://arxiv.org/pdf/2303.13402v1</a><br> <br> <br> <font size='5'> 204 </font> <div style="text-align: right"> 2023-03-23 12:59:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Black Holes Up Close</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent developments have ushered in a new era in the field of black hole
astrophysics, providing our first direct view of the remarkable environment
near black hole event horizons. These observations have enabled astronomers to
confirm long-standing ideas on the physics of gas flowing into black holes with
temperatures that are hundreds of times greater than at the center of the Sun.
At the same time, the observations have conclusively shown that light rays near
a black hole experience large deflections which cause a dark shadow in the
center of the image, an effect predicted by Einstein's theory of General
Relativity. With further investment, this field is poised to deliver decades of
advances in our understanding of gravity and black holes through new and
stringent tests of General Relativity, as well as new insights into the role of
black holes as the central engines powering a wide range of astronomical
phenomena. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13229v1' target="_blank">http://arxiv.org/pdf/2303.13229v1</a><br> <br> <br> <font size='5'> 205 </font> <div style="text-align: right"> 2023-03-23 12:27:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Today, many systems use artificial intelligence (AI) to solve complex
problems. While this often increases system effectiveness, developing a
production-ready AI-based system is a difficult task. Thus, solid AI
engineering practices are required to ensure the quality of the resulting
system and to improve the development process. While several practices have
already been proposed for the development of AI-based systems, detailed
practical experiences of applying these practices are rare.
  In this paper, we aim to address this gap by collecting such experiences
during a case study, namely the development of an autonomous stock trading
system that uses machine learning functionality to invest in stocks. We
selected 10 AI engineering practices from the literature and systematically
applied them during development, with the goal to collect evidence about their
applicability and effectiveness. Using structured field notes, we documented
our experiences. Furthermore, we also used field notes to document challenges
that occurred during the development, and the solutions we applied to overcome
them. Afterwards, we analyzed the collected field notes, and evaluated how each
practice improved the development. Lastly, we compared our evidence with
existing literature.
  Most applied practices improved our system, albeit to varying extent, and we
were able to overcome all major challenges. The qualitative results provide
detailed accounts about 10 AI engineering practices, as well as challenges and
solutions associated with such a project. Our experiences therefore enrich the
emerging body of evidence in this field, which may be especially helpful for
practitioner teams new to AI engineering. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13216v1' target="_blank">http://arxiv.org/pdf/2303.13216v1</a><br> <br> <br> <font size='5'> 206 </font> <div style="text-align: right"> 2023-03-22 22:38:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Wireless Network Demands of Data Products from Small Uncrewed Aerial Systems at Hurricane Ian</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Data collected at Hurricane Ian (2022) quantifies the demands that small
uncrewed aerial systems (UAS), or drones, place on the network communication
infrastructure and identifies gaps in the field. Drones have been increasingly
used since Hurricane Katrina (2005) for disaster response, however getting the
data from the drone to the appropriate decision makers throughout incident
command in a timely fashion has been problematic. These delays have persisted
even as countries such as the USA have made significant investments in wireless
infrastructure, rapidly deployable nodes, and an increase in commercial
satellite solutions. Hurricane Ian serves as a case study of the mismatch
between communications needs and capabilities. In the first four days of the
response, nine drone teams flew 34 missions under the direction of the State of
Florida FL-UAS1, generating 636GB of data. The teams had access to six
different wireless communications networks but had to resort to physically
transferring data to the nearest intact emergency operations center in order to
make the data available to the relevant agencies. The analysis of the mismatch
contributes a model of the drone data-to-decision workflow in a disaster and
quantifies wireless network communication requirements throughout the workflow
in five factors. Four of the factors-availability, bandwidth, burstiness, and
spatial distribution-were previously identified from analyses of Hurricanes
Harvey (2017) and Michael (2018). This work adds upload rate as a fifth
attribute. The analysis is expected to improve drone design and edge computing
schemes as well as inform wireless communication research and development. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12937v1' target="_blank">http://arxiv.org/pdf/2303.12937v1</a><br> <br> <br> <font size='5'> 207 </font> <div style="text-align: right"> 2023-03-22 05:55:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Lessons learned to boost a bioinformatics knowledge base reusability, the Bgee experience</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background, enhancing interoperability of bioinformatics knowledge bases is a
high priority requirement to maximize data reusability, and thus increase their
utility such as the return on investment for biomedical research. A knowledge
base may provide useful information for life scientists and other knowledge
bases, but it only acquires exchange value once the knowledge base is (re)used,
and without interoperability the utility lies dormant. Results, in this
article, we discuss several approaches to boost interoperability depending on
the interoperable parts. The findings are driven by several real-world scenario
examples that were mostly implemented by Bgee, a well-established gene
expression database. To better justify the findings are transferable, for each
Bgee interoperability experience, we also highlight similar implementations by
major bioinformatics knowledge bases. Moreover, we discuss ten general main
lessons learnt. These lessons can be applied in the context of any
bioinformatics knowledge base to foster data reusability. Conclusions, this
work provides pragmatic methods and transferable skills to promote reusability
of bioinformatics knowledge bases by focusing on interoperability. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12329v2' target="_blank">http://arxiv.org/pdf/2303.12329v2</a><br> <br> <br> <font size='5'> 208 </font> <div style="text-align: right"> 2023-03-21 18:45:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Language Models gained significant attention in late 2022 / early
2023, notably with the introduction of models refined to act consistently with
users' expectations of interactions with AI (conversational models). Arguably
the focal point of public attention has been such a refinement of the GPT3
model -- the ChatGPT and its subsequent integration with auxiliary
capabilities, including search as part of Microsoft Bing. Despite extensive
prior research invested in their development, their performance and
applicability to a range of daily tasks remained unclear and niche. However,
their wider utilization without a requirement for technical expertise, made in
large part possible through conversational fine-tuning, revealed the extent of
their true capabilities in a real-world environment. This has garnered both
public excitement for their potential applications and concerns about their
capabilities and potential malicious uses. This review aims to provide a brief
overview of the history, state of the art, and implications of Generative
Language Models in terms of their principles, abilities, limitations, and
future prospects -- especially in the context of cyber-defense, with a focus on
the Swiss operational environment. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12132v1' target="_blank">http://arxiv.org/pdf/2303.12132v1</a><br> <br> <br> <font size='5'> 209 </font> <div style="text-align: right"> 2023-03-21 12:40:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Non-Market Allocation Mechanisms: Optimal Design and Investment Incentives</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study how to optimally design selection mechanisms, accounting for agents'
investment incentives. A principal wishes to allocate a resource of homogeneous
quality to a heterogeneous population of agents. The principal commits to a
possibly random selection rule that depends on a one-dimensional characteristic
of the agents she intrinsically values. Agents have a strict preference for
being selected by the principal and may undertake a costly investment to
improve their characteristic before it is revealed to the principal. We show
that even if random selection rules foster agents' investments, especially at
the top of the characteristic distribution, deterministic "pass-fail" selection
rules are in fact optimal. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11805v1' target="_blank">http://arxiv.org/pdf/2303.11805v1</a><br> <br> <br> <font size='5'> 210 </font> <div style="text-align: right"> 2023-03-20 16:02:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Report of the US ITER Research Program Research Needs Workshop</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The US ITER Research Program Basic Research Needs Workshop, held over the
course of several months in 2022 with over 400 participants, sought to identify
steps to be taken to both maximize the return of the US investment in ITER
construction and operation and to ensure US research priorities on ITER
strengthen the domestic program aimed at the development of a fusion pilot
plant (FPP). </font><br> Link: <a href='http://arxiv.org/pdf/2303.12094v2' target="_blank">http://arxiv.org/pdf/2303.12094v2</a><br> <br> <br> <font size='5'> 211 </font> <div style="text-align: right"> 2023-03-20 14:01:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Sustainable DevOps: A Decision Making Framework</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In software industry, the DevOps is an increasingly adopting software
development paradigm. Towards the sustainable DevOps adoption, there is a need
to transform the organization Culture, Automation, Measurement and Sharing
(CAMS) aspects concerning to core theme of continues development and
operations. The software organizations face several complexities while
implementing the DevOps principles. The sustainable DevOps implementation
assist the software organizations to develop the quality projects with good
return on investment. This evidence-based study aims to explore the guidelines
of sustainable DevOps implementation, reported in literature and industry
practices. Using systematic literature review and questionnaire survey, we
identified the 48 guidelines for sustainable DevOps implementation. We further
develop a decision-making framework aiming to assist the practitioners to
consider the most significant set of guidelines on priority. The results show
that out of CAMS, culture is the most important principle for sustainable
DevOps implementation. Moreover, (i) enterprises should focus on building a
collaborative culture with shared goals, (ii) assess your organization
readiness to utilize a microservices architecture and (iii) educate executives
at your company about the benefits of DevOps to gain resource and budget
support are the highest priority guidelines for sustainable DevOps
implementation. We believe that this in-depth study helps the practitioners to
understand the core principles and guidelines for sustainable DevOps
implementation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11121v1' target="_blank">http://arxiv.org/pdf/2303.11121v1</a><br> <br> <br> <font size='5'> 212 </font> <div style="text-align: right"> 2023-03-20 13:01:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI and the Digital Commons</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many generative foundation models (or GFMs) are trained on publicly available
data and use public infrastructure, but 1) may degrade the "digital commons"
that they depend on, and 2) do not have processes in place to return value
captured to data producers and stakeholders. Existing conceptions of data
rights and protection (focusing largely on individually-owned data and
associated privacy concerns) and copyright or licensing-based models offer some
instructive priors, but are ill-suited for the issues that may arise from
models trained on commons-based data. We outline the risks posed by GFMs and
why they are relevant to the digital commons, and propose numerous
governance-based solutions that include investments in standardized
dataset/model disclosure and other kinds of transparency when it comes to
generative models' training and capabilities, consortia-based funding for
monitoring/standards/auditing organizations, requirements or norms for GFM
companies to contribute high quality data to the commons, and structures for
shared ownership based on individual or community provision of fine-tuning
data. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11074v1' target="_blank">http://arxiv.org/pdf/2303.11074v1</a><br> <br> <br> <font size='5'> 213 </font> <div style="text-align: right"> 2023-03-20 07:04:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Financial Structure, Firm Size and Financial Growth of Non-Financial Firms Listed at the Nairobi Securities Exchange</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A significant number of the non-financial firms listed at the Nairobi
Securities Exchange have been experiencing declining financial performance and
financial growth, which deter investors from investing in such firms. Hence,
the study aimed at establishing the effect of financial structure on the
financial growth of non-financial firms listed at the Nairobi Securities
Exchange. </font><br> Link: <a href='http://arxiv.org/pdf/2303.10910v1' target="_blank">http://arxiv.org/pdf/2303.10910v1</a><br> <br> <br> <font size='5'> 214 </font> <div style="text-align: right"> 2023-03-17 12:27:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Integrated investment, retrofit and abandonment planning of energy systems with short-term and long-term uncertainty using enhanced Benders decomposition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose the REORIENT (REnewable resOuRce Investment for the ENergy
Transition) model for energy systems planning with the following novelties: (1)
integrating capacity expansion, retrofit and abandonment planning, and (2)
using multi-horizon stochastic mixed-integer linear programming with short-term
and long-term uncertainty. We apply the model to the European energy system
considering: (a) investment in new hydrogen infrastructures, (b) capacity
expansion of the European power system, (c) retrofitting oil and gas
infrastructures in the North Sea region for hydrogen production and
distribution, and abandoning existing infrastructures, and (d) long-term
uncertainty in oil and gas prices and short-term uncertainty in time series
parameters. We utilise the special structure of multi-horizon stochastic
programming and propose an enhanced Benders decomposition to solve the model
efficiently. We first conduct a sensitivity analysis on retrofitting costs of
oil and gas infrastructures. We then compare the REORIENT model with a
conventional investment planning model regarding costs and investment
decisions. Finally, the computational performance of the algorithm is
presented. The results show that: (1) when the retrofitting cost is below 20%
of the cost of building new ones, retrofitting is economical for most of the
existing pipelines, (2) platform clusters keep producing oil due to the massive
profit, and the clusters are abandoned in the last investment stage, (3)
compared with a traditional investment planning model, the REORIENT model
yields 24% lower investment cost in the North Sea region, and (4) the enhanced
Benders algorithm is up to 6.8 times faster than the reference algorithm. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09927v1' target="_blank">http://arxiv.org/pdf/2303.09927v1</a><br> <br> <br> <font size='5'> 215 </font> <div style="text-align: right"> 2023-03-16 09:38:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Real Options Technique as a Tool of Strategic Risk Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The real options approach is now considered an effective alternative to the
corporate DCF model for a feasibility study. The current paper offers a
practical methodology employing binomial trees and real options techniques for
evaluating investment projects. A general computation procedure is suggested
for the decision tree with two active stages of real options, which correspond
to additional investments. The suggested technique can be used for most real
options, which are practically essential regarding enterprise strategy. The
special case named Binomial-Random-Cash-Flow Real Options Model with random
outcomes is developed as the next step of real options modelling. Project Value
at Risk is introduced and used as a criterion of investment project feasibility
under the assumption regarding random outcomes. In particular, the Gaussian
probability distribution is used for modelling option outcomes uncertainty. The
choice of the Gaussian distribution is caused by the desire to obtain estimates
in the final analytical form. Choosing another distribution for random outcomes
leads to using Monte Carlo simulation, for which a general framework is
developed by demonstrating some instances. The author could avoid the
computational complexity that makes these solutions feasible for business
practice. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09176v1' target="_blank">http://arxiv.org/pdf/2303.09176v1</a><br> <br> <br> <font size='5'> 216 </font> <div style="text-align: right"> 2023-03-16 00:00:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion Problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Effective investment planning decisions are crucial to ensure cyber-physical
infrastructures satisfy performance requirements over an extended time horizon.
Computing these decisions often requires solving Capacity Expansion Problems
(CEPs). In the context of regional-scale energy systems, these problems are
prohibitively expensive to solve due to large network sizes, heterogeneous node
characteristics, and a large number of operational periods. To maintain
tractability, traditional approaches aggregate network nodes and/or select a
set of representative time periods. Often, these reductions do not capture
supply-demand variations that crucially impact CEP costs and constraints,
leading to suboptimal decisions. Here, we propose a novel graph convolutional
autoencoder approach for spatio-temporal aggregation of a generic CEP with
heterogeneous nodes (CEPHN). Our architecture leverages graph pooling to
identify nodes with similar characteristics and minimizes a multi-objective
loss function. This loss function is tailored to induce desirable spatial and
temporal aggregations with regard to tractability and optimality. In
particular, the output of the graph pooling provides a spatial aggregation
while clustering the low-dimensional encoded representations yields a temporal
aggregation. We apply our approach to generation expansion planning of a
coupled 88-node power and natural gas system in New England. The resulting
aggregation leads to a simpler CEPHN with 6 nodes and a small set of
representative days selected from one year. We evaluate aggregation outcomes
over a range of hyperparameters governing the loss function and compare
resulting upper bounds on the original problem with those obtained using
benchmark methods. We show that our approach provides upper bounds that are 33%
(resp. 10%) lower those than obtained from benchmark spatial (resp. temporal)
aggregation approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08996v2' target="_blank">http://arxiv.org/pdf/2303.08996v2</a><br> <br> <br> <font size='5'> 217 </font> <div style="text-align: right"> 2023-03-15 22:37:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A parsimonious neural network approach to solve portfolio optimization problems without using dynamic programming</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present a parsimonious neural network approach, which does not rely on
dynamic programming techniques, to solve dynamic portfolio optimization
problems subject to multiple investment constraints. The number of parameters
of the (potentially deep) neural network remains independent of the number of
portfolio rebalancing events, and in contrast to, for example, reinforcement
learning, the approach avoids the computation of high-dimensional conditional
expectations. As a result, the approach remains practical even when considering
large numbers of underlying assets, long investment time horizons or very
frequent rebalancing events. We prove convergence of the numerical solution to
the theoretical optimal solution of a large class of problems under fairly
general conditions, and present ground truth analyses for a number of popular
formulations, including mean-variance and mean-conditional value-at-risk
problems. We also show that it is feasible to solve Sortino ratio-inspired
objectives (penalizing only the variance of wealth outcomes below the mean) in
dynamic trading settings with the proposed approach. Using numerical
experiments, we demonstrate that if the investment objective functional is
separable in the sense of dynamic programming, the correct time-consistent
optimal investment strategy is recovered, otherwise we obtain the correct
pre-commitment (time-inconsistent) investment strategy. The proposed approach
remains agnostic as to the underlying data generating assumptions, and results
are illustrated using (i) parametric models for underlying asset returns, (ii)
stationary block bootstrap resampling of empirical returns, and (iii)
generative adversarial network (GAN)-generated synthetic asset returns. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08968v1' target="_blank">http://arxiv.org/pdf/2303.08968v1</a><br> <br> <br> <font size='5'> 218 </font> <div style="text-align: right"> 2023-03-15 11:03:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment in ambiguous financial markets with learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the classical multi-asset Merton investment problem under drift
uncertainty, i.e. the asset price dynamics are given by geometric Brownian
motions with constant but unknown drift coefficients. The investor assumes a
prior drift distribution and is able to learn by observing the asset prize
realizations during the investment horizon. While the solution of an expected
utility maximizing investor with constant relative risk aversion (CRRA) is well
known, we consider the optimization problem under risk and ambiguity
preferences by means of the KMM (Klibanoff et al. (2005)) approach. Here, the
investor maximizes a double certainty equivalent. The inner certainty
equivalent is for given drift coefficient, the outer is based on a drift
distribution. Assuming also a CRRA type ambiguity function, it turns out that
the optimal strategy can be stated in terms of the solution without ambiguity
preferences but an adjusted drift distribution. To the best of our knowledge an
explicit solution method in this setting is new. We rely on some duality
theorems to prove our statements.
  Based on our theoretical results, we are able to shed light on the impact of
the prior drift distribution as well as the consequences of ambiguity
preferences via the transfer to an adjusted drift distribution, i.e. we are
able to explain the interaction of risk and ambiguity preferences. We compare
our results with the ones in a pre-commitment setup where the investor is
restricted to deterministic strategies. It turns out that (under risk and
ambiguity aversion) an infinite investment horizon implies in both cases a
maximin decision rule, i.e. the investor follows the worst (best) Merton
fraction (over all realizations of it) if she is more (less) risk averse than a
log-investor. We illustrate our findings with an extensive numerical study. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08521v1' target="_blank">http://arxiv.org/pdf/2303.08521v1</a><br> <br> <br> <font size='5'> 219 </font> <div style="text-align: right"> 2023-03-15 09:06:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment in Defined Contribution Pension Schemes with Forward Utility Preferences</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Optimal investment strategies of an individual worker during the accumulation
phase in the defined contribution pension scheme have been well studied in the
literature. Most of them adopted the classical backward model and approach, but
any pre-specifications of retirement time, preferences, and market environment
models do not often hold in such a prolonged horizon of the pension scheme.
Pre-commitment to ensure the time-consistency of an optimal investment strategy
derived from the backward model and approach leads the supposedly optimal
strategy to be sub-optimal in the actual realizations. This paper revisits the
optimal investment problem for the worker during the accumulation phase in the
defined contribution pension scheme, via the forward preferences which resolve
the pre-specification issues in the backward model and approach. Stochastic
partial differential equation representation for the worker's forward
preferences is illustrated. This paper constructs two of the forward utility
preferences and solves the corresponding optimal investment strategies, in the
cases of initial power and exponential utility functions. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08462v1' target="_blank">http://arxiv.org/pdf/2303.08462v1</a><br> <br> <br> <font size='5'> 220 </font> <div style="text-align: right"> 2023-03-14 06:58:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Code Will Tell: Visual Identification of Ponzi Schemes on Ethereum</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Ethereum has become a popular blockchain with smart contracts for investors
nowadays. Due to the decentralization and anonymity of Ethereum, Ponzi schemes
have been easily deployed and caused significant losses to investors. However,
there are still no explainable and effective methods to help investors easily
identify Ponzi schemes and validate whether a smart contract is actually a
Ponzi scheme. To fill the research gap, we propose PonziLens, a novel
visualization approach to help investors achieve early identification of Ponzi
schemes by investigating the operation codes of smart contracts. Specifically,
we conduct symbolic execution of opcode and extract the control flow for
investing and rewarding with critical opcode instructions. Then, an intuitive
directed-graph based visualization is proposed to display the investing and
rewarding flows and the crucial execution paths, enabling easy identification
of Ponzi schemes on Ethereum. Two usage scenarios involving both Ponzi and
non-Ponzi schemes demonstrate the effectiveness of PonziLens. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07657v1' target="_blank">http://arxiv.org/pdf/2303.07657v1</a><br> <br> <br> <font size='5'> 221 </font> <div style="text-align: right"> 2023-03-13 16:15:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Stock Price Relationship between Holding Companies and Subsidiaries: A Case study of Indonesia Multiholding Companies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aimed to examine the correlation between the stock prices of two
major Indonesian holding companies, MNC Group and Elang Mahkota Teknologi
(Emtek) Group, and their respective subsidiaries as case studies. The data for
the analysis were collected from 2013 to 2022, and Spearman correlation was
used to determine the strength and direction of the relationship between the
stock prices of the holding companies and their subsidiaries. The results of
the analysis revealed that there were varying degrees of correlation between
the stock prices of the holding companies and their subsidiaries. The strongest
positive correlation was observed between BHIT and BMTR, while the weakest
correlations were found between BHIT and IPTV, and BHIT and MSIN. The
correlations were also found to have changed over time, possibly due to market
conditions, company-specific events, or changes in industry sectors.In the case
of Emtek Group, the analysis suggested that EMTK's stock price movements had a
significant impact on the stock prices of its subsidiaries, with varying
strengths of relationships. The negative correlation between EMTK and SCMA over
the entire period suggested an inverse relationship, while positive
correlations with BUKA, AMOR, BBHI, and RSGK indicated a tendency to move in
the same direction as EMTK's stock price. The correlations were found to have
increased over time, possibly due to market conditions and EMTK's ownership
stake in these companies. Overall, the findings of this study suggest that
there is a complex interplay between the stock prices of parent companies and
their subsidiaries, and that there are a variety of factors that can influence
these relationships over time. These findings may be useful for investors in
making informed decisions about their investment portfolios, as changes in the
correlations could impact their portfolio's performance. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07244v1' target="_blank">http://arxiv.org/pdf/2303.07244v1</a><br> <br> <br> <font size='5'> 222 </font> <div style="text-align: right"> 2023-03-12 16:33:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Composite Sorting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a new tractable general framework for sorting - composite sorting.
Composite sorting comprises of (1) distinct workers being assigned to the same
job and (2) a given worker type simultaneously being part of both positive and
negative sorting. We show that composite sorting generically arises in a class
of sorting models when fixed investments can mitigate the variable costs of
mismatch. We develop a complete characterization of equilibrium sorting as well
as the corresponding equilibrium wages. Wages exhibit a local hierarchical
structure meaning the relative wages depend solely on sorting patterns within
narrow skill groups. Using this framework, we study within-job wage dispersion
and demonstrate that quantitatively composite sorting may explain a sizable
portion of wage dispersion within occupations in the United States. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06701v1' target="_blank">http://arxiv.org/pdf/2303.06701v1</a><br> <br> <br> <font size='5'> 223 </font> <div style="text-align: right"> 2023-03-12 01:32:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantifying Technical Debt: A Systematic Mapping Study and a Conceptual Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: To effectively manage Technical Debt (TD), we need reliable means to quantify
it. We conducted a Systematic Mapping Study (SMS) where we identified TD
quantification approaches that focus on different aspects of TD. Some
approaches base the quantification on the identification of smells, some
quantify the Return on Investment (ROI) of refactoring, some compare an ideal
state with the current state of a software in terms of the software quality,
and some compare alternative development paths to reduce TD. It is unclear if
these approaches are quantifying the same thing and if they support similar or
different decisions regarding TD Management (TDM). This creates the problem of
not being able to effectively compare and evaluate approaches. To solve this
problem, we developed a novel conceptual model, the Technical Debt
Quantification Model (TDQM), that captures the important concepts related to TD
quantification and illustrates the relationships between them. TDQM can
represent varied TD quantification approaches via a common uniform
representation, the TDQM Approach Comparison Matrix, that allows performing
useful comparisons and evaluations between approaches. This paper reports on
the mapping study, the development of TDQM, and on applying TDQM to compare and
evaluate TD quantification approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06535v1' target="_blank">http://arxiv.org/pdf/2303.06535v1</a><br> <br> <br> <font size='5'> 224 </font> <div style="text-align: right"> 2023-03-11 19:03:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Knowledge Distillation for Efficient Sequences of Training Runs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In many practical scenarios -- like hyperparameter search or continual
retraining with new data -- related training runs are performed many times in
sequence. Current practice is to train each of these models independently from
scratch. We study the problem of exploiting the computation invested in
previous runs to reduce the cost of future runs using knowledge distillation
(KD). We find that augmenting future runs with KD from previous runs
dramatically reduces the time necessary to train these models, even taking into
account the overhead of KD. We improve on these results with two strategies
that reduce the overhead of KD by 80-90% with minimal effect on accuracy and
vast pareto-improvements in overall cost. We conclude that KD is a promising
avenue for reducing the cost of the expensive preparatory work that precedes
training final models in practice. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06480v1' target="_blank">http://arxiv.org/pdf/2303.06480v1</a><br> <br> <br> <font size='5'> 225 </font> <div style="text-align: right"> 2023-03-11 09:25:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: PowerMat: context-aware recommender system without user item rating values that solves the cold-start problem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recommender systems serves as an important technical asset in many modern
companies. With the increasing demand for higher precision of the technology,
more and more research and investment has been allocated to the field. One
important sub-field of recommender systems that has been stagnating is
context-aware recommender systems. Due to the difficulty of collecting input
dataset, the amount of research on context-aware recommender systems is much
less than other sub-fields of recommender systems. In this paper, we propose a
new algorithm named PowerMat to tackle the context-aware recommendation
problem. We build our theory on matrix factorization and Zipf's law, and also
more recent research work such as DotMat. We prove by experiments that our
method achieves superior results to the classic matrix factorization algorithm
and other context-aware recommender systems such as MovieMat+. In addition, by
theoretical analysis, we show that our algorithm solves the cold-start problem
for context-aware recommendation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06356v1' target="_blank">http://arxiv.org/pdf/2303.06356v1</a><br> <br> <br> <font size='5'> 226 </font> <div style="text-align: right"> 2023-03-10 16:22:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine learning for sports betting: should predictive models be optimised for accuracy or calibration?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sports betting's recent federal legalisation in the USA coincides with the
golden age of machine learning. If bettors can leverage data to reliably
predict the probability of an outcome, they can recognise when the bookmaker's
odds are in their favour. As sports betting is a multi-billion dollar industry
in the USA alone, identifying such opportunities could be extremely lucrative.
Many researchers have applied machine learning to the sports outcome prediction
problem, generally using accuracy to evaluate the performance of predictive
models. We hypothesise that for the sports betting problem, model calibration
is more important than accuracy. To test this hypothesis, we train models on
NBA data over several seasons and run betting experiments on a single season,
using published odds. We show that optimising the predictive model for
calibration leads to greater returns than optimising for accuracy, on average
(return on investment of $+34.69\%$ versus $-35.17\%$) and in the best case
($+36.93\%$ versus $+5.56\%$). These findings suggest that for sports betting
(or any probabilistic decision-making problem), calibration is a more important
metric than accuracy. Sports bettors who wish to increase profits should
therefore optimise their predictive model for calibration. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06021v3' target="_blank">http://arxiv.org/pdf/2303.06021v3</a><br> <br> <br> <font size='5'> 227 </font> <div style="text-align: right"> 2023-03-10 15:03:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Sizing of Isolated Renewable Power Systems with Ammonia Synthesis: Model and Solution Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Isolated renewable power to ammonia (IRePtA) has been recognized as a
promising way to decarbonize the chemical industry. Optimal sizing of the
renewable power system is significant to improve the techno-economic of IRePtA
since the investment of power sources exceeds 80\% of the total investment.
However, multi-timescale electricity, hydrogen, and ammonia storages, minimum
power supply for system safety, and the multi-year uncertainty of renewable
generation lead to difficulties in planning. To address the issues above, an
IGDT-MILFP model is proposed. First, the levelized cost of ammonia (LCOA) is
directly formulated as the objective, rendering a mixed integer linear
fractional programming (MILFP) problem. Information gap decision theory (IGDT)
is utilized to handle the multi-year uncertainty of renewable generation.
Second, a combined Charnes-Cooper (C&C) transformation and Branch-and-Bound
(B&B) method is proposed to efficiently solve the large-scale IGDT-MILFP model,
giving robust and opportunistic planning results. Then, Markov Chain Monte
Carlo (MCMC) sampling-based posteriori analysis is leveraged to quantify the
long-run performance. Finally, a real-life system in Inner Mongolia, China, is
studied. The results indicate that the proposed methods could reduce the
computational burden by orders of magnitude for solving a large-scale MILFP
problem. Moreover, the proposed IGDT-MILFP model is necessary and accurate to
obtain an optimal capacity allocation with the lowest expected LCOA (3610
RMB/t) in long-run simulations. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05971v1' target="_blank">http://arxiv.org/pdf/2303.05971v1</a><br> <br> <br> <font size='5'> 228 </font> <div style="text-align: right"> 2023-03-09 12:19:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Real-time scheduling of renewable power systems through planning-based reinforcement learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The growing renewable energy sources have posed significant challenges to
traditional power scheduling. It is difficult for operators to obtain accurate
day-ahead forecasts of renewable generation, thereby requiring the future
scheduling system to make real-time scheduling decisions aligning with
ultra-short-term forecasts. Restricted by the computation speed, traditional
optimization-based methods can not solve this problem. Recent developments in
reinforcement learning (RL) have demonstrated the potential to solve this
challenge. However, the existing RL methods are inadequate in terms of
constraint complexity, algorithm performance, and environment fidelity. We are
the first to propose a systematic solution based on the state-of-the-art
reinforcement learning algorithm and the real power grid environment. The
proposed approach enables planning and finer time resolution adjustments of
power generators, including unit commitment and economic dispatch, thus
increasing the grid's ability to admit more renewable energy. The well-trained
scheduling agent significantly reduces renewable curtailment and load shedding,
which are issues arising from traditional scheduling's reliance on inaccurate
day-ahead forecasts. High-frequency control decisions exploit the existing
units' flexibility, reducing the power grid's dependence on hardware
transformations and saving investment and operating costs, as demonstrated in
experimental results. This research exhibits the potential of reinforcement
learning in promoting low-carbon and intelligent power systems and represents a
solid step toward sustainable electricity generation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05205v2' target="_blank">http://arxiv.org/pdf/2303.05205v2</a><br> <br> <br> <font size='5'> 229 </font> <div style="text-align: right"> 2023-03-09 03:07:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Parallel Computing Based Solution for Reliability-Constrained Distribution Network Planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The main goal of distribution network (DN) expansion planning is essentially
to achieve minimal investment constrained with specified reliability
requirements. The reliability-constrained distribution network planning (RcDNP)
problem can be cast an instance of mixed-integer linear programming (MILP)
which involves ultra-heavy computation burden especially for large scale DNs.
In this paper, we propose a parallel computing-based acceleration algorithm for
solve RcDNP problem. The RcDNP is decomposed into a backbone grid and several
lateral grid problems with coordination. Then a parallelizable augmented
Lagrangian algorithm with acceleration strategy is developed to solve the
coordination planning problems. In this method, the lateral grid problems are
solved in parallel through coordinating with the backbone grid planning
problem. To address the presence of nonconvexity, Gauss-Seidel iteration is
adopted on the convex hull of the feasible region constructed by the
decomposition method. Under mild conditions, the optimality and convergence of
this algorithm is proven. The numerical tests show the proposed method can
significantly reduce the computation time and make the RcDNP applicable for
real-world problems. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05005v1' target="_blank">http://arxiv.org/pdf/2303.05005v1</a><br> <br> <br> <font size='5'> 230 </font> <div style="text-align: right"> 2023-03-08 08:23:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Balancing DSO interests and PV system economics with alternative tariffs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Distributed rooftop photovoltaics (PV) is one of the pillars of the energy
transition. However, the massive integration of distributed PV systems
challenges the existing grid, with high amounts of PV injection possibly
leading to over-voltage and reverse power flow, with line and transformer
overloading, among other issues. Moreover, the increase in PV self-consumption
and consequently the reduction of imported electricity poses a problem in
recovering Transmission System Operators (TSOs) and Distribution System
Operators (DSOs) grid costs, that until now have been directly linked to the
amount of electricity consumed due to the volumetric nature of traditional
tariffs. To investigate whether alternative tariffs could mitigate PV impacts
at the distribution level without hampering PV development, we assess five
electricity tariffs that could help the DSOs to recover the costs of
maintaining the distribution grid. Additionally, we evaluate how such tariffs
may affect private investment in storage and their impact on three types of
low-voltage networks (i.e., urban, semi-urban, and rural). We found that
tariffs with a capacity-based component promote further adoption of PV and
storage. At the same time, they allow the DSOs to recover the grid cost without
incurring relevant economic differences for the customer. However, all assessed
tariffs were found to have a limited role in mitigating PV impacts at the
distribution level. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04433v1' target="_blank">http://arxiv.org/pdf/2303.04433v1</a><br> <br> <br> <font size='5'> 231 </font> <div style="text-align: right"> 2023-03-07 21:45:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Vulnerability Mimicking Mutants</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the increasing release of powerful language models trained on large code
corpus (e.g. CodeBERT was trained on 6.4 million programs), a new family of
mutation testing tools has arisen with the promise to generate more "natural"
mutants in the sense that the mutated code aims at following the implicit rules
and coding conventions typically produced by programmers. In this paper, we
study to what extent the mutants produced by language models can semantically
mimic the observable behavior of security-related vulnerabilities (a.k.a.
Vulnerability-mimicking Mutants), so that designing test cases that are failed
by these mutants will help in tackling mimicked vulnerabilities. Since
analyzing and running mutants is computationally expensive, it is important to
prioritize those mutants that are more likely to be vulnerability mimicking
prior to any analysis or test execution. Taking this into account, we introduce
VMMS, a machine learning based approach that automatically extracts the
features from mutants and predicts the ones that mimic vulnerabilities. We
conducted our experiments on a dataset of 45 vulnerabilities and found that
16.6% of the mutants fail one or more tests that are failed by 88.9% of the
respective vulnerabilities. More precisely, 3.9% of the mutants from the entire
mutant set are vulnerability-mimicking mutants that mimic 55.6% of the
vulnerabilities. Despite the scarcity, VMMS predicts vulnerability-mimicking
mutants with 0.63 MCC, 0.80 Precision, and 0.51 Recall, demonstrating that the
features of vulnerability-mimicking mutants can be automatically learned by
machine learning models to statically predict these without the need of
investing effort in defining such features. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04247v1' target="_blank">http://arxiv.org/pdf/2303.04247v1</a><br> <br> <br> <font size='5'> 232 </font> <div style="text-align: right"> 2023-03-07 20:59:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment with insurable background risk and nonlinear portfolio allocation frictions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study investment and insurance demand decisions for an agent in a
theoretical continuous-time expected utility maximization model that combines
risky assets with an (exogenous) insurable background risk. This risk takes the
form of a jump-diffusion process with negative jumps in the return rate of the
(self-financed) wealth. The main distinctive feature of our model is that the
agent's decision on portfolio choice and insurance demand causes nonlinear
friction in the dynamics of the wealth process. We use the dynamic programming
approach to find optimality conditions under which the agent assumes the
insurable risk entirely, or partially, or purchases total insurance against it.
In particular, we consider differential and piece-wise linear portfolio
allocation frictions, with differential borrowing and lending rates as our most
emblematic example. Finally, we present a mutual-fund separation result and
illustrate our results with several numerical examples when the adverse jump
risk has Beta distribution. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04236v1' target="_blank">http://arxiv.org/pdf/2303.04236v1</a><br> <br> <br> <font size='5'> 233 </font> <div style="text-align: right"> 2023-03-07 15:35:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Decomposition Methods for Dynamically Monotone Two-Time-Scale Stochastic Optimization Problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In energy management, it is common that strategic investment decisions
(storage capacity, production units) are made at a slow time scale, whereas
operational decisions (storage, production) are made at a fast time scale: for
such problems, the total number of decision stages may be huge. In this paper,
we consider multistage stochastic optimization problems with two time-scales,
and we propose a time block decomposition scheme to address them numerically.
More precisely, our approach relies on two assumptions. On the one hand, we
suppose slow time scale stagewise independence of the noise process: the random
variables that occur during a slow time scale interval are independent of those
at another slow time scale interval. This makes it possible to use Dynamic
Programming at the slow time scale. On the other hand, we suppose a dynamically
monotone property for the problem under consideration, which makes it possible
to obtain bounds. Then, we present two algorithmic methods to compute upper and
lower bounds for slow time scale Bellman value functions. Both methods rely
respectively on primal and dual decomposition of the Bellman equation applied
at the slow time scale. We assess the methods tractability and validate their
efficiency by solving a battery management problem where the fast time scale
operational decisions have an impact on the storage current capacity, hence on
the strategic decisions to renew the battery at the slow time scale. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03985v1' target="_blank">http://arxiv.org/pdf/2303.03985v1</a><br> <br> <br> <font size='5'> 234 </font> <div style="text-align: right"> 2023-03-07 06:33:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Training Machine Learning Models to Characterize Temporal Evolution of Disadvantaged Communities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Disadvantaged communities (DAC), as defined by the Justice40 initiative of
the Department of Energy (DOE), USA, identifies census tracts across the USA to
determine where benefits of climate and energy investments are or are not
currently accruing. The DAC status not only helps in determining the
eligibility for future Justice40-related investments but is also critical for
exploring ways to achieve equitable distribution of resources. However,
designing inclusive and equitable strategies not just requires a good
understanding of current demographics, but also a deeper analysis of the
transformations that happened in those demographics over the years. In this
paper, machine learning (ML) models are trained on publicly available census
data from recent years to classify the DAC status at the census tracts level
and then the trained model is used to classify DAC status for historical years.
A detailed analysis of the feature and model selection along with the evolution
of disadvantaged communities between 2013 and 2018 is presented in this study. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03677v1' target="_blank">http://arxiv.org/pdf/2303.03677v1</a><br> <br> <br> <font size='5'> 235 </font> <div style="text-align: right"> 2023-03-06 14:42:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03174v1' target="_blank">http://arxiv.org/pdf/2303.03174v1</a><br> <br> <br> <font size='5'> 236 </font> <div style="text-align: right"> 2023-03-06 10:25:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identification of Ex Ante Returns Using Elicited Choice Probabilities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the identification of perceived ex ante returns in the
context of binary human capital investment decisions. The environment is
characterised by uncertainty about future outcomes, with some uncertainty being
resolved over time. In this context, each individual holds a probability
distribution over different levels of returns. The paper uses the hypothetical
choice methodology to identify nonparametrically the population distribution of
several individual-specific distribution parameters, which are crucial for
counterfactual policy analyses. The empirical application estimates perceived
returns on overstaying for Afghan asylum seekers in Germany and evaluates the
effect of assisted voluntary return policies. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03009v1' target="_blank">http://arxiv.org/pdf/2303.03009v1</a><br> <br> <br> <font size='5'> 237 </font> <div style="text-align: right"> 2023-03-06 09:28:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Leveraging the Existing German Transmission Grid with Dynamic Line Rating</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The integration of large shares of wind and solar power into the power system
benefits from transmission network expansion. However, the construction of new
power lines requires long planning phases and is often delayed by citizen
protests. As a non-invasive alternative, Dynamic Line Rating (DLR) offers the
potential to leverage the existing grid by dynamically adjusting the
transmission line capacities to the prevailing weather conditions. In this
study, we present the first investment model that includes DLR in a large-scale
power system with real-world network data and a high temporal resolution. Using
Germany as an example, we show that a system-wide integration of DLR improves
the integration of existing and additional renewables while reducing grid
congestion. The evolving synergies between DLR and increased wind generation
result in total cost savings of about 3% of all system costs for a scenario
with 80% renewable power production, mainly due to reduced storage and solar
capacity needs. If considering a fully decarbonized electricity system, the
cost savings from DLR amount to up to 5.5% of the system costs, i.e. 4 billion
Euro per year. Our results underscore the importance of a rapid implementation
of DLR in power systems to support the energy transition and relieve grid
congestion. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02987v1' target="_blank">http://arxiv.org/pdf/2303.02987v1</a><br> <br> <br> <font size='5'> 238 </font> <div style="text-align: right"> 2023-03-05 03:18:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a novel approach for explainability in financial analysis
by utilizing the Pearson correlation coefficient to establish a relationship
between aspect-based sentiment analysis and stock prices. The proposed
methodology involves constructing an aspect list from financial news articles
and analyzing sentiment intensity scores for each aspect. These scores are then
compared to the stock prices for the relevant companies using the Pearson
coefficient to determine any significant correlations. The results indicate
that the proposed approach provides a more detailed and accurate understanding
of the relationship between sentiment analysis and stock prices, which can be
useful for investors and financial analysts in making informed decisions.
Additionally, this methodology offers a transparent and interpretable way to
explain the sentiment analysis results and their impact on stock prices.
Overall, the findings of this paper demonstrate the importance of
explainability in financial analysis and highlight the potential benefits of
utilizing the Pearson coefficient for analyzing aspect-based sentiment analysis
and stock prices. The proposed approach offers a valuable tool for
understanding the complex relationships between financial news sentiment and
stock prices, providing a new perspective on the financial market and aiding in
making informed investment decisions. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02563v3' target="_blank">http://arxiv.org/pdf/2303.02563v3</a><br> <br> <br> <font size='5'> 239 </font> <div style="text-align: right"> 2023-03-04 00:33:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Augmented smartphone bilirubinometer enabled by a mobile app that turns smartphone into multispectral imager</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present the development of SpeCamX, a mobile application that transforms
any unmodified smartphone into a powerful multispectral imager capable of
capturing multispectral information. Our application includes an augmented
bilirubinometer, enabling accurate prediction of blood bilirubin levels (BBL).
In a clinical study involving 320 patients with liver diseases, we used SpeCamX
to image the bulbar conjunctiva region, and we employed a hybrid machine
learning prediction model to predict BBL. We observed a high correlation with
blood test results, demonstrating the efficacy of our approach. Furthermore, we
compared our method, which uses spectrally augmented learning (SAL), with
traditional learning based on RGB photographs (RGBL), and our results clearly
indicate that SpeCamX outperforms RGBL in terms of prediction accuracy,
efficiency, and stability. This study highlights the potential of SpeCamX to
improve the prediction of bio-chromophores, and its ability to transform an
ordinary smartphone into a powerful medical tool without the need for
additional investments or expertise. This makes it suitable for widespread use,
particularly in areas where medical resources are scarce. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02277v1' target="_blank">http://arxiv.org/pdf/2303.02277v1</a><br> <br> <br> <font size='5'> 240 </font> <div style="text-align: right"> 2023-03-03 20:33:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Awkward World of Python and C++</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There are undeniable benefits of binding Python and C++ to take advantage of
the best features of both languages. This is especially relevant to the HEP and
other scientific communities that have invested heavily in the C++ frameworks
and are rapidly moving their data analyses to Python. Version 2 of Awkward
Array, a Scikit-HEP Python library, introduces a set of header-only C++
libraries that do not depend on any application binary interface. Users can
directly include these libraries in their compilation rather than linking
against platform-specific libraries. This new development makes the integration
of Awkward Arrays into other projects easier and more portable as the
implementation is easily separable from the rest of the Awkward Array codebase.
The code is minimal, it does not include all of the code needed to use Awkward
Arrays in Python, nor does it include references to Python or pybind11. The C++
users can use it to make arrays and then copy them to Python without any
specialized data types - only raw buffers, strings, and integers. This C++ code
also simplifies the process of just-in-time (JIT) compilation in ROOT. This
implementation approach solves some of the drawbacks, like packaging projects
where native dependencies can be challenging. In this paper, we demonstrate the
technique to integrate C++ and Python by using a header-only approach. We also
describe the implementation of a new LayoutBuilder and a GrowableBuffer.
Furthermore, examples of wrapping the C++ data into Awkward Arrays and exposing
Awkward Arrays to C++ without copying them are discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02205v1' target="_blank">http://arxiv.org/pdf/2303.02205v1</a><br> <br> <br> <font size='5'> 241 </font> <div style="text-align: right"> 2023-03-03 20:31:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Linked Data Science Powered by Knowledge Graphs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, we have witnessed a growing interest in data science not
only from academia but particularly from companies investing in data science
platforms to analyze large amounts of data. In this process, a myriad of data
science artifacts, such as datasets and pipeline scripts, are created. Yet,
there has so far been no systematic attempt to holistically exploit the
collected knowledge and experiences that are implicitly contained in the
specification of these pipelines, e.g., compatible datasets, cleansing steps,
ML algorithms, parameters, etc. Instead, data scientists still spend a
considerable amount of their time trying to recover relevant information and
experiences from colleagues, trial and error, lengthy exploration, etc. In this
paper, we, therefore, propose a scalable system (KGLiDS) that employs machine
learning to extract the semantics of data science pipelines and captures them
in a knowledge graph, which can then be exploited to assist data scientists in
various ways. This abstraction is the key to enabling Linked Data Science since
it allows us to share the essence of pipelines between platforms, companies,
and institutions without revealing critical internal information and instead
focusing on the semantics of what is being processed and how. Our comprehensive
evaluation uses thousands of datasets and more than thirteen thousand pipeline
scripts extracted from data discovery benchmarks and the Kaggle portal and
shows that KGLiDS significantly outperforms state-of-the-art systems on related
tasks, such as dataset recommendation and pipeline classification. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02204v2' target="_blank">http://arxiv.org/pdf/2303.02204v2</a><br> <br> <br> <font size='5'> 242 </font> <div style="text-align: right"> 2023-03-03 11:18:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An adaptive volatility method for probabilistic forecasting and its application to the M6 financial forecasting competition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this note, we address the problem of probabilistic forecasting using an
adaptive volatility method based on classical time-varying volatility models
and stochastic optimization algorithms. These principles were successfully
applied in the recent M6 financial forecasting competition for both
probabilistic forecasting and investment decision-making under the team named
AdaGaussMC. The key points of our strategy are: (a) apply a univariate
time-varying volatility model, called AdaVol, (b) obtain probabilistic
forecasts of future returns, and (c) optimize the competition metrics using
stochastic gradient-based algorithms. We claim that the frugality of the
methods implies its robustness and consistency. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01855v1' target="_blank">http://arxiv.org/pdf/2303.01855v1</a><br> <br> <br> <font size='5'> 243 </font> <div style="text-align: right"> 2023-03-03 06:30:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automatic Increase Market Systems (AIMS): Towards a deterministic theory for cryptocurrencies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The popularity of cryptocurrencies has grown significantly in recent years,
and they have become an important asset for internet trading. One of the main
drawbacks of cryptocurrencies is the high volatility and fluctuation in value.
The value of cryptocurrencies can change rapidly and dramatically, making them
a risky investment. Cryptocurrencies are largely unregulated, which can
exacerbate their volatility. The high volatility of cryptocurrencies has also
led to a speculative bubble, with many investors buying and selling
cryptocurrencies based on short-term price fluctuations rather than their
underlying values. Therefore, how to reduce the fluctuation risk introduced by
exchanges, transform uncertain prices to deterministic value, and promote the
benefits of decentralized finance are critical for the future development of
cryptos and Web 3.0.
  To address the issues, this paper proposes a novel theory as Automatic
Increase Market Systems (AIMS) for cryptos, which could potentially be designed
to automatically adjust the value of a cryptocurrency helping to stabilize the
price and increase its value over time in a deterministic manner. We build a
crypto, WISH (https://wishbank.wtf), based on AIMS in order to demonstrate how
the automatic increase market system would work in practice, and how it would
influence the supply of the cryptocurrency in response to market demand and
finally make itself to be a stable medium of exchange, ensuring that the AIMS
is fair and transparent. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01735v1' target="_blank">http://arxiv.org/pdf/2303.01735v1</a><br> <br> <br> <font size='5'> 244 </font> <div style="text-align: right"> 2023-03-02 11:04:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How will Language Modelers like ChatGPT Affect Occupations and Industries?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent dramatic increases in AI language modeling capabilities has led to
many questions about the effect of these technologies on the economy. In this
paper we present a methodology to systematically assess the extent to which
occupations, industries and geographies are exposed to advances in AI language
modeling capabilities. We find that the top occupations exposed to language
modeling include telemarketers and a variety of post-secondary teachers such as
English language and literature, foreign language and literature, and history
teachers. We find the top industries exposed to advances in language modeling
are legal services and securities, commodities, and investments. We also find a
positive correlation between wages and exposure to AI language modeling. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01157v2' target="_blank">http://arxiv.org/pdf/2303.01157v2</a><br> <br> <br> <font size='5'> 245 </font> <div style="text-align: right"> 2023-03-02 09:47:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Stock Price Movement as an Image Classification Problem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper studies intraday price movement of stocks that is considered as an
image classification problem. Using a CNN-based model we make a compelling case
for the high-level relationship between the first hour of trading and the
close. The algorithm managed to adequately separate between the two opposing
classes and investing according to the algorithm's predictions outperformed all
alternative constructs but the theoretical maximum. To support the thesis, we
ran several additional tests. The findings in the paper highlight the
suitability of computer vision techniques for studying financial markets and in
particular prediction of stock price movements. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01111v1' target="_blank">http://arxiv.org/pdf/2303.01111v1</a><br> <br> <br> <font size='5'> 246 </font> <div style="text-align: right"> 2023-03-01 17:25:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Techno-economic assessment of long-distance supply chains of energy carriers: Comparing hydrogen and iron for carbon-free electricity generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Effective usage of renewable energy requires ways of storage and delivery to
balance energy demand and availability divergences. Carbon-free chemical energy
carriers are proposed solutions, converting clean electricity into stable media
for storage and long-distance energy trade. Hydrogen (H$_2$) is the subject of
significant investment and research. Metal fuels, such as iron (Fe), are
promising solutions for a clean energy supply, but establishing an
interconnected ecosystem still requires considerable research and development.
A model is proposed to assess the supply chain of hydrogen and iron as clean,
carbon-free energy carriers and then examines case studies of possible trade
routes between the potential energy exporters Morocco, Saudi Arabia, and
Australia and importers Germany and Japan. The work comprehends the assessment
of economic (levelized cost of electricity - LCOE), energetic (thermodynamic
efficiency) and environmental (CO$_2$ emissions) aspects, quantified by the
comprehensive model accounting for the most critical processes in the supply
chain. Sensitivity and uncertainty analyses identify the main drivers for
energy costs. Iron is shown to be lower-cost and more efficient to transport in
longer routes and for long-term storage, but potentially more expensive and
less efficient than H$_2$ to produce and convert. Uncertainties related to the
supply chain specifications and the sensitivity to the used variables indicate
that the path to viable energy carriers fundamentally depends on efficient
synthesis, conversion, storage, and transport. A break-even analysis
demonstrated that clean energy carriers could be competitive with conventional
energy carriers at low renewable energy prices, while carbon taxes might be
needed to level the playing field. Thereby, green iron is an important
potential energy carrier for long-distance trade in a globalized clean energy
market. </font><br> Link: <a href='http://arxiv.org/pdf/2303.00681v2' target="_blank">http://arxiv.org/pdf/2303.00681v2</a><br> <br> <br> <font size='5'> 247 </font> <div style="text-align: right"> 2023-02-28 12:08:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economics of nuclear power in decarbonized energy systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many governments consider the construction of new nuclear power plants to
support the decarbonization of the energy system. On the one hand, dispatchable
nuclear plants can complement fluctuating generation from wind and solar, but
on the other hand, escalating construction costs and times raise economic
concerns. Using a detailed energy planning model, our analysis finds that even
if, despite the historic trend, overnight construction costs of nuclear half to
4,000 US-$2018 per kW and construction times remain below 10 years, the cost
efficient share of nuclear power in European electricity generation is only
around 10%. This analysis still omits social costs of nuclear power, such as
the risk of accidents or waste management. To recover their investment costs,
nuclear plants must operate inflexibly and at utilization rates close to 90%.
As a result, grid infrastructure, flexible demand, and storage are more
efficient options to integrate fluctuating wind and PV generation. </font><br> Link: <a href='http://arxiv.org/pdf/2302.14515v2' target="_blank">http://arxiv.org/pdf/2302.14515v2</a><br> <br> <br> <font size='5'> 248 </font> <div style="text-align: right"> 2023-02-27 11:51:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Accelerating and scaling mentoring strategies to build infrastructure that supports underrepresented groups in STEM</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The vision of 2030STEM is to address systemic barriers in institutional
structures and funding mechanisms required to achieve full inclusion in
Science, Technology, Engineering, and Mathematics (STEM) and accelerate
leadership pathways for individuals from underrepresented populations across
STEM sectors. 2030STEM takes a systems-level approach to create a community of
practice that affirms diverse cultural identities in STEM.
  Accelerated systemic change is needed to achieve parity and representation in
the STEM workforce, and mentorship - due to its impact on retaining talent - is
crucial to ensure those underrepresented in STEM feel that they belong and can
thrive. To support the studies and careers of those underrepresented in STEM,
we must increase access to mentors who have received adequate training on both
the discipline of mentorship in addition to cross-cultural mentoring, use
evidence-based mentorship tools to improve the outcomes of mentor/mentee
relationships, and create a persistent culture of mentorship at the
institutional versus individual level. This white paper provides a summary of
research-based mentorship practices that have worked at improving the
experience in STEM for underrepresented groups.
  This is the second in a series of white papers based on 2030STEM Salons that
bring together innovative thinkers invested in creating a better STEM world for
all. Our first salon focused on the power of social media campaigns like the
#XinSTEM initiatives, to accelerate change towards inclusion and leadership by
underrepresented communities in STEM. Read our first white paper entitled
#Change: How Social Media is Accelerating STEM Inclusion for more information. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13691v3' target="_blank">http://arxiv.org/pdf/2302.13691v3</a><br> <br> <br> <font size='5'> 249 </font> <div style="text-align: right"> 2023-02-26 06:43:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Physical Momentum in the Indian Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Our study focuses on determining the presence of abnormal returns for
physical momentum portfolios in the context of the Indian market. The physical
momentum portfolios, comprising stocks from the NSE 500, are constructed for
the daily, weekly, monthly, and yearly timescales. In the aforementioned
timescales, we empirically evaluate the historical returns and varied risk
profiles of these portfolios for the years 2014-2021. It has been observed that
the best-performing physical momentum portfolios from each of the four
timescales achieved higher returns and better risk measures when compared to
the benchmark NIFTY 50 portfolio. We further find that the high-frequency daily
time scale exhibits the strongest reversal in the physical momentum effect,
wherein the portfolio yielded a 16-fold profit over the initial investment. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13245v1' target="_blank">http://arxiv.org/pdf/2302.13245v1</a><br> <br> <br> <font size='5'> 250 </font> <div style="text-align: right"> 2023-02-25 17:03:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Contribution of Tourism in National Economies: Evidence of Greece</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Greece constitutes a coastal country with a lot of geomorphologic, climatic,
cultural and historic peculiarities favoring the development of many aspects of
tourism. Within this framework, this article examines what are the effects of
tourism in Greece and how determinative these effects are, by applying a
macroscopic analysis on empirical data for the estimation of the contribution
of tourism in the Greek Economy. The available data regard records of the
Balance of Payments in Greece and of the major components of the Balance of the
Invisible Revenues, where a measurable aspect of tourism, the Travel or Tourism
Exchange, is included. At the time period of the available data (2000-2012) two
events of the recent Greek history are distinguished as the most significant
(the Olympic Games in the year 2004 and the economic crisis initiated in the
year 2009) and their impact on the diachronic evolution in the tourism is
discussed. Under an overall assessment, the analysis illustrated that tourism
is a sector of the Greek economy, which is described by a significant
resilience, but it seems that it has not yet been submitted to an effective
developmental plan exploiting the endogenous tourism dynamics of the country,
suggesting currently a promising investment of low risk for the economic growth
of country and the exit of the economic crisis </font><br> Link: <a href='http://arxiv.org/pdf/2302.13121v1' target="_blank">http://arxiv.org/pdf/2302.13121v1</a><br> <br> <br> <font size='5'> 251 </font> <div style="text-align: right"> 2023-02-24 19:38:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FLINT: A Platform for Federated Learning Integration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cross-device federated learning (FL) has been well-studied from algorithmic,
system scalability, and training speed perspectives. Nonetheless, moving from
centralized training to cross-device FL for millions or billions of devices
presents many risks, including performance loss, developer inertia, poor user
experience, and unexpected application failures. In addition, the corresponding
infrastructure, development costs, and return on investment are difficult to
estimate. In this paper, we present a device-cloud collaborative FL platform
that integrates with an existing machine learning platform, providing tools to
measure real-world constraints, assess infrastructure capabilities, evaluate
model training performance, and estimate system resource requirements to
responsibly bring FL into production. We also present a decision workflow that
leverages the FL-integrated platform to comprehensively evaluate the trade-offs
of cross-device FL and share our empirical evaluations of business-critical
machine learning applications that impact hundreds of millions of users. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12862v2' target="_blank">http://arxiv.org/pdf/2302.12862v2</a><br> <br> <br> <font size='5'> 252 </font> <div style="text-align: right"> 2023-02-24 14:23:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-objective near-optimal necessary conditions for multi-sectoral planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper extends the concepts of epsilon-optimal spaces and necessary
conditions for near-optimality from single-objective to multi-objective
optimisation. These notions are first presented for single-objective
optimisation, and the mathematical formulation is adapted to address the
multi-objective framework. Afterwards, we illustrate the newly developed
methodology by conducting multi-sectoral planning of the Belgian energy system
with an open-source model called EnergyScope TD. The cost and energy invested
in the system are used as objectives. Optimal and efficient solutions for these
two objectives are computed and analysed. These results are then used to obtain
necessary conditions corresponding to the minimum amount of energy from
different sets of resources, including endogenous and exogenous resources. This
case study highlights the high dependence of Belgium on imported energy while
demonstrating that no individual resource is essential on its own. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12654v1' target="_blank">http://arxiv.org/pdf/2302.12654v1</a><br> <br> <br> <font size='5'> 253 </font> <div style="text-align: right"> 2023-02-23 17:09:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Principal-Agent Framework for Optimal Incentives in Renewable Investments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the optimal regulation of energy production reflecting the
long-term goals of the Paris Climate Agreement. We analyze the optimal
regulatory incentives to foster the development of non-emissive electricity
generation when the demand for power is served either by a monopoly or by two
competing agents. The regulator wishes to encourage green investments to limit
carbon emissions, while simultaneously reducing intermittency of the total
energy production. We find that the regulation of a competitive market is more
efficient than the one of the monopoly as measured with the certainty
equivalent of the Principal's value function. This higher efficiency is
achieved thanks to a higher degree of freedom of the incentive mechanisms which
involves cross-subsidies between firms. A numerical study quantifies the impact
of the designed second-best contract in both market structures compared to the
business-as-usual scenario.
  In addition, we expand the monopolistic and competitive setup to a more
general class of tractable Principal-Multi-Agent incentives problems when both
the drift and the volatility of a multi-dimensional diffusion process can be
controlled by the Agents. We follow the resolution methodology of Cvitani\'c et
al. (2018) in an extended linear quadratic setting with exponential utilities
and a multi-dimensional state process of Ornstein-Uhlenbeck type. We provide
closed-form expression of the second-best contracts. In particular, we show
that they are in rebate form involving time-dependent prices of each
state-variable. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12167v1' target="_blank">http://arxiv.org/pdf/2302.12167v1</a><br> <br> <br> <font size='5'> 254 </font> <div style="text-align: right"> 2023-02-23 01:28:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Factor Exposure Heterogeneity in Green and Brown Stocks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Using the peer-exposure ratio, we explore the factor exposure heterogeneity
in green and brown stocks. By looking at peer groups of S&P 500 index firms
over 2014-2020 based on their greenhouse gas emission levels, we find that, on
average, green stocks exhibit less factor exposure heterogeneity than brown
stocks for most of the traditional equity factors but the value factor. Hence,
investment managers shifting their investments from brown stocks to green
stocks have less room to differentiate themselves regarding their factor
exposures. Finally, we find that factor exposure heterogeneity has increased
for green stocks compared to earlier periods. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11729v2' target="_blank">http://arxiv.org/pdf/2302.11729v2</a><br> <br> <br> <font size='5'> 255 </font> <div style="text-align: right"> 2023-02-22 22:42:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On ruin probabilities in the presence of risky investments and random switching</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the asymptotic behavior of ruin probabilities, as the initial
reserve goes to infinity, for a reserve process model where claims arrive
according to a renewal process, while between the claim times the process has
the dynamics of geometric Brownian motion-type It\^o processes with
time-dependent random coefficients. These coefficients are ``reset" after each
claim time, switching to new values independent of the past history of the
process. We use the implicit renewal theory to obtain power-function bounds for
the eventual ruin probability. In the special case when the random drift and
diffusion coefficients of the investment returns process remain unchanged
between consecutive claim arrivals, we obtain conditions for existence of
Lundberg's exponent for our model ensuring the power function behaviour for the
ruin probability. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11682v1' target="_blank">http://arxiv.org/pdf/2302.11682v1</a><br> <br> <br> <font size='5'> 256 </font> <div style="text-align: right"> 2023-02-22 13:49:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Institutional reforms and the employment effects of spatially targeted investment grants: The case of Germany's GRW</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Spatially targeted investment grant schemes are a common tool to support
firms in lagging regions. We exploit exogenous variations in Germany's main
regional policy instrument (GRW) arriving from institutional reforms to analyse
local employment effects of investment grants. Findings for reduced-form and IV
regressions point to a significant policy channel running from higher funding
rates to increased firm-level investments and newly created jobs. When we
contrast effects for regions with high but declining funding rates to those
with low but rising rates, we find that GRW reforms led to diminishing
employment increases. Especially small firms responded to changing funding
conditions. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11376v1' target="_blank">http://arxiv.org/pdf/2302.11376v1</a><br> <br> <br> <font size='5'> 257 </font> <div style="text-align: right"> 2023-02-22 08:52:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Feasible Recourse Plan via Diverse Interpolation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Explaining algorithmic decisions and recommending actionable feedback is
increasingly important for machine learning applications. Recently, significant
efforts have been invested in finding a diverse set of recourses to cover the
wide spectrum of users' preferences. However, existing works often neglect the
requirement that the recourses should be close to the data manifold; hence, the
constructed recourses might be implausible and unsatisfying to users. To
address these issues, we propose a novel approach that explicitly directs the
diverse set of actionable recourses towards the data manifold. We first find a
diverse set of prototypes in the favorable class that balances the trade-off
between diversity and proximity. We demonstrate two specific methods to find
these prototypes: either by finding the maximum a posteriori estimate of a
determinantal point process or by solving a quadratic binary program. To ensure
the actionability constraints, we construct an actionability graph in which the
nodes represent the training samples and the edges indicate the feasible action
between two instances. We then find a feasible path to each prototype, and this
path demonstrates the feasible actions for each recourse in the plan. The
experimental results show that our method produces a set of recourses that are
close to the data manifold while delivering a better cost-diversity trade-off
than existing approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11213v1' target="_blank">http://arxiv.org/pdf/2302.11213v1</a><br> <br> <br> <font size='5'> 258 </font> <div style="text-align: right"> 2023-02-22 08:52:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Simple Analytics of the Government Investment Multiplier</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: What are the effects of investing in public infrastructure? We answer this
question with a New Keynesian model. We recast the model as a Markov chain and
develop a general solution method that nests existing ones inside and outside
the lower bound as special cases. Our framework delivers a simple expression
for the contribution of public infrastructure. We show that it provides a
unified framework to study the effects of public investment in three scenarios:
$(i)$ normal times $(ii)$ a short-lived liquidity trap $(iii)$ a long-lived
liquidity trap. We also provide closed-form results for intermediate cases with
a liquidity trap of arbitrary duration. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11212v1' target="_blank">http://arxiv.org/pdf/2302.11212v1</a><br> <br> <br> <font size='5'> 259 </font> <div style="text-align: right"> 2023-02-22 03:49:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ignorance Is Bliss: The Screening Effect of (Noisy) Information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies how the firm designs its internal information system when
facing an adverse selection problem arising from unobservable managerial
abilities. While more precise information allows the firm to make ex-post more
efficient investment decisions, noisier information has an ex-ante screening
effect that allows the firm to attract on-average better managers. The tradeoff
between more effective screening of managers and more informed investment
implies a non-monotonic relationship between firm value and information
quality, and a marginal improvement of information quality does not necessarily
lead to an overall improvement of firm value. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11128v2' target="_blank">http://arxiv.org/pdf/2302.11128v2</a><br> <br> <br> <font size='5'> 260 </font> <div style="text-align: right"> 2023-02-21 09:38:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal transmission expansion planning in the context of renewable energy integration policies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In light of increasing pressure to curb greenhouse gas emissions, many
countries have focused on the development of strategies that encourage
renewable generation in liberalised energy markets. This paper presents a
modelling assessment of a renewable-driven expansion of the transmission system
infrastructure that accounts for decentralized energy market settings. The
mathematical optimisation problem formulation involves a bi-level model in
which a welfare-maximizing transmission system operator makes investments in
transmission lines at the upper level while considering power market dynamics
at the lower level. To account for deregulated energy market structure, we
assume that the generation companies at the lower level make generation
capacity investment decisions as price-takers in perfect competition.
Considering alternative levels for a transmission infrastructure expansion
budget, carbon emission taxes and monetary incentives for renewable generation
capacity expansion, we study how alternative compositions of these three
factors affect the share of renewable generation in the total generation mix.
We apply the proposed modelling assessment to an illustrative three-node
instance and a case study considering a simplified representation of the energy
system of the Nordic and Baltic countries. The results suggest the limited
efficiency of three renewable-driven measures when applied individually.
Nevertheless, applied in composition, these three measures demonstrated a
positive impact on Nordics' and Baltics' energy system welfare, VRE share and
total generation amount. However, the amplitude of this impact differs
depending on the composition of values used for three renewable-driven
measures. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10562v2' target="_blank">http://arxiv.org/pdf/2302.10562v2</a><br> <br> <br> <font size='5'> 261 </font> <div style="text-align: right"> 2023-02-21 07:19:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment with a noisy signal of future stock prices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider an investor who is dynamically informed about the future
evolution of one of the independent Brownian motions driving a stock's price
fluctuations. With linear temporary price impact the resulting optimal
investment problem with exponential utility turns out to be not only well
posed, but it even allows for a closed-form solution. We describe this solution
and the resulting problem value for this stochastic control problem with
partial observation by solving its convex-analytic dual problem. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10485v1' target="_blank">http://arxiv.org/pdf/2302.10485v1</a><br> <br> <br> <font size='5'> 262 </font> <div style="text-align: right"> 2023-02-21 02:43:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Scenario-based Optimization Models for Power Grid Resilience to Extreme Flooding Events</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose two scenario-based optimization models for power grid resilience
decision making that integrate output from a hydrology model with a power flow
model. The models are used to identify an optimal substation hardening strategy
against potential flooding from storms for a given investment budget, which if
implemented enhances the resilience of the power grid, minimizing the power
demand that is shed. The same models can alternatively be used to determine the
optimal budget that should be allocated for substation hardening when long-term
forecasts of storm frequency and impact (specifically restoration times) are
available. The two optimization models differ in terms of capturing risk
attitude: one minimizes the average load shed for given scenario probabilities
and the other minimizes the worst-case load shed without needing scenario
probabilities. To demonstrate the efficacy of the proposed models, we further
develop a case study for the Texas Gulf Coast using storm surge maps developed
by the National Oceanic and Atmospheric Administration and a synthetic power
grid for the state of Texas developed as part of an ARPA-E project. For a
reasonable choice of parameters, we show that a scenario-based representation
of uncertainty can offer a significant improvement in minimizing load shed as
compared to using point estimates or average flood values. We further show that
when the available investment budget is relatively high, solutions that
minimize the worst-case load shed can offer several advantages as compared to
solutions obtained from minimizing the average load shed. Lastly, we show that
even for relatively low values of load loss and short post-hurricane power
restoration times, it is optimal to make significant investments in substation
hardening to deal with the storm surge considered in the NOAA flood scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10408v1' target="_blank">http://arxiv.org/pdf/2302.10408v1</a><br> <br> <br> <font size='5'> 263 </font> <div style="text-align: right"> 2023-02-20 15:42:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A computationally efficient Benders decomposition for energy systems planning problems with detailed operations and time-coupling constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Energy systems planning models identify least-cost strategies for expansion
and operation of energy systems and provide decision support for investment,
planning, regulation, and policy. Most are formulated as linear programming
(LP) or mixed integer linear programming (MILP) problems. Despite the relative
efficiency and maturity of solvers, large scale problems are often intractable
without abstractions that impact quality of results and generalizability of
findings. We consider a macro-energy systems planning problem with detailed
operations and policy constraints and formulate a computationally efficient
Benders decomposition that separates investments from operations and further
decouples operational timesteps using budgeting variables in the the master
model. This novel approach enables parallelization of operational subproblems
and permits modeling of relevant constraints that couple decisions across time
periods (e.g. policy constraints.) Runtime scales linearly with temporal
resolution; numerical tests demonstrate substantial improvement in runtime
relative to monolithic problems using state-of-the-art commercial solvers for
all MILP formulations and for some LP formulations depending on problem size.
Our algorithm is applicable to similar planning problems in other domains (e.g.
water, transportation networks, production processes) and can solve
large-scale, otherwise intractable, planning problems. We show that the
increased resolution enabled by this algorithm mitigates structural uncertainty
and improves recommendation accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10037v1' target="_blank">http://arxiv.org/pdf/2302.10037v1</a><br> <br> <br> <font size='5'> 264 </font> <div style="text-align: right"> 2023-02-20 10:10:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economic Dynamics of Agents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Post-pandemic world has thrown up several challenges, such as, high
inflation, low growth, high debt, collapse of economies, political instability,
job losses, lowering of income in addition to damages caused natural disasters,
more convincing attributed to climate change, apart from existing inequalities.
Efforts are being made to mitigate these challenges at various levels. To the
best of the knowledge of the author, most of the prior researches have focussed
on specific scenarios, use cases, inter-relationships between couple of sectors
and more so on optimal policies, such as, impact of carbon tax on individuals,
interaction between taxes and welfare, etc. However, not much effort have been
made to understand the actual impact on individual agents due to diverse policy
changes and how agents cope with changing economic dynamics. This paper
considers progressive deteriorating conditions of increase in expense,
degrading environmental utility, increase in taxation, decrease in welfare and
lowering of income with recourse to inherited properties, credits and return on
investments, and tries to understand how the agents cope with the changing
situations using an agent based model with matrices related to savings,
credits, assets. Results indicate that collapse of agents' economic conditions
can be quite fast, sudden and drastic for all income groups in most cases. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09877v1' target="_blank">http://arxiv.org/pdf/2302.09877v1</a><br> <br> <br> <font size='5'> 265 </font> <div style="text-align: right"> 2023-02-19 11:08:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Globalization-Inequality Nexus: A Comparative Study of Developed and Developing Countries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study examines the relationship between globalization and income
inequality, utilizing panel data spanning from 1992 to 2020. Globalization is
measured by the World Bank global-link indicators such as FDI, Remittance,
Trade Openness, and Migration while income inequality is measured by Gini
Coefficient and the median income of 50% of the population. The fixed effect
panel data analysis provides empirical evidence indicating that globalization
tends to reduce income inequality, though its impact varies between developed
and developing countries. The analysis reveals a strong negative correlation
between net foreign direct investment (FDI) inflows and inequality in
developing countries, while no such relationship was found for developed
countries.The relationship holds even if we consider an alternative measure of
inequality. However, when dividing countries by developed and developing
groups, no statistically significant relationship was observed. Policymakers
can use these findings to support efforts to increase FDI, trade, tourism, and
migration to promote growth and reduce income inequality. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09537v1' target="_blank">http://arxiv.org/pdf/2302.09537v1</a><br> <br> <br> <font size='5'> 266 </font> <div style="text-align: right"> 2023-02-18 15:41:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cryptocurrencies Are Becoming Part of the World Global Financial Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this study the cross-correlations between the cryptocurrency market
represented by the two most liquid and highest-capitalized cryptocurrencies:
bitcoin and ethereum, on the one side, and the instruments representing the
traditional financial markets: stock indices, Forex, commodities, on the other
side, are measured in the period: January 2020--October 2022. Our purpose is to
address the question whether the cryptocurrency market still preserves its
autonomy with respect to the traditional financial markets or it has already
aligned with them in expense of its independence. We are motivated by the fact
that some previous related studies gave mixed results. By calculating the
$q$-dependent detrended cross-correlation coefficient based on the high
frequency 10 s data in the rolling window, the dependence on various time
scales, different fluctuation magnitudes, and different market periods are
examined. There is a strong indication that the dynamics of the bitcoin and
ethereum price changes since the March 2020 Covid-19 panic is no longer
independent. Instead, it is related to the dynamics of the traditional
financial markets, which is especially evident now in 2022, when the bitcoin
and ethereum coupling to the US tech stocks is observed during the market bear
phase. It is also worth emphasizing that the cryptocurrencies have begun to
react to the economic data such as the Consumer Price Index readings in a
similar way as traditional instruments. Such a spontaneous coupling of the so
far independent degrees of freedom can be interpreted as a kind of phase
transition that resembles the collective phenomena typical for the complex
systems. Our results indicate that the cryptocurrencies cannot be considered as
a safe haven for the financial investments. </font><br> Link: <a href='http://arxiv.org/pdf/2303.00495v1' target="_blank">http://arxiv.org/pdf/2303.00495v1</a><br> <br> <br> <font size='5'> 267 </font> <div style="text-align: right"> 2023-02-17 15:05:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unique Identification of 50,000+ Virtual Reality Users from Head & Hand Motion Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the recent explosive growth of interest and investment in virtual
reality (VR) and the so-called "metaverse," public attention has rightly
shifted toward the unique security and privacy threats that these platforms may
pose. While it has long been known that people reveal information about
themselves via their motion, the extent to which this makes an individual
globally identifiable within virtual reality has not yet been widely
understood. In this study, we show that a large number of real VR users
(N=55,541) can be uniquely and reliably identified across multiple sessions
using just their head and hand motion relative to virtual objects. After
training a classification model on 5 minutes of data per person, a user can be
uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy
from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of
motion. This work is the first to truly demonstrate the extent to which
biomechanics may serve as a unique identifier in VR, on par with widely used
biometrics such as facial or fingerprint recognition. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08927v1' target="_blank">http://arxiv.org/pdf/2302.08927v1</a><br> <br> <br> <font size='5'> 268 </font> <div style="text-align: right"> 2023-02-17 14:08:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantum Computing Toolkit From Nuts and Bolts to Sack of Tools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum computing has the potential to provide exponential performance
benefits in processing over classical computing. It utilizes quantum mechanics
phenomena (such as superposition, entanglement, and interference) to solve a
computational problem. It can explore atypical patterns over data that
classical computers can't perform efficiently. Quantum computers are in the
nascent stage of development and are noisy due to decoherence, i.e., quantum
bits deteriorate with environmental interactions. It will take a long time for
quantum computers to achieve fault tolerance although quantum algorithms can be
developed in advance. Heavy investment in developing quantum hardware, software
development kits, and simulators has led to multiplicity of quantum development
tools. Selection of a suitable development platform requires a proper
understanding of the capabilities and limitations of these tools. Although a
comprehensive comparison of the different quantum development tools would be of
great value, to the best of our knowledge, no such extensive study is currently
available. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08884v2' target="_blank">http://arxiv.org/pdf/2302.08884v2</a><br> <br> <br> <font size='5'> 269 </font> <div style="text-align: right"> 2023-02-17 11:56:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Great year, bad Sharpe? A note on the joint distribution of performance and risk-adjusted return</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Returns distributions are heavy-tailed across asset classes. In this note, I
examine the implications of this stylized fact for the joint statistics of
performance and risk-adjusted return. Using both synthetic and real data, I
show that the Sharpe ratio does not increase monotonically with performance: in
a sample of price trajectories, the largest Sharpe ratios are associated with
suboptimal mean returns; conversely, the best performance never corresponds to
the largest Sharpe ratios. This counter-intuitive effect is unrelated to the
risk-return tradeoff familiar from portfolio theory: it is a consequence of
asymptotic correlations between the sample mean and sample standard deviation
of heavy-tailed variables (which are absent in the Gaussian case). In addition
to its very large sample noise, the non-monotonic association of the Sharpe
ratio with performance puts into question its status as the gold standard of
investment quality. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08829v1' target="_blank">http://arxiv.org/pdf/2302.08829v1</a><br> <br> <br> <font size='5'> 270 </font> <div style="text-align: right"> 2023-02-17 07:00:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal management of DB pension fund under both underfunded and overfunded cases</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates the optimal management of an aggregated defined
benefit pension plan in a stochastic environment. The interest rate follows the
Ornstein-Uhlenbeck model, the benefits follow the geometric Brownian motion
while the contribution rate is determined by the spread method of fund
amortization. The pension manager invests in the financial market with three
assets: cash, bond and stock. Regardless of the initial status of the plan, we
suppose that the pension fund may become underfunded or overfunded in the
planning horizon. The optimization goal of the manager is to maximize the
expected utility in the overfunded region minus the weighted solvency risk in
the underfunded region. By introducing an auxiliary process and related
equivalent optimization problems and using the martingale method, the optimal
wealth process, optimal portfolio and efficient frontier are obtained under
four cases (high tolerance towards solvency risk, low tolerance towards
solvency risk, a specific lower bound, and high lower bound). Moreover, we also
obtain the probabilities that the optimal terminal wealth falls in the
overfunded and underfunded regions. At last, we present numerical analyses to
illustrate the manager's economic behaviors. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08731v1' target="_blank">http://arxiv.org/pdf/2302.08731v1</a><br> <br> <br> <font size='5'> 271 </font> <div style="text-align: right"> 2023-02-15 18:16:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optical Networks and Interconnects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid evolution of communication technologies such as 5G and beyond, rely
on optical networks to support the challenging and ambitious requirements that
include both capacity and reliability. This chapter begins by giving an
overview of the evolution of optical access networks, focusing on Passive
Optical Networks (PONs). The development of the different PON standards and
requirements aiming at longer reach, higher client count and delivered
bandwidth are presented. PON virtualization is also introduced as the
flexibility enabler. Triggered by the increase of bandwidth supported by access
and aggregation network segments, core networks have also evolved, as presented
in the second part of the chapter. Scaling the physical infrastructure requires
high investment and hence, operators are considering alternatives to optimize
the use of the existing capacity. This chapter introduces different planning
problems such as Routing and Spectrum Assignment problems, placement problems
for regenerators and wavelength converters, and how to offer resilience to
different failures. An overview of control and management is also provided.
Moreover, motivated by the increasing importance of data storage and data
processing, this chapter also addresses different aspects of optical data
center interconnects. Data centers have become critical infrastructure to
operate any service. They are also forced to take advantage of optical
technology in order to keep up with the growing capacity demand and power
consumption. This chapter gives an overview of different optical data center
network architectures as well as some expected directions to improve the
resource utilization and increase the network capacity. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07829v1' target="_blank">http://arxiv.org/pdf/2302.07829v1</a><br> <br> <br> <font size='5'> 272 </font> <div style="text-align: right"> 2023-02-15 16:19:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Evaluation of Researchers' Migration Patterns in Europe using Digital Trace Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The comprehension of the mechanisms behind the mobility of skilled workers is
of paramount importance for policy making. The lacking nature of official
measurements motivates the use of digital trace data extracted from ORCID
public records. We use such data to investigate European regions, studied at
NUTS2 level, over the time horizon of 2009 to 2020. We present a novel
perspective where regions roles are dictated by the overall activity of the
research community, contradicting the common brain drain interpretation of the
phenomenon. We find that a high mobility is usually correlated with strong
university prestige, high magnitude of investments and an overall good
schooling level in a region. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07764v1' target="_blank">http://arxiv.org/pdf/2302.07764v1</a><br> <br> <br> <font size='5'> 273 </font> <div style="text-align: right"> 2023-02-14 16:35:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Security Reputation Metrics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Security reputation metrics (aka. security metrics) quantify the security
levels of organization (e.g., hosting or Internet access providers) relative to
comparable entities. They enable benchmarking and are essential tools for
decision and policy-making in security, and may be used to govern and steer
responsible parties towards investing in security when economic or other
decision-making factors may drive them to do otherwise. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07172v1' target="_blank">http://arxiv.org/pdf/2302.07172v1</a><br> <br> <br> <font size='5'> 274 </font> <div style="text-align: right"> 2023-02-14 16:15:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Semiconductor manufacturing is a notoriously complex and costly multi-step
process involving a long sequence of operations on expensive and
quantity-limited equipment. Recent chip shortages and their impacts have
highlighted the importance of semiconductors in the global supply chains and
how reliant on those our daily lives are. Due to the investment cost,
environmental impact, and time scale needed to build new factories, it is
difficult to ramp up production when demand spikes.
  This work introduces a method to successfully learn to schedule a
semiconductor manufacturing facility more efficiently using deep reinforcement
and self-supervised learning. We propose the first adaptive scheduling approach
to handle complex, continuous, stochastic, dynamic, modern semiconductor
manufacturing models. Our method outperforms the traditional hierarchical
dispatching strategies typically used in semiconductor manufacturing plants,
substantially reducing each order's tardiness and time until completion. As a
result, our method yields a better allocation of resources in the semiconductor
manufacturing process. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07162v1' target="_blank">http://arxiv.org/pdf/2302.07162v1</a><br> <br> <br> <font size='5'> 275 </font> <div style="text-align: right"> 2023-02-14 15:22:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Control of Emerging-Market Target, Abnormal Stock Return: Evidence in Vietnam</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Joining with the upward trend of Global Foreign direct investment and FDI in
emerging economies and emerging Asian economies, FDI to Vietnam, especially
M&As have increased significantly in both numbers and value of deals from 1995
to 2015... </font><br> Link: <a href='http://arxiv.org/pdf/2302.07117v2' target="_blank">http://arxiv.org/pdf/2302.07117v2</a><br> <br> <br> <font size='5'> 276 </font> <div style="text-align: right"> 2023-02-14 13:08:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Revisiting the Second Law and Weak Cosmic Censorship Conjecture in High-Dimensional Charged-AdS Black Hole: an Additional Assumption</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The verification of the second law of black hole mechanics and the WCCC in
the context of enthalpy as mass of the black hole and its related thermodynamic
properties has not been tested through a vast number of literature in the
recent past. Such studies are of great physical importance as they provide us
with a large number of information regarding the thermodynamics and the
dynamics of AdS black hole systems. We invest the prior limited surveys of such
analysis to investigate the WCCC for the $D$- dimensional asymptotically
AdS-charged black holes characterized by its mass ($M$), electric charge ($Q$),
and AdS radius ($l$) under the absorption of scalar particles of charge $q$. We
examine the WCCC by analyzing the energy-momentum condition of the electrically
charged particles as absorbed by the black holes. We prove that the conjecture
is well verified irrespective of whether the initial black hole configurations
are extremal or non-extremal by changing its charge, the AdS radius, and their
variations. We show that the first law and the WCCC are valid for all spacetime
dimensions ($D$) independent of the choice of the parameters characterizing the
black holes. But to verify the second law in the extremal and non-extremal
configurations one has to be very cautious as it gets strongly affected by the
choices of the values of the black hole parameters and their variations...
  In the context of the extended phase space, taking the grand canonical
potential into account allow us to obtain the missing information about the
variation of the cosmological constant necessary to construct the extended
phase space, namely the notion of the black hole pressure, and which is absent
in the previous literature so far. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07026v2' target="_blank">http://arxiv.org/pdf/2302.07026v2</a><br> <br> <br> <font size='5'> 277 </font> <div style="text-align: right"> 2023-02-12 01:19:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Difficulty of Characterizing Network Formation with Endogenous Behavior</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Bolletta (2021, Math. Soc. Sci. 114:1-10) studies a model in which a network
is strategically formed and then agents play a linear best-response investment
game in it. The model is motivated by an application in which people choose
both their study partners and their levels of educational effort. Agents have
different one-dimensional types $\unicode{x2013}$ private returns to effort. A
main result claims that pairwise Nash stable networks have a locally complete
structure consisting of possibly overlapping cliques: if two agents are linked,
they are part of a clique composed of all agents with types between theirs. We
offer a counterexample showing that the claimed characterization is incorrect,
highlight where the analysis errs, and discuss implications for network
formation models. </font><br> Link: <a href='http://arxiv.org/pdf/2302.05831v2' target="_blank">http://arxiv.org/pdf/2302.05831v2</a><br> <br> <br> <font size='5'> 278 </font> <div style="text-align: right"> 2023-02-10 18:26:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Impact of Network Design Interventions on the Security of Interdependent Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the problem of defending a Cyber-Physical System (CPS) consisting of
interdependent components with heterogeneous sensitivity to investments. In
addition to the optimal allocation of limited security resources, we analyze
the impact of an orthogonal set of defense strategies in the form of network
design interventions in the CPS to protect it against the attacker. We first
propose an algorithm to simplify the CPS attack graph to an equivalent form
which reduces the computational requirements for characterizing the defender's
optimal security investments. We then evaluate four types of design
interventions in the network in the form of adding nodes in the attack graph,
interpreted as introducing additional safeguards, introducing structural
redundancies, introducing functional redundancies, and introducing new
functionalities. We identify scenarios in which interventions that strengthen
internal components of the CPS may be more beneficial than traditional
approaches such as perimeter defense. We showcase our proposed approach in two
practical use cases: a remote attack on an industrial CPS and a remote attack
on an automotive system. We highlight how our results closely match
recommendations made by security organizations and discuss the implications of
our findings for CPS design. </font><br> Link: <a href='http://arxiv.org/pdf/2302.05411v1' target="_blank">http://arxiv.org/pdf/2302.05411v1</a><br> <br> <br> <font size='5'> 279 </font> <div style="text-align: right"> 2023-02-10 15:17:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bayesian Optimization of ESG Financial Investments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Financial experts and analysts seek to predict the variability of financial
markets. In particular, the correct prediction of this variability ensures
investors successful investments. However, there has been a big trend in
finance in the last years, which are the ESG criteria. Concretely, ESG
(Economic, Social and Governance) criteria have become more significant in
finance due to the growing importance of investments being socially
responsible, and because of the financial impact companies suffer when not
complying with them. Consequently, creating a stock portfolio should not only
take into account its performance but compliance with ESG criteria. Hence, this
paper combines mathematical modelling, with ESG and finance. In more detail, we
use Bayesian optimization (BO), a sequential state-of-the-art design strategy
to optimize black-boxes with unknown analytical and costly-to compute
expressions, to maximize the performance of a stock portfolio under the
presence of ESG criteria soft constraints incorporated to the objective
function. In an illustrative experiment, we use the Sharpe ratio, that takes
into consideration the portfolio returns and its variance, in other words, it
balances the trade-off between maximizing returns and minimizing risks. In the
present work, ESG criteria have been divided into fourteen independent
categories used in a linear combination to estimate a firm total ESG score.
Most importantly, our presented approach would scale to alternative black-box
methods of estimating the performance and ESG compliance of the stock
portfolio. In particular, this research has opened the door to many new
research lines, as it has proved that a portfolio can be optimized using a BO
that takes into consideration financial performance and the accomplishment of
ESG criteria. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01485v1' target="_blank">http://arxiv.org/pdf/2303.01485v1</a><br> <br> <br> <font size='5'> 280 </font> <div style="text-align: right"> 2023-02-09 16:34:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Benchmarks for Automated Commonsense Reasoning: A Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: More than one hundred benchmarks have been developed to test the commonsense
knowledge and commonsense reasoning abilities of artificial intelligence (AI)
systems. However, these benchmarks are often flawed and many aspects of common
sense remain untested. Consequently, we do not currently have any reliable way
of measuring to what extent existing AI systems have achieved these abilities.
This paper surveys the development and uses of AI commonsense benchmarks. We
discuss the nature of common sense; the role of common sense in AI; the goals
served by constructing commonsense benchmarks; and desirable features of
commonsense benchmarks. We analyze the common flaws in benchmarks, and we argue
that it is worthwhile to invest the work needed ensure that benchmark examples
are consistently high quality. We survey the various methods of constructing
commonsense benchmarks. We enumerate 139 commonsense benchmarks that have been
developed: 102 text-based, 18 image-based, 12 video based, and 7 simulated
physical environments. We discuss the gaps in the existing benchmarks and
aspects of commonsense reasoning that are not addressed in any existing
benchmark. We conclude with a number of recommendations for future development
of commonsense AI benchmarks. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04752v2' target="_blank">http://arxiv.org/pdf/2302.04752v2</a><br> <br> <br> <font size='5'> 281 </font> <div style="text-align: right"> 2023-02-09 13:20:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Employing Channel Probing to Derive End-of-Life Service Margins for Optical Spectrum Services. To appear in OPTICA Journal of Optical Communications and Networking</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Optical Spectrum as a Service (OSaaS) spanning over multiple transparent
optical network domains, can significantly reduce the investment and
operational costs of the end-to-end service. Based on the black-link approach,
these services are empowered by reconfigurable transceivers and the emerging
disaggregation trend in optical transport networks. This work investigates the
accuracy aspects of the channel probing method used in Generalized Signal to
Noise Ratio (GSNR)-based OSaaS characterization in terrestrial brownfield
systems. OSaaS service margins to accommodate impacts from enabling neighboring
channels and end-of-life channel loads are experimentally derived in a
systematic lab study carried out in the Open Ireland testbed. The applicability
of the lab-derived margins is then verified in the HEAnet production network
using a 400 GHz wide OSaaS. Finally, the probing accuracy is tested by
depleting the GSNR margin through power adjustments utilizing the same 400 GHz
OSaaS in the HEAnet live network. A minimum of 0.92 dB and 1.46 dB of service
margin allocation is recommended to accommodate the impacts of enabling
neighboring channels and end-of-life channel loads. Further 0.6 dB of GSNR
margin should be allocated to compensate for probing inaccuracies. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04623v1' target="_blank">http://arxiv.org/pdf/2302.04623v1</a><br> <br> <br> <font size='5'> 282 </font> <div style="text-align: right"> 2023-02-08 22:14:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sentiment analysis and opinion mining on educational data: A survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sentiment analysis AKA opinion mining is one of the most widely used NLP
applications to identify human intentions from their reviews. In the education
sector, opinion mining is used to listen to student opinions and enhance their
learning-teaching practices pedagogically. With advancements in sentiment
annotation techniques and AI methodologies, student comments can be labelled
with their sentiment orientation without much human intervention. In this
review article, (1) we consider the role of emotional analysis in education
from four levels: document level, sentence level, entity level, and aspect
level, (2) sentiment annotation techniques including lexicon-based and
corpus-based approaches for unsupervised annotations are explored, (3) the role
of AI in sentiment analysis with methodologies like machine learning, deep
learning, and transformers are discussed, (4) the impact of sentiment analysis
on educational procedures to enhance pedagogy, decision-making, and evaluation
are presented. Educational institutions have been widely invested to build
sentiment analysis tools and process their student feedback to draw their
opinions and insights. Applications built on sentiment analysis of student
feedback are reviewed in this study. Challenges in sentiment analysis like
multi-polarity, polysemous, negation words, and opinion spam detection are
explored and their trends in the research space are discussed. The future
directions of sentiment analysis in education are discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04359v1' target="_blank">http://arxiv.org/pdf/2302.04359v1</a><br> <br> <br> <font size='5'> 283 </font> <div style="text-align: right"> 2023-02-06 15:43:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: NPV, IRR, PI, PP, and DPP: a unified view</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduces a class of investment project's profitability metrics
that includes the net present value criterion (which labels a project as weakly
profitable if its NPV is nonnegative), the internal rate of return (IRR), the
profitability index (PI), the payback period (PP) and its discounted
counterpart (DPP) as special cases. An axiomatic characterization of this
class, as well as of the mentioned conventional metrics within the class, is
presented. This approach is useful at least in three respects. First, it
suggests a unified interpretation for profitability metrics as measures of
financial stability of a project with respect to a collection of scenarios of
economic environment. Second, it shows that, with the exception of the NPV
criterion, a profitability metric is necessarily incomplete (i.e., there are
incomparable projects). In particular, this implies that any extension of the
IRR to the space of all projects does not meet a set of reasonable conditions.
A similar conclusion is valid for the other mentioned conventional metrics. For
each of these metrics, we provide a complete characterization of pairs of
compatible projects and describe the largest subset of projects to which the
metric can be unambiguously extended. Third, it determines the conditions under
which the use of one metric is superior to the others. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02875v5' target="_blank">http://arxiv.org/pdf/2302.02875v5</a><br> <br> <br> <font size='5'> 284 </font> <div style="text-align: right"> 2023-02-06 14:51:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What may future electricity markets look like?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Should the organization, design and functioning of electricity markets be
taken for granted? Definitely not. While decades of evolution of electricity
markets in countries that committed early to restructure their electric power
sector made us believe that we may have found the right and future-proof model,
the substantially and rapidly evolving context of our power and energy systems
is challenging this idea in many ways. Actually, that situation brings both
challenges and opportunities. Challenges include accommodation of renewable
energy generation, decentralization and support to investment, while
opportunities are mainly that advances in technical and social sciences provide
us with many more options in terms of future market design. We here take a
holistic point of view, by trying to understand where we are coming from with
electricity markets and where we may be going. Future electricity markets
should be made fit for purpose by considering them as a way to organize and
operate a socio-techno-economic system. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02833v2' target="_blank">http://arxiv.org/pdf/2302.02833v2</a><br> <br> <br> <font size='5'> 285 </font> <div style="text-align: right"> 2023-02-05 23:05:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Gradient of Generative AI Release: Methods and Considerations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As increasingly powerful generative AI systems are developed, the release
method greatly varies. We propose a framework to assess six levels of access to
generative AI systems: fully closed; gradual or staged access; hosted access;
cloud-based or API access; downloadable access; and fully open. Each level,
from fully closed to fully open, can be viewed as an option along a gradient.
We outline key considerations across this gradient: release methods come with
tradeoffs, especially around the tension between concentrating power and
mitigating risks. Diverse and multidisciplinary perspectives are needed to
examine and mitigate risk in generative AI systems from conception to
deployment. We show trends in generative system release over time, noting
closedness among large companies for powerful systems and openness among
organizations founded on principles of openness. We also enumerate safety
controls and guardrails for generative systems and necessary investments to
improve future releases. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04844v1' target="_blank">http://arxiv.org/pdf/2302.04844v1</a><br> <br> <br> <font size='5'> 286 </font> <div style="text-align: right"> 2023-02-05 21:36:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Facts of US Firm Scale and Growth 1970-2019: An Illustrated Guide</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This work analyzes data on all public US firms in the 50 year period
1970-2019, and presents 18 stylized facts of their scale, income, growth,
return, investment, and dynamism. Special attention is given to (i) identifying
distributional forms; and (ii) scale effects -- systematic difference between
firms based on their scale of operations. Notable findings are that the
Difference-of-Log-Normals (DLN) distribution has a central role in describing
firm data, scale-dependent heteroskedasticity is rampant, and small firms are
systematically different from large firms. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02485v1' target="_blank">http://arxiv.org/pdf/2302.02485v1</a><br> <br> <br> <font size='5'> 287 </font> <div style="text-align: right"> 2023-02-05 18:44:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ODEWS: The Overdraft Early Warning System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: When a customer overdraws their account and their balance is negative they
are assessed an overdraft fee. Americans pay approximately \$15 billion in
unnecessary overdraft fees a year, often in \$35 increments; users of the Mint
personal finance app pay approximately \$250 million in fees a year in
particular. These overdraft fees are an excessive financial burden and lead to
cascading overdraft fees trapping customers in financial hardship. To address
this problem, we have created an ML-driven overdraft early warning system
(ODEWS) that assesses a customer's risk of overdrafting within the next week
using their banking and transaction data in the Mint app. At-risk customers are
sent an alert so they can take steps to avoid the fee, ultimately changing
their behavior and financial habits. The system deployed resulted in a \$3
million savings in overdraft fees for Mint customers compared to a control
group. Moreover, the methodology outlined here can be generalized to provide
ML-driven personalized financial advice for many different personal finance
goals--increase credit score, build emergency savings fund, pay down debut,
allocate capital for investment. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02455v1' target="_blank">http://arxiv.org/pdf/2302.02455v1</a><br> <br> <br> <font size='5'> 288 </font> <div style="text-align: right"> 2023-02-05 09:52:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment problem for a hybrid pension with intergenerational risk-sharing and longevity trend under model uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the optimal investment problem for a hybrid pension plan
under model uncertainty, where both the contribution and the benefit are
adjusted depending on the performance of the plan. Furthermore, an age and
time-dependent force of mortality and a linear maximum age are considered to
capture the longevity trend. Suppose that the plan manager is ambiguity averse
and is allowed to invest in a risk-free asset and a stock. The plan manager
aims to find optimal investment strategies and optimal intergenerational
risk-sharing arrangements by minimizing the cost of unstable contribution risk,
the cost of unstable benefit risk and discontinuity risk under the worst-case
scenario. By applying the stochastic optimal control approach, closed-form
solutions are derived under a penalized quadratic cost function. Through
numerical analysis and three special cases, we find that the intergeneration
risk-sharing is achieved in our collective hybrid pension plan effectively. And
it also shows that when people live longer, postponing the retirement seems a
feasible way to alleviate the stress of the aging problem. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02351v1' target="_blank">http://arxiv.org/pdf/2302.02351v1</a><br> <br> <br> <font size='5'> 289 </font> <div style="text-align: right"> 2023-02-04 15:48:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: AI advice is becoming increasingly popular, e.g., in investment and medical
treatment decisions. As this advice is typically imperfect, decision-makers
have to exert discretion as to whether actually follow that advice: they have
to "appropriately" rely on correct and turn down incorrect advice. However,
current research on appropriate reliance still lacks a common definition as
well as an operational measurement concept. Additionally, no in-depth
behavioral experiments have been conducted that help understand the factors
influencing this behavior. In this paper, we propose Appropriateness of
Reliance (AoR) as an underlying, quantifiable two-dimensional measurement
concept. We develop a research model that analyzes the effect of providing
explanations for AI advice. In an experiment with 200 participants, we
demonstrate how these explanations influence the AoR, and, thus, the
effectiveness of AI advice. Our work contributes fundamental concepts for the
analysis of reliance behavior and the purposeful design of AI advisors. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02187v3' target="_blank">http://arxiv.org/pdf/2302.02187v3</a><br> <br> <br> <font size='5'> 290 </font> <div style="text-align: right"> 2023-02-03 15:47:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Portfolio Optimisation via the Heston Model Calibrated to Real Asset Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The debate between active and passive investment strategies has been ongoing
for many years and is far from being over. In this paper, we show that the
choice of an optimal portfolio management strategy depends on an investment
climate, which we measure via the parameters of the Heston model calibrated to
the real stock market data. Depending on the values of those parameters, the
passive strategy may namely outperform the active ones or vice versa. The
method is tested on three stock market indices: S\&P500, DAX and WIG20. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01816v1' target="_blank">http://arxiv.org/pdf/2302.01816v1</a><br> <br> <br> <font size='5'> 291 </font> <div style="text-align: right"> 2023-02-03 11:56:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain
technologies and smart contracts, of unique crypto assets on digital art forms
(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,
NFTs have attracted the attention of crypto enthusiasts and investors intent on
placing promising investments in this profitable market. However, the NFT
financial performance prediction has not been widely explored to date.
  In this work, we address the above problem based on the hypothesis that NFT
images and their textual descriptions are essential proxies to predict the NFT
selling prices. To this purpose, we propose MERLIN, a novel multimodal deep
learning framework designed to train Transformer-based language and visual
models, along with graph neural network models, on collections of NFTs' images
and texts. A key aspect in MERLIN is its independence on financial features, as
it exploits only the primary data a user interested in NFT trading would like
to deal with, i.e., NFT images and textual descriptions. By learning dense
representations of such data, a price-category classification task is performed
by MERLIN models, which can also be tuned according to user preferences in the
inference phase to mimic different risk-return investment profiles.
Experimental evaluation on a publicly available dataset has shown that MERLIN
models achieve significant performances according to several financial
assessment criteria, fostering profitable investments, and also beating
baseline machine-learning classifiers based on financial features. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01676v2' target="_blank">http://arxiv.org/pdf/2302.01676v2</a><br> <br> <br> <font size='5'> 292 </font> <div style="text-align: right"> 2023-02-03 03:38:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-channel Autobidding with Budget and ROI Constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In digital online advertising, advertisers procure ad impressions
simultaneously on multiple platforms, or so-called channels, such as Google
Ads, Meta Ads Manager, etc., each of which consists of numerous ad auctions. We
study how an advertiser maximizes total conversion (e.g. ad clicks) while
satisfying aggregate return-on-investment (ROI) and budget constraints across
all channels. In practice, an advertiser does not have control over, and thus
cannot globally optimize, which individual ad auctions she participates in for
each channel, and instead authorizes a channel to procure impressions on her
behalf: the advertiser can only utilize two levers on each channel, namely
setting a per-channel budget and per-channel target ROI. In this work, we first
analyze the effectiveness of each of these levers for solving the advertiser's
global multi-channel problem. We show that when an advertiser only optimizes
over per-channel ROIs, her total conversion can be arbitrarily worse than what
she could have obtained in the global problem. Further, we show that the
advertiser can achieve the global optimal conversion when she only optimizes
over per-channel budgets. In light of this finding, under a bandit feedback
setting that mimics real-world scenarios where advertisers have limited
information on ad auctions in each channels and how channels procure ads, we
present an efficient learning algorithm that produces per-channel budgets whose
resulting conversion approximates that of the global optimal problem. Finally,
we argue that all our results hold for both single-item and multi-item auctions
from which channels procure impressions on advertisers' behalf. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01523v3' target="_blank">http://arxiv.org/pdf/2302.01523v3</a><br> <br> <br> <font size='5'> 293 </font> <div style="text-align: right"> 2023-02-02 22:57:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Insurance Paradigm for Improving Power System Resilience via Distributed Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Extreme events, exacerbated by climate change, pose significant risks to the
energy system and its consumers. However there are natural limits to the degree
of protection that can be delivered from a centralised market architecture.
Distributed energy resources provide resilience to the energy system, but their
value remains inadequately recognized by regulatory frameworks. We propose an
insurance framework to align residual outage risk exposure with locational
incentives for distributed investment. We demonstrate that leveraging this
framework in large-scale electricity systems could improve consumer welfare
outcomes in the face of growing risks from extreme events via investment in
distributed energy. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01456v1' target="_blank">http://arxiv.org/pdf/2302.01456v1</a><br> <br> <br> <font size='5'> 294 </font> <div style="text-align: right"> 2023-02-02 18:44:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: IRIS-HEP Strategic Plan for the Next Phase of Software Upgrades for HL-LHC Physics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The quest to understand the fundamental building blocks of nature and their
interactions is one of the oldest and most ambitious of human scientific
endeavors. CERN's Large Hadron Collider (LHC) represents a huge step forward in
this quest. The discovery of the Higgs boson, the observation of exceedingly
rare decays of $B$ mesons, and stringent constraints on many viable theories of
physics beyond the Standard Model (SM) demonstrate the great scientific value
of the LHC physics program. The next phase of this global scientific project
will be the High-Luminosity LHC (HL-LHC) which will collect data starting circa
2029 and continue through the 2030s. The primary science goal is to search for
physics beyond the SM and, should it be discovered, to study its implications.
In the HL-LHC era, the ATLAS and CMS experiments will record around 100 times
as many collisions as were used to discover the Higgs boson (and at twice the
energy). Both NSF and DOE are making large detector upgrade investments so the
HL-LHC can operate in this high-rate environment. Similar investment in
software R&D for acquiring, managing, processing and analyzing HL-LHC data is
critical to maximize the return-on-investment in the upgraded accelerator and
detectors. This report presents a strategic plan for a possible second 5-year
funded phase (2023 through 2028) for the Institute for Research and Innovation
in Software for High Energy Physics (IRIS-HEP) which will close remaining
software and computing gaps to deliver HL-LHC science. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01317v1' target="_blank">http://arxiv.org/pdf/2302.01317v1</a><br> <br> <br> <font size='5'> 295 </font> <div style="text-align: right"> 2023-02-02 16:53:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Evology: a Market Ecology Agent-Based Model of US Equity Mutual Funds II</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Agent-based models (ABMs) are fit to model heterogeneous, interacting systems
like financial markets. We present the latest advances in Evology: a
heterogeneous, empirically calibrated market ecology agent-based model of the
US stock market. Prices emerge endogenously from the interactions of market
participants with diverse investment behaviours and their reactions to
fundamentals. This approach allows testing trading strategies while accounting
for the interactions of this strategy with other market participants and
conditions. Those early results encourage a closer association between ABMs and
ML algorithms for testing and optimising investment strategies using machine
learning algorithms. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01216v1' target="_blank">http://arxiv.org/pdf/2302.01216v1</a><br> <br> <br> <font size='5'> 296 </font> <div style="text-align: right"> 2023-02-02 16:30:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Online Learning under Budget and ROI Constraints and Applications to Bidding in Non-Truthful Auctions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study online learning problems in which a decision maker has to make a
sequence of costly decisions, with the goal of maximizing their expected reward
while adhering to budget and return-on-investment (ROI) constraints. Previous
work requires the decision maker to know beforehand some specific parameters
related to the degree of strict feasibility of the offline problem. Moreover,
when inputs are adversarial, it requires the existence of a strictly feasible
solution to the offline optimization problem at each round. Both requirements
are unrealistic for practical applications such as bidding in online ad
auctions. We propose a best-of-both-worlds primal-dual framework which
circumvents both assumptions by exploiting the notion of interval regret,
providing guarantees under both stochastic and adversarial inputs. Our proof
techniques can be applied to both input models with minimal modifications,
thereby providing a unified perspective on the two problems. Finally, we show
how to instantiate the framework to optimally bid in various mechanisms of
practical relevance, such as first- and second-price auctions. </font><br> Link: <a href='http://arxiv.org/pdf/2302.01203v2' target="_blank">http://arxiv.org/pdf/2302.01203v2</a><br> <br> <br> <font size='5'> 297 </font> <div style="text-align: right"> 2023-02-02 10:26:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Deep Dive into NFT Whales: A Longitudinal Study of the NFT Trading Ecosystem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: NFT (Non-fungible Token) has drastically increased in its size, accounting
for over \$16.9B of total market capitalization. Despite the rapid growth of
NFTs, this market has not been examined thoroughly from a financial
perspective. In this paper, we conduct methodical analyses to identify NFT
market movers who play a significant role in potentially manipulating and
oscillating NFT values. We collect over 3.8M NFT transaction data from the
Ethereum Blockchain from January 2021 to February 2022 to extract trading
information in line with the NFT lifecycle: (i) mint, (ii) transfer/sale, and
(iii) burn. Based on the size of held NFT values, we classify NFT traders into
three groups (whales, dolphins, and minnows). In total, we analyze 430K traders
from 91 different NFT collection sources. We find that the top 0.1\% of NFT
traders (i.e., whales) drive the NFT market with consistent, high returns. We
then identify and characterize the NFT whales' unique investment strategies
(e.g., mint/sale patterns, wash trading) to empirically understand the whales
in the NFT market for the first time. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09393v1' target="_blank">http://arxiv.org/pdf/2303.09393v1</a><br> <br> <br> <font size='5'> 298 </font> <div style="text-align: right"> 2023-02-02 09:01:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantifying optimal resource allocation strategies for controlling epidemics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Frequent emergence of communicable diseases has been a major concern
worldwide. Lack of sufficient resources to mitigate the disease-burden makes
the situation even more challenging for lower-income countries. Hence, strategy
development towards disease eradication and optimal management of the social
and economic burden has garnered a lot of attention in recent years. In this
context, we quantify the optimal fraction of resources that can be allocated to
two major intervention measures, namely reduction of disease transmission and
improvement of healthcare infrastructure. Our results demonstrate that the
effectiveness of each of the interventions has a significant impact on the
optimal resource allocation in both long-term disease dynamics and outbreak
scenarios. Often allocating resources to both strategies is optimal. For
long-term dynamics, a non-monotonic behavior of optimal resource allocation
with intervention effectiveness is observed which is different from the more
intuitive strategy recommended in case of outbreaks. Further, our result
indicates that the relationship between investment into interventions and the
corresponding outcomes has a decisive role in determining optimal strategies.
Intervention programs with decreasing returns promote the necessity for
resource sharing. Our study provides a fundamental insight into determining the
best response strategy in case of controlling epidemics under
resource-constrained situations. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00960v1' target="_blank">http://arxiv.org/pdf/2302.00960v1</a><br> <br> <br> <font size='5'> 299 </font> <div style="text-align: right"> 2023-02-01 21:25:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-Leverage Puzzle</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, I examine why some firms have zero leverage. I fail to find
evidence that firms are unlevered because of managerial entrenchment since
these firms do not have weaker corporate governance. I reject the hypothesis
that firms become zero-leverage after prolonged periods of high market
valuation, since before levering these firms do not suffer from declining
valuations and continue to issue large amounts of equity. I find strong
evidence in favor of the financial constraints explanation of the zero-leverage
puzzle. Zero-leverage firms appear to be financially constrained using three
different measures of financial constraints. I obtain mixed evidence on the
financial flexibility hypothesis since all-equity firms increase investments
and acquisitions after levering, but the probability of their levering
decreased during the financial crisis. My results suggest that financial
constraints are the first-order the driver of zero-leverage behavior and are
more important than less obvious explanations such as managerial entrenchment. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00761v1' target="_blank">http://arxiv.org/pdf/2302.00761v1</a><br> <br> <br> <font size='5'> 300 </font> <div style="text-align: right"> 2023-02-01 17:17:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Investment Management Game: Extending the Scope of the Notion of Core</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The core is a dominant solution concept in economics and game theory. In this
context, the following question arises, ``How versatile is this solution
concept?'' We note that within game theory, this notion has been used for
profit -- equivalently, cost or utility -- sharing only. In this paper, we show
a completely different use for it: in an {\em investment management game},
under which an agent needs to allocate her money among investment firms in such
a way that {\em in each of exponentially many future scenarios}, sufficient
money is available in the ``right'' firms so she can buy an ``optimal
investment'' for that scenario.
  We study a restriction of this game to {\em perfect graphs} and characterize
its core. Our characterization is analogous to Shapley and Shubik's
characterization of the core of the assignment game. The difference is the
following: whereas their characterization follows from {\em total
unimodularity}, ours follows from {\em total dual integrality}. The latter is
another novelty of our work. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00608v3' target="_blank">http://arxiv.org/pdf/2302.00608v3</a><br> <br> <br> <font size='5'> 301 </font> <div style="text-align: right"> 2023-02-01 14:34:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Subgame-perfect equilibrium strategies for time-inconsistent recursive stochastic control problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study time-inconsistent recursive stochastic control problems. Since for
this class of problems classical optimal controls may fail to exist or to be
relevant in practice, we focus on subgame-perfect equilibrium policies. The
approach followed in our work relies on the stochastic maximum principle: we
adapt the classical spike variation technique to obtain a characterization of
equilibrium strategies in terms of a generalized second-order Hamiltonian
function defined through a pair of backward stochastic differential equations.
The theoretical results are applied in the financial field to finite horizon
investment-consumption policies with non-exponential actualization. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00471v1' target="_blank">http://arxiv.org/pdf/2302.00471v1</a><br> <br> <br> <font size='5'> 302 </font> <div style="text-align: right"> 2023-02-01 13:10:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the follow-up efforts of long-period transiting planet candidates detected with Gaia astrometry</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The class of transiting cold Jupiters, orbiting at $\gtrsim0.5-1.0$ au, is
to-date underpopulated. Probing their atmospheric composition and physical
characteristics is particularly valuable, as it allows for direct comparisons
with the Solar System giant planets. We investigate some aspects of the synergy
between Gaia astrometry and other ground-based and space-borne programs for
detection and characterization of such companions. We carry out numerical
simulations of Gaia observations of systems with one cold transiting gas giant,
using Jovian planets around a sample of nearby low-mass stars as proxies. Using
state-of-the-art orbit fitting tools, we gauge the potential of Gaia astrometry
to predict the time of transit centre $T_c$ for the purpose of follow-up
observations to verify that the companions are indeed transiting. Typical
uncertainties on $T_c$ will be on the order of a few months, reduced to several
weeks for high astrometric signal-to-noise ratios and periods shorter than
$\sim3$ yr. We develop a framework for the combined analysis of Gaia astrometry
and radial-velocity data from representative ground-based campaigns and show
that combined orbital fits would allow to significantly reduce the transit
windows to be searched for, down to about $\pm2$ weeks ($2-\sigma$ level) in
the most favourable cases. These results are achievable with a moderate
investment of observing time ($\sim0.5$ nights per candidate, $\sim50$ nights
for the top 100 candidates), reinforcing the notion that Gaia astrometric
detections of potentially transiting cold giant planets, starting with Data
Release 4, will constitute a valuable sample worthy of synergistic follow-up
efforts with a variety of techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00420v1' target="_blank">http://arxiv.org/pdf/2302.00420v1</a><br> <br> <br> <font size='5'> 303 </font> <div style="text-align: right"> 2023-02-01 11:18:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Autobidding Auctions in the Presence of User Costs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study autobidding ad auctions with user costs, where each bidder is
value-maximizing subject to a return-over-investment (ROI) constraint, and the
seller aims to maximize the social welfare taking into consideration the user's
cost of viewing an ad. We show that in the worst case, the approximation ratio
of social welfare by running the vanilla VCG auctions with user costs could as
bad as 0. To improve the performance of VCG, We propose a new variant of VCG
based on properly chosen cost multipliers, and prove that there exist
auction-dependent and bidder-dependent cost multipliers that guarantee
approximation ratios of 1/2 and 1/4 respectively in terms of the social
welfare. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00377v1' target="_blank">http://arxiv.org/pdf/2302.00377v1</a><br> <br> <br> <font size='5'> 304 </font> <div style="text-align: right"> 2023-01-31 21:59:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Translation Impact in E-commerce Multilingual Search</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Previous work suggests that performance of cross-lingual information
retrieval correlates highly with the quality of Machine Translation. However,
there may be a threshold beyond which improving query translation quality
yields little or no benefit to further improve the retrieval performance. This
threshold may depend upon multiple factors including the source and target
languages, the existing MT system quality and the search pipeline. In order to
identify the benefit of improving an MT system for a given search pipeline, we
investigate the sensitivity of retrieval quality to the presence of different
levels of MT quality using experimental datasets collected from actual traffic.
We systematically improve the performance of our MT systems quality on language
pairs as measured by MT evaluation metrics including Bleu and Chrf to determine
their impact on search precision metrics and extract signals that help to guide
the improvement strategies. Using this information we develop techniques to
compare query translations for multiple language pairs and identify the most
promising language pairs to invest and improve. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00119v1' target="_blank">http://arxiv.org/pdf/2302.00119v1</a><br> <br> <br> <font size='5'> 305 </font> <div style="text-align: right"> 2023-01-31 21:08:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detecting Harmful Agendas in News Articles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Manipulated news online is a growing problem which necessitates the use of
automated systems to curtail its spread. We argue that while misinformation and
disinformation detection have been studied, there has been a lack of investment
in the important open challenge of detecting harmful agendas in news articles;
identifying harmful agendas is critical to flag news campaigns with the
greatest potential for real world harm. Moreover, due to real concerns around
censorship, harmful agenda detectors must be interpretable to be effective. In
this work, we propose this new task and release a dataset, NewsAgendas, of
annotated news articles for agenda identification. We show how interpretable
systems can be effective on this task and demonstrate that they can perform
comparably to black-box models. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00102v1' target="_blank">http://arxiv.org/pdf/2302.00102v1</a><br> <br> <br> <font size='5'> 306 </font> <div style="text-align: right"> 2023-01-31 12:00:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utility-based indifference pricing of pure endowments in a Markov-modulated market model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper we study exponential utility indifference pricing of pure
endowment policies in a stochastic-factor model for an insurance company, which
can also invest in a financial market. Specifically, we propose a modeling
framework where the hazard rate is described by an observable general diffusion
process and the risky asset price evolves as a jump diffusion affected by a
continuous-time finite-state Markov chain representing regimes of the economy.
Using the classical stochastic control approach based on the
Hamilton-Jacobi-Bellman equation, we describe the optimal investment strategies
with and without the insurance derivative and characterize the indifference
price in terms of a classical solution to a linear PDE. We also provide its
probabilistic representation via an extension of the Feynman-Kac formula show
that it satisfies a final value problem. Furthermore, we also discuss the
indifference price for a portfolio of insurance policies and for a term life
insurance. Finally, some numerical experiments are performed to address
sensitivity analyses. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13575v1' target="_blank">http://arxiv.org/pdf/2301.13575v1</a><br> <br> <br> <font size='5'> 307 </font> <div style="text-align: right"> 2023-01-30 21:59:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study a game between autobidding algorithms that compete in an online
advertising platform. Each autobidder is tasked with maximizing its
advertiser's total value over multiple rounds of a repeated auction, subject to
budget and/or return-on-investment constraints. We propose a gradient-based
learning algorithm that is guaranteed to satisfy all constraints and achieves
vanishing individual regret. Our algorithm uses only bandit feedback and can be
used with the first- or second-price auction, as well as with any
"intermediate" auction format. Our main result is that when these autobidders
play against each other, the resulting expected liquid welfare over all rounds
is at least half of the expected optimal liquid welfare achieved by any
allocation. This holds whether or not the bidding dynamics converges to an
equilibrium and regardless of the correlation structure between advertiser
valuations. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13306v2' target="_blank">http://arxiv.org/pdf/2301.13306v2</a><br> <br> <br> <font size='5'> 308 </font> <div style="text-align: right"> 2023-01-30 08:58:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Planning for Electrical Collector System of Offshore Wind Farm with Double-sided Ring Topology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a planning method for offshore wind farm electrical collector
system (OWF-ECS) with double-sided ring topology meeting the "N-1" criterion on
cable faults, in which the submarine cables layout of OWF is optimized
considering cable length and power losses. The proposed mixed-integer quadratic
programming (MIQP) model is based on the Capacitated Vehicle Routing Problem
(CVRP) formulation and power network expansion planning, which could
approximate the power losses in OWF-ECS. In addition, cross-avoidance
constraints are proposed to avoid crossing cables, and the minimum k-degree
center tree model is included to improve the convergence. Case studies on OWFs
with 30 and 62 WTs demonstrate the effectiveness of the proposed method.
Considering the potential outage cost in the radial topology, the total cost of
the planning result is reduced by up to 25.9% with reliability improvement. The
cable investment is reduced by 4%~8% with the proposed method compared with
conventional heuristic methods and Google OR-tools. The proposed method/model
can also achieve acceptable computation efficiency and OWF-ECS planning results
with good optimality. Moreover, it could be solved by modern commercial
solvers/optimization software, thus it's easy to use even for large-scale OWF. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12734v1' target="_blank">http://arxiv.org/pdf/2301.12734v1</a><br> <br> <br> <font size='5'> 309 </font> <div style="text-align: right"> 2023-01-29 04:01:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Long-Term Modeling of Financial Machine Learning for Active Portfolio Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the practical business of asset management by investment trusts and the
like, the general practice is to manage over the medium to long term owing to
the burden of operations and increase in transaction costs with the increase in
turnover ratio. However, when machine learning is used to construct a
management model, the number of learning data decreases with the increase in
the long-term time scale; this causes a decline in the learning precision.
Accordingly, in this study, data augmentation was applied by the combined use
of not only the time scales of the target tasks but also the learning data of
shorter term time scales, demonstrating that degradation of the generalization
performance can be inhibited even if the target tasks of machine learning have
long-term time scales. Moreover, as an illustration of how this data
augmentation can be applied, we conducted portfolio management in which machine
learning of a multifactor model was done by an autoencoder and mispricing was
used from the estimated theoretical values. The effectiveness could be
confirmed in not only the stock market but also the FX market, and a
general-purpose management model could be constructed in various financial
markets. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12346v1' target="_blank">http://arxiv.org/pdf/2301.12346v1</a><br> <br> <br> <font size='5'> 310 </font> <div style="text-align: right"> 2023-01-28 17:26:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The impact of surplus sharing on the outcomes of specific investments under negotiated transfer pricing: An agent-based simulation with fuzzy Q-learning agents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper focuses on specific investments under negotiated transfer pricing.
Reasons for transfer pricing studies are primarily to find conditions that
maximize the firm's overall profit, especially in cases with bilateral trading
problems with specific investments. However, the transfer pricing problem has
been developed in the context where managers are fully individual rational
utility maximizers. The underlying assumptions are rather heroic and, in
particular, how managers process information under uncertainty, do not
perfectly match with human decision-making behavior. Therefore, this paper
relaxes key assumptions and studies whether cognitively bounded agents achieve
the same results as fully rational utility maximizers and, in particular,
whether the recommendations on managerial-compensation arrangements and
bargaining infrastructures are designed to maximize headquarters' profit in
such a setting. Based on an agent-based simulation with fuzzy Q-learning
agents, it is shown that in case of symmetric marginal cost parameters, myopic
fuzzy Q-learning agents invest only as much as in the classic hold-up problem,
while non-myopic fuzzy Q-learning agents invest optimally. However, in
scenarios with non-symmetric marginal cost parameters, a deviation from the
previously recommended surplus sharing rules can lead to higher investment
decisions and, thus, to an increase in the firm's overall profit. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12255v1' target="_blank">http://arxiv.org/pdf/2301.12255v1</a><br> <br> <br> <font size='5'> 311 </font> <div style="text-align: right"> 2023-01-27 21:05:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-powered weigh-in-motion system combining vibration energy harvesting and self-sensing composite pavements</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Overloaded vehicles are the primary cause of accelerated degradation of road
infrastructures. In this context, although weigh-in-motion (WIM) systems are
most efficient to enforce weight regulations, current technologies require
costly investments limiting their extensive implementation. Recent advances in
multifunctional composites enabled cost-efficient alternatives in the form of
smart pavements. Nevertheless, the need for a stable power supply still
represents a major practical limitation. This work presents a novel
proof-of-concept self-sustainable WIM technology combining smart pavements and
vibration-based energy harvesting (EH). The feasibility of piezoelectric
bimorph cantilevered beams to harvest traffic-induced vibrations is firstly
investigated, followed by the demonstration of the proposed technology under
laboratory conditions. The main original contributions of this work comprise
(i) the development of a new self-powered data acquisition system, (ii) a novel
approach for the fabrication and electromechanical testing of the
piezoresistive composite pavement, and (iii) laboratory feasibility analysis of
the developed EH unit to conduct traffic load identification through electrical
resistivity measurements of the smart pavement. While the presented results
conclude the need for dense EH networks or combinations of different EH
technologies to attain complete self-sustainability, this work represents an
initial feasibility evidence paving the way towards the development of
self-powered low-cost WIM systems. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06388v2' target="_blank">http://arxiv.org/pdf/2302.06388v2</a><br> <br> <br> <font size='5'> 312 </font> <div style="text-align: right"> 2023-01-25 16:21:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Overview on Cloud Distributed Databases for Business Environments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cloud-based distributed databases are a popular choice for many current
applications, especially those that run over the Internet. By incorporating
distributed database systems within cloud environments, it has enabled
businesses to scale operations to a global level, all while achieving desired
standards of system reliability, availability, and responsiveness. Cloud
providers offer infrastructure and management tools for distributed databases
as Database-as-a-Service (DBaaS), re-purposing the investment by businesses
towards database services. This paper reviews the functionality of these
services, by highlighting Amazon Relational Data Service (RDS), suited for
handling relational distributed databases. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10673v2' target="_blank">http://arxiv.org/pdf/2301.10673v2</a><br> <br> <br> <font size='5'> 313 </font> <div style="text-align: right"> 2023-01-25 12:58:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Early life exposure to measles and later-life outcomes: Evidence from the introduction of a vaccine</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Until the mid 1960s, the UK experienced regular measles epidemics, with the
vast majority of children being infected in early childhood. The introduction
of a measles vaccine substantially reduced its incidence. The first part of
this paper examines the long-term human capital and health effects of this
change in the early childhood disease environment. The second part investigates
interactions between the vaccination campaign and individuals' endowments as
captured using molecular genetic data, shedding light on complementarities
between public health investments and individual endowments. We use two
identification approaches, based on the nationwide introduction of the vaccine
in 1968 and local vaccination trials in 1966. Our results show that exposure to
the vaccination in early childhood positively affects adult height, but only
among those with high genetic endowments for height. We find no effects on
years of education; neither a direct effect, nor evidence of complementarities. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10558v1' target="_blank">http://arxiv.org/pdf/2301.10558v1</a><br> <br> <br> <font size='5'> 314 </font> <div style="text-align: right"> 2023-01-25 12:14:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Educational Game on Cryptocurrency Investment: Using Microeconomic Decision Making to Understand Macroeconomics Principles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Gamification is an effective strategy for motivating and engaging users,
which is grounded in business, marketing, and management by designing games in
nongame contexts. Gamifying education, which consists of the design and study
of educational games, is an emerging trend. However, the existing classroom
games for understanding macroeconomics have weak connections to the
microfoundations of individual decision-making. We design an educational game
on cryptocurrency investment for understanding macroeconomic concepts in
microeconomic decisions. We contribute to the literature by designing
game-based learning that engages students in understanding macroeconomics in
incentivized individual investment decisions. Our game can be widely
implemented in online, in-person, and hybrid classrooms. We also reflect on
strategies for improving the user experience for future educational game
implementations. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10541v3' target="_blank">http://arxiv.org/pdf/2301.10541v3</a><br> <br> <br> <font size='5'> 315 </font> <div style="text-align: right"> 2023-01-25 03:37:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey of Process-Oriented Data Science and Analytics for supporting Business Process Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Process analytics approaches allow organizations to support the practice of
Business Process Management and continuous improvement by leveraging all
process-related data to extract knowledge, improve process performance and
support decision-making across the organization. Process execution data once
collected will contain hidden insights and actionable knowledge that are of
considerable business value enabling firms to take a data-driven approach for
identifying performance bottlenecks, reducing costs, extracting insights and
optimizing the utilization of available resources. Understanding the properties
of 'current deployed process' (whose execution trace is often available in
these logs), is critical to understanding the variation across the process
instances, root-causes of inefficiencies and determining the areas for
investing improvement efforts. In this survey, we discuss various methods that
allow organizations to understand the behaviour of their processes, monitor
currently running process instances, predict the future behavior of those
instances and provide better support for operational decision-making across the
organization. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10398v1' target="_blank">http://arxiv.org/pdf/2301.10398v1</a><br> <br> <br> <font size='5'> 316 </font> <div style="text-align: right"> 2023-01-24 23:29:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Need for a Near-Earth Object Characterization Constellation in Low-Earth Orbit</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In 2005, the United States Congress passed a bill mandating the detection,
tracking, cataloguing and characterization of 90\% of the 140 meter and larger
near-Earth objects (NEOs) by 2020. At the deadline $\sim$35\% were detected,
tracked and catalogued, but only a small fraction were characterized. At the
present rate, it will take 40 years to meet the detection mandate, and there
are insufficient global facilities dedicated to NEO characterization to come
close to the characterization threshold. The major surveys focus mainly on
detection and initial orbit determination, which must be refined in order to
fully be tracked and catalogued. Characterization requires observations
spanning multiple wavelengths, cadences, and instruments, so it is challenging
for observers to acquire the requisite data in a timely manner for planetary
defense. Two upcoming surveys will easily meet the 90\% threshold for
detection, but each will require separate facilities to tip and queue to refine
orbits and characterize new discoveries, and they will provide too many
discoveries for ground and space-based assets to keep up with. Here, I argue
for a constellation of proliferating small satellites carrying visible and
infrared sensors that would offer the needed coverage and flexibility to follow
up detections from current and upcoming surveys in a timely manner. Such a
constellation would enable NASA to move beyond the detection focused
investments and fully meet the 2005 Congressional mandate. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10348v2' target="_blank">http://arxiv.org/pdf/2301.10348v2</a><br> <br> <br> <font size='5'> 317 </font> <div style="text-align: right"> 2023-01-24 08:09:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Macroeconomic forecasting and sovereign risk assessment using deep learning techniques</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this study, we propose a novel approach of nowcasting and forecasting the
macroeconomic status of a country using deep learning techniques. We focus
particularly on the US economy but the methodology can be applied also to other
economies. Specifically US economy has suffered a severe recession from 2008 to
2010 which practically breaks out conventional econometrics model attempts.
Deep learning has the advantage that it models all macro variables
simultaneously taking into account all interdependencies among them and
detecting non-linear patterns which cannot be easily addressed under a
univariate modelling framework. Our empirical results indicate that the deep
learning methods have a superior out-of-sample performance when compared to
traditional econometric techniques such as Bayesian Model Averaging (BMA).
Therefore our results provide a concise view of a more robust method for
assessing sovereign risk which is a crucial component in investment and
monetary decisions. </font><br> Link: <a href='http://arxiv.org/pdf/2301.09856v1' target="_blank">http://arxiv.org/pdf/2301.09856v1</a><br> <br> <br> <font size='5'> 318 </font> <div style="text-align: right"> 2023-01-24 04:13:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Demystifying NFT Promotion and Phishing Scams</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The popularity and hype around purchasing digital assets such as art, video,
and music in the form of Non-fungible tokens (NFTs) has rapidly made them a
lucrative investment opportunity, with NFT-based sales surpassing $25B in 2021
alone. However, the volatility and scarcity of NFTs, combined with the general
lack of familiarity with the technical aspects of this ecosystem, encourage the
spread of several scams. The success of an NFT is majorly impacted by its
online virality. There have been sparse reports about scammers emulating this
virality by either promoting their fraudulent NFT projects on social media or
imitating other popular NFT projects. This paper presents a longitudinal
analysis of 439 unique Twitter accounts that consistently promote fraudulent
NFT collections through giveaway competitions and 1,028 NFT phishing attacks.
Our findings indicate that most accounts interacting with these promotions are
bots, which can rapidly increase the popularity of the fraudulent NFT
collections by inflating their likes, followers, and retweet counts. This leads
to significant engagement from real users, who then proceed to invest in the
scams. On the other hand, we identify two novel attack vectors which are
utilized by NFT phishing scams to steal funds and digital assets from the
victim's wallet. We also identify several gaps in the prevalent anti-phishing
ecosystem by evaluating the performance of popular anti-phishing blocklists and
security tools against NFT phishing attacks. We utilize our findings to develop
a machine learning classifier that can automatically detect NFT phishing scams
at scale. </font><br> Link: <a href='http://arxiv.org/pdf/2301.09806v2' target="_blank">http://arxiv.org/pdf/2301.09806v2</a><br> <br> <br> <font size='5'> 319 </font> <div style="text-align: right"> 2023-01-23 14:08:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Energy Worker Profiler from Technologies to Skills to Realize Energy Efficiency in Manufacturing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, the manufacturing sector has been responsible for nearly 55
percent of total energy consumption, inducing a major impact on the global
ecosystem. Although stricter regulations, restrictions on heavy manufacturing
and technological advances are increasing its sustainability, zero-emission and
fuel-efficient manufacturing is still considered a utopian target. In
parallel,companies that have invested in digital innovation now need to align
their internal competencies to maximize their return on investment. Moreover, a
primary feature of Industry 4.0 is the digitization of production processes,
which offers the opportunity to optimize energy consumption. However, given the
speed with which innovation manifests itself, tools capable of measuring the
impact that technology is having on digital and green professions and skills
are still being designed. In light of the above, in this article we present the
Worker Profiler, a software designed to map the skills currently possessed by
workers, identifying misalignment with those they should ideally possess to
meet the renewed demands that digital innovation and environmental preservation
impose. The creation of the Worker Profiler consists of two steps: first, the
authors inferred the key technologies and skills for the area of interest,
isolating those with markedly increasing patent trends and identifying green
and digital enabling skills and occupations. Thus, the software was designed
and implemented at the user-interface level. The output of the self-assessment
is the definition of the missing digital and green skills and the job roles
closest to the starting one in terms of current skills; both the results enable
the definition of a customized retraining strategy. The tool has shown evidence
of being user-friendly, effective in identifying skills gaps and easily
adaptable to other contexts. </font><br> Link: <a href='http://arxiv.org/pdf/2301.09445v1' target="_blank">http://arxiv.org/pdf/2301.09445v1</a><br> <br> <br> <font size='5'> 320 </font> <div style="text-align: right"> 2023-01-22 13:40:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Selecting a suitable Parallel Label-propagation based algorithm for Disjoint Community Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Community detection is an essential task in network analysis as it helps
identify groups and patterns within a network. High-speed community detection
algorithms are necessary to analyze large-scale networks in a reasonable amount
of time. Researchers have made significant contributions in the development of
high-speed community detection algorithms, particularly in the area of
label-propagation based disjoint community detection. These algorithms have
been proven to be highly effective in analyzing large-scale networks in a
reasonable amount of time. However, it is important to evaluate the performance
and accuracy of these existing methods to determine which algorithm is best
suited for a particular type of network and specific research problem. In this
report, we investigate the RAK, COPRA, and SLPA, three label-propagation-based
static community discovery techniques. We pay close attention to each
algorithm's minute details as we implement both its single-threaded and
multi-threaded OpenMP-based variants, making any necessary adjustments or
optimizations and obtaining the right parameter values. The RAK algorithm is
found to perform well with a tolerance of 0.05 and OpenMP-based strict RAK with
12 threads was 6.75x faster than the sequential non-strict RAK. The COPRA
algorithm works well with a single label for road networks and max labels of
4-16 for other classes of graphs. The SLPA algorithm performs well with
increasing memory size, but overall doesn't offer a favourable return on
investment. The RAK algorithm is recommended for label-propagation based
disjoint community detection. </font><br> Link: <a href='http://arxiv.org/pdf/2301.09125v1' target="_blank">http://arxiv.org/pdf/2301.09125v1</a><br> <br> <br> <font size='5'> 321 </font> <div style="text-align: right"> 2023-01-21 06:31:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Corporate Culture and Organizational Fragility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Complex organizations accomplish tasks through many steps of collaboration
among workers. Corporate culture supports collaborations by establishing norms
and reducing misunderstandings. Because a strong corporate culture relies on
costly, voluntary investments by many workers, we model it as an organizational
public good, subject to standard free-riding problems, which become severe in
large organizations. Our main finding is that voluntary contributions to
culture can nevertheless be sustained, because an organization's equilibrium
productivity is endogenously highly sensitive to individual contributions.
However, the completion of complex tasks is then necessarily fragile to small
shocks that damage the organization's culture. </font><br> Link: <a href='http://arxiv.org/pdf/2301.08907v1' target="_blank">http://arxiv.org/pdf/2301.08907v1</a><br> <br> <br> <font size='5'> 322 </font> <div style="text-align: right"> 2023-01-19 20:24:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Environmentally-Extended Input-Output analyses efficiently sketch large-scale environmental transition plans -- illustration by Canada's road industry</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Industries struggle to build robust environmental transition plans as they
lack the tools to quantify their ecological responsibility over their value
chain. Companies mostly turn to sole greenhouse gas (GHG) emissions reporting
or time-intensive Life Cycle Assessment (LCA), while Environmentally-Extended
Input-Output (EEIO) analysis is more efficient on a wider scale. We illustrate
EEIO analysis usefulness to sketch transition plans on the example of Canada s
road industry - estimation of national environmental contributions, most
important environmental issues, main potential transition levers of the sector,
and metrics prioritization for green purchase plans). To do so, openIO-Canada,
a new Canadian EEIO database, coupled with IMPACT World plus v1.30-1.48
characterization method, provides a multicriteria environmental diagnosis of
Canada s economy. The road industry generates a limited impact (0.5-1.8
percent) but must reduce the environmental burden from material purchases -
mainly concrete and asphalt products - through green purchase plans and
eco-design and invest in new machinery powered with cleaner energies such as
low-carbon electricity or bioenergies. EEIO analysis also captures impacts
often neglected in process-based pavement LCAs - amortization of capital goods,
staff consumptions, and services - and shows some substantial impacts
advocating for enlarging system boundaries in standard LCA. Yet, pavement
construction and maintenance only explain 5 percent of the life cycle carbon
footprint of Canada s road network, against 95 percent for the roads usage.
Thereby, a carbon-neutral pathway for the road industry must first focus on
reducing vehicle consumption and wear through better design and maintenance of
roads (...) </font><br> Link: <a href='http://arxiv.org/pdf/2301.08302v1' target="_blank">http://arxiv.org/pdf/2301.08302v1</a><br> <br> <br> <font size='5'> 323 </font> <div style="text-align: right"> 2023-01-19 10:55:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From prosumer to flexumer: Case study on the value of flexibility in decarbonizing the multi-energy system of a manufacturing company</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Digitalization and sector coupling enable companies to turn into flexumers.
By using the flexibility of their multi-energy system (MES), they reduce costs
and carbon emissions while stabilizing the electricity system. However, to
identify the necessary investments in energy conversion and storage
technologies to leverage demand response (DR) potentials, companies need to
assess the value of flexibility. Therefore, this study quantifies the
flexibility value of a production company's MES by optimizing the synthesis,
design, and operation of a decarbonizing MES considering self-consumption
optimization, peak shaving, and integrated DR based on hourly prices and carbon
emission factors (CEFs). The detailed case study of a beverage company in
northern Germany considers vehicle-to-X of powered industrial trucks,
power-to-heat on multiple temperatures, wind turbines, photovoltaic systems,
and energy storage systems (thermal energy, electricity, and hydrogen). We
propose and apply novel data-driven metrics to evaluate the intensity of
price-based and CEF-based DR. The results reveal that flexibility usage reduces
decarbonization costs (by 19-80% depending on electricity and carbon removal
prices), total annual costs, operating carbon emissions, energy-weighted
average prices and CEFs, and fossil energy dependency. The results also suggest
that a net-zero operational carbon emission MES requires flexibility, which, in
an economic case, is provided by a combination of different flexible
technologies and storage systems that complement each other. While the value of
flexibility depends on various market and consumer-specific factors such as
electricity or carbon removal prices, this study highlights the importance of
demand flexibility for the decarbonization of MESs. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07997v1' target="_blank">http://arxiv.org/pdf/2301.07997v1</a><br> <br> <br> <font size='5'> 324 </font> <div style="text-align: right"> 2023-01-19 01:04:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Restless multi-arm bandits (RMABs) is a popular decision-theoretic framework
that has been used to model real-world sequential decision making problems in
public health, wildlife conservation, communication systems, and beyond.
Deployed RMAB systems typically operate in two stages: the first predicts the
unknown parameters defining the RMAB instance, and the second employs an
optimization algorithm to solve the constructed RMAB instance.
  In this work we provide and analyze the results from a first-of-its-kind
deployment of an RMAB system in public health domain, aimed at improving
maternal and child health. Our analysis is focused towards understanding the
relationship between prediction accuracy and overall performance of deployed
RMAB systems. This is crucial for determining the value of investing in
improving predictive accuracy towards improving the final system performance,
and is useful for diagnosing, monitoring deployed RMAB systems.
  Using real-world data from our deployed RMAB system, we demonstrate that an
improvement in overall prediction accuracy may even be accompanied by a
degradation in the performance of RMAB system -- a broad investment of
resources to improve overall prediction accuracy may not yield expected
results. Following this, we develop decision-focused evaluation metrics to
evaluate the predictive component and show that it is better at explaining
(both empirically and theoretically) the overall performance of a deployed RMAB
system. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07835v1' target="_blank">http://arxiv.org/pdf/2301.07835v1</a><br> <br> <br> <font size='5'> 325 </font> <div style="text-align: right"> 2023-01-18 19:16:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On solving decision and risk management problems subject to uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Uncertainty is a pervasive challenge in decision and risk management and it
is usually studied by quantification and modeling. Interestingly, engineers and
other decision makers usually manage uncertainty with strategies such as
incorporating robustness, or by employing decision heuristics. The focus of
this paper is then to develop a systematic understanding of such strategies,
determine their range of application, and develop a framework to better employ
them.
  Based on a review of a dataset of 100 decision problems, this paper found
that many decision problems have pivotal properties, i.e. properties that
enable solution strategies, and finds 14 such properties. Therefore, an analyst
can first find these properties in a given problem, and then utilize the
strategies they enable. Multi-objective optimization methods could be used to
make investment decisions quantitatively. The analytical complexity of decision
problems can also be scored by evaluating how many of the pivotal properties
are available. Overall, we find that in the light of pivotal properties,
complex problems under uncertainty frequently appear surprisingly tractable. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10244v1' target="_blank">http://arxiv.org/pdf/2301.10244v1</a><br> <br> <br> <font size='5'> 326 </font> <div style="text-align: right"> 2023-01-18 17:32:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Navigating the energy trilemma during geopolitical and environmental crises</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There are many indicators of energy security. Few measure what really matters
-- affordable and reliable energy supply -- and the trade-offs between the two.
Reliability is physical, affordability is economic. Russia's latest invasion of
Ukraine highlights some of the problems with energy security, from long-term
contracts being broken to supposedly secure supplies being diverted to retired
power plants being recommissioned to spillovers to other markets. The
transition to carbon-free energy poses new challenges for energy security, from
a shift in dependence from some resources (coal, oil, gas) to others (rare
earths, wind, sunshine) to substantial redundancies in the energy capital stock
to undercapitalized energy companies, while regulatory uncertainty deters
investment. Renewables improve energy security in one dimension, but worsen it
in others, particularly long spells of little wind. Security problems with rare
earths and borrowed capital are less pronounced, as stock rather than flow. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07671v1' target="_blank">http://arxiv.org/pdf/2301.07671v1</a><br> <br> <br> <font size='5'> 327 </font> <div style="text-align: right"> 2023-01-18 05:40:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative Portfolio Optimization with Attention-Powered Sequential Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The dynamic portfolio construction problem requires dynamic modeling of the
joint distribution of multivariate stock returns. To achieve this, we propose a
dynamic generative factor model which uses random variable transformation as an
implicit way of distribution modeling and relies on the Attention-GRU network
for the dynamic modeling. The proposed model captures the dynamic dependence
among multivariate stock returns, especially focusing on the tail-side
properties. We also propose a two-step iterative algorithm to train the model
and then predict the time-varying model parameters (including the
time-invariant tail parameters). At each time, we can easily simulate new
samples from the learned generative model, and we further perform CVaR
portfolio optimization with the simulated samples to form a dynamic portfolio
strategy. The numerical experiment on stock data shows that our strategy using
the proposed model leads to wiser investments that promise higher rewards while
presenting lower tail risks and smaller maximum drawdowns. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07318v2' target="_blank">http://arxiv.org/pdf/2301.07318v2</a><br> <br> <br> <font size='5'> 328 </font> <div style="text-align: right"> 2023-01-18 05:23:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Auctions without commitment in the auto-bidding world</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Advertisers in online ad auctions are increasingly using auto-bidding
mechanisms to bid into auctions instead of directly bidding their value
manually. One prominent auto-bidding format is the target cost-per-acquisition
(tCPA) which maximizes the volume of conversions subject to a
return-of-investment constraint. From an auction theoretic perspective however,
this trend seems to go against foundational results that postulate that for
profit-maximizing bidders, it is optimal to use a classic bidding system like
marginal CPA (mCPA) bidding rather than using strategies like tCPA.
  In this paper we rationalize the adoption of such seemingly sub-optimal
bidding within the canonical quasi-linear framework. The crux of the argument
lies in the notion of commitment. We consider a multi-stage game where first
the auctioneer declares the auction rules; then bidders select either the tCPA
or mCPA bidding format and then, if the auctioneer lacks commitment, it can
revisit the rules of the auction (e.g., may readjust reserve prices depending
on the observed bids). Our main result is that so long as a bidder believes
that the auctioneer lacks commitment to follow the rule of the declared auction
then the bidder will make a higher profit by choosing the tCPA format over the
mCPA format.
  We then explore the commitment consequences for the auctioneer. In a
simplified version of the model where there is only one bidder, we show that
the tCPA subgame admits a credible equilibrium while the mCPA format does not.
That is, when the bidder chooses the tCPA format the auctioneer can credibly
implement the auction rules announced at the beginning of the game. We also
show that, under some mild conditions, the auctioneer's revenue is larger when
the bidder uses the tCPA format rather than mCPA. We further quantify the value
for the auctioneer to be able to commit to the declared auction rules. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07312v2' target="_blank">http://arxiv.org/pdf/2301.07312v2</a><br> <br> <br> <font size='5'> 329 </font> <div style="text-align: right"> 2023-01-17 12:17:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generalizing Impermanent Loss on Decentralized Exchanges with Constant Function Market Makers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Liquidity providers are essential for the function of decentralized exchanges
to ensure liquidity takers can be guaranteed a counterparty for their trades.
However, liquidity providers investing in liquidity pools face many risks, the
most prominent of which is impermanent loss. Currently, analysis of this metric
is difficult to conduct due to different market maker algorithms, fee
structures and concentrated liquidity dynamics across the various exchanges. To
this end, we provide a framework to generalize impermanent loss for multiple
asset pools obeying any constant function market maker with optional
concentrated liquidity. We also discuss how pool fees fit into the framework,
and identify the condition for which liquidity provisioning becomes profitable
when earnings from trading fees exceed impermanent loss. Finally, we
demonstrate the utility and generalizability of this framework with simulations
in BalancerV2 and UniswapV3. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06831v1' target="_blank">http://arxiv.org/pdf/2301.06831v1</a><br> <br> <br> <font size='5'> 330 </font> <div style="text-align: right"> 2023-01-17 02:39:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: UMIRobot: An Open-{Software, Hardware} Low-Cost Robotic Manipulator for Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Robot teleoperation has been studied for the past 70 years and is relevant in
many contexts, such as in the handling of hazardous materials and telesurgery.
The COVID19 pandemic has rekindled interest in this topic, but the existing
robotic education kits fail short of being suitable for teleoperated robotic
manipulator learning. In addition, the global restrictions of motion motivated
large investments in online/hybrid education. In this work, a newly developed
robotics education kit and its ecosystem are presented which is used as the
backbone of an online/hybrid course in teleoperated robots. The students are
assembled into teams, design, fabricate, and control a master device and
gripper, and compete in a teleoperation challenge. The kit is low cost (<
100USD), which allows higher-learning institutions to provide one kit per
student and they can learn in a risk-free environment. As of now, 53 such kits
have been assembled and sent to course participants in eight countries. As
major success stories, we show an example of gripper and master designed for
the proposed course. In addition, we show a teleoperated task between Japan and
Bangladesh executed by course participants. Design files, videos, and more
information available at https://mmmarinho.github.io/UMIRobot/ </font><br> Link: <a href='http://arxiv.org/pdf/2301.06668v1' target="_blank">http://arxiv.org/pdf/2301.06668v1</a><br> <br> <br> <font size='5'> 331 </font> <div style="text-align: right"> 2023-01-16 21:57:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does Spending More Always Ensure Higher Cooperation? An Analysis of Institutional Incentives on Heterogeneous Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Humans have developed considerable machinery used at scale to create policies
and to distribute incentives, yet we are forever seeking ways in which to
improve upon these, our institutions. Especially when funding is limited, it is
imperative to optimise spending without sacrificing positive outcomes, a
challenge which has often been approached within several areas of social, life
and engineering sciences. These studies often neglect the availability of
information, cost restraints, or the underlying complex network structures,
which define real-world populations. Here, we have extended these models,
including the aforementioned concerns, but also tested the robustness of their
findings to stochastic social learning paradigms. Akin to real-world decisions
on how best to distribute endowments, we study several incentive schemes, which
consider information about the overall population, local neighbourhoods, or the
level of influence which a cooperative node has in the network, selectively
rewarding cooperative behaviour if certain criteria are met. Following a
transition towards a more realistic network setting and stochastic behavioural
update rule, we found that carelessly promoting cooperators can often lead to
their downfall in socially diverse settings. These emergent cyclic patterns not
only damage cooperation, but also decimate the budgets of external investors.
Our findings highlight the complexity of designing effective and cogent
investment policies in socially diverse populations. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06620v1' target="_blank">http://arxiv.org/pdf/2301.06620v1</a><br> <br> <br> <font size='5'> 332 </font> <div style="text-align: right"> 2023-01-16 21:53:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sustainability and coordination in a socially responsible supply chain using a combined incentive contract and a social marketing strategy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Due to the growing concerns for sustainable development, supply chains seek
to invest in social sustainability issues to seize more market share in today's
competitive business environment. This study aims to develop a coordination
scheme for a manufacturer-retailer supply chain (SC) contributing to social
donation (SD) activity under a cause-related marketing (CRM) campaign. In the
presence of consumer social awareness (CSA), the manufacturer notices consumers
through some activities (i.e. labelling) that he participates in a CRM campaign
by donating a proportion of the retail price to a cause whenever a consumer
makes a purchase. In this study, the market demand depends on the retail price,
the retailer's stock level and donation size. The proposed problem is designed
under three decision-making systems. Firstly, a decentralized decision-making
system (traditional structure), where the SC's members aim to optimize their
profits regardless of the other member's profitability, is investigated. Then,
the problem is designed under a centralized decision-making system to obtain
the best values of the retail price and replenishment decisions from the entire
SC perspective. Afterwards, an incentive mechanism based on a cost and
revenue-sharing (RCS) factor is developed in the coordination system to
persuade the SC members to accept the optimal results of the centralized system
without suffering any profit loss. Moreover, the surplus profit obtained in the
centralized system is divided between the members based on their bargaining
power. The numerical investigations and the blocked decision-making on SD
activity are presented to evaluate the proposed model. Not only does the
proposed coordination model increase the SC members' profit, but it is also
desirable in achieving a more socially responsible SC. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06618v1' target="_blank">http://arxiv.org/pdf/2301.06618v1</a><br> <br> <br> <font size='5'> 333 </font> <div style="text-align: right"> 2023-01-15 20:22:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Equitable Data-Driven Resource Allocation to Fight the Opioid Epidemic: A Mixed-Integer Optimization Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The opioid epidemic is a crisis that has plagued the United States (US) for
decades. One central issue of the epidemic is inequitable access to treatment
for opioid use disorder (OUD), which puts certain populations at a higher risk
of opioid overdose. We integrate a predictive dynamical model and a
prescriptive optimization problem to compute high-quality opioid treatment
facility and treatment budget allocations for each US state. Our predictive
model is a differential equation-based epidemiological model that captures the
dynamics of the opioid epidemic. We use neural ordinary differential equations
to fit this model to opioid epidemic data for each state and obtain estimates
for unknown parameters in the model. We then incorporate this epidemiological
model into a corresponding mixed-integer optimization problem (MIP) that aims
to minimize the number of opioid overdose deaths and the number of people with
OUD. We develop strong relaxations based on McCormick envelopes to efficiently
compute approximate solutions to our MIPs that have less than 1% optimality
gaps. Our method provides socioeconomically equitable solutions, as it
incentivizes investments in areas with higher social vulnerability (from the US
Centers for Disease Control's Social Vulnerability Index) and opioid
prescribing rates. On average, our approach decreases the number of people with
OUD by 6.08 $\pm$ 0.863%, increases the number of people in treatment by 22.57
$\pm$ 3.633%, and decreases the number of opioid-related deaths by 0.55 $\pm$
0.105% after 2 years compared to the baseline epidemiological model's
predictions. We identify that treatment facilities should be moved or added to
counties that have significantly less facilities than their population share
and higher social vulnerability. Future iterations of our approach could be
implemented as a decision-making tool to tackle opioid treatment
inaccessibility. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06179v2' target="_blank">http://arxiv.org/pdf/2301.06179v2</a><br> <br> <br> <font size='5'> 334 </font> <div style="text-align: right"> 2023-01-15 20:22:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bike Frames: Understanding the Implicit Portrayal of Cyclists in the News</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Increasing the number of cyclists, whether for general transport or
recreation, can provide health improvements and reduce the environmental impact
of vehicular transportation. However, the public's perception of cycling may be
driven by the ideologies and reporting standards of news agencies. For
instance, people may identify cyclists on the road as "dangerous" if news
agencies overly report cycling accidents, limiting the number of people that
cycle for transportation. Moreover, if fewer people cycle, there may be less
funding from the government to invest in safe infrastructure. In this paper, we
explore the perceived perception of cyclists within news headlines. To
accomplish this, we introduce a new dataset, "Bike Frames", that can help
provide insight into how headlines portray cyclists and help detect
accident-related headlines. Next, we introduce a multi-task (MT) regularization
approach that increases the detection accuracy of accident-related posts,
demonstrating improvements over traditional MT frameworks. Finally, we compare
and contrast the perceptions of cyclists with motorcyclist-related headlines to
ground the findings with another related activity for both male- and
female-related posts. Our findings show that general news websites are more
likely to report accidents about cyclists than other events. Moreover,
cyclist-specific websites are more likely to report about accidents than
motorcycling-specific websites, even though there is more potential danger for
motorcyclists. Finally, we show substantial differences in the reporting about
male vs. female-related persons, e.g., more male-related cyclists headlines are
related to accidents, but more female-related motorcycling headlines about
accidents. WARNING: This paper contains descriptions of accidents and death. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06178v1' target="_blank">http://arxiv.org/pdf/2301.06178v1</a><br> <br> <br> <font size='5'> 335 </font> <div style="text-align: right"> 2023-01-15 08:01:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient anomaly detection method for rooftop PV systems using big data and permutation entropy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The number of rooftop photovoltaic (PV) systems has significantly increased
in recent years around the globe, including in Australia. This trend is
anticipated to continue in the next few years. Given their high share of
generation in power systems, detecting malfunctions and abnormalities in
rooftop PV systems is essential for ensuring their high efficiency and safety.
In this paper, we present a novel anomaly detection method for a large number
of rooftop PV systems installed in a region using big data and a time series
complexity measure called weighted permutation entropy (WPE). This efficient
method only uses the historical PV generation data in a given region to
identify anomalous PV systems and requires no new sensor or smart device. Using
a real-world PV generation dataset, we discuss how the hyperparameters of WPE
should be tuned for the purpose. The proposed PV anomaly detection method is
then tested on rooftop PV generation data from over 100 South Australian
households. The results demonstrate that anomalous systems detected by our
method have indeed encountered problems and require a close inspection. The
detection and resolution of potential faults would result in better rooftop PV
systems, longer lifetimes, and higher returns on investment. </font><br> Link: <a href='http://arxiv.org/pdf/2301.06035v1' target="_blank">http://arxiv.org/pdf/2301.06035v1</a><br> <br> <br> <font size='5'> 336 </font> <div style="text-align: right"> 2023-01-14 06:39:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The financial markets, which involve more than $90 trillion market capitals,
attract the attention of innumerable investors around the world. Recently,
reinforcement learning in financial markets (FinRL) has emerged as a promising
direction to train agents for making profitable investment decisions. However,
the evaluation of most FinRL methods only focuses on profit-related measures
and ignores many critical axes, which are far from satisfactory for financial
practitioners to deploy these methods into real-world financial markets.
Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability,
Risk-control, Universality, Diversity, rEliability, and eXplainability, with a
total of 17 measures for a systematic evaluation. Specifically, i) we propose
AlphaMix+ as a strong FinRL baseline, which leverages mixture-of-experts (MoE)
and risk-sensitive approaches to make diversified risk-aware investment
decisions, ii) we evaluate 8 FinRL methods in 4 long-term real-world datasets
of influential financial markets to demonstrate the usage of our
PRUDEX-Compass, iii) PRUDEX-Compass together with 4 real-world datasets,
standard implementation of 8 FinRL methods and a portfolio management
environment is released as public resources to facilitate the design and
comparison of new FinRL methods. We hope that PRUDEX-Compass can not only shed
light on future FinRL research to prevent untrustworthy results from stagnating
FinRL into successful industry deployment but also provide a new challenging
algorithm evaluation scenario for the reinforcement learning (RL) community. </font><br> Link: <a href='http://arxiv.org/pdf/2302.00586v2' target="_blank">http://arxiv.org/pdf/2302.00586v2</a><br> <br> <br> <font size='5'> 337 </font> <div style="text-align: right"> 2023-01-13 09:49:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: UK Astronomy Science and Technology Roadmap: STFC Astronomy Advisory Panel Roadmap 2022</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This document summarises the UK astronomy community's science and technology
priorities for funding and investments in the coming decades, following a
series of national community consultations by the Astronomy Advisory Panel of
the Science and Technology Facilities Council (STFC). The facility remit of
STFC is ground-based so the infrastructure recommendations are necessarily also
ground-based, but the report also recognises the importance of STFC-funded
technology development for, and science exploitation of, the ESA science
program including but not limited to X-ray, gamma-ray and multimessenger
astronomy. </font><br> Link: <a href='http://arxiv.org/pdf/2301.05457v1' target="_blank">http://arxiv.org/pdf/2301.05457v1</a><br> <br> <br> <font size='5'> 338 </font> <div style="text-align: right"> 2023-01-13 08:57:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ASEAN's Portfolio Investment in a Gravity Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the elasticity of portfolio investment to geographical
distance in a gravity model utilizing a bilateral panel of 86 reporting and 241
counterparty countries/territories for 2007-2017. We find that the elasticity
is more negative for ASEAN than OECD members. The difference is larger if we
exclude Singapore. This indicates that Singapore's behavior is very different
from other ASEAN members. While Singapore tends to invest in faraway OECD
countries, other ASEAN members tend to invest in nearby countries. Our study
also shows the emergence of China as a significant investment destination for
ASEAN members. </font><br> Link: <a href='http://arxiv.org/pdf/2301.05443v1' target="_blank">http://arxiv.org/pdf/2301.05443v1</a><br> <br> <br> <font size='5'> 339 </font> <div style="text-align: right"> 2023-01-13 00:48:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Parameters, Properties, and Process: Conditional Neural Generation of Realistic SEM Imagery Towards ML-assisted Advanced Manufacturing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The research and development cycle of advanced manufacturing processes
traditionally requires a large investment of time and resources. Experiments
can be expensive and are hence conducted on relatively small scales. This poses
problems for typically data-hungry machine learning tools which could otherwise
expedite the development cycle. We build upon prior work by applying
conditional generative adversarial networks (GANs) to scanning electron
microscope (SEM) imagery from an emerging manufacturing process, shear assisted
processing and extrusion (ShAPE). We generate realistic images conditioned on
temper and either experimental parameters or material properties. In doing so,
we are able to integrate machine learning into the development cycle, by
allowing a user to immediately visualize the microstructure that would arise
from particular process parameters or properties. This work forms a technical
backbone for a fundamentally new approach for understanding manufacturing
processes in the absence of first-principle models. By characterizing
microstructure from a topological perspective we are able to evaluate our
models' ability to capture the breadth and diversity of experimental scanning
electron microscope (SEM) samples. Our method is successful in capturing the
visual and general microstructural features arising from the considered
process, with analysis highlighting directions to further improve the
topological realism of our synthetic imagery. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08495v1' target="_blank">http://arxiv.org/pdf/2302.08495v1</a><br> <br> <br> <font size='5'> 340 </font> <div style="text-align: right"> 2023-01-12 21:22:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Open Case Studies: Statistics and Data Science Education through Real-World Applications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With unprecedented and growing interest in data science education, there are
limited educator materials that provide meaningful opportunities for learners
to practice statistical thinking, as defined by Wild and Pfannkuch (1999), with
messy data addressing real-world challenges. As a solution, Nolan and Speed
(1999) advocated for bringing applications to the forefront in undergraduate
statistics curriculum with the use of in-depth case studies to encourage and
develop statistical thinking in the classroom. Limitations to this approach
include the significant time investment required to develop a case study --
namely, to select a motivating question and to create an illustrative data
analysis -- and the domain expertise needed. As a result, case studies based on
realistic challenges, not toy examples, are scarce. To address this, we
developed the Open Case Studies (https://www.opencasestudies.org) project,
which offers a new statistical and data science education case study model.
This educational resource provides self-contained, multimodal, peer-reviewed,
and open-source guides (or case studies) from real-world examples for active
experiences of complete data analyses. We developed an educator's guide
describing how to most effectively use the case studies, how to modify and
adapt components of the case studies in the classroom, and how to contribute
new case studies. (https://www.opencasestudies.org/OCS_Guide). </font><br> Link: <a href='http://arxiv.org/pdf/2301.05298v1' target="_blank">http://arxiv.org/pdf/2301.05298v1</a><br> <br> <br> <font size='5'> 341 </font> <div style="text-align: right"> 2023-01-12 05:28:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Data-centric AI: Perspectives and Challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The role of data in building AI systems has recently been significantly
magnified by the emerging concept of data-centric AI (DCAI), which advocates a
fundamental shift from model advancements to ensuring data quality and
reliability. Although our community has continuously invested efforts into
enhancing data in different aspects, they are often isolated initiatives on
specific tasks. To facilitate the collective initiative in our community and
push forward DCAI, we draw a big picture and bring together three general
missions: training data development, inference data development, and data
maintenance. We provide a top-level discussion on representative DCAI tasks and
share perspectives. Finally, we list open challenges. More resources are
summarized at https://github.com/daochenzha/data-centric-AI </font><br> Link: <a href='http://arxiv.org/pdf/2301.04819v3' target="_blank">http://arxiv.org/pdf/2301.04819v3</a><br> <br> <br> <font size='5'> 342 </font> <div style="text-align: right"> 2023-01-12 00:13:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Closely estimating the entropy of sparse graph models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce an algorithm for estimating the entropy of pairwise,
probabilistic graph models by leveraging bridges between social communities and
an accurate entropy estimator on sparse samples. We propose using a measure of
investment from the sociological literature, Burt's structural constraint, as a
heuristic for identifying bridges that partition a graph into conditionally
independent components. We combine this heuristic with the
Nemenman-Shafee-Bialek entropy estimator to obtain a faster and more accurate
estimator. We demonstrate it on the pairwise maximum entropy, or Ising, models
of judicial voting, to improve na\"ive entropy estimates. We use our algorithm
to estimate the partition function closely, which we then apply to the problem
of model selection, where estimating the likelihood is difficult. This serves
as an improvement over existing methods that rely on point correlation
functions to test fit can be extended to other graph models with a
straightforward modification of the open-source implementation. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04768v1' target="_blank">http://arxiv.org/pdf/2301.04768v1</a><br> <br> <br> <font size='5'> 343 </font> <div style="text-align: right"> 2023-01-11 13:27:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utilizing Technical Data to Discover Similar Companies in Dhaka Stock Exchange</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stock market investment have been an ideal form of investment for many years.
Investing capitals smartly in stock market yields high profit returns. But
there are many companies available in a market. Currently there are more than
$345$ active companies who have stocks in Dhaka Stock Exchange (DSE). Analyzing
all these companies is quite impossible. However, many companies tend to move
together. This study aims at finding which companies in DSE have a close
connection and move alongside each other. By analyzing this relation, the
investors and traders will be able to analyze a lot of companies' statistics
from a calculating just a handful number of companies. The conducted experiment
yielded promising results. It was found that though the system was not given
anything other than technical data, it was able to identify companies that show
domain specific outcomes. In other words, a relation between technical data and
fundamental data was discovered from the conducted experiment. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04455v1' target="_blank">http://arxiv.org/pdf/2301.04455v1</a><br> <br> <br> <font size='5'> 344 </font> <div style="text-align: right"> 2023-01-11 12:46:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uniform Inference in Linear Error-in-Variables Models: Divide-and-Conquer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It is customary to estimate error-in-variables models using higher-order
moments of observables. This moments-based estimator is consistent only when
the coefficient of the latent regressor is assumed to be non-zero. We develop a
new estimator based on the divide-and-conquer principle that is consistent for
any value of the coefficient of the latent regressor. In an application on the
relation between investment, (mismeasured) Tobin's $q$ and cash flow, we find
time periods in which the effect of Tobin's $q$ is not statistically different
from zero. The implausibly large higher-order moment estimates in these periods
disappear when using the proposed estimator. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04439v1' target="_blank">http://arxiv.org/pdf/2301.04439v1</a><br> <br> <br> <font size='5'> 345 </font> <div style="text-align: right"> 2023-01-10 16:08:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal social security timing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The optimal age that a retiree claims social security retirement benefits is
in general a complicated function of many factors. However, if the
beneficiary's finances and health are not the constraining factors, it is
possible to formally derive mathematical models that maximize a well-defined
measure of his total benefits. A model that takes into account various factors
such as the increase in the benefits for delayed claims and the penalties for
early retirement, the advantages of investing some of the benefits in the
financial markets, and the effects of cost-of-living adjustments shows that not
waiting until age 70 is almost always the better option. The optimal claiming
age that maximizes the total benefits, however, depends on the expected market
returns and the rate of cost-of-living adjustments, with the higher market
rates in general pushing the optimal age lower. The models presented here can
be easily tailored to address the particular circumstances and goals of any
individual. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04052v1' target="_blank">http://arxiv.org/pdf/2301.04052v1</a><br> <br> <br> <font size='5'> 346 </font> <div style="text-align: right"> 2023-01-10 08:30:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Practitioners' Expectations on Code Completion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Code completion has become a common practice for programmers during their
daily programming activities. It aims at automatically predicting the next
tokens or lines that the programmers tend to use. A good code completion tool
can substantially save keystrokes and improve the programming efficiency for
programmers. Recently, various techniques for code completion have been
proposed for usage in practice. However, it is still unclear what are
practitioners' expectations on code completion and whether existing research
has met their demands. To fill the gap, we perform an empirical study by first
interviewing 15 practitioners and then surveying 599 practitioners from 18 IT
companies about their expectations on code completion. We then compare the
practitioners' demands with current research via conducting a literature review
of papers on code completion published in premier publication venues from 2012
to 2022. Based on the comparison, we highlight the directions desirable for
researchers to invest efforts towards developing code completion techniques for
meeting practitioners' expectations. </font><br> Link: <a href='http://arxiv.org/pdf/2301.03846v1' target="_blank">http://arxiv.org/pdf/2301.03846v1</a><br> <br> <br> <font size='5'> 347 </font> <div style="text-align: right"> 2023-01-09 17:26:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient Attack Detection in IoT Devices using Feature Engineering-Less Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Through the generalization of deep learning, the research community has
addressed critical challenges in the network security domain, like malware
identification and anomaly detection. However, they have yet to discuss
deploying them on Internet of Things (IoT) devices for day-to-day operations.
IoT devices are often limited in memory and processing power, rendering the
compute-intensive deep learning environment unusable. This research proposes a
way to overcome this barrier by bypassing feature engineering in the deep
learning pipeline and using raw packet data as input. We introduce a feature
engineering-less machine learning (ML) process to perform malware detection on
IoT devices. Our proposed model, "Feature engineering-less-ML (FEL-ML)," is a
lighter-weight detection algorithm that expends no extra computations on
"engineered" features. It effectively accelerates the low-powered IoT edge. It
is trained on unprocessed byte-streams of packets. Aside from providing better
results, it is quicker than traditional feature-based methods. FEL-ML
facilitates resource-sensitive network traffic security with the added benefit
of eliminating the significant investment by subject matter experts in feature
engineering. </font><br> Link: <a href='http://arxiv.org/pdf/2301.03532v1' target="_blank">http://arxiv.org/pdf/2301.03532v1</a><br> <br> <br> <font size='5'> 348 </font> <div style="text-align: right"> 2023-01-08 18:51:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Synergy between NP and HEP research goals and efforts in fundamental symmetries and interactions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The aim of this white paper is to highlight several areas for which the
Department of Energy's Office of Nuclear Physics has primary stewardship or
significant investment and expertise, and for which there is also significant
interest and expertise within the HEP community. These areas of overlap offer
exciting opportunities for collaboration. </font><br> Link: <a href='http://arxiv.org/pdf/2301.03086v1' target="_blank">http://arxiv.org/pdf/2301.03086v1</a><br> <br> <br> <font size='5'> 349 </font> <div style="text-align: right"> 2023-01-07 15:09:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Learning to Estimate Gross Loss of Jewelry for Wax Patterns</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In mass manufacturing of jewellery, the gross loss is estimated before
manufacturing to calculate the wax weight of the pattern that would be
investment casted to make multiple identical pieces of jewellery. Machine
learning is a technology that is a part of AI which helps create a model with
decision-making capabilities based on a large set of user-defined data. In this
paper, the authors found a way to use Machine Learning in the jewellery
industry to estimate this crucial Gross Loss. Choosing a small data set of
manufactured rings and via regression analysis, it was found out that there is
a potential of reducing the error in estimation from +-2-3 to +-0.5 using ML
Algorithms from historic data and attributes collected from the CAD file during
the design phase itself. To evaluate the approach's viability, additional study
must be undertaken with a larger data set. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02872v1' target="_blank">http://arxiv.org/pdf/2301.02872v1</a><br> <br> <br> <font size='5'> 350 </font> <div style="text-align: right"> 2023-01-07 12:46:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Network Slicing: Market Mechanism and Competitive Equilibria</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Towards addressing spectral scarcity and enhancing resource utilization in 5G
networks, network slicing is a promising technology to establish end-to-end
virtual networks without requiring additional infrastructure investments. By
leveraging Software Defined Networks (SDN) and Network Function Virtualization
(NFV), we can realize slices completely isolated and dedicated to satisfy the
users' diverse Quality of Service (QoS) prerequisites and Service Level
Agreements (SLAs). This paper focuses on the technical and economic challenges
that emerge from the application of the network slicing architecture to
real-world scenarios. We consider a market where multiple Network Providers
(NPs) own the physical infrastructure and offer their resources to multiple
Service Providers (SPs). Then, the SPs offer those resources as slices to their
associated users. We propose a holistic iterative model for the network slicing
market along with a clock auction that converges to a robust
$\epsilon$-competitive equilibrium. At the end of each cycle of the market, the
slices are reconfigured and the SPs aim to learn the private parameters of
their users. Numerical results are provided that validate and evaluate the
convergence of the clock auction and the capability of the proposed market
architecture to express the incentives of the different entities of the system. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02840v2' target="_blank">http://arxiv.org/pdf/2301.02840v2</a><br> <br> <br> <font size='5'> 351 </font> <div style="text-align: right"> 2023-01-07 00:43:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Frequency-Based Optimal Portfolio with Transaction Costs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The aim of this paper is to investigate the impact of rebalancing frequency
and transaction costs on the log-optimal portfolio, which is a portfolio that
maximizes the expected logarithmic growth rate of an investor's wealth. We
prove that the frequency-dependent log-optimal portfolio problem with costs is
equivalent to a concave program and provide a version of the dominance theorem
with costs to determine when an investor should invest all available funds in a
particular asset. Then, we show that transaction costs may cause a bankruptcy
issue for the frequency-dependent log-optimal portfolio. To address this issue,
we approximate the problem to obtain a quadratic concave program and derive
necessary and sufficient optimality conditions. Additionally, we prove a
version of the two-fund theorem, which states that any convex combination of
two optimal weights from the optimality conditions is still optimal. We test
our proposed methods using both intraday and daily price data. Finally, we
extend our empirical studies to an online trading scenario by implementing a
sliding window approach. This approach enables us to solve a sequence of
concave programs rather than a potentially computational complex stochastic
dynamic programming problem. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02754v1' target="_blank">http://arxiv.org/pdf/2301.02754v1</a><br> <br> <br> <font size='5'> 352 </font> <div style="text-align: right"> 2023-01-06 16:08:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cognitive Endurance, Talent Selection, and the Labor Market Returns to Human Capital</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cognitive endurance -- the ability to sustain performance on a
cognitively-demanding task over time -- is thought to be a crucial productivity
determinant. However, a lack of data on this variable has limited researchers'
ability to understand its role for success in college and the labor market.
This paper uses college-admission-exam records from 15 million Brazilian high
school students to measure cognitive endurance based on changes in performance
throughout the exam. By exploiting exogenous variation in the order of exam
questions, I show that students are 7.1 percentage points more likely to
correctly answer a given question when it appears at the beginning of the day
versus the end (relative to a sample mean of 34.3%). I develop a method to
decompose test scores into fatigue-adjusted ability and cognitive endurance. I
then merge these measures into a higher-education census and the earnings
records of the universe of Brazilian formal-sector workers to quantify the
association between endurance and long-run outcomes. I find that cognitive
endurance has a statistically and economically significant wage return.
Controlling for fatigue-adjusted ability and other student characteristics, a
one-standard-deviation higher endurance predicts a 5.4% wage increase. This
wage return to endurance is sizable, equivalent to a third of the wage return
to ability. I also document positive associations between endurance and college
attendance, college quality, college graduation, firm quality, and other
outcomes. Finally, I show how systematic differences in endurance across
students interact with the exam design to determine the sorting of students to
colleges. I discuss the implications of these findings for the use of cognitive
assessments for talent selection and investments in interventions that build
cognitive endurance. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02575v1' target="_blank">http://arxiv.org/pdf/2301.02575v1</a><br> <br> <br> <font size='5'> 353 </font> <div style="text-align: right"> 2023-01-06 09:32:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Myths and Legends in High-Performance Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this humorous and thought provoking article, we discuss certain myths and
legends that are folklore among members of the high-performance computing
community. We collected those myths from conversations at conferences and
meetings, product advertisements, papers, and other communications such as
tweets, blogs, and news articles within (and beyond) our community. We believe
they represent the zeitgeist of the current era of massive change, driven by
the end of many scaling laws such as Dennard scaling and Moore's law. While
some laws end, new directions open up, such as algorithmic scaling or novel
architecture research. However, these myths are rarely based on scientific
facts but often on some evidence or argumentation. In fact, we believe that
this is the very reason for the existence of many myths and why they cannot be
answered clearly. While it feels like there should be clear answers for each,
some may remain endless philosophical debates such as the question whether
Beethoven was better than Mozart. We would like to see our collection of myths
as a discussion of possible new directions for research and industry
investment. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02432v1' target="_blank">http://arxiv.org/pdf/2301.02432v1</a><br> <br> <br> <font size='5'> 354 </font> <div style="text-align: right"> 2023-01-06 04:36:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Multi-Objective Planning and Scheduling Framework for Community Energy Storage Systems in Low Voltage Distribution Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a methodology for optimizing the planning and scheduling
aspects of a community energy storage (CES) system in the presence of solar
photovoltaic (SPV) power in low voltage (LV) distribution networks. To this
end, we develop a multi-objective optimization framework that minimizes the
real power loss, the energy trading cost of LV customers and the CES provider
with the grid, and the investment cost for the CES. Distribution network limits
including the voltage constraint are also taken into account by combining the
optimization problem with a linearized power flow model. Simulations for the
proposed optimization framework with real power consumption and SPV generation
data of the customers, highlight both real power loss and energy trading cost
with the grid are reduced compared with the case without a CES by nearly 29%
and 16%, respectively. Moreover, a case study justifies our methodology is
competent in attaining the three objectives better than the optimization models
which optimize only the CES scheduling. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02372v1' target="_blank">http://arxiv.org/pdf/2301.02372v1</a><br> <br> <br> <font size='5'> 355 </font> <div style="text-align: right"> 2023-01-05 11:57:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cryptocurrency co-investment network: token returns reflect investment patterns</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since the introduction of Bitcoin in 2009, the dramatic and unsteady
evolution of the cryptocurrency market has also been driven by large
investments by traditional and cryptocurrency-focused hedge funds.
Notwithstanding their critical role, our understanding of the relationship
between institutional investments and the evolution of the cryptocurrency
market has remained limited, also due to the lack of comprehensive data
describing investments over time. In this study, we present a quantitative
study of cryptocurrency institutional investments based on a dataset collected
for 1324 currencies in the period between 2014 and 2022 from Crunchbase, one of
the largest platforms gathering business information. We show that the
evolution of the cryptocurrency market capitalization is highly correlated with
the size of institutional investments, thus confirming their important role.
Further, we find that the market is dominated by the presence of a group of
prominent investors who tend to specialise by focusing on particular
technologies. Finally, studying the co-investment network of currencies that
share common investors, we show that assets with shared investors tend to be
characterized by similar market behavior. Our work sheds light on the role
played by institutional investors and provides a basis for further research on
their influence in the cryptocurrency ecosystem. </font><br> Link: <a href='http://arxiv.org/pdf/2301.02027v3' target="_blank">http://arxiv.org/pdf/2301.02027v3</a><br> <br> <br> <font size='5'> 356 </font> <div style="text-align: right"> 2023-01-05 09:04:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ruin Probabilities for a Sparre Andersen Model with Investments: the Case of Annuity Payments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This note is a complement to the paper by Eberlein, Kabanov, and Schmidt on
the asymptotic of the ruin probability in a Sparre Andersen non-life insurance
model with investments a risky asset whose price follows a geometric L\'evy
process. Using the techniques of semi-Markov processes we extend the result of
the mentioned paper to the case of annuities and models with two-sided jumps. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01966v1' target="_blank">http://arxiv.org/pdf/2301.01966v1</a><br> <br> <br> <font size='5'> 357 </font> <div style="text-align: right"> 2023-01-04 23:00:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Peace Dividends: The Economic Effects of Colombia's Peace Agreement</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The last decades have seen a resurgence of armed conflict around the world,
renewing the need for durable peace agreements. In this paper, I evaluate the
economic effects of the peace agreement between the Colombian government and
the largest guerrilla group in the country, the FARC, putting an end to one of
the longest and most violent armed conflicts in recent history. Using a
difference-in-difference strategy comparing municipalities that historically
had FARC presence and those with presence of a similar, smaller guerrilla
group, the ELN, before and after the start of a unilateral ceasefire by the
FARC, I establish three sets of results. First, violence indicators
significantly and sizably decreased in historically FARC municipalities.
Second, despite this large reduction in violence, I find precisely-estimated
null effects across a variety of economic indicators, suggesting no effect of
the peace agreement on economic activity. Furthermore, I use a sharp
discontinuity in eligibility to the government's flagship business and job
creation program for conflict-affected areas to evaluate the policy's impact,
also finding precisely-estimated null effects on the same economic indicators.
Third, I present evidence that suggests the reason why historically FARC
municipalities could not reap the economic benefits from the reduction in
violence is a lack of state capacity, caused both by their low initial levels
of state capacity and the lack of state entry post-ceasefire. These results
indicate that peace agreements require complementary investments in state
capacity to yield an economic dividend. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01843v1' target="_blank">http://arxiv.org/pdf/2301.01843v1</a><br> <br> <br> <font size='5'> 358 </font> <div style="text-align: right"> 2023-01-04 18:10:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Episodes Discovery Recommendation with Multi-Source Augmentations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recommender systems (RS) commonly retrieve potential candidate items for
users from a massive number of items by modeling user interests based on
historical interactions. However, historical interaction data is highly sparse,
and most items are long-tail items, which limits the representation learning
for item discovery. This problem is further augmented by the discovery of novel
or cold-start items. For example, after a user displays interest in bitcoin
financial investment shows in the podcast space, a recommender system may want
to suggest, e.g., a newly released blockchain episode from a more technical
show. Episode correlations help the discovery, especially when interaction data
of episodes is limited. Accordingly, we build upon the classical Two-Tower
model and introduce the novel Multi-Source Augmentations using a Contrastive
Learning framework (MSACL) to enhance episode embedding learning by
incorporating positive episodes from numerous correlated semantics. Extensive
experiments on a real-world podcast recommendation dataset from a large audio
streaming platform demonstrate the effectiveness of the proposed framework for
user podcast exploration and cold-start episode recommendation. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01737v2' target="_blank">http://arxiv.org/pdf/2301.01737v2</a><br> <br> <br> <font size='5'> 359 </font> <div style="text-align: right"> 2023-01-04 06:52:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Stochastic Multi-Objective Optimization Framework for Planning and Scheduling of Community Energy Storage Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores a methodology to optimize the planning and the scheduling
of a community energy storage (CES) considering the uncertainty of real power
consumption and solar photovoltaic (SPV) generation of the customers in low
voltage (LV) distribution networks. To this end, we develop a stochastic
multi-objective optimization framework which minimizes the investment and the
operation costs of the CES provider, and the social costs of the customers
(i.e. cost of customers for trading energy with the grid and the CES). The
uncertainty of SPV generation and real power consumption are modelled to follow
the beta and normal distributions, respectively. Then, the roulette wheel
mechanism (RWM) is exploited to formulate a scenario-based stochastic program.
The initial scenarios obtained from the RWM, are then reduced by using the
K-Means clustering algorithm, to keep the problem tractability. A case study
highlights our model provides 10-21% more cumulative economic benefits for the
customers and the CES provider, compared with the models that optimize only the
CES scheduling. Also, the simulation results for different energy price schemes
of the CES provider reflect, the customers change their power exchange with the
CES and the grid significantly, to minimize their social costs. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01462v1' target="_blank">http://arxiv.org/pdf/2301.01462v1</a><br> <br> <br> <font size='5'> 360 </font> <div style="text-align: right"> 2023-01-04 05:19:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Building Coverage Estimation with Low-resolution Remote Sensing Imagery</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building coverage statistics provide crucial insights into the urbanization,
infrastructure, and poverty level of a region, facilitating efforts towards
alleviating poverty, building sustainable cities, and allocating infrastructure
investments and public service provision. Global mapping of buildings has been
made more efficient with the incorporation of deep learning models into the
pipeline. However, these models typically rely on high-resolution satellite
imagery which are expensive to collect and infrequently updated. As a result,
building coverage data are not updated timely especially in developing regions
where the built environment is changing quickly. In this paper, we propose a
method for estimating building coverage using only publicly available
low-resolution satellite imagery that is more frequently updated. We show that
having a multi-node quantile regression layer greatly improves the model's
spatial and temporal generalization. Our model achieves a coefficient of
determination ($R^2$) as high as 0.968 on predicting building coverage in
regions of different levels of development around the world. We demonstrate
that the proposed model accurately predicts the building coverage from raw
input images and generalizes well to unseen countries and continents,
suggesting the possibility of estimating global building coverage using only
low-resolution remote sensing data. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01449v2' target="_blank">http://arxiv.org/pdf/2301.01449v2</a><br> <br> <br> <font size='5'> 361 </font> <div style="text-align: right"> 2023-01-03 19:53:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Competitive Leverage Paradox Effect on Information Systems Life Cycle</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The fierce market competition has put pressure on organizations leveraging
their value chains. The continuous development in strategic technologies such
as Artificial Intelligence (AI) has pushed organizations to continuously
acquire new Intelligent Information Systems (IIS) while underutilizing existing
ones leading to the competitive leverage paradox. However, research on
underutilizing IIS has focused on the social and organizational aspects of the
problem, ignoring the flaws in designing and evaluating IIS. One of the
overlooked factors is the effective life span of an IIS. This research
conducted a systematic literature review to profoundly investigate the
determinants of the competitive leverage paradox and its effect on the IIS life
cycle. The research studies the IISs from economic and design perspectives. We
also explore the design and strategic factors that led to defects in the
effective life cycle of IIS. This research calls to consider the economic, and
design factors in addressing the underutilization of IIS. The study also
presents future research propositions to enhance IIS life cycle and return on
investment. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10006v1' target="_blank">http://arxiv.org/pdf/2301.10006v1</a><br> <br> <br> <font size='5'> 362 </font> <div style="text-align: right"> 2023-01-03 14:47:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Gender Diversity in Ownership and Firm Innovativeness in Emerging Markets. The Mediating Roles of R&D Investments and External Capital</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite recent evidence linking gender diversity in the firm with firm
innovativeness, we know little about the underlying mechanisms. Building on and
extending the Upper Echelon and entrepreneurship literature, we address two
lingering questions: why and how does gender diversity in firm ownership affect
firm innovativeness? We use survey data collected from 7,848 owner-managers of
SMEs across 29 emerging markets to test our hypotheses. Our findings
demonstrate that firms with higher gender diversity in ownership are more
likely to invest in R&D and rely upon a breadth of external capital, with such
differentials explaining sizeable proportions of the higher likelihood of
overall firm innovativeness, product and process, as well as organizational and
marketing innovations exhibited by their firms. Our findings are robust to
corrections for alternative measurement of focal variables, sensitivity to
outliers and subsamples, and endogenous self-selection concerns. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01127v1' target="_blank">http://arxiv.org/pdf/2301.01127v1</a><br> <br> <br> <font size='5'> 363 </font> <div style="text-align: right"> 2023-01-02 03:54:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Reinforcement Learning for Asset Allocation: Reward Clipping</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, there are many trials to apply reinforcement learning in asset
allocation for earning more stable profits. In this paper, we compare
performance between several reinforcement learning algorithms - actor-only,
actor-critic and PPO models. Furthermore, we analyze each models' character and
then introduce the advanced algorithm, so called Reward clipping model. It
seems that the Reward Clipping model is better than other existing models in
finance domain, especially portfolio optimization - it has strength both in
bull and bear markets. Finally, we compare the performance for these models
with traditional investment strategies during decreasing and increasing
markets. </font><br> Link: <a href='http://arxiv.org/pdf/2301.05300v1' target="_blank">http://arxiv.org/pdf/2301.05300v1</a><br> <br> <br> <font size='5'> 364 </font> <div style="text-align: right"> 2022-12-31 21:07:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Inference for Large Panel Data with Many Covariates</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper proposes a novel testing procedure for selecting a sparse set of
covariates that explains a large dimensional panel. Our selection method
provides correct false detection control while having higher power than
existing approaches. We develop the inferential theory for large panels with
many covariates by combining post-selection inference with a novel multiple
testing adjustment. Our data-driven hypotheses are conditional on the sparse
covariate selection. We control for family-wise error rates for covariate
discovery for large cross-sections. As an easy-to-use and practically relevant
procedure, we propose Panel-PoSI, which combines the data-driven adjustment for
panel multiple testing with valid post-selection p-values of a generalized
LASSO, that allows us to incorporate priors. In an empirical study, we select a
small number of asset pricing factors that explain a large cross-section of
investment strategies. Our method dominates the benchmarks out-of-sample due to
its better size and power. </font><br> Link: <a href='http://arxiv.org/pdf/2301.00292v6' target="_blank">http://arxiv.org/pdf/2301.00292v6</a><br> <br> <br> <font size='5'> 365 </font> <div style="text-align: right"> 2022-12-31 10:51:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient Methods for Approximating the Shapley Value for Asset Sharing in Energy Communities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the emergence of energy communities, where a number of prosumers invest
in shared generation and storage, the issue of fair allocation of benefits is
increasingly important. The Shapley value has attracted increasing interest for
redistribution in energy settings - however, computing it exactly is
intractable beyond a few dozen prosumers. In this paper, we first conduct a
systematic review of the literature on the use of Shapley value in
energy-related applications, as well as efforts to compute or approximate it.
Next, we formalise the main methods for approximating the Shapley value in
community energy settings, and propose a new one, which we call the stratified
expected value approximation. To compare the performance of these methods, we
design a novel method for exact Shapley value computation, which can be applied
to communities of up to several hundred agents by clustering the prosumers into
a smaller number of demand profiles. We perform a large-scale experimental
comparison of the proposed methods, for communities of up to 200 prosumers,
using large-scale, publicly available data from two large-scale energy trials
in the UK (UKERC Energy Data Centre, 2017, UK Power Networks Innovation, 2021).
Our analysis shows that, as the number of agents in the community increases,
the relative difference to the exact Shapley value converges to under 1% for
all the approximation methods considered. In particular, for most experimental
scenarios, we show that there is no statistical difference between the newly
proposed stratified expected value method and the existing state-of-the-art
method that uses adaptive sampling (O'Brien et al., 2015), although the cost of
computation for large communities is an order of magnitude lower. </font><br> Link: <a href='http://arxiv.org/pdf/2301.00174v1' target="_blank">http://arxiv.org/pdf/2301.00174v1</a><br> <br> <br> <font size='5'> 366 </font> <div style="text-align: right"> 2022-12-31 10:09:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Democratization of Retail Trading: Can Reddit's WallStreetBets Outperform Investment Bank Analysts?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent hype around Reddit's WallStreetBets (WSB) community has inspired
research on its impact on our economy and society. Still, one important
question remains: Can WSB's community of anonymous contributors actually
provide valuable investment advice and possibly even outperform top financial
institutions? We present a data-driven empirical study of investment
recommendations of WSB in comparison to recommendations made by leading
investment banks, based on more than 1.6 million WSB posts published since
2018. %enriched with stock market data. To this end, we extract and evaluate
investment recommendations from WSB's raw text for all S&P 500 stocks and
compare their performance to more than 16,000 analyst recommendations from the
largest investment banks. While not all WSB recommendations prove profitable,
our results show that they achieve average returns that compete with the best
banks and outperform them in certain cases. Furthermore, the WSB community has
been better than almost all investment banks at detecting top-performing
stocks. We conclude that WSB may indeed constitute a freely accessible,
valuable source of investment advice. </font><br> Link: <a href='http://arxiv.org/pdf/2301.00170v1' target="_blank">http://arxiv.org/pdf/2301.00170v1</a><br> <br> <br> <font size='5'> 367 </font> <div style="text-align: right"> 2022-12-30 16:03:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A deep real options policy for sequential service region design and timing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As various city agencies and mobility operators navigate toward innovative
mobility solutions, there is a need for strategic flexibility in well-timed
investment decisions in the design and timing of mobility service regions, i.e.
cast as "real options" (RO). This problem becomes increasingly challenging with
multiple interacting RO in such investments. We propose a scalable machine
learning based RO framework for multi-period sequential service region design &
timing problem for mobility-on-demand services, framed as a Markov decision
process with non-stationary stochastic variables. A value function
approximation policy from literature uses multi-option least squares Monte
Carlo simulation to get a policy value for a set of interdependent investment
decisions as deferral options (CR policy). The goal is to determine the optimal
selection and timing of a set of zones to include in a service region. However,
prior work required explicit enumeration of all possible sequences of
investments. To address the combinatorial complexity of such enumeration, we
propose a new variant "deep" RO policy using an efficient recurrent neural
network (RNN) based ML method (CR-RNN policy) to sample sequences to forego the
need for enumeration, making network design & timing policy tractable for large
scale implementation. Experiments on multiple service region scenarios in New
York City (NYC) shows the proposed policy substantially reduces the overall
computational cost (time reduction for RO evaluation of > 90% of total
investment sequences is achieved), with zero to near-zero gap compared to the
benchmark. A case study of sequential service region design for expansion of
MoD services in Brooklyn, NYC show that using the CR-RNN policy to determine
optimal RO investment strategy yields a similar performance (0.5% within CR
policy value) with significantly reduced computation time (about 5.4 times
faster). </font><br> Link: <a href='http://arxiv.org/pdf/2212.14800v1' target="_blank">http://arxiv.org/pdf/2212.14800v1</a><br> <br> <br> <font size='5'> 368 </font> <div style="text-align: right"> 2022-12-30 11:18:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Solar Poles: The Last Great Frontier of the Sun</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite investments in multiple space and ground-based solar observatories by
the global community, the Sun's polar regions remain unchartered territory -
the last great frontier for solar observations. Breaching this frontier is
fundamental to understanding the solar cycle - the ultimate driver of
short-to-long term solar activity that encompasses space weather and space
climate. Magnetohydrodynamic dynamo models and empirically observed
relationships have established that the polar field is the primary determinant
of the future solar cycle amplitude. Models of solar surface evolution of
tilted active regions indicate that the mid to high latitude surges of magnetic
flux govern dynamics leading to the reversal and build-up of polar fields. Our
theoretical understanding and numerical models of this high latitude magnetic
field dynamics and plasma flows - that are a critical component of the sunspot
cycle - lack precise observational constraints. This limitation compromises our
ability to observe the enigmatic kilo Gauss polar flux patches and constrain
the polar field distribution at high latitudes. The lack of these observations
handicap our understanding of how high latitude magnetic fields power polar
jets, plumes, and the fast solar wind that extend to the boundaries of the
heliosphere and modulate solar open flux and cosmic ray flux within the solar
system. Accurate observation of the Sun's polar regions, therefore, is the
single most outstanding challenge that confronts Heliophysics. This paper
argues the scientific case for novel out of ecliptic observations of the Sun's
polar regions, in conjunction with existing, or future multi-vantage point
heliospheric observatories. Such a mission concept can revolutionize the field
of Heliophysics like no other mission concept has - with relevance that
transcends spatial regimes from the solar interior to the heliosphere. </font><br> Link: <a href='http://arxiv.org/pdf/2301.00010v1' target="_blank">http://arxiv.org/pdf/2301.00010v1</a><br> <br> <br> <font size='5'> 369 </font> <div style="text-align: right"> 2022-12-29 22:12:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An electronic warfare approach for deploying a software-based Wi-Fi jammer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Some prominent instances have been centered on electronic warfare. For
example, the American military has made significant investments in automation
through UAV programs, only for competitors like the Iranians to create
strategies to interfere with these systems. Iran managed to capture a
top-secret U.S. surveil-lance drone by fooling it into descending in the
incorrect place by jamming its control signals and providing it with bogus GPS
data. In this paper, the authors have focused on the electronic warfare
approach for deploying a software-based Wi-Fi jammer. The software-based Wi-Fi
jammer can disconnect the targets using the DoS pursuit mode. The paper
describes the unique methodology of how software can also be used for jamming
wireless signals. </font><br> Link: <a href='http://arxiv.org/pdf/2212.14470v1' target="_blank">http://arxiv.org/pdf/2212.14470v1</a><br> <br> <br> <font size='5'> 370 </font> <div style="text-align: right"> 2022-12-29 18:19:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT Takes the Bar Exam</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Nearly all jurisdictions in the United States require a professional license
exam, commonly referred to as "the Bar Exam," as a precondition for law
practice. To even sit for the exam, most jurisdictions require that an
applicant completes at least seven years of post-secondary education, including
three years at an accredited law school. In addition, most test-takers also
undergo weeks to months of further, exam-specific preparation. Despite this
significant investment of time and capital, approximately one in five
test-takers still score under the rate required to pass the exam on their first
try. In the face of a complex task that requires such depth of knowledge, what,
then, should we expect of the state of the art in "AI?" In this research, we
document our experimental evaluation of the performance of OpenAI's
`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate
multiple choice (MBE) section of the exam. While we find no benefit in
fine-tuning over GPT-3.5's zero-shot performance at the scale of our training
data, we do find that hyperparameter optimization and prompt engineering
positively impacted GPT-3.5's zero-shot performance. For best prompt and
parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete
NCBE MBE practice exam, significantly in excess of the 25% baseline guessing
rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's
ranking of responses is also highly-correlated with correctness; its top two
and top three choices are correct 71% and 88% of the time, respectively,
indicating very strong non-entailment performance. While our ability to
interpret these results is limited by nascent scientific understanding of LLMs
and the proprietary nature of GPT, we believe that these results strongly
suggest that an LLM will pass the MBE component of the Bar Exam in the near
future. </font><br> Link: <a href='http://arxiv.org/pdf/2212.14402v1' target="_blank">http://arxiv.org/pdf/2212.14402v1</a><br> <br> <br> <font size='5'> 371 </font> <div style="text-align: right"> 2022-12-29 14:45:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Stackelberg reinsurance-investment game under $$-maxmin mean-variance criterion and stochastic volatility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates a Stackelberg game between an insurer and a reinsurer
under the $\alpha$-maxmin mean-variance criterion. The insurer can purchase
per-loss reinsurance from the reinsurer. With the insurer's feedback
reinsurance strategy, the reinsurer optimizes the reinsurance premium in the
Stackelberg game. The financial market consists of cash and stock with Heston's
stochastic volatility. Both the insurer and reinsurer maximize their respective
$\alpha$-maxmin mean-variance preferences in the market. The criterion is
time-inconsistent and we derive the equilibrium strategies by the extended
Hamilton-Jacobi-Bellman equations. Similar to the non-robust case in Li and
Young (2022), excess-of-loss reinsurance is the optimal form of reinsurance
strategy for the insurer. The equilibrium investment strategy is determined by
a system of Riccati differential equations. Besides, the equations determining
the equilibrium reinsurance strategy and reinsurance premium rate are given
semi-explicitly, which is simplified to an algebraic equation in a specific
example. Numerical examples illustrate that the game between the insurer and
reinsurer makes the insurance more radical when the agents become more
ambiguity aversion or risk aversion. Furthermore, the level of ambiguity,
ambiguity attitude, and risk attitude of the insurer (reinsurer) have similar
effects on the equilibrium reinsurance strategy, reinsurance premium, and
investment strategy. </font><br> Link: <a href='http://arxiv.org/pdf/2212.14327v1' target="_blank">http://arxiv.org/pdf/2212.14327v1</a><br> <br> <br> <font size='5'> 372 </font> <div style="text-align: right"> 2022-12-28 07:57:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Historical Patterns and Recent Impacts of Chinese Investors in United States Real Estate</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since supplanting Canada in 2014, Chinese investors have been the lead
foreign buyers of U.S. real estate, concentrating their purchases in urban
areas with higher Chinese populations like California. The reasons for
investment include prestige, freedom from capital confiscation, and safe,
diversified opportunities from abroad simply being more lucrative and available
than in their home country, where the market is eroding. Interestingly, since
2019, Chinese investors have sold a net 23.6 billion dollars of U.S. commercial
real estate, a stark contrast to past acquisitions between 2013 to 2018 where
they were net buyers of almost 52 billion dollars worth of properties. A
similar trend appears in the residential real estate segment too. In both 2017
and 2018, Chinese buyers purchased over 40,000 U.S. residential properties
which were halved in 2019 and steadily declined to only 6,700 in the past year.
This turnaround in Chinese investment can be attributed to a deteriorating
relationship between the U.S. and China during the Trump Presidency, financial
distress in China, and new Chinese government regulations prohibiting outbound
investments. Additionally, while Chinese investment is a small share of U.S.
real estate (~1.5% at its peak), it has outsized impacts on market valuations
of home prices in U.S. zip codes with higher populations of foreign-born
Chinese, increasing property prices and exacerbating the issue of housing
affordability in these areas. This paper investigates the rapid growth and
decline of Chinese investment in U.S. real estate and its effect on U.S. home
prices in certain demographics. </font><br> Link: <a href='http://arxiv.org/pdf/2301.00681v2' target="_blank">http://arxiv.org/pdf/2301.00681v2</a><br> <br> <br> <font size='5'> 373 </font> <div style="text-align: right"> 2022-12-28 00:38:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Electric-Gas Infrastructure Planning for Deep Decarbonization of Energy Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The transition to a deeply decarbonized energy system requires coordinated
planning of infrastructure investments and operations serving multiple end-uses
while considering technology and policy-enabled interactions across sectors.
Electricity and natural gas (NG), which are vital vectors of today's energy
system, are likely to be coupled in different ways in the future, resulting
from increasing electrification, adoption of variable renewable energy (VRE)
generation in the power sector and policy factors such as cross-sectoral
emissions trading. This paper develops a least-cost investment and operations
model for joint planning of electricity and NG infrastructures that considers a
wide range of available and emerging technology options across the two vectors,
including carbon capture and storage (CCS) equipped power generation,
low-carbon drop-in fuels (LCDF) as well as long-duration energy storage (LDES).
The model incorporates the main operational constraints of both systems and
allows each system to operate under different temporal resolutions consistent
with their typical scheduling timescales. We apply the modeling framework to
evaluate power-NG system outcomes for the U.S. New England region under
different technology, decarbonization goals, and demand scenarios. Under a
global emissions constraint, ranging between 80-95\% emissions reduction
compared to 1990 levels, the least-cost solution disproportionately relies on
using the available emissions budget to serve non-power NG demand and results
in the power sector using only 15-43\% of the emissions budget. </font><br> Link: <a href='http://arxiv.org/pdf/2212.13655v2' target="_blank">http://arxiv.org/pdf/2212.13655v2</a><br> <br> <br> <font size='5'> 374 </font> <div style="text-align: right"> 2022-12-27 16:03:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Building a Culture of Reproducibility in Academic Research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reproducibility is an ideal that no researcher would dispute "in the
abstract", but when aspirations meet the cold hard reality of the academic
grind, reproducibility often "loses out". In this essay, I share some personal
experiences grappling with how to operationalize reproducibility while
balancing its demands against other priorities. My research group has had some
success building a "culture of reproducibility" over the past few years, which
I attempt to distill into lessons learned and actionable advice, organized
around answering three questions: why, what, and how. I believe that
reproducibility efforts should yield easy-to-use, well-packaged, and
self-contained software artifacts that allow others to reproduce and generalize
research findings. At the core, my approach centers on self interest: I argue
that the primary beneficiaries of reproducibility efforts are, in fact, those
making the investments. I believe that (unashamedly) appealing to self
interest, augmented with expectations of reciprocity, increases the chances of
success. Building from repeatability, social processes and standardized tools
comprise the two important additional ingredients that help achieve
aspirational ideals. The dogfood principle nicely ties these ideas together. </font><br> Link: <a href='http://arxiv.org/pdf/2212.13534v1' target="_blank">http://arxiv.org/pdf/2212.13534v1</a><br> <br> <br> <font size='5'> 375 </font> <div style="text-align: right"> 2022-12-27 00:30:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Learning for Space Weather Prediction: Bridging the Gap between Heliophysics Data and Theory</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Traditionally, data analysis and theory have been viewed as separate
disciplines, each feeding into fundamentally different types of models. Modern
deep learning technology is beginning to unify these two disciplines and will
produce a new class of predictively powerful space weather models that combine
the physical insights gained by data and theory. We call on NASA to invest in
the research and infrastructure necessary for the heliophysics' community to
take advantage of these advances. </font><br> Link: <a href='http://arxiv.org/pdf/2212.13328v1' target="_blank">http://arxiv.org/pdf/2212.13328v1</a><br> <br> <br> <font size='5'> 376 </font> <div style="text-align: right"> 2022-12-26 08:53:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Motivations and locational factors of FDI in CIS countries: Empirical evidence from South Korean FDI in Kazakhstan, Russia, and Uzbekistan</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Considering the growing significance of Eurasian economic ties because of
South Korea s New Northern Policy and Russia s New Eastern Policy, this study
investigates the motivations and locational factors of South Korean foreign
direct investment (FDI) in three countries in the Commonwealth of Independent
States (CIS: Kazakhstan, Russia, and Uzbekistan) by employing panel analysis
(pooled ordinary least squares (OLS), fixed effects, random effects) using data
from 1993 to 2017. The results show the positive and significant coefficients
of GDP, resource endowments, and inflation. Unlike conventional South Korean
outward FDI, labour-seeking is not defined as a primary purpose. Exchange
rates, political rights, and civil liberties are identified as insignificant.
The authors conclude that South Korean FDI in Kazakhstan, Russia, and
Uzbekistan is associated with market-seeking (particularly in Kazakhstan and
Russia) and natural resource-seeking, especially the former. From a policy
perspective, our empirical evidence suggests that these countries host
governments could implement mechanisms to facilitate the movement of goods
across regions and countries to increase the attractiveness of small local
markets. The South Korean government could develop financial support and risk
sharing programmes to enhance natural resource-seeking investments and mutual
exchange programmes to overcome the red syndrome complex in South Korean
society. </font><br> Link: <a href='http://arxiv.org/pdf/2212.13841v1' target="_blank">http://arxiv.org/pdf/2212.13841v1</a><br> <br> <br> <font size='5'> 377 </font> <div style="text-align: right"> 2022-12-26 08:32:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do not Waste Money on Advertising Spend: Bid Recommendation via Concavity Changes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In computational advertising, a challenging problem is how to recommend the
bid for advertisers to achieve the best return on investment (ROI) given budget
constraint. This paper presents a bid recommendation scenario that discovers
the concavity changes in click prediction curves. The recommended bid is
derived based on the turning point from significant increase (i.e. concave
downward) to slow increase (convex upward). Parametric learning based method is
applied by solving the corresponding constraint optimization problem. Empirical
studies on real-world advertising scenarios clearly demonstrate the performance
gains for business metrics (including revenue increase, click increase and
advertiser ROI increase). </font><br> Link: <a href='http://arxiv.org/pdf/2212.13923v1' target="_blank">http://arxiv.org/pdf/2212.13923v1</a><br> <br> <br> <font size='5'> 378 </font> <div style="text-align: right"> 2022-12-24 09:20:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automatic stabilization of finite-element simulations using neural networks and hierarchical matrices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Petrov-Galerkin formulations with optimal test functions allow for the
stabilization of finite element simulations. In particular, given a discrete
trial space, the optimal test space induces a numerical scheme delivering the
best approximation in terms of a problem-dependent energy norm. This ideal
approach has two shortcomings: first, we need to explicitly know the set of
optimal test functions; and second, the optimal test functions may have large
supports inducing expensive dense linear systems. Nevertheless, parametric
families of PDEs are an example where it is worth investing some (offline)
computational effort to obtain stabilized linear systems that can be solved
efficiently, for a given set of parameters, in an online stage. Therefore, as a
remedy for the first shortcoming, we explicitly compute (offline) a function
mapping any PDE-parameter, to the matrix of coefficients of optimal test
functions (in a basis expansion) associated with that PDE-parameter. Next, as a
remedy for the second shortcoming, we use the low-rank approximation to
hierarchically compress the (non-square) matrix of coefficients of optimal test
functions. In order to accelerate this process, we train a neural network to
learn a critical bottleneck of the compression algorithm (for a given set of
PDE-parameters). When solving online the resulting (compressed) Petrov-Galerkin
formulation, we employ a GMRES iterative solver with inexpensive matrix-vector
multiplications thanks to the low-rank features of the compressed matrix. We
perform experiments showing that the full online procedure as fast as the
original (unstable) Galerkin approach. In other words, we get the stabilization
with hierarchical matrices and neural networks practically for free. We
illustrate our findings by means of 2D Eriksson-Johnson and Hemholtz model
problems. </font><br> Link: <a href='http://arxiv.org/pdf/2212.12695v1' target="_blank">http://arxiv.org/pdf/2212.12695v1</a><br> <br> <br> <font size='5'> 379 </font> <div style="text-align: right"> 2022-12-24 07:49:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards a Multimodal Charging Network: Joint Planning of Charging Stations and Battery Swapping Stations for Electrified Ride-Hailing Fleets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper considers a multimodal charging network in which charging stations
and battery swapping stations are jointly built to support the electrified
ride-hailing fleet in a synergistic manner. Our central thesis is predicated on
the observation that charging stations are cost-effective, making them ideal
for scaling up electric vehicles in ride-hailing fleets in the beginning, while
battery swapping stations offer quick turnaround and can be deployed in tandem
with charging stations to improve fleet utilization and reduce operational
costs for the ride-hailing platform. To fulfill this vision, we consider a
ride-hailing platform that expands the multimodal charging network with a
multi-stage investment budget and operates a ride-hailing fleet to maximize its
profit. A multi-stage network expansion model is proposed to characterize the
coupled planning and operational decisions, which captures demand elasticity,
passenger waiting time, charging and swapping waiting times, as well as their
dependence on fleet status and charging infrastructure. The overall problem is
formulated as a nonconvex program. Instead of pursuing the globally optimal
solution, we establish a theoretical upper bound through relaxation,
reformulation, and decomposition so that the global optimality of solutions to
the nonconvex problem is verifiable. In the case study for Manhattan, we find
that ... </font><br> Link: <a href='http://arxiv.org/pdf/2212.12677v2' target="_blank">http://arxiv.org/pdf/2212.12677v2</a><br> <br> <br> <font size='5'> 380 </font> <div style="text-align: right"> 2022-12-23 03:00:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Survival of Tongue Cancer Patients by Machine Learning Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Tongue cancer is a common oral cavity malignancy that originates in the mouth
and throat. Much effort has been invested in improving its diagnosis,
treatment, and management. Surgical removal, chemotherapy, and radiation
therapy remain the major treatment for tongue cancer. The survival of patients
determines the treatment effect. Previous studies have identified certain
survival and risk factors based on descriptive statistics, ignoring the
complex, nonlinear relationship among clinical and demographic variables. In
this study, we utilize five cutting-edge machine learning models and clinical
data to predict the survival of tongue cancer patients after treatment.
Five-fold cross-validation, bootstrap analysis, and permutation feature
importance are applied to estimate and interpret model performance. The
prognostic factors identified by our method are consistent with previous
clinical studies. Our method is accurate, interpretable, and thus useable as
additional evidence in tongue cancer treatment and management. </font><br> Link: <a href='http://arxiv.org/pdf/2212.12114v1' target="_blank">http://arxiv.org/pdf/2212.12114v1</a><br> <br> <br> <font size='5'> 381 </font> <div style="text-align: right"> 2022-12-21 21:08:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Statistical Challenges in Online Controlled Experiments: A Review of A/B Testing Methodology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of internet-based services and products in the late 1990's brought
about an unprecedented opportunity for online businesses to engage in large
scale data-driven decision making. Over the past two decades, organizations
such as Airbnb, Alibaba, Amazon, Baidu, Booking, Alphabet's Google, LinkedIn,
Lyft, Meta's Facebook, Microsoft, Netflix, Twitter, Uber, and Yandex have
invested tremendous resources in online controlled experiments (OCEs) to assess
the impact of innovation on their customers and businesses. Running OCEs at
scale has presented a host of challenges requiring solutions from many domains.
In this paper we review challenges that require new statistical methodologies
to address them. In particular, we discuss the practice and culture of online
experimentation, as well as its statistics literature, placing the current
methodologies within their relevant statistical lineages and providing
illustrative examples of OCE applications. Our goal is to raise academic
statisticians' awareness of these new research opportunities to increase
collaboration between academia and the online industry. </font><br> Link: <a href='http://arxiv.org/pdf/2212.11366v2' target="_blank">http://arxiv.org/pdf/2212.11366v2</a><br> <br> <br> <font size='5'> 382 </font> <div style="text-align: right"> 2022-12-21 14:30:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What should clubs monitor to predict future value of football players</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Huge amounts of money are invested every year by football clubs on transfers.
For both growth and survival, it is crucial for recruiting departments to make
smart choices when targeting players. Therefore, it is very important to
identify the right parameters to monitor to predict market value. The following
paper aims at determining the relevant features that successfully forecast
future value for football players. Success is measured against their market
value from TransferMarkt. To select prominent features, we use Lasso
regressions and Random Forest algorithms. Some obvious variables are selected
but we also observe some subtle dependencies between features and future market
value. Finally, we rank the Golden Boy nominees using our forecasts and show
our methodology can successfully compare football players based on their
quality. </font><br> Link: <a href='http://arxiv.org/pdf/2212.11041v1' target="_blank">http://arxiv.org/pdf/2212.11041v1</a><br> <br> <br> <font size='5'> 383 </font> <div style="text-align: right"> 2022-12-20 16:08:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Differentiability and Regularization of Parametric Convex Value Functions in Stochastic Multistage Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In multistage decision problems, it is often the case that an initial
strategic decision (such as investment) is followed by many operational ones
(operating the investment). Such initial strategic decision can be seen as a
parameter affecting a multistage decision problem. More generally, we study in
this paper a standard multistage stochastic optimization problem depending on a
parameter. When the parameter is fixed, Stochastic Dynamic Programming provides
a way to compute the optimal value of the problem. Thus, the value function
depends both on the state (as usual) and on the parameter. Our aim is to
investigate on the possibility to efficiently compute gradients of the value
function with respect to the parameter, when these objects exist. When
nondifferentiable, we propose a regularization method based on the
Moreau-Yosida envelope. We present a numerical test case from day-ahead power
scheduling. </font><br> Link: <a href='http://arxiv.org/pdf/2212.10384v2' target="_blank">http://arxiv.org/pdf/2212.10384v2</a><br> <br> <br> <font size='5'> 384 </font> <div style="text-align: right"> 2022-12-16 17:42:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Thermal expansion and the glass transition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Melting is well understood in terms of the Lindemann criterion, essentially
stating that crystalline materials melt when the thermal vibrations of their
atoms become such vigorous that they shake themselves free of the binding
forces. However, how about another common type of solids: glasses, where the
nature of the solid-liquid crossover is highly controversial? The Lindemann
criterion implies that the thermal expansion coefficients alpha of crystals are
inversely proportional to their melting temperatures. Here we find that,
unexpectedly, alpha of glasses decreases much stronger with increasing
glass-transition temperature Tg marking the liquid-solid crossover in this
material class. However, scaling alpha by the fragility m, a measure of
particle cooperativity, restores the proportionality, i.e., alpha/m ~ 1/Tg.
Obviously, for a glass to become liquid, it is not sufficient to simply
overcome the interparticle binding energies. Instead, more energy has to be
invested to break up the typical cooperative particle network which is
considered a hallmark feature of glassy materials. Surprisingly, alpha of the
liquid phase reveals similar anomalous behaviour and is universally enhanced by
a constant factor of ~3. The found universalities allow estimating
glass-transition temperatures from thermal expansion and vice versa. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08612v1' target="_blank">http://arxiv.org/pdf/2212.08612v1</a><br> <br> <br> <font size='5'> 385 </font> <div style="text-align: right"> 2022-12-16 01:45:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Geometry-aware Autoregressive Models for Calorimeter Shower Simulations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Calorimeter shower simulations are often the bottleneck in simulation time
for particle physics detectors. A lot of effort is currently spent on
optimizing generative architectures for specific detector geometries, which
generalize poorly. We develop a geometry-aware autoregressive model on a range
of calorimeter geometries such that the model learns to adapt its energy
deposition depending on the size and position of the cells. This is a key
proof-of-concept step towards building a model that can generalize to new
unseen calorimeter geometries with little to no additional training. Such a
model can replace the hundreds of generative models used for calorimeter
simulation in a Large Hadron Collider experiment. For the study of future
detectors, such a model will dramatically reduce the large upfront investment
usually needed to generate simulations. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08233v1' target="_blank">http://arxiv.org/pdf/2212.08233v1</a><br> <br> <br> <font size='5'> 386 </font> <div style="text-align: right"> 2022-12-16 01:14:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rationally Inattentive Statistical Discrimination: Arrow Meets Phelps</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: When information acquisition is costly but flexible, a principal may
rationally acquire information that favors a ``majority'' group over
``minorities'' unless the latter are strictly more productive than the former
(the relative size of the groups plays no actual role). Majorities therefore
face incentives to invest in being productive to the principal, whereas
minorities are discouraged from such investments. The principal, in turn,
focuses scarce attentional resources on majorities precisely because they are
likely to invest. Our results have welfare and policy implications, as they add
to the discussion of affirmative action, as well as the empirical literature on
implicit bias and discrimination in performance evaluation. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08219v1' target="_blank">http://arxiv.org/pdf/2212.08219v1</a><br> <br> <br> <font size='5'> 387 </font> <div style="text-align: right"> 2022-12-15 23:48:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economic impacts of AI-augmented R&D</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since its emergence around 2010, deep learning has rapidly become the most
important technique in Artificial Intelligence (AI), producing an array of
scientific firsts in areas as diverse as protein folding, drug discovery,
integrated chip design, and weather prediction. As more scientists and
engineers adopt deep learning, it is important to consider what effect
widespread deployment would have on scientific progress and, ultimately,
economic growth. We assess this impact by estimating the idea production
function for AI in two computer vision tasks that are considered key test-beds
for deep learning and show that AI idea production is notably more
capital-intensive than traditional R&D. Because increasing the
capital-intensity of R&D accelerates the investments that make scientists and
engineers more productive, our work suggests that AI-augmented R&D has the
potential to speed up technological change and economic growth. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08198v2' target="_blank">http://arxiv.org/pdf/2212.08198v2</a><br> <br> <br> <font size='5'> 388 </font> <div style="text-align: right"> 2022-12-15 16:11:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Study on the Intersection of GPU Utilization and CNN Inference</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There has been significant progress in developing neural network
architectures that both achieve high predictive performance and that also
achieve high application-level inference throughput (e.g., frames per second).
Another metric of increasing importance is GPU utilization during inference:
the measurement of how well a deployed neural network uses the computational
capabilities of the GPU on which it runs. Achieving high GPU utilization is
critical to increasing application-level throughput and ensuring a good return
on investment for deploying GPUs.
  This paper analyzes the GPU utilization of convolutional neural network (CNN)
inference. We first survey the GPU utilization of CNNs to show that there is
room to improve the GPU utilization of many of these CNNs. We then investigate
the GPU utilization of networks within a neural architecture search (NAS)
search space, and explore how using GPU utilization as a metric could
potentially be used to accelerate NAS itself. Our study makes the case that
there is room to improve the inference-time GPU utilization of CNNs and that
knowledge of GPU utilization has the potential to benefit even applications
that do not target utilization itself. We hope that the results of this study
will spur future innovation in designing GPU-efficient neural networks. </font><br> Link: <a href='http://arxiv.org/pdf/2212.07936v1' target="_blank">http://arxiv.org/pdf/2212.07936v1</a><br> <br> <br> <font size='5'> 389 </font> <div style="text-align: right"> 2022-12-15 11:55:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Defending against cybersecurity threats to the payments and banking system</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cyber security threats to the payment and banking system have become a
worldwide menace. The phenomenon has forced financial institutions to take
risks as part of their business model. Hence, deliberate investment in
sophisticated technologies and security measures has become imperative to
safeguard against heavy financial losses and information breaches that may
occur due to cyber-attacks. The proliferation of cyber crimes is a huge concern
for various stakeholders in the banking sector. Usually, cyber-attacks are
carried out via software systems running on a computing system in cyberspace.
As such, to prevent risks of cyber-attacks on software systems, entities
operating within cyberspace must be identified and the threats to the
application security isolated after analyzing the vulnerabilities and
developing defense mechanisms. This paper will examine various approaches that
identify assets in cyberspace, classify the cyber threats, provide security
defenses and map security measures to control types and functionalities. Thus,
adopting the right application to the security threats and defenses will aid IT
professionals and users alike in making decisions for developing a strong
defense-in-depth mechanism. </font><br> Link: <a href='http://arxiv.org/pdf/2212.12307v1' target="_blank">http://arxiv.org/pdf/2212.12307v1</a><br> <br> <br> <font size='5'> 390 </font> <div style="text-align: right"> 2022-12-14 02:37:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Market Mechanisms for Low-Carbon Electricity Investments: A Game-Theoretical Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Electricity markets are transforming from the dominance of conventional
energy resources (CERs), e.g., fossil fuels, to low-carbon energy resources
(LERs), e.g., renewables and energy storage. This work examines market
mechanisms to incentivize LER investments, while ensuring adequate market
revenues for investors, guiding investors' strategic investments towards social
optimum, and protecting consumers from scarcity prices. To reduce the impact of
excessive scarcity prices, we present a new market mechanism, which consists of
a Penalty payment for lost load, a supply Incentive, and an energy price Uplift
(PIU). We establish a game-theoretical framework to analyze market equilibrium.
We prove that one Nash equilibrium under the penalty payment and supply
incentive can reach the social optimum given quadratic supply costs of CERs.
Although the price uplift can ensure adequate revenues, the resulting system
cost deviates from the social optimum while the gap decreases as more CERs
retire. Furthermore, under the traditional marginal-cost pricing (MCP)
mechanism, investors may withhold investments to cause scarcity prices, but
such behavior is absent under the PIU mechanism. Simulation results show that
the PIU mechanism can reduce consumers' costs by over 30% compared with the MCP
mechanism by reducing excessive revenues of low-cost CERs from scarcity prices. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06984v1' target="_blank">http://arxiv.org/pdf/2212.06984v1</a><br> <br> <br> <font size='5'> 391 </font> <div style="text-align: right"> 2022-12-13 18:58:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adversarial Attacks and Defences for Skin Cancer Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There has been a concurrent significant improvement in the medical images
used to facilitate diagnosis and the performance of machine learning techniques
to perform tasks such as classification, detection, and segmentation in recent
years. As a result, a rapid increase in the usage of such systems can be
observed in the healthcare industry, for instance in the form of medical image
classification systems, where these models have achieved diagnostic parity with
human physicians. One such application where this can be observed is in
computer vision tasks such as the classification of skin lesions in
dermatoscopic images. However, as stakeholders in the healthcare industry, such
as insurance companies, continue to invest extensively in machine learning
infrastructure, it becomes increasingly important to understand the
vulnerabilities in such systems. Due to the highly critical nature of the tasks
being carried out by these machine learning models, it is necessary to analyze
techniques that could be used to take advantage of these vulnerabilities and
methods to defend against them. This paper explores common adversarial attack
techniques. The Fast Sign Gradient Method and Projected Descent Gradient are
used against a Convolutional Neural Network trained to classify dermatoscopic
images of skin lesions. Following that, it also discusses one of the most
popular adversarial defense techniques, adversarial training. The performance
of the model that has been trained on adversarial examples is then tested
against the previously mentioned attacks, and recommendations to improve neural
networks robustness are thus provided based on the results of the experiment. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06822v1' target="_blank">http://arxiv.org/pdf/2212.06822v1</a><br> <br> <br> <font size='5'> 392 </font> <div style="text-align: right"> 2022-12-13 17:14:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image and Time Series Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The automated machine learning (AutoML) field has become increasingly
relevant in recent years. These algorithms can develop models without the need
for expert knowledge, facilitating the application of machine learning
techniques in the industry. Neural Architecture Search (NAS) exploits deep
learning techniques to autonomously produce neural network architectures whose
results rival the state-of-the-art models hand-crafted by AI experts. However,
this approach requires significant computational resources and hardware
investments, making it less appealing for real-usage applications. This article
presents the third version of Pareto-Optimal Progressive Neural Architecture
Search (POPNASv3), a new sequential model-based optimization NAS algorithm
targeting different hardware environments and multiple classification tasks.
Our method is able to find competitive architectures within large search
spaces, while keeping a flexible structure and data processing pipeline to
adapt to different tasks. The algorithm employs Pareto optimality to reduce the
number of architectures sampled during the search, drastically improving the
time efficiency without loss in accuracy. The experiments performed on images
and time series classification datasets provide evidence that POPNASv3 can
explore a large set of assorted operators and converge to optimal architectures
suited for the type of data provided under different scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06735v1' target="_blank">http://arxiv.org/pdf/2212.06735v1</a><br> <br> <br> <font size='5'> 393 </font> <div style="text-align: right"> 2022-12-13 15:45:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Observation of Acoustic Non-Hermitian Bloch Braids and Associated Topological Phase Transitions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Topological features embedded in ancient braiding and knotting arts endow
significant impacts on our daily life and even cutting-edge science. Recently,
fast growing efforts are invested to the braiding topology of complex Bloch
bands in non-Hermitian systems. This new classification of band topology goes
far beyond those established in Hermitian counterparts. Here, we present the
first acoustic realization of the topological non-Hermitian Bloch braids, based
on a two-band model easily accessible for realizing any desired knot structure.
The non-Hermitian bands are synthesized by a simple binary cavity-tube system,
where the long-range, complex-valued, and momentum-resolved couplings are
accomplished by a well-controlled unidirectional coupler. In addition to
directly visualizing various two-band braiding patterns, we unambiguously
observe the highly-elusive topological phase transitions between them. Not only
do our results provide a direct demonstration for the non-Hermitian band
topology, but also the experimental techniques open new avenues for designing
unconventional acoustic metamaterials. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06667v1' target="_blank">http://arxiv.org/pdf/2212.06667v1</a><br> <br> <br> <font size='5'> 394 </font> <div style="text-align: right"> 2022-12-13 15:12:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Twisted mass ensemble generation on GPU machines</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present how we ported the Hybrid Monte Carlo implementation in the tmLQCD
software suite to GPUs through offloading its most expensive parts to the QUDA
library. We discuss our motivations and some of the technical challenges that
we encountered as we added the required functionality to both tmLQCD and QUDA.
We further present some performance details, focussing in particular on the
usage of QUDA's multigrid solver for poorly conditioned light quark monomials
as well as the multi-shift solver for the non-degenerate strange and charm
sector in $N_f=2+1+1$ simulations using twisted mass clover fermions, comparing
the efficiency of state-of-the-art simulations on CPU and GPU machines. We also
take a look at the performance-portability question through preliminary tests
of our HMC on a machine based on AMD's MI250 GPU, finding good performance
after a very minor additional porting effort. Finally, we conclude that we
should be able to achieve GPU utilisation factors acceptable for the current
generation of (pre-)exascale supercomputers with subtantial efficiency
improvements and real time speedups compared to just running on CPUs. At the
same time, we find that future challenges will require different approaches
and, most importantly, a very significant investment of personnel for software
development. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06635v1' target="_blank">http://arxiv.org/pdf/2212.06635v1</a><br> <br> <br> <font size='5'> 395 </font> <div style="text-align: right"> 2022-12-13 11:53:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artificial Intelligence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantitative investment (``quant'') is an interdisciplinary field combining
financial engineering, computer science, mathematics, statistics, etc. Quant
has become one of the mainstream investment methodologies over the past
decades, and has experienced three generations: Quant 1.0, trading by
mathematical modeling to discover mis-priced assets in markets; Quant 2.0,
shifting quant research pipeline from small ``strategy workshops'' to large
``alpha factories''; Quant 3.0, applying deep learning techniques to discover
complex nonlinear pricing rules. Despite its advantage in prediction, deep
learning relies on extremely large data volume and labor-intensive tuning of
``black-box'' neural network models. To address these limitations, in this
paper, we introduce Quant 4.0 and provide an engineering perspective for
next-generation quant. Quant 4.0 has three key differentiating components.
First, automated AI changes quant pipeline from traditional hand-craft modeling
to the state-of-the-art automated modeling, practicing the philosophy of
``algorithm produces algorithm, model builds model, and eventually AI creates
AI''. Second, explainable AI develops new techniques to better understand and
interpret investment decisions made by machine learning black-boxes, and
explains complicated and hidden risk exposures. Third, knowledge-driven AI is a
supplement to data-driven AI such as deep learning and it incorporates prior
knowledge into modeling to improve investment decision, in particular for
quantitative value investing. Moreover, we discuss how to build a system that
practices the Quant 4.0 concept. Finally, we propose ten challenging research
problems for quant technology, and discuss potential solutions, research
directions, and future trends. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04020v1' target="_blank">http://arxiv.org/pdf/2301.04020v1</a><br> <br> <br> <font size='5'> 396 </font> <div style="text-align: right"> 2022-12-12 07:15:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Decentralized lending and its users: Insights from Compound</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Permissionless blockchains offer an information environment where users can
interact privately without fear of censorship. Financial services can be
programmatically coded via smart contracts to automate transactions without the
need for human intervention or knowing user identity. This new paradigm is
known as decentralized finance (DeFi). We investigate Compound (a leading DeFi
lending protocol) to show how it works in this novel information environment,
who its users are, and what factors determine their participation. On-chain
transaction data shows that loan durations are short (31 days on average), and
many users borrow to support leveraged investment strategies (yield farming).
We show that systemic risk in DeFi arises from concentration and
interconnection, and how traditional risk management practices can be
challenging for DeFi. </font><br> Link: <a href='http://arxiv.org/pdf/2212.05734v1' target="_blank">http://arxiv.org/pdf/2212.05734v1</a><br> <br> <br> <font size='5'> 397 </font> <div style="text-align: right"> 2022-12-11 14:22:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why Should and How Can Quantum Technologies Be Leveraged at National Levels?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum technologies (QT) promise to change the landscape of technologies
disruptively in diverse industries. For this reason, many nations around the
globe are investing to emerge within the global quantum ecosystem through
initiating national programs and international partnerships. Nonetheless, some
other countries are still running behind and yet their governments need to take
series actions to help their private and public sectors adapt to the looming
changes, considering the new regulations required and the huge influence that
QT will present in the near future. In this opinion piece, we provide, for the
best of our knowledge, the first generally applicable, yet comprehensive and
brief, framework for leveraging the emerging quantum technologies to facilitate
the establishment of national initiatives properly. The insights presented in
this article were driven based on investigating various approaches,
initiatives, and roadmaps adopted globally and meeting with local and regional
leaders, professionals, and governmental officials. Furthermore, taken into
account socioeconomic and institutional dimensions of the Libyan society, we
project the framework for the Libyan nation. This opinion piece is intended to
inspire researchers, technical industrial experts, stakeholders, and
governmental bodies to find roles they need to play to bring QT forward. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08040v1' target="_blank">http://arxiv.org/pdf/2212.08040v1</a><br> <br> <br> <font size='5'> 398 </font> <div style="text-align: right"> 2022-12-10 14:52:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Consumption Decision, Portfolio Choice and Healthcare Irreversible Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a tractable dynamic framework for the joint determination of
optimal consumption, portfolio choice, and healthcare irreversible investment.
Our model is based on a Merton's portfolio and consumption problem, where, in
addition, the agent can choose the time at which undertaking a costly lump sum
health investment decision. Health depreciates with age and directly affects
the agent's mortality force, so that investment into healthcare reduces the
agent's mortality risk. The resulting optimization problem is formulated as a
stochastic control-stopping problem with a random time-horizon and
state-variables given by the agent's wealth and health capital. We transform
this problem into its dual version, which is now a two-dimensional optimal
stopping problem with interconnected dynamics and finite time-horizon.
Regularity of the optimal stopping value function is derived and the related
free boundary surface is proved to be Lipschitz continuous and it is
characterized as the unique solution to a nonlinear integral equation. In the
original coordinates, the agent thus invests into healthcare whenever her
wealth exceeds an age- and health-dependent transformed version of the optimal
stopping boundary. </font><br> Link: <a href='http://arxiv.org/pdf/2212.05317v1' target="_blank">http://arxiv.org/pdf/2212.05317v1</a><br> <br> <br> <font size='5'> 399 </font> <div style="text-align: right"> 2022-12-10 03:52:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Walkability Optimization: Formulations, Algorithms, and a Case Study of Toronto</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The concept of walkable urban development has gained increased attention due
to its public health, economic, and environmental sustainability benefits.
Unfortunately, land zoning and historic under-investment have resulted in
spatial inequality in walkability and social inequality among residents. We
tackle the problem of Walkability Optimization through the lens of
combinatorial optimization. The task is to select locations in which additional
amenities (e.g., grocery stores, schools, restaurants) can be allocated to
improve resident access via walking while taking into account existing
amenities and providing multiple options (e.g., for restaurants). To this end,
we derive Mixed-Integer Linear Programming (MILP) and Constraint Programming
(CP) models. Moreover, we show that the problem's objective function is
submodular in special cases, which motivates an efficient greedy heuristic. We
conduct a case study on 31 underserved neighborhoods in the City of Toronto,
Canada. MILP finds the best solutions in most scenarios but does not scale well
with network size. The greedy algorithm scales well and finds near-optimal
solutions. Our empirical evaluation shows that neighbourhoods with low
walkability have a great potential for transformation into pedestrian-friendly
neighbourhoods by strategically placing new amenities. Allocating 3 additional
grocery stores, schools, and restaurants can improve the "WalkScore" by more
than 50 points (on a scale of 100) for 4 neighbourhoods and reduce the walking
distances to amenities for 75% of all residential locations to 10 minutes for
all amenity types. Our code and paper appendix are available at
https://github.com/khalil-research/walkability. </font><br> Link: <a href='http://arxiv.org/pdf/2212.05192v1' target="_blank">http://arxiv.org/pdf/2212.05192v1</a><br> <br> <br> <font size='5'> 400 </font> <div style="text-align: right"> 2022-12-09 13:48:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Achieving a Given Financial Goal with Optimal Deferred Term Insurance Purchasing Policy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper researches the problem of purchasing deferred term insurance in
the context of financial planning to maximize the probability of achieving a
personal financial goal. Specifically, our study starts from the perspective of
hedging death risk and longevity risk, and considers the purchase of deferred
term life insurance and deferred term pure endowment to achieve a given
financial goal for the first time in both deterministic and stochastic
framework. In particular, we consider income, consumption and risky investment
in the stochastic framework, extending previous results in
\cite{Bayraktar2016}. The time cutoff m and n make the work more difficult.
However, by establishing new controls,``\emph{quasi-ideal value}"
and``\emph{ideal value}", we solve the corresponding ordinary differential
equations or stochastic differential equations, and give the specific
expressions for the maximum probability. Then we provide the optimal life
insurance purchasing strategies and the optimal risk investment strategies. In
general, when m \geqslant 0, n>0, deferred term insurance or term life
insurance is a better choice for those who want to achieve their financial or
bequest goals but are not financially sound. In particular, if m >0, n
\rightarrow \infty, our viewpoint also sheds light on reaching a bequest goal
by purchasing deferred whole life insurance. It is worth noting that when m=0,
n \rightarrow \infty, our problem is equivalent to achieving the just mentioned
bequest goal by purchasing whole life insurance, at which point the maximum
probability and the life insurance purchasing strategies we provide are
consistent with those in \cite{Bayraktar2014, Bayraktar2016}. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04118v1' target="_blank">http://arxiv.org/pdf/2301.04118v1</a><br> <br> <br> <font size='5'> 401 </font> <div style="text-align: right"> 2022-12-09 10:19:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Sizing and Pricing of Renewable Power to Ammonia Systems Considering the Limited Flexibility of Ammonia Synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Converting renewable energy into ammonia has been recognized as a promising
way to realize ``green hydrogen substitution" in the chemical industry.
However, renewable power to ammonia (RePtA) requires an essential investment in
facilities to provide a buffer against the strong volatility of renewable
energy and the limited flexibility of ammonia synthesis, which involves the
three main stakeholders, namely, power, hydrogen, and ammonia. Therefore, the
sizing and pricing of RePtA play a core role in balancing the interest demands
of investors. This paper proposes an optimal sizing and pricing method for
RePtA system planning. First, power to ammonia (P2A) is modeled as a flexible
load, especially considering the limited flexibility of ammonia synthesis,
which has been verified using real dynamic regulation data. Second, the
multi-investor economic (MIE) model is established considering both external
and internal trading modes. Then, a two-stage decomposed sizing and pricing
method is proposed to solve the problem caused by the strong coupling of
planning, operation, and trading, and information gap decision theory (IGDT)
method is utilized to handle the uncertainty of renewable generation. Finally,
real data from a real-life system in Inner Mongolia are utilized to verify the
proposed approach. The results show that the system proposed has a yield of
8.15%. </font><br> Link: <a href='http://arxiv.org/pdf/2212.04754v1' target="_blank">http://arxiv.org/pdf/2212.04754v1</a><br> <br> <br> <font size='5'> 402 </font> <div style="text-align: right"> 2022-12-09 01:32:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Arbitrage theory in a market of stochastic dimension</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies an equity market of stochastic dimension, where the number
of assets fluctuates over time. In such a market, we develop the fundamental
theorem of asset pricing, which provides the equivalence of the following
statements: (i) there exists a supermartingale num\'eraire portfolio; (ii) each
dissected market, which is of a fixed dimension between dimensional jumps, has
locally finite growth; (iii) there is no arbitrage of the first kind; (iv)
there exists a local martingale deflator; (v) the market is viable. We also
present the optional decomposition theorem, which characterizes a given
nonnegative process as the wealth process of some investment-consumption
strategy. Furthermore, similar results still hold in an open market embedded in
the entire market of stochastic dimension, where investors can only invest in a
fixed number of large capitalization stocks. These results are developed in an
equity market model where the price process is given by a piecewise continuous
semimartingale of stochastic dimension. Without the continuity assumption on
the price process, we present similar results but without explicit
characterization of the num\'eraire portfolio. </font><br> Link: <a href='http://arxiv.org/pdf/2212.04623v2' target="_blank">http://arxiv.org/pdf/2212.04623v2</a><br> <br> <br> <font size='5'> 403 </font> <div style="text-align: right"> 2022-12-08 21:42:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Language-conditioned policies allow robots to interpret and execute human
instructions. Learning such policies requires a substantial investment with
regards to time and compute resources. Still, the resulting controllers are
highly device-specific and cannot easily be transferred to a robot with
different morphology, capability, appearance or dynamics. In this paper, we
propose a sample-efficient approach for training language-conditioned
manipulation policies that allows for rapid transfer across different types of
robots. By introducing a novel method, namely Hierarchical Modularity, and
adopting supervised attention across multiple sub-modules, we bridge the divide
between modular and end-to-end learning and enable the reuse of functional
building blocks. In both simulated and real world robot manipulation
experiments, we demonstrate that our method outperforms the current
state-of-the-art methods and can transfer policies across 4 different robots in
a sample-efficient manner. Finally, we show that the functionality of learned
sub-modules is maintained beyond the training process and can be used to
introspect the robot decision-making process. Code is available at
https://github.com/ir-lab/ModAttn. </font><br> Link: <a href='http://arxiv.org/pdf/2212.04573v1' target="_blank">http://arxiv.org/pdf/2212.04573v1</a><br> <br> <br> <font size='5'> 404 </font> <div style="text-align: right"> 2022-12-08 16:52:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment under partial observations and robust VaR-type constraint</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The present paper extends the literature on utility maximization by combining
the framework of partial information and (robust) regulatory constraints.
Partial information is characterized by the fact that the stock price itself is
observable to the optimizing financial institution, but the outcome of the
market price of risk $\theta$ is unknown to the institution. The regulator
builds the same or a different belief about the market price of risk as the
financial institution. The solution to our optimization problem takes the same
form as in the full information case: the optimal wealth can be expressed as a
decreasing function of the state price density, and the regulatory threshold is
ensured in the intermediate economic states. The main difference lies in the
terminal state price density depending on the entire evolution of the estimated
market price of risk $\hat{\theta}(s)$. The subjective evaluation of the
regulatory constraint influences the width of the ensured region. </font><br> Link: <a href='http://arxiv.org/pdf/2212.04394v1' target="_blank">http://arxiv.org/pdf/2212.04394v1</a><br> <br> <br> <font size='5'> 405 </font> <div style="text-align: right"> 2022-12-08 16:13:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DECO2 An Open-source Energy System Decarbonisation Planning Software Including Negative Emissions Technologies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The deployment of CO2 capture and storage (CCS) and negative emissions
technologies (NETs) are crucial to meet the net-zero target by year 2050, as
emphasised by the Glasgow Climate Pact. Over the years, several energy planning
models have been developed to address the temporal aspects of carbon
management. However, limited works have incorporated CCS and NETs for bottom-up
energy planning at the individual plant scale, which is considered in this
work. The novel formulation is implemented in an open-source energy system
software that has been developed in this work for optimal decarbonisation
planning. The DECarbonation Options Optimisation (DECO2) software considers
multiperiod energy planning with a superstructural model and was developed in
Python with an integrated user interface in Microsoft Excel. The software
application is demonstrated with two scenarios that differ in terms of the
availabilities of mitigation technologies. Results demonstrated the potential
of fuel substitutions for low-carbon alternatives in existing coal and natural
gas power plants. Additionally, once NETs are mature and are available for
commercial deployment, their deployment is crucial in aiding CO2 removal in
minimal investment costs scenarios. Overall, the newly developed open-source
software demonstrates the importance of determining the optimal deployment of
mitigation technologies in meeting climate change targets for each period. </font><br> Link: <a href='http://arxiv.org/pdf/2212.04372v1' target="_blank">http://arxiv.org/pdf/2212.04372v1</a><br> <br> <br> <font size='5'> 406 </font> <div style="text-align: right"> 2022-12-07 04:39:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fallen Angel Bonds Investment and Bankruptcy Predictions Using Manual Models and Automated Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The primary aim of this research was to find a model that best predicts which
fallen angel bonds would either potentially rise up back to investment grade
bonds and which ones would fall into bankruptcy. To implement the solution, we
thought that the ideal method would be to create an optimal machine learning
model that could predict bankruptcies. Among the many machine learning models
out there we decided to pick four classification methods: logistic regression,
KNN, SVM, and NN. We also utilized an automated methods of Google Cloud's
machine learning.
  The results of our model comparisons showed that the models did not predict
bankruptcies very well on the original data set with the exception of Google
Cloud's machine learning having a high precision score. However, our
over-sampled and feature selection data set did perform very well. This could
likely be due to the model being over-fitted to match the narrative of the
over-sampled data (as in, it does not accurately predict data outside of this
data set quite well). Therefore, we were not able to create a model that we are
confident that would predict bankruptcies.
  However, we were able to find value out of this project in two key ways. The
first is that Google Cloud's machine learning model in every metric and in
every data set either outperformed or performed on par with the other models.
The second is that we found that utilizing feature selection did not reduce
predictive power that much. This means that we can reduce the amount of data to
collect for future experimentation regarding predicting bankruptcies. </font><br> Link: <a href='http://arxiv.org/pdf/2212.03454v2' target="_blank">http://arxiv.org/pdf/2212.03454v2</a><br> <br> <br> <font size='5'> 407 </font> <div style="text-align: right"> 2022-12-07 00:57:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robo-Chargers: Optimal Operation and Planning of a Robotic Charging System to Alleviate Overstay</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Charging infrastructure availability is a major concern for plug-in electric
vehicle users. Nowadays, the limited public chargers are commonly occupied by
vehicles which have already been fully charged. Such phenomenon, known as
overstay, hinders other vehicles' accessibility to charging resources. In this
paper, we analyze a charging facility innovation to tackle the challenge of
overstay, leveraging the idea of Robo-chargers - automated chargers that can
rotate in a charging station and proactively plug or unplug plug-in electric
vehicles. We formalize an operation model for stations incorporating
Fixed-chargers and Robo-chargers. Optimal scheduling can be solved with the
recognition of the combinatorial nature of vehicle-charger assignments,
charging dynamics, and customer waiting behaviors. Then, with operation model
nested, we develop a planning model to guide economical investment on both
types of chargers so that the total cost of ownership is minimized. In the
planning phase, it further considers charging demand variances and service
capacity requirements. In this paper, we provide systematic techno-economical
methods to evaluate if introducing Robo-chargers is beneficial given a specific
application scenario. Comprehensive sensitivity analysis based on real-world
data highlights the advantages of Robo-chargers, especially in a scenario where
overstay is severe. Validations also suggest the tractability of operation
model and robustness of planning results for real-time application under
reasonable model mismatches, uncertainties and disturbances. </font><br> Link: <a href='http://arxiv.org/pdf/2212.03391v2' target="_blank">http://arxiv.org/pdf/2212.03391v2</a><br> <br> <br> <font size='5'> 408 </font> <div style="text-align: right"> 2022-12-05 12:16:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rethinking Generative Methods for Image Restoration in Physics-based Vision: A Theoretical Analysis from the Perspective of Information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: End-to-end generative methods are considered a more promising solution for
image restoration in physics-based vision compared with the traditional
deconstructive methods based on handcrafted composition models. However,
existing generative methods still have plenty of room for improvement in
quantitative performance. More crucially, these methods are considered black
boxes due to weak interpretability and there is rarely a theory trying to
explain their mechanism and learning process. In this study, we try to
re-interpret these generative methods for image restoration tasks using
information theory. Different from conventional understanding, we analyzed the
information flow of these methods and identified three sources of information
(extracted high-level information, retained low-level information, and external
information that is absent from the source inputs) are involved and optimized
respectively in generating the restoration results. We further derived their
learning behaviors, optimization objectives, and the corresponding information
boundaries by extending the information bottleneck principle. Based on this
theoretic framework, we found that many existing generative methods tend to be
direct applications of the general models designed for conventional generation
tasks, which may suffer from problems including over-invested abstraction
processes, inherent details loss, and vanishing gradients or imbalance in
training. We analyzed these issues with both intuitive and theoretical
explanations and proved them with empirical evidence respectively. Ultimately,
we proposed general solutions or ideas to address the above issue and validated
these approaches with performance boosts on six datasets of three different
image restoration tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2212.02198v2' target="_blank">http://arxiv.org/pdf/2212.02198v2</a><br> <br> <br> <font size='5'> 409 </font> <div style="text-align: right"> 2022-12-05 10:38:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interactive Planning and Operations using Peak Load Pricing in Distribution Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of Distributed Energy Resources (DERs) provides both challenges
and opportunities for the planning and operations of distribution systems.
These resources can be deployed in a manner that is either complementary to or
in competition with traditional network operations and planning as the DERs can
provide numerous important services to grid operators and utilities. This paper
presents a novel method to estimate the trade off between DERs and traditional
investments using a dynamic Peak-Load Pricing (PLP) methodology. PLP is a
pricing strategy for a time-dependent quantity of a non-storable commodity and
is based on the theory of long-run marginal costs. Importantly PLP deals with
the trade-off between capacity utilization and consumer welfare. Therefore, the
capacity price is set at a point where the cost of investment is exactly offset
by the additional social welfare that the investment would bring. Importantly
it allows for capital cost recovery with no uplift payments. This dynamic PLP
methodology is an interactive planning and operations model based on the
Dynamic Monitoring and Decision Systems (DyMonDS) Framework which helps to
align physical, information, and economic incentives across many stakeholders
within the electric energy system. The DyMonDS framework helps to solve the
drawbacks of PLP, which are related to the computational complexity of the PLP
models considering different technologies with different payback periods over a
long investment horizon. Results show that the dynamic PLP model can accurately
value the impact of different technologies in reducing congestion and
increasing the number of customers served. This allows the network operator to
easily identify which technologies should be chosen in each investment cycle. </font><br> Link: <a href='http://arxiv.org/pdf/2212.02145v1' target="_blank">http://arxiv.org/pdf/2212.02145v1</a><br> <br> <br> <font size='5'> 410 </font> <div style="text-align: right"> 2022-12-02 15:27:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Bumpy Road of Taking Automated Debugging to Industry</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Debugging is arguably among the most difficult and extremely time consuming
tasks of the software development life cycle. Therefore, it comes as no
surprise that researchers have invested a considerable amount of effort in
developing automated techniques and tools to support developers excel in these
tasks. Despite the significant advances, including demonstrations of
usefulness, efficacy, and efficiency, these techniques are yet to find their
way into industrial adoption. In this paper, we reflect upon the
commercialization efforts of a particular automated debugging technique and lay
down potential reasons for lack of success stories as well as ideas to move
forward. </font><br> Link: <a href='http://arxiv.org/pdf/2212.01237v1' target="_blank">http://arxiv.org/pdf/2212.01237v1</a><br> <br> <br> <font size='5'> 411 </font> <div style="text-align: right"> 2022-12-02 09:47:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment and reinsurance policies for the Cram{}r-Lundberg risk model under monotone mean-variance preference</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, an optimization problem for the monotone mean-variance(MMV)
criterion is considered in the perspective of the insurance company. The MMV
criterion is an amended version of the classical mean-variance(MV) criterion
which guarantees the monotonicity of the utility function. With this criterion
we study the optimal investment and reinsurance problem which is formulated as
a zero-sum game between the insurance company and an imaginary player. We apply
the dynamic programming principle to obtain the corresponding
Hamilton-Jacobi-Bellman-Isaacs(HJBI) equation. As the main conclusion of this
paper, by solving the HJBI equation explicitly, the closed forms of the optimal
strategy and the value function are obtained. Moreover, the MMV efficient
frontier is also provided. At the end of the paper, a numerical example is
presented. </font><br> Link: <a href='http://arxiv.org/pdf/2212.01056v1' target="_blank">http://arxiv.org/pdf/2212.01056v1</a><br> <br> <br> <font size='5'> 412 </font> <div style="text-align: right"> 2022-12-01 17:30:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantity restrictions and price discounts on Russian oil</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Following Russia's invasion of Ukraine, Western countries have looked for
ways to limit Russia's oil income. This paper considers, theoretically and
quantitatively, two such options: 1) an export-quantity restriction and 2) a
forced discount on Russian oil. We build a quantifiable model of the global oil
market and analyze how each of these policies affect: which Russian oil fields
fall out of production; the global oil supply; and the global oil price. By
these statics we derive the effects of the policies on Russian oil profits and
oil-importers' economic surplus. The effects on Russian oil profits are
substantial. In the short run (within the first year), a quantity restriction
of 20% yields Russian losses of 62 million USD per day, equivalent to 1.2% of
GDP and 32% of military spending. In the long run (beyond a year) new
investments become unprofitable. Losses rise to 100 million USD per day, 2% of
GDP and 56% of military spending. A price discount of 20% is even more harmful
to Russia, yielding losses of 152 million USD per day, equivalent to 3.1% of
GDP and 85% of military spending in the short run and long run. A price
discount puts generally more burden on Russia and less on importers compared to
a quantity restriction. In fact, a price discount implies net gains for oil
importers as it essentially redistributes oil rents from Russia to importers.
If the restrictions are expected to last for long, the burden on oil importers
decreases. Overall, both policies at all levels imply larger relative losses
for Russia than for oil importers (in shares of their GDP). The case for a
price discount on Russian oil is thus strong. However, Russia may choose not to
export at the discounted price, in which case the price-discount sanction
becomes a de facto supply restriction. </font><br> Link: <a href='http://arxiv.org/pdf/2212.00674v2' target="_blank">http://arxiv.org/pdf/2212.00674v2</a><br> <br> <br> <font size='5'> 413 </font> <div style="text-align: right"> 2022-12-01 09:37:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dynamic and static fund separations and their stability for long-term optimal investments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates dynamic and static fund separations and their
stability for long-term optimal investments under three model classes. An
investor maximizes the expected utility with constant relative risk aversion
under an incomplete market consisting of a safe asset, several risky assets,
and a single state variable. The state variables in two of the model classes
follow a 3/2 process and an inverse Bessel process, respectively. The other
market model has the partially observed state variable modeled as an
Ornstein-Uhlenbeck state process. We show that the dynamic optimal portfolio of
this utility maximization consists of m+3 portfolios: the safe asset, the
myopic portfolio, the m time-independent portfolios, and the intertemporal
portfolio. Over time, the intertemporal portfolio eventually vanishes, leading
the dynamic portfolio to converge to m+2 portfolios, referred to as the static
portfolio. We also prove that the convergence is stable under model parameter
perturbations. In addition, sensitivities of the intertemporal portfolio with
respect to small parameters perturbations also vanish in the long run. The
convergence rate for the intertemporal portfolio and its sensitivities are
computed explicitly for the presented models. </font><br> Link: <a href='http://arxiv.org/pdf/2212.00391v2' target="_blank">http://arxiv.org/pdf/2212.00391v2</a><br> <br> <br> <font size='5'> 414 </font> <div style="text-align: right"> 2022-11-30 17:05:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Computational drug repurposing is a cost- and time-efficient
approach that aims to identify new therapeutic targets or diseases
(indications) of existing drugs/compounds. It is especially critical for
emerging and/or orphan diseases due to its cheaper investment and shorter
research cycle compared with traditional wet-lab drug discovery approaches.
However, the underlying mechanisms of action (MOAs) between repurposed drugs
and their target diseases remain largely unknown, which is still a main
obstacle for computational drug repurposing methods to be widely adopted in
clinical settings.
  Results: In this work, we propose KGML-xDTD: a Knowledge Graph-based Machine
Learning framework for explainably predicting Drugs Treating Diseases. It is a
two-module framework that not only predicts the treatment probabilities between
drugs/compounds and diseases but also biologically explains them via knowledge
graph (KG) path-based, testable mechanisms of action (MOAs). We leverage
knowledge-and-publication based information to extract biologically meaningful
"demonstration paths" as the intermediate guidance in the Graph-based
Reinforcement Learning (GRL) path-finding process. Comprehensive experiments
and case study analyses show that the proposed framework can achieve
state-of-the-art performance in both predictions of drug repurposing and
recapitulation of human-curated drug MOA paths.
  Conclusions: KGML-xDTD is the first model framework that can offer KG-path
explanations for drug repurposing predictions by leveraging the combination of
prediction outcomes and existing biological knowledge and publications. We
believe it can effectively reduce "black-box" concerns and increase prediction
confidence for drug repurposing based on predicted path-based explanations, and
further accelerate the process of drug discovery for emerging diseases. </font><br> Link: <a href='http://arxiv.org/pdf/2212.01384v2' target="_blank">http://arxiv.org/pdf/2212.01384v2</a><br> <br> <br> <font size='5'> 415 </font> <div style="text-align: right"> 2022-11-30 15:31:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Security Investment Over Networks with Bounded Rational Agents: Analysis and Distributed Algorithm</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper considers the security investment problem over a network in which
the resource owners aim to allocate their constrained security resources to
heterogeneous targets strategically. Investing in each target makes it less
vulnerable, and thus lowering its probability of a successful attack. However,
humans tend to perceive such probabilities inaccurately yielding bounded
rational behaviors; a phenomenon frequently observed in their decision-making
when facing uncertainties. We capture this human nature through the lens of
cumulative prospect theory and establish a behavioral resource allocation
framework to account for the human's misperception in security investment. We
analyze how this misperception behavior affects the resource allocation plan by
comparing it with the accurate perception counterpart. The network can become
highly complex with a large number of participating agents. To this end, we
further develop a fully distributed algorithm to compute the behavioral
security investment strategy efficiently. Finally, we corroborate our results
and illustrate the impacts of human's bounded rationality on the resource
allocation scheme using cases studies. </font><br> Link: <a href='http://arxiv.org/pdf/2211.17072v2' target="_blank">http://arxiv.org/pdf/2211.17072v2</a><br> <br> <br> <font size='5'> 416 </font> <div style="text-align: right"> 2022-11-30 13:42:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Crowdfunding as Entrepreneurial Investment: The Role of Local Knowledge Spillover</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores the role of local knowledge spillover and human capital
as a driver of crowdfunding investment. The role of territory has already been
studied in terms of campaign success, but the impact of territory on the use of
financial sources like equity crowdfunding is not yet known. Using a sample of
435 equity crowdfunding campaigns in 20 Italian regions during a 4-year period
(from 2016 to 2019), this paper evaluates the impact of human capital flow on
the adoption of crowdfunding campaigns. Our results show that inbound knowledge
in the region, measured in terms of ability to attract national and
international students, has a significant effect on the adoption of
crowdfunding campaigns in the region itself. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16984v1' target="_blank">http://arxiv.org/pdf/2211.16984v1</a><br> <br> <br> <font size='5'> 417 </font> <div style="text-align: right"> 2022-11-30 13:20:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Federated deep clustering with GAN-based data synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Clustering has been extensively studied in centralized settings, but
relatively unexplored in federated ones that data are distributed among
multiple clients and can only be kept local at the clients. The necessity to
invest more resources in improving federated clustering methods is twofold: 1)
The performance of supervised federated learning models can benefit from
clustering. 2) It is non-trivial to extend centralized ones to perform
federated clustering tasks. In centralized settings, various deep clustering
methods that perform dimensionality reduction and clustering jointly have
achieved great success. To obtain high-quality cluster information, it is
natural but non-trivial to extend these methods to federated settings. For this
purpose, we propose a simple but effective federated deep clustering method. It
requires only one communication round between the central server and clients,
can run asynchronously, and can handle device failures. Moreover, although most
studies have highlighted adverse effects of the non-independent and identically
distributed (non-IID) data across clients, experimental results indicate that
the proposed method can significantly benefit from this scenario. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16965v1' target="_blank">http://arxiv.org/pdf/2211.16965v1</a><br> <br> <br> <font size='5'> 418 </font> <div style="text-align: right"> 2022-11-30 08:54:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Measurement of Investment activity in China based on Natural language processing technology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The purpose of this study is to propose a new index to measure and reflect
China's investment activity in time, and to analyze the changes of China's
investment activity in the past five years. This study first uses the NEZHA
model for semantic representation, and expand the indicator system based on
semantic similarity. Then we calculate China's investment activity index by
using the network search data. This study shows that China's investment
activity began to decline in 2019, rebounded for a period of time after the
outbreak of COVID-19 in 2020, and then continued to maintain a downward trend.
Private investment activity has declined significantly, while government
investment activity has increased. Among the provinces in Chinese Mainland, the
investment activity of economically developed provinces has decreased
significantly, while the investment activity of some economically less
developed provinces in the north and south is higher. After the outbreak of
COVID-19, the investment period became shorter. Our research will provide
timely investment information for the government, decision makers and managers,
as well as provide other researchers who also pay attention to investment with
a perspective other than investment in fixed asset. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16829v4' target="_blank">http://arxiv.org/pdf/2211.16829v4</a><br> <br> <br> <font size='5'> 419 </font> <div style="text-align: right"> 2022-11-30 04:48:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding transit ridership in an equity context through a comparison of statistical and machine learning algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building an accurate model of travel behaviour based on individuals'
characteristics and built environment attributes is of importance for
policy-making and transportation planning. Recent experiments with big data and
Machine Learning (ML) algorithms toward a better travel behaviour analysis have
mainly overlooked socially disadvantaged groups. Accordingly, in this study, we
explore the travel behaviour responses of low-income individuals to transit
investments in the Greater Toronto and Hamilton Area, Canada, using statistical
and ML models. We first investigate how the model choice affects the prediction
of transit use by the low-income group. This step includes comparing the
predictive performance of traditional and ML algorithms and then evaluating a
transit investment policy by contrasting the predicted activities and the
spatial distribution of transit trips generated by vulnerable households after
improving accessibility. We also empirically investigate the proposed transit
investment by each algorithm and compare it with the city of Brampton's future
transportation plan. While, unsurprisingly, the ML algorithms outperform
classical models, there are still doubts about using them due to
interpretability concerns. Hence, we adopt recent local and global
model-agnostic interpretation tools to interpret how the model arrives at its
predictions. Our findings reveal the great potential of ML algorithms for
enhanced travel behaviour predictions for low-income strata without
considerably sacrificing interpretability. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16736v1' target="_blank">http://arxiv.org/pdf/2211.16736v1</a><br> <br> <br> <font size='5'> 420 </font> <div style="text-align: right"> 2022-11-29 14:39:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utility Maximizer or Value Maximizer: Mechanism Design for Mixed Bidders in Online Advertising</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Digital advertising constitutes one of the main revenue sources for online
platforms. In recent years, some advertisers tend to adopt auto-bidding tools
to facilitate advertising performance optimization, making the classical
\emph{utility maximizer} model in auction theory not fit well. Some recent
studies proposed a new model, called \emph{value maximizer}, for auto-bidding
advertisers with return-on-investment (ROI) constraints. However, the model of
either utility maximizer or value maximizer could only characterize partial
advertisers in real-world advertising platforms. In a mixed environment where
utility maximizers and value maximizers coexist, the truthful ad auction design
would be challenging since bidders could manipulate both their values and
affiliated classes, leading to a multi-parameter mechanism design problem. In
this work, we address this issue by proposing a payment rule which combines the
corresponding ones in classical VCG and GSP mechanisms in a novel way. Based on
this payment rule, we propose a truthful auction mechanism with an
approximation ratio of $2$ on social welfare, which is close to the lower bound
of at least $\frac{5}{4}$ that we also prove. The designed auction mechanism is
a generalization of VCG for utility maximizers and GSP for value maximizers. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16251v2' target="_blank">http://arxiv.org/pdf/2211.16251v2</a><br> <br> <br> <font size='5'> 421 </font> <div style="text-align: right"> 2022-11-29 12:24:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Business-cycles and Cash-on-Market: Pre-money Startup Valuation in the Macroeconomic Environment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: How do business-cycles impact startup-valuations? While several studies
explore VC startupecosystems and pre-money valuations, relatively-few delve
deeper into the role of macro-level economic factors in influencing those
startup deals valuations. Using a dataset of 1,089 venturecapital investments
in European Union and European Economic Area markets, this article examines
macroeconomic, cyclical and macro-sectoral influences on VC startups pre-money
VC valuations. Our findings show that business-cycles impact startup-valuation
both directly and indirectly. Beyond DCF factors, startup-valuations are
impacted via by business-cycles directly, and via local venture-capital
market-size. By using a Structural Equation Model approach, our findings
contribute to entrepreneurship and financial-intermediary literature by
exploring indirect and endogenous relationship possibilities finding that most
determinants are transmission-channels rather than independent drivers. Our
findings effectively tie-together startup-valuations, intermediary markets, and
macroeconomic determinants. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16151v1' target="_blank">http://arxiv.org/pdf/2211.16151v1</a><br> <br> <br> <font size='5'> 422 </font> <div style="text-align: right"> 2022-11-29 04:01:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimizing Stock Option Forecasting with the Assembly of Machine Learning Models and Improved Trading Strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper introduced key aspects of applying Machine Learning (ML) models,
improved trading strategies, and the Quasi-Reversibility Method (QRM) to
optimize stock option forecasting and trading results. It presented the
findings of the follow-up project of the research "Application of Convolutional
Neural Networks with Quasi-Reversibility Method Results for Option
Forecasting". First, the project included an application of Recurrent Neural
Networks (RNN) and Long Short-Term Memory (LSTM) networks to provide a novel
way of predicting stock option trends. Additionally, it examined the dependence
of the ML models by evaluating the experimental method of combining multiple ML
models to improve prediction results and decision-making. Lastly, two improved
trading strategies and simulated investing results were presented. The Binomial
Asset Pricing Model with discrete time stochastic process analysis and
portfolio hedging was applied and suggested an optimized investment
expectation. These results can be utilized in real-life trading strategies to
optimize stock option investment results based on historical data. </font><br> Link: <a href='http://arxiv.org/pdf/2211.15912v1' target="_blank">http://arxiv.org/pdf/2211.15912v1</a><br> <br> <br> <font size='5'> 423 </font> <div style="text-align: right"> 2022-11-28 19:32:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Football Match Outcomes with eXplainable Machine Learning and the Kelly Index</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, a machine learning approach is developed for predicting the
outcomes of football matches. The novelty of this research lies in the
utilisation of the Kelly Index to first classify matches into categories where
each one denotes the different levels of predictive difficulty. Classification
models using a wide suite of algorithms were developed for each category of
matches in order to determine the efficacy of the approach. In conjunction to
this, a set of previously unexplored features were engineering including
Elo-based variables.
  The dataset originated from the Premier League match data covering the
2019-2021 seasons. The findings indicate that the process of decomposing the
predictive problem into sub-tasks was effective and produced competitive
results with prior works, while the ensemble-based methods were the most
effective.
  The paper also devised an investment strategy in order to evaluate its
effectiveness by benchmarking against bookmaker odds. An approach was developed
that minimises risk by combining the Kelly Index with the predefined confidence
thresholds of the predictive models. The experiments found that the proposed
strategy can return a profit when following a conservative approach that
focuses primarily on easy-to-predict matches where the predictive models
display a high confidence level. </font><br> Link: <a href='http://arxiv.org/pdf/2211.15734v1' target="_blank">http://arxiv.org/pdf/2211.15734v1</a><br> <br> <br> <font size='5'> 424 </font> <div style="text-align: right"> 2022-11-28 17:37:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Digital Asset Prices using Natural Language Processing: a survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Blockchain technology has changed how people think about how they used to
store and trade their assets, as it introduced us to a whole new way to
transact: using digital currencies. One of the major innovations of blockchain
technology is decentralization, meaning that traditional financial
intermediaries, such as asset-backed security issuers and banks, are eliminated
in the process. Even though blockchain technology has been utilized in a wide
range of industries, its most prominent application is still cryptocurrencies,
with Bitcoin being the first proposed. At its peak in 2021, the market cap for
Bitcoin once surpassed 1 trillion US dollars. The open nature of the crypto
market poses various challenges and concerns for both potential retail
investors and institutional investors, as the price of the investment is highly
volatile, and its fluctuations are unpredictable. The rise of Machine Learning,
and Natural Language Processing, in particular, has shed some light on
monitoring and predicting the price behaviors of cryptocurrencies. This paper
aims to review and analyze the recent efforts in applying Machine Learning and
Natural Language Processing methods to predict the prices and analyze the
behaviors of digital assets such as Bitcoin and Ethereum. </font><br> Link: <a href='http://arxiv.org/pdf/2212.00726v1' target="_blank">http://arxiv.org/pdf/2212.00726v1</a><br> <br> <br> <font size='5'> 425 </font> <div style="text-align: right"> 2022-11-28 13:08:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FLOWViZ: Framework for Phylogenetic Processing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The increasing risk of epidemics and a fast-growing world population has
contributed to a great investment in phylogenetic analysis, in order to track
numerous diseases and conceive effective medication and treatments.
  Phylogenetic analysis requires large quantities of information to be analyzed
and processed for knowledge extraction, using suitable techniques and,
nowadays, specific software and algorithms, to deliver results as efficiently
and fast as possible. These algorithms and techniques are already provided by
several free and available frameworks and tools. Usually, the process of
phylogenetic analysis consists of several processing steps, which define a
pipeline. Some phylogenetic frameworks have available more than one processing
step, such as inferring phylogenetic trees, data integration, and
visualization, but due to the continuous growth in involved data amounts, each
step may last several hours or days.
  Scientific workflow systems may use high performance computing facilities, if
available, for processing large volumes of data, concurrently. But most of
these scientific workflow systems cannot be easily installed and configured,
are available as centralized services, and, usually, it is not easy to
integrate tools and processing steps available in phylogenetic frameworks.
  This paper summarizes the thesis document of the FLOWViZ framework, which
main goal is to provide a software integration framework between a phylogenetic
framework and a scientific workflow system. This framework makes it possible to
build a customized integration with much fewer lines of code, while providing
existing phylogenetic frameworks with workflow building and execution, to
manage the processing of great amounts of data.
  The project was supported by funds, for a student grant of FCT - NGPHYLO
PTDC/CCI-BIO/29676/2017 and an IPL project - IPL/2021/DIVA. </font><br> Link: <a href='http://arxiv.org/pdf/2211.15282v1' target="_blank">http://arxiv.org/pdf/2211.15282v1</a><br> <br> <br> <font size='5'> 426 </font> <div style="text-align: right"> 2022-11-28 12:26:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ETF construction on CRIX</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investments in cryptocurrencies (CCs) remain risky due to high volatility.
Exchange Traded Funds (ETFs) are a suitable tool to diversify risk and to
benefit from the growth of the whole CC sector. We construct an ETF on the
CRIX, the CRyptocurrency IndeX that maps the non-stationary CC dynamics closely
by adapting its constituents weights dynamically. The scenario analysis
considers the fee schedules of regulated CC exchanges, spreads obtained from
high-frequency order book data, and models capital deposits to the ETF
stochastically. The analysis yields valuable insights into the mechanisms,
costs and risks of this new financial product: i) although the composition of
the CRIX ETF changes frequently (from 5 to 30 constituents), it remains robust
in its core, as the weights of Bitcoin (BTC) and Ethereum (ETH) are robust over
time, ii) on average, a portion of 5.2% needed to be rebalanced at the
rebalancing dates, iii) trading costs are low compared to traditional assets,
iv) the liquidity of the CC sector has increased significantly during the
analysis period, spreads occur especially for altcoins and increase by the size
of the transactions. But since BTC and ETH are most affected by rebalancing,
the cost of spreads remains limited. </font><br> Link: <a href='http://arxiv.org/pdf/2211.15260v2' target="_blank">http://arxiv.org/pdf/2211.15260v2</a><br> <br> <br> <font size='5'> 427 </font> <div style="text-align: right"> 2022-11-27 18:14:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Strategically revealing capabilities in General Lotto games</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Can revealing one's competitive capabilities to an opponent offer strategic
benefits? In this paper, we address this question in the context of General
Lotto games, a class of two-player competitive resource allocation models. We
consider an asymmetric information setting where the opponent is uncertain
about the resource budget of the other player, and holds a prior belief on its
value. We assume the other player, called the signaler, is able to send a noisy
signal about its budget to the opponent. With its updated belief, the opponent
then must decide to invest in costly resources that it will deploy against the
signaler's resource budget in a General Lotto game. We derive the subgame
perfect equilibrium to this extensive-form game. In particular, we identify
necessary and sufficient conditions for which a signaling policy improves the
signaler's resulting performance in comparison to the scenario where it does
not send any signal. Moreover, we provide the optimal signaling policy when
these conditions are met. Notably we find that for some scenarios, the signaler
can effectively double its performance. </font><br> Link: <a href='http://arxiv.org/pdf/2211.14907v2' target="_blank">http://arxiv.org/pdf/2211.14907v2</a><br> <br> <br> <font size='5'> 428 </font> <div style="text-align: right"> 2022-11-26 18:18:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Familiarity Facilitates Adoption: Evidence from Electric Vehicles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper shows that a non-price intervention which increased the prevalence
of a new technology facilitated its further adoption. The BlueLA program put
Electric Vehicles (EVs) for public use in many heavily trafficked areas,
primarily (but not exclusively) aimed at low-to-middle income households. We
show, using data on subsidies for these households and a difference-in
differences strategy, that BlueLA is associated with a 33\% increase of new EV
adoptions, justifying a substantial portion of public investment. While the
program provides a substitute to car ownership, our findings are consistent
with the hypothesis that increasing familiarity with EVs could facilitate
adoption. </font><br> Link: <a href='http://arxiv.org/pdf/2211.14634v1' target="_blank">http://arxiv.org/pdf/2211.14634v1</a><br> <br> <br> <font size='5'> 429 </font> <div style="text-align: right"> 2022-11-26 04:13:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparison Between Mean-Variance and Monotone Mean-Variance Preferences Under Jump Diffusion and Stochastic Factor Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper considers optimal investment problems based on monotone
mean-variance and mean-variance preferences in L\'evy market with an untradable
stochastic factor. We focus on the comparison of optimal strategies and value
functions in two problems. It is an open question proposed by Trybula and
Zawisza. Using dynamic programming and the Lagrange multiplier method, we get
Hamilton-Jacobi-Bellman-Isaacs equations (HJBI) and Hamilton-Jacobi-Bellman
equations (HJB) corresponding to the two investment problems. The equations are
transformed into a new-type parabolic equation, from which the optimal
strategies under both preferences are derived. We prove that optimal strategies
and value functions coincide in two investment problems, which means that
investors with mean-variance preference act as they have a monotone preference.
This phenomenon is interesting as that contradicts the results in single-period
investment problems, even under our discontinuous market model. In addition, we
derive the efficient frontier and analyze the economic impact of jump diffusion
part in the risky asset. </font><br> Link: <a href='http://arxiv.org/pdf/2211.14473v1' target="_blank">http://arxiv.org/pdf/2211.14473v1</a><br> <br> <br> <font size='5'> 430 </font> <div style="text-align: right"> 2022-11-25 12:56:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semantic Table Detection with LayoutLMv3</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents an application of the LayoutLMv3 model for semantic table
detection on financial documents from the IIIT-AR-13K dataset. The motivation
behind this paper's experiment was that LayoutLMv3's official paper had no
results for table detection using semantic information. We concluded that our
approach did not improve the model's table detection capabilities, for which we
can give several possible reasons. Either the model's weights were unsuitable
for our purpose, or we needed to invest more time in optimising the model's
hyperparameters. It is also possible that semantic information does not improve
a model's table detection accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2211.15504v1' target="_blank">http://arxiv.org/pdf/2211.15504v1</a><br> <br> <br> <font size='5'> 431 </font> <div style="text-align: right"> 2022-11-25 09:56:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantum Software Engineering: A New Genre of Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum computing (QC) is no longer only a scientific interest but is rapidly
becoming an industrially available technology that can potentially tackle the
limitations of classical computing. Over the last few years, major technology
giants have invested in developing hardware and programming frameworks to
develop quantum-specific applications. QC hardware technologies are gaining
momentum, however, operationalizing the QC technologies trigger the need for
software-intensive methodologies, techniques, processes, tools, roles, and
responsibilities for developing industrial-centric quantum software
applications. This paper presents the vision of the quantum software
engineering (QSE) life cycle consisting of quantum requirements engineering,
quantum software design, quantum software implementation, quantum software
testing, and quantum software maintenance. This paper particularly calls for
joint contributions of software engineering research and industrial community
to present real-world solutions to support the entire quantum software
development activities. The proposed vision facilitates the researchers and
practitioners to propose new processes, reference architectures, novel tools,
and practices to leverage quantum computers and develop emerging and next
generations of quantum software. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13990v1' target="_blank">http://arxiv.org/pdf/2211.13990v1</a><br> <br> <br> <font size='5'> 432 </font> <div style="text-align: right"> 2022-11-25 09:17:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sponsored messaging about climate change on Facebook: Actors, content, frames</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Online communication about climate change is central to public discourse
around this contested issue. Facebook is a dominant social media platform known
to be a major source of information and online influence, yet discussion of
climate change on the platform has remained largely unstudied due to
difficulties in accessing data. This paper utilises Facebook's repository of
social/political ads to study how climate change is framed as an issue in
adverts placed by different actors. Sponsored content is a strategic investment
and presumably intended to be persuasive, so patterns of who pays for adverts
and how those adverts frame the issue can reveal large-scale trends in public
discourse. We show that most money spent on climate-related messaging is
targeted at users in the US, GB and CA. While the number of advert impressions
correlates with total spend by an actor, there is a secondary effect of unpaid
social sharing which can substantially affect the number of impressions per
dollar spent. Most spend in the US is by political actors, while environmental
non-governmental organisations dominate spend in GB. Analysis shows that
climate change solutions are well represented in GB, while climate change
impacts such as extreme weather events are strongly represented in the US and
CA. Different actor types frame the issue of climate change in different ways;
political actors position the issue as party political and a point of
difference between candidates, whereas environmental NGOs frame climate change
as the focus of collective action and social mobilisation. Overall, our study
provides a first empirical exploration of climate-related advertising on
Facebook. It shows the diversity of actors seeking to use Facebook as a
platform for their campaigns and how they utilise different topic frames to
persuade users to act. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13965v1' target="_blank">http://arxiv.org/pdf/2211.13965v1</a><br> <br> <br> <font size='5'> 433 </font> <div style="text-align: right"> 2022-11-25 02:43:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Machine Learning, Natural Language Processing Analysis of Youth Perspectives: Key Trends and Focus Areas for Sustainable Youth Development Policies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investing in children and youth is a critical step towards inclusive,
equitable, and sustainable development for current and future generations.
Several international agendas for accomplishing common global goals emphasize
the need for active youth participation and engagement for sustainable
development. The 2030 Agenda for Sustainable Development emphasizes the need
for youth engagement and the inclusion of youth perspectives as an important
step toward addressing each of the 17 Sustainable Development Goals. The aim of
this study is to analyze youth perspectives, values, and sentiments towards
issues addressed by the 17 Sustainable Development Goals through social network
analysis using machine learning. Social network data collected during 7 major
sustainability conferences aimed at engaging children and youth is analyzed
using natural language processing techniques for sentiment analysis. This data
categorized using a natural language processing text classifier trained on a
sample dataset of social network data during the 7 youth sustainability
conferences for deeper understanding of youth perspectives in relation to the
SDGs. Machine learning identified demographic and location attributes and
features are utilized in order to identify bias and demographic differences
between ages, gender, and race among youth. Using natural language processing,
the qualitative data collected from over 7 different countries in 3 languages
are systematically translated, categorized, and analyzed, revealing key trends
and focus areas for sustainable youth development policies. The obtained
results reveal the general youth's depth of knowledge on sustainable
development and their attitudes towards each of the 17 SDGs. The findings of
this study serve as a guide toward better understanding the interests, roles,
and perspectives of children and youth in achieving the goals of Agenda 2030. </font><br> Link: <a href='http://arxiv.org/pdf/2211.14321v1' target="_blank">http://arxiv.org/pdf/2211.14321v1</a><br> <br> <br> <font size='5'> 434 </font> <div style="text-align: right"> 2022-11-24 14:20:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Value of Flexibility in a Carbon Neutral Power System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we use a formulation of the generation expansion planning
problem with hourly temporal resolution, to investigate the impact of the
availability of different sources of flexibility on a carbon-neutral central
European power system (with a focus on Switzerland) for the year 2040. We
assess the role of flexible generation, load shifting and imports on the
investment and operation of existing and newly built units. Our results show
that including load shifting as part of the optimization could reduce the need
for investments in both RES and conventional technologies. The combination of
newly built flexible generators (gas turbines) and load shifting increases the
flexibility of the simulated power system and results in the overall lowest
system costs. The reduction of cross-border transmission capacity between
Switzerland and its neighbors has a significant impact on the domestic
operation and investments but could also affect the surrounding countries. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13625v1' target="_blank">http://arxiv.org/pdf/2211.13625v1</a><br> <br> <br> <font size='5'> 435 </font> <div style="text-align: right"> 2022-11-24 07:32:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass 2021 Underground Facilities & Infrastructure Frontier Report</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The decade since Snowmass 2013 has seen extraordinary progress of high energy
physics research performed--or planned for--at underground facilities. Drs. T.
Kajita and A.B. McDonald were awarded the 2015 Nobel Prize in Physics for the
discovery of neutrino oscillation, which show that neutrinos have mass. The
U.S. has embarked on the development of the world-class LBNF/DUNE science
program to investigate neutrino properties. The Generation 2 dark matter
program is advancing to full data collection in the coming 5 years, a Dark
Matter New Initiatives program has begun, and the U.S. dark matter community is
looking toward a Generation 3 program of large-scale dark matter direct
detection searches. The Sanford Underground Research Facility has become a
focal point for U.S. underground facilities and infrastructure investment. The
status since the 2013 Snowmass process as well as the outcome from the 2014 P5
program of recommendations is reviewed. These are then evaluated based on the
activities and discussions of the Snowmass 2021 process resulting in
conclusions looking forward to the coming decade of high energy physics
research performed in underground facilities. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13450v1' target="_blank">http://arxiv.org/pdf/2211.13450v1</a><br> <br> <br> <font size='5'> 436 </font> <div style="text-align: right"> 2022-11-24 05:49:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identifying discreditable firms in a large-scale ownership network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Violations of laws and regulations about food safety, production safety,
quality standard and environmental protection, or negative consequences from
loan, guarantee and pledge contracts, may result in operating and credit risks
of firms. The above illegal or trust-breaking activities are collectively
called discreditable activities, and firms with discreditable activities are
named as discreditable firms. Identification of discreditable firms is of great
significance for investment attraction, bank lending, equity investment,
supplier selection, job seeking, and so on. In this paper, we collect
registration records of about 113 million Chinese firms and construct an
ownership network with about 6 million nodes, where each node is a firm who has
invested at least one firm or has been invested by at least one firm. Analysis
of publicly available records of discreditable activities show strong network
effect, namely the probability of a firm to be discreditable is remarkably
higher than the average probability given the fact that one of its investors or
investees is discreditable. In comparison, for the risk of being a
discreditable firm, an investee has higher impact than an investor in average.
The impact of a firm on surrounding firms decays along with the increasing
topological distance, analogous to the well-known "three degrees of separation"
phenomenon. The uncovered correlation of discreditable activities can be
considered as a representative example of network effect, in addition to the
propagation of diseases, opinions and human behaviors. Lastly, we show that the
utilization of the network effect largely improves the accuracy of the
algorithm to identify discreditable firms. </font><br> Link: <a href='http://arxiv.org/pdf/2211.14316v1' target="_blank">http://arxiv.org/pdf/2211.14316v1</a><br> <br> <br> <font size='5'> 437 </font> <div style="text-align: right"> 2022-11-24 04:35:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the drive-by sensing power of bus fleet through active scheduling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Vehicle-based mobile sensing (a.k.a drive-by sensing) is an important means
of surveying urban environment by leveraging the mobility of public or private
transport vehicles. Buses, for their extensive spatial coverage and reliable
operations, have received much attention in drive-by sensing. Existing studies
have focused on the assignment of sensors to a set of lines or buses with no
operational intervention, which is typically formulated as set covering or
subset selection problems. This paper aims to boost the sensing power of bus
fleets through active scheduling, by allowing instrumented buses to circulate
across multiple lines to deliver optimal sensing outcome. We consider a fleet
consisting of instrumented and normal buses, and jointly optimize sensor
assignment, bus dispatch, and intra- or inter-line relocations, with the
objectives of maximizing sensing quality and minimizing operational costs,
while serving all timetabled trips. By making general assumptions on the
sensing utility function, we formulate the problem as a nonlinear integer
program based on a time-expanded network. A batch scheduling algorithm is
developed following linearization techniques to solve the problem efficiently,
which is tested in a real-world case study in Chengdu, China. The results show
that the proposed scheme can improve the sensing objective by 12.0%-20.5%
(single-line scheduling) and 16.3%-32.1% (multi-line scheduling), respectively,
while managing to save operational costs by 1.0%. Importantly, to achieve the
same level of sensing quality, we found that the sensor investment can be
reduced by over 33% when considering active bus scheduling. Comprehensive
comparative and sensitivity analyses are presented to generate managerial
insights and recommendations for practice. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13414v1' target="_blank">http://arxiv.org/pdf/2211.13414v1</a><br> <br> <br> <font size='5'> 438 </font> <div style="text-align: right"> 2022-11-23 18:55:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Costs and benefits of automation for astronomical facilities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Observatorio Astrof\'isico de Javalambre (OAJ{\dag}1) in Spain is a young
astronomical facility, conceived and developed from the beginning as a fully
automated observatory with the main goal of optimizing the processes in the
scientific and general operation of the Observatory. The OAJ has been
particularly conceived for carrying out large sky surveys with two
unprecedented telescopes of unusually large fields of view (FoV): the JST/T250,
a 2.55m telescope of 3deg field of view, and the JAST/T80, an 83cm telescope of
2deg field of view. The most immediate objective of the two telescopes for the
next years is carrying out two unique photometric surveys of several thousands
square degrees, J-PAS{\dag}2 and J-PLUS{\dag}3, each of them with a wide range
of scientific applications, like e.g. large structure cosmology and Dark
Energy, galaxy evolution, supernovae, Milky Way structure, exoplanets, among
many others. To do that, JST and JAST are equipped with panoramic cameras under
development within the J-PAS collaboration, JPCam and T80Cam respectively,
which make use of large format (~ 10k x 10k) CCDs covering the entire focal
plane. This paper describes in detail, from operations point of view, a
comparison between the detailed cost of the global automation of the
Observatory and the standard automation cost for astronomical facilities, in
reference to the total investment and highlighting all benefits obtained from
this approach and difficulties encountered. The paper also describes the
engineering development of the overall facilities and infrastructures for the
fully automated observatory and a global overview of current status,
pinpointing lessons learned in order to boost observatory operations
performance, achieving scientific targets, maintaining quality requirements,
but also minimizing operation cost and human resources. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13214v2' target="_blank">http://arxiv.org/pdf/2211.13214v2</a><br> <br> <br> <font size='5'> 439 </font> <div style="text-align: right"> 2022-11-23 16:40:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Necessity of Rational Asset Price Bubbles in Two-Sector Growth Economies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present plausible economic models in which an equilibrium with rational
asset price bubbles exists but equilibria with asset prices equal to
fundamental values do not. These economies feature multiple sectors with faster
economic growth than dividend growth. In our two-sector endogenous growth
model, entrepreneurs have access to a production technology subject to
idiosyncratic investment risk (tech sector) and trade a dividend-paying asset
(land). When leverage is relaxed beyond a critical value, the unique trend
stationary equilibrium exhibits a phase transition from the fundamental regime
to the bubbly regime with growth, implying the inevitability of bubbles with
loose financial conditions. </font><br> Link: <a href='http://arxiv.org/pdf/2211.13100v4' target="_blank">http://arxiv.org/pdf/2211.13100v4</a><br> <br> <br> <font size='5'> 440 </font> <div style="text-align: right"> 2022-11-22 22:49:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Spatial-temporal dynamics of employment shocks in declining coal mining regions and potentialities of the 'just transition'</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The United States, much like other countries around the world, faces
significant obstacles to achieving a rapid decarbonization of its economy.
Crucially, decarbonization disproportionately affects the communities that have
been historically, politically, and socially embedded in the nation's fossil
fuel production. However, this effect has rarely been quantified in the
literature. Using econometric estimation methods that control for unobserved
heterogeneity via two-way fixed effects, spatial effects, heterogeneous time
trends, and grouped fixed effects, we demonstrate that mine closures induce a
significant and consistent contemporaneous rise in the unemployment rate across
US counties. A single mine closure can raise a county's unemployment rate by
0.056 percentage points in a given year; this effect is amplified by a factor
of four when spatial econometric dynamics are considered. Although this
response in the unemployment rate fades within 2-3 years, it has far-reaching
effects in its immediate vicinity. Furthermore, we use cluster analysis to
build a novel typology of coal counties based on qualities that are thought to
facilitate a successful recovery in the face of local industrial decline. The
combined findings of the econometric analysis and typology point to the
importance of investing in alternative sectors in places with promising levels
of economic diversity, retraining job seekers in places with lower levels of
educational attainment, providing relocation (or telecommuting) support in
rural areas, and subsidizing childcare and after school programs in places with
low female labor force participation due to the gendered division of domestic
work. </font><br> Link: <a href='http://arxiv.org/pdf/2211.12619v1' target="_blank">http://arxiv.org/pdf/2211.12619v1</a><br> <br> <br> <font size='5'> 441 </font> <div style="text-align: right"> 2022-11-22 20:36:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting the Type and Target of Offensive Social Media Posts in Marathi</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The presence of offensive language on social media is very common motivating
platforms to invest in strategies to make communities safer. This includes
developing robust machine learning systems capable of recognizing offensive
content online. Apart from a few notable exceptions, most research on automatic
offensive language identification has dealt with English and a few other high
resource languages such as French, German, and Spanish. In this paper we
address this gap by tackling offensive language identification in Marathi, a
low-resource Indo-Aryan language spoken in India. We introduce the Marathi
Offensive Language Dataset v.2.0 or MOLD 2.0 and present multiple experiments
on this dataset. MOLD 2.0 is a much larger version of MOLD with expanded
annotation to the levels B (type) and C (target) of the popular OLID taxonomy.
MOLD 2.0 is the first hierarchical offensive language dataset compiled for
Marathi, thus opening new avenues for research in low-resource Indo-Aryan
languages. Finally, we also introduce SeMOLD, a larger dataset annotated
following the semi-supervised methods presented in SOLID. </font><br> Link: <a href='http://arxiv.org/pdf/2211.12570v1' target="_blank">http://arxiv.org/pdf/2211.12570v1</a><br> <br> <br> <font size='5'> 442 </font> <div style="text-align: right"> 2022-11-22 10:51:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Reinsurance-Investment Strategy for a Monotone Mean-Variance Insurer in the Cramr-Lundberg Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As classical mean-variance preferences have the shortcoming of
non-monotonicity, portfolio selection theory based on monotone mean-variance
preferences is becoming an important research topic recently. In
continuous-time Cram\'er-Lundberg insurance and Black-Scholes financial market
model, we solve the optimal reinsurance-investment strategies of insurers under
mean-variance preferences and monotone mean-variance preferences by the HJB
equation and the HJBI equation, respectively. We prove the validity of
verification theorems and find that the optimal strategies under the two
preferences are the same. This illustrates that neither the continuity nor the
completeness of the market is necessary for the consistency of two optimal
strategies. We make detailed explanations for this result. Thus, we develop the
existing theory of portfolio selection problems under the monotone
mean-variance criterion. </font><br> Link: <a href='http://arxiv.org/pdf/2211.12168v2' target="_blank">http://arxiv.org/pdf/2211.12168v2</a><br> <br> <br> <font size='5'> 443 </font> <div style="text-align: right"> 2022-11-22 07:29:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Financing Urban Infrastructure through Land Leasing: Evidence from Bahir Dar City, Ethiopia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The provision of essential urban infrastructure and services for the
expanding population is a persistent financial challenge for many of the
rapidly expanding cities in developing nations like Ethiopia. The land lease
system has received little academic attention as a means of financing urban
infrastructure in developing countries. Therefore, the main objective of this
study is to assess the contribution of land leasing in financing urban
infrastructure and services using evidence from Bahir Dar city, Ethiopia.
Primary and secondary data-gathering techniques have been used. Descriptive
statistics and qualitative analysis have been adopted. The results show land
lease revenue is a dominant source of extra-budgetary revenue for Bahir Dar
city. As evidenced by Bahir Dar city, a significant portion of urban
infrastructure expenditure is financed by revenues from land leasing. However,
despite the critical importance of land lease revenue to investments in urban
infrastructure, there is inefficiency in the collection of potential lease
revenue due to weak information exchange, inadequate land provision for various
uses, lack of transparency in tender committees, and the existence of poor
documentation. Our findings suggest that Bahir Dar City needs to manage lease
revenue more effectively to increase investment in urban infrastructure while
giving due consideration to availing more land for leasing.
  Keywords: urban, land, revenue, inefficiency, lease, financing, Bahir Dar
City </font><br> Link: <a href='http://arxiv.org/pdf/2211.12061v2' target="_blank">http://arxiv.org/pdf/2211.12061v2</a><br> <br> <br> <font size='5'> 444 </font> <div style="text-align: right"> 2022-11-20 19:03:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semantic Similarity-Based Clustering of Findings From Security Testing Tools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the last years, software development in domains with high security
demands transitioned from traditional methodologies to uniting modern
approaches from software development and operations (DevOps). Key principles of
DevOps gained more importance and are now applied to security aspects of
software development, resulting in the automation of security-enhancing
activities. In particular, it is common practice to use automated security
testing tools that generate reports after inspecting a software artifact from
multiple perspectives. However, this raises the challenge of generating
duplicate security findings. To identify these duplicate findings manually, a
security expert has to invest resources like time, effort, and knowledge. A
partial automation of this process could reduce the analysis effort, encourage
DevOps principles, and diminish the chance of human error. In this study, we
investigated the potential of applying Natural Language Processing for
clustering semantically similar security findings to support the identification
of problem-specific duplicate findings. Towards this goal, we developed a web
application for annotating and assessing security testing tool reports and
published a human-annotated corpus of clustered security findings. In addition,
we performed a comparison of different semantic similarity techniques for
automatically grouping security findings. Finally, we assess the resulting
clusters using both quantitative and qualitative evaluation methods. </font><br> Link: <a href='http://arxiv.org/pdf/2211.11057v1' target="_blank">http://arxiv.org/pdf/2211.11057v1</a><br> <br> <br> <font size='5'> 445 </font> <div style="text-align: right"> 2022-11-20 17:52:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Revealing Robust Oil and Gas Company Macro-Strategies using Deep Multi-Agent Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The energy transition potentially poses an existential risk for major
international oil companies (IOCs) if they fail to adapt to low-carbon business
models. Projections of energy futures, however, are met with diverging
assumptions on its scale and pace, causing disagreement among IOC
decision-makers and their stakeholders over what the business model of an
incumbent fossil fuel company should be. In this work, we used deep multi-agent
reinforcement learning to solve an energy systems wargame wherein players
simulate IOC decision-making, including hydrocarbon and low-carbon investments
decisions, dividend policies, and capital structure measures, through an
uncertain energy transition to explore critical and non-linear governance
questions, from leveraged transitions to reserve replacements. Adversarial play
facilitated by state-of-the-art algorithms revealed decision-making strategies
robust to energy transition uncertainty and against multiple IOCs. In all
games, robust strategies emerged in the form of low-carbon business models as a
result of early transition-oriented movement. IOCs adopting such strategies
outperformed business-as-usual and delayed transition strategies regardless of
hydrocarbon demand projections. In addition to maximizing value, these
strategies benefit greater society by contributing substantial amounts of
capital necessary to accelerate the global low-carbon energy transition. Our
findings point towards the need for lenders and investors to effectively
mobilize transition-oriented finance and engage with IOCs to ensure responsible
reallocation of capital towards low-carbon business models that would enable
the emergence of fossil fuel incumbents as future low-carbon leaders. </font><br> Link: <a href='http://arxiv.org/pdf/2211.11043v1' target="_blank">http://arxiv.org/pdf/2211.11043v1</a><br> <br> <br> <font size='5'> 446 </font> <div style="text-align: right"> 2022-11-19 03:20:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A 2030 United States Macro Grid Unlocking Geographical Diversity to Accomplish Clean Energy Goals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Some U.S. states have set clean energy goals and targets in an effort to
decarbonize their electricity sectors. There are many reasons for such goals
and targets, including the increasingly apparent effects of climate change. A
handful of states (Washington, California, New York, and Virginia) are aiming
for deep decarbonization by 2050 or earlier, a mere 30 years or less from
today. The urgency of substantial carbon emissions reduction (50% or more by
2030) needed to avoid catastrophic climate impacts requires even more ambitious
efforts than some of the original targets (e.g., a 30% renewable portfolio
standard) set for between now and 2030. With the cost of solar and wind energy
falling faster than expected in recent years, economics are also driving rapid
expansion of clean energy investments. With this in mind, this report examines
combinations of interregional AC and High-Voltage DC (HVDC) transmission
upgrades and additions to evaluate the benefits of large-scale transmission
expansion. </font><br> Link: <a href='http://arxiv.org/pdf/2211.10574v1' target="_blank">http://arxiv.org/pdf/2211.10574v1</a><br> <br> <br> <font size='5'> 447 </font> <div style="text-align: right"> 2022-11-18 21:07:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal performance of a tontine overlay subject to withdrawal constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the holder of an individual tontine retirement account, with
maximum and minimum withdrawal amounts (per year) specified. The tontine
account holder initiates the account at age 65, and earns mortality credits
while alive, but forfeits all wealth in the account upon death. The holder
desires to maximize total withdrawals, and minimize the expected shortfall,
assuming the holder survives to age 95. The investor controls the amount
withdrawn each year and the fraction of the investments in stocks and bonds.
The optimal controls are determined based on a parametric model fitted to
almost a century of market data. The optimal control algorithm is based on
dynamic programming and solution of a partial integro differential equation
(PIDE) using Fourier methods. The optimal strategy (based on the parametric
model) is tested out of sample using stationary block bootstrap resampling of
the historical data. In terms of an expected total withdrawal, expected
shortfall (EW-ES) efficient frontier, the tontine overlay greatly outperforms
an optimal strategy (without the tontine overlay), which in turn outperforms a
constant weight strategy with withdrawals based on the ubiquitous four per cent
rule. </font><br> Link: <a href='http://arxiv.org/pdf/2211.10509v1' target="_blank">http://arxiv.org/pdf/2211.10509v1</a><br> <br> <br> <font size='5'> 448 </font> <div style="text-align: right"> 2022-11-18 00:23:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Discrete Choice Models, Market Shares, and Density Functional Theory: Application to Monolayer Nanomaterials</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite possessing unique and often superior physical properties,
nanomaterials rarely attract attention beyond academic circles. In order for
nanomaterials to attract investment for large-scale production, it is necessary
to clarify how physical properties relate to consumer demand. In this paper, we
show how density functional theory (DFT), a technique from computational
physics, can be integrated into econometrics to develop models for nanomaterial
market shares. Concretely, we study a simple discrete-choice model in which
consumers select one of two firms from which to purchase platinum catalyst
materials for hydrogen fuel cells. One of these firms produces an ordinary
material catalyst (platinum) and the other produces a nanomaterial catalyst
(monolayer platinum). Consumers make their selections according to hedonic
utility expressions, which depend upon the physical characteristics of the
materials in question. In turn, these physical characteristics can be
calculated using the DFT technique. As an application of our model, we show how
marginal production costs necessary for the nanomaterial catalyst to achieve a
particular market share can be inferred, thereby providing a means for
entrepreneurs to formulate cost targets for large-scale nanomaterial
production. Our model, which can be generalised beyond the situation considered
here, represents the first step towards the unification of econometrics and
computational physics, an intellectual advancement which may facilitate future
efforts at nanomaterial commercialisation. </font><br> Link: <a href='http://arxiv.org/pdf/2211.09952v1' target="_blank">http://arxiv.org/pdf/2211.09952v1</a><br> <br> <br> <font size='5'> 449 </font> <div style="text-align: right"> 2022-11-16 21:27:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modeling 100% Electrified Transportation in NYC</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Envisioning a future 100% electrified transportation sector, this paper uses
socio-economic, demographic, and geographic data to assess electric energy
demand from commuter traffic. We explore the individual mode choices, which
allows to create mode-mix scenarios for the entire population, and quantify the
electric energy demand for each scenario using technical specifications of
battery and electric drives technology in combination with different charging
scenarios. Using data sets for New York City, our results highlight the need
for infrastructure investments, the usefulness of flexible charging policies,
and the positive impact of incentivizing micromobility and mass-transit
options. Our model and results are publicly available as interactive dashboard. </font><br> Link: <a href='http://arxiv.org/pdf/2211.11581v3' target="_blank">http://arxiv.org/pdf/2211.11581v3</a><br> <br> <br> <font size='5'> 450 </font> <div style="text-align: right"> 2022-11-16 15:51:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dynamical Linear Bandits</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In many real-world sequential decision-making problems, an action does not
immediately reflect on the feedback and spreads its effects over a long time
frame. For instance, in online advertising, investing in a platform produces an
instantaneous increase of awareness, but the actual reward, i.e., a conversion,
might occur far in the future. Furthermore, whether a conversion takes place
depends on: how fast the awareness grows, its vanishing effects, and the
synergy or interference with other advertising platforms. Previous work has
investigated the Multi-Armed Bandit framework with the possibility of delayed
and aggregated feedback, without a particular structure on how an action
propagates in the future, disregarding possible dynamical effects. In this
paper, we introduce a novel setting, the Dynamical Linear Bandits (DLB), an
extension of the linear bandits characterized by a hidden state. When an action
is performed, the learner observes a noisy reward whose mean is a linear
function of the hidden state and of the action. Then, the hidden state evolves
according to linear dynamics, affected by the performed action too. We start by
introducing the setting, discussing the notion of optimal policy, and deriving
an expected regret lower bound. Then, we provide an optimistic regret
minimization algorithm, Dynamical Linear Upper Confidence Bound (DynLin-UCB),
that suffers an expected regret of order $\widetilde{\mathcal{O}} \Big( \frac{d
\sqrt{T}}{(1-\overline{\rho})^{3/2}} \Big)$, where $\overline{\rho}$ is a
measure of the stability of the system, and $d$ is the dimension of the action
vector. Finally, we conduct a numerical validation on a synthetic environment
and on real-world data to show the effectiveness of DynLin-UCB in comparison
with several baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2211.08997v2' target="_blank">http://arxiv.org/pdf/2211.08997v2</a><br> <br> <br> <font size='5'> 451 </font> <div style="text-align: right"> 2022-11-16 14:05:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient implementation of portfolio strategies involving cryptocurrencies and VIX INDEX and Gold</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research mainly explores the characteristics of different strategies and
whether VIX INDEX positively influences the investment portfolio in any period.
Our portfolio has six significant cryptocurrencies, VIX INDEX and gold. We
perform parameter estimation on all raw data and bring the two types into
different investment strategies, complete them effectively according to other
characteristics, and compare the results. At the same time, we make two
different portfolios, one contains VIX INDEX, and one does not have VIX INDEX.
We use different portfolios in different portfolio strategies and find that VIX
INDEX can positively impact the investment portfolio of cryptocurrencies, no
matter in the standard market or the downward market. The research shows that
gold has the same attributes as VIX INDEX and should have a specific positive
effect, but no comparative experiment has been done. </font><br> Link: <a href='http://arxiv.org/pdf/2211.08919v1' target="_blank">http://arxiv.org/pdf/2211.08919v1</a><br> <br> <br> <font size='5'> 452 </font> <div style="text-align: right"> 2022-11-15 11:56:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Extending the OSLC standard for ECA-based automation in DevOps environments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The DevOps paradigm is taking over software development systems, helping
businesses increase efficiency, accelerate production, and adapt quickly to
market changes. However, adopting these principles can be challenging.
Practitioners often face an important issue known as vendor lock-in caused by
the cost of tool replacement. In addition, automating the processes that
involve these tools also requires investment. These issues could be addressed
by standardizing service interfaces to facilitate their integration. Linked
Data is an attractive choice for implementing such a standard without
sacrificing versatility. An exciting and promising proposal in this direction
is the OSLC standard specification. Its purpose is to build an environment
where services can interoperate using standard Linked Data models. However, the
current specification version still lacks standard definitions for concepts
that are critical to automating the execution of actions in fast-changing
environments. Therefore, this paper proposes a new specification to extend
OSLC, based on the ECA model, for event-based interoperable automation,
especially for DevOps environments, which are our motivational scenario. A
simple DevOps architecture is built as a prototype to validate the proposed
model. Using that architecture, the proposed model is validated in a real-world
workflow to prove its contribution to the OSLC standard and the DevOps field. </font><br> Link: <a href='http://arxiv.org/pdf/2211.08075v1' target="_blank">http://arxiv.org/pdf/2211.08075v1</a><br> <br> <br> <font size='5'> 453 </font> <div style="text-align: right"> 2022-11-14 16:49:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning to Answer Multilingual and Code-Mixed Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Question-answering (QA) that comes naturally to humans is a critical
component in seamless human-computer interaction. It has emerged as one of the
most convenient and natural methods to interact with the web and is especially
desirable in voice-controlled environments. Despite being one of the oldest
research areas, the current QA system faces the critical challenge of handling
multilingual queries. To build an Artificial Intelligent (AI) agent that can
serve multilingual end users, a QA system is required to be language versatile
and tailored to suit the multilingual environment. Recent advances in QA models
have enabled surpassing human performance primarily due to the availability of
a sizable amount of high-quality datasets. However, the majority of such
annotated datasets are expensive to create and are only confined to the English
language, making it challenging to acknowledge progress in foreign languages.
Therefore, to measure a similar improvement in the multilingual QA system, it
is necessary to invest in high-quality multilingual evaluation benchmarks. In
this dissertation, we focus on advancing QA techniques for handling end-user
queries in multilingual environments. This dissertation consists of two parts.
In the first part, we explore multilingualism and a new dimension of
multilingualism referred to as code-mixing. Second, we propose a technique to
solve the task of multi-hop question generation by exploiting multiple
documents. Experiments show our models achieve state-of-the-art performance on
answer extraction, ranking, and generation tasks on multiple domains of MQA,
VQA, and language generation. The proposed techniques are generic and can be
widely used in various domains and languages to advance QA systems. </font><br> Link: <a href='http://arxiv.org/pdf/2211.07522v1' target="_blank">http://arxiv.org/pdf/2211.07522v1</a><br> <br> <br> <font size='5'> 454 </font> <div style="text-align: right"> 2022-11-14 01:29:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating Distribution System Reliability with Hyperstructures Graph Convolutional Nets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Nowadays, it is broadly recognized in the power system community that to meet
the ever expanding energy sector's needs, it is no longer possible to rely
solely on physics-based models and that reliable, timely and sustainable
operation of energy systems is impossible without systematic integration of
artificial intelligence (AI) tools. Nevertheless, the adoption of AI in power
systems is still limited, while integration of AI particularly into
distribution grid investment planning is still an uncharted territory. We make
the first step forward to bridge this gap by showing how graph convolutional
networks coupled with the hyperstructures representation learning framework can
be employed for accurate, reliable, and computationally efficient distribution
grid planning with resilience objectives. We further propose a Hyperstructures
Graph Convolutional Neural Networks (Hyper-GCNNs) to capture hidden higher
order representations of distribution networks with attention mechanism. Our
numerical experiments show that the proposed Hyper-GCNNs approach yields
substantial gains in computational efficiency compared to the prevailing
methodology in distribution grid planning and also noticeably outperforms seven
state-of-the-art models from deep learning (DL) community. </font><br> Link: <a href='http://arxiv.org/pdf/2211.07645v1' target="_blank">http://arxiv.org/pdf/2211.07645v1</a><br> <br> <br> <font size='5'> 455 </font> <div style="text-align: right"> 2022-11-13 22:29:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FinTech for Social Good: A Research Agenda from NLP Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Making our research results positively impact on society and environment is
one of the goals our community has been pursuing recently. Although financial
technology (FinTech) is one of the popular application fields, we notice that
there is no discussion on how NLP can help in FinTech for the social good. When
mentioning FinTech for social good, people are talking about financial
inclusion and green finance. However, the role of NLP in these directions only
gets limited discussions. To fill this gap, this paper shares our idea of how
we can use NLP in FinTech for social good. We hope readers can rethink the
relationship between finance and NLP based on our sharing, and further join us
in improving the financial literacy of individual investors and improving the
supports for impact investment. </font><br> Link: <a href='http://arxiv.org/pdf/2211.06431v1' target="_blank">http://arxiv.org/pdf/2211.06431v1</a><br> <br> <br> <font size='5'> 456 </font> <div style="text-align: right"> 2022-11-13 17:03:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interlacing in atomic resolution scanning transmission electron microscopy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Fast frame-rates are desirable in scanning transmission electron microscopy
for a number of reasons: controlling electron beam dose, capturing in-situ
events or reducing the appearance of scan distortions. Whilst several
strategies exist for increasing frame-rates, many impact image quality or
require investment in advanced scan hardware. Here we present an interlaced
imaging approach to achieve minimal loss of image quality with faster
frame-rates that can be implemented on many existing scan controllers. We
further demonstrate that our interlacing approach provides the best possible
strain precision for a given electron dose compared with other contemporary
approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2211.06954v1' target="_blank">http://arxiv.org/pdf/2211.06954v1</a><br> <br> <br> <font size='5'> 457 </font> <div style="text-align: right"> 2022-11-13 11:23:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Companies' ESG Ratings from News Articles Using Multivariate Timeseries Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Environmental, social and governance (ESG) engagement of companies moved into
the focus of public attention over recent years. With the requirements of
compulsory reporting being implemented and investors incorporating
sustainability in their investment decisions, the demand for transparent and
reliable ESG ratings is increasing. However, automatic approaches for
forecasting ESG ratings have been quite scarce despite the increasing
importance of the topic. In this paper, we build a model to predict ESG ratings
from news articles using the combination of multivariate timeseries
construction and deep learning techniques. A news dataset for about 3,000 US
companies together with their ratings is also created and released for
training. Through the experimental evaluation we find out that our approach
provides accurate results outperforming the state-of-the-art, and can be used
in practice to support a manual determination or analysis of ESG ratings. </font><br> Link: <a href='http://arxiv.org/pdf/2212.11765v1' target="_blank">http://arxiv.org/pdf/2212.11765v1</a><br> <br> <br> <font size='5'> 458 </font> <div style="text-align: right"> 2022-11-11 15:13:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FinBERT-LSTM: Deep Learning based stock price prediction using News Sentiment Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Economy is severely dependent on the stock market. An uptrend usually
corresponds to prosperity while a downtrend correlates to recession. Predicting
the stock market has thus been a centre of research and experiment for a long
time. Being able to predict short term movements in the market enables
investors to reap greater returns on their investments. Stock prices are
extremely volatile and sensitive to financial market. In this paper we use Deep
Learning networks to predict stock prices, assimilating financial, business and
technology news articles which present information about the market. First, we
create a simple Multilayer Perceptron (MLP) network and then expand into more
complex Recurrent Neural Network (RNN) like Long Short Term Memory (LSTM), and
finally propose FinBERT-LSTM model, which integrates news article sentiments to
predict stock price with greater accuracy by analysing short-term market
information. We then train the model on NASDAQ-100 index stock data and New
York Times news articles to evaluate the performance of MLP, LSTM, FinBERT-LSTM
models using mean absolute error (MAE), mean absolute percentage error (MAPE)
and accuracy metrics. </font><br> Link: <a href='http://arxiv.org/pdf/2211.07392v1' target="_blank">http://arxiv.org/pdf/2211.07392v1</a><br> <br> <br> <font size='5'> 459 </font> <div style="text-align: right"> 2022-11-10 16:36:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Holder Recommendations using Graph Representation Learning & Link Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Lead recommendations for financial products such as funds or ETF is
potentially challenging in investment space due to changing market scenarios,
and difficulty in capturing financial holder's mindset and their philosophy.
Current methods surface leads based on certain product categorization and
attributes like returns, fees, category etc. to suggest similar product to
investors which may not capture the holder's investment behavior holistically.
Other reported works does subjective analysis of institutional holder's
ideology. This paper proposes a comprehensive data driven framework for
developing a lead recommendations system in holder's space for financial
products like funds by using transactional history, asset flows and product
specific attributes. The system assumes holder's interest implicitly by
considering all investment transactions made and collects possible meta
information to detect holder's investment profile/persona like investment
anticipation and investment behavior. This paper focusses on holder
recommendation component of framework which employs a bi-partite graph
representation of financial holders and funds using variety of attributes and
further employs GraphSage model for learning representations followed by link
prediction model for ranking recommendation for future period. The performance
of the proposed approach is compared with baseline model i.e., content-based
filtering approach on metric hits at Top-k (50, 100, 200) recommendations. We
found that the proposed graph ML solution outperform baseline by absolute 42%,
22% and 14% with a look ahead bias and by absolute 18%, 19% and 18% on
completely unseen holders in terms of hit rate for top-k recommendations: 50,
100 and 200 respectively. </font><br> Link: <a href='http://arxiv.org/pdf/2212.09624v1' target="_blank">http://arxiv.org/pdf/2212.09624v1</a><br> <br> <br> <font size='5'> 460 </font> <div style="text-align: right"> 2022-11-10 08:13:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Relative growth rate optimization under behavioral criterion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies a continuous-time optimal portfolio selection problem in
the complete market for a behavioral investor whose preference is of the
prospect type with probability distortion. The investor concerns about the
terminal relative growth rate (log-return) instead of absolute capital value.
This model can be regarded as an extension of the classical growth optimal
problem to the behavioral framework. It leads to a new type of M-shaped utility
maximization problem under nonlinear Choquet expectation. Due to the presence
of probability distortion, the classical stochastic control methods are not
applicable. By the martingale method, concavification and quantile optimization
techniques, we derive the closed-form optimal growth rate. We find that the
benchmark growth rate has a significant impact on investment behaviors.
Compared to Zhang et al where the same preference measure is applied to the
terminal relative wealth, we find a new phenomenon when the investor's risk
tolerance level is high and the market states are bad. In addition, our optimal
wealth in every scenario is less sensitive to the pricing kernel and thus more
stable than theirs. </font><br> Link: <a href='http://arxiv.org/pdf/2211.05402v1' target="_blank">http://arxiv.org/pdf/2211.05402v1</a><br> <br> <br> <font size='5'> 461 </font> <div style="text-align: right"> 2022-11-10 06:18:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment and consumption under logarithmic utility and uncertainty model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study a robust utility maximization problem in the case of an incomplete
market and logarithmic utility with general stochastic constraints, not
necessarily convex. Our problem is equivalent to maximizing of nonlinear
expected logarithmic utility. We characterize the optimal solution using
quadratic BSDE. </font><br> Link: <a href='http://arxiv.org/pdf/2211.05367v1' target="_blank">http://arxiv.org/pdf/2211.05367v1</a><br> <br> <br> <font size='5'> 462 </font> <div style="text-align: right"> 2022-11-10 03:35:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal growth strategies for a representative agent in a continuous-time asset market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a multi-agent model of an asset market and study conditions that
guarantee that the strategy of an individual agent cannot outperform the
market. The model assumes a mean-field approximation of the market by
considering an infinite number of infinitesimal agents who use the same
strategy and another infinitesimal agent with a different strategy who tries to
outperform the market. We show that the optimal strategy for the market agents
is to split their investment budgets among the assets proportionally to their
discounted expected relative dividend intensities. </font><br> Link: <a href='http://arxiv.org/pdf/2211.05316v1' target="_blank">http://arxiv.org/pdf/2211.05316v1</a><br> <br> <br> <font size='5'> 463 </font> <div style="text-align: right"> 2022-11-10 01:58:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal consumption-investment with coupled constraints on consumption and investment strategies in a regime switching market with random coefficients</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies finite-time optimal consumption-investment problems with
power, logarithmic and exponential utilities, in a regime switching market with
random coefficients, subject to coupled constraints on the consumption and
investment strategies. We provide explicit optimal consumption-investment
strategies and optimal values for the problems in terms of the solutions to
some diagonally quadratic backward stochastic differential equation (BSDE)
systems and linear BSDE systems with unbound coefficients. Some of these BSDEs
are new in the literature and solving them is one of the main theoretical
contributions of this paper. We accomplish the latter by applying the
truncation, approximation technique to get some a priori uniformly lower and
upper bounds for their solutions. </font><br> Link: <a href='http://arxiv.org/pdf/2211.05291v1' target="_blank">http://arxiv.org/pdf/2211.05291v1</a><br> <br> <br> <font size='5'> 464 </font> <div style="text-align: right"> 2022-11-09 05:37:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Solution for a Fundamental Problem of 3D Inference based on 2D Representations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: 3D inference from monocular vision using neural networks is an important
research area of computer vision. Applications of the research area are various
with many proposed solutions and have shown remarkable performance. Although
many efforts have been invested, there are still unanswered questions, some of
which are fundamental. In this paper, I discuss a problem that I hope will come
to be known as a generalization of the Blind Perspective-n-Point (Blind PnP)
problem for object-driven 3D inference based on 2D representations. The vital
difference between the fundamental problem and the Blind PnP problem is that 3D
inference parameters in the fundamental problem are attached directly to 3D
points and the camera concept will be represented through the sharing of the
parameters of these points. By providing an explainable and robust
gradient-decent solution based on 2D representations for an important special
case of the problem, the paper opens up a new approach for using available
information-based learning methods to solve problems related to 3D object pose
estimation from 2D images. </font><br> Link: <a href='http://arxiv.org/pdf/2211.04691v1' target="_blank">http://arxiv.org/pdf/2211.04691v1</a><br> <br> <br> <font size='5'> 465 </font> <div style="text-align: right"> 2022-11-08 17:23:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Profit Shifting Frictions and the Geography of Multinational Activity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a quantitative general equilibrium model of multinational activity
embedding corporate taxation and profit shifting. In addition to trade and
investment frictions, our model shows that profit-shifting frictions shape the
geography of multinational production. Key to our model is the distinction
between the corporate tax elasticity of real activity and profit shifting. The
quantification of our model requires estimates of shifted profits flows. We
provide a new, model-consistent methodology to calibrate bilateral
profit-shifting frictions based on accounting identities. We simulate various
tax reforms aimed at curbing tax-dodging practices of multinationals and their
impact on a range of outcomes, including tax revenues and production. Our
results show that the effects of the international relocation of firms across
countries are of comparable magnitude as the direct gains in taxable income. </font><br> Link: <a href='http://arxiv.org/pdf/2211.04388v2' target="_blank">http://arxiv.org/pdf/2211.04388v2</a><br> <br> <br> <font size='5'> 466 </font> <div style="text-align: right"> 2022-11-08 16:57:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Macroeconomic evaluation of the growth of the UK economy over the period 2000 to 2019</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: An information entropy statistical methodology was used to evaluate the
growth of the UK economy over the period 2000 to 2019, with an emphasis on the
impact of labour productivity on gross domestic product (GDP) per capita and
the average growth in real wages, during this time period. The growth of the UK
economy over the period 2000 to 2019 can be described in terms of three
distinct phases: 1) 2000 to 2007 - strong sustained economic growth 2) 2008 to
2013 - the impact of the international financial crisis, its immediate
aftermath, and period of recovery 3) 2014 to 2019 - weak sustained economic
growth The key determinant of the UK economic performance over this period
would appear to the annual rate of growth in labour productivity. It was
closely related to the annual rate of growth in GDP per capita, and it was
significantly weaker in the period 2014 to 2019 compared to the period 2000 to
2007. This also corresponded with a weaker rate of growth in annual average
real wages over the period 2014 to 2019 compared to the period 2000 to 2007.
Throughout the period 2000 to 2019, UK CPI was maintained, on average, at
approximately 2.1% per annum. More rapid UK economic growth would be expected
to be achieved by sustained investment in measures that enhance labour
productivity, with the further expectation that a sustained improvement in
labour productivity would increase the annual rate of growth of UK GDP per
capita and average real wages. While the results given in this paper are
specific to the UK over the time period 2000 to 2019, the expectation is that
the methodology and approach adopted can be applied to quantifying the dynamics
of any developed economy over any time period. </font><br> Link: <a href='http://arxiv.org/pdf/2212.03947v1' target="_blank">http://arxiv.org/pdf/2212.03947v1</a><br> <br> <br> <font size='5'> 467 </font> <div style="text-align: right"> 2022-11-07 14:50:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dynamic Estimates Of The Arrow-Pratt Absolute And Relative Risk Aversion Coefficients</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We derive a closed-form expression capturing the degree of Relative Risk
Aversion (RRA) of investors for non-"fair" lotteries. We argue that our formula
is superior to earlier methods that have been proposed, as it is a function of
only three variables. Namely, the Treasury yields, the returns and the market
capitalization of a specific market index. Our formula, is tested on CAC 40,
EURO, S&P 500 and STOXX 600, with respect to the market capitalization of each
index, for different time periods. We deduce that the investors in these
markets exhibit Decreasing Absolute Risk Aversion (DARA) through all the
different time periods that we consider, while the degree of RRA has altered
between being constant, decreasing or increasing. Furthermore, we propose a
simple and intuitive way to measure the degree to which a wrong assumption with
respect to the utility function of an investor will affect the structure of his
portfolio. Our method is built on a two asset portfolio framework. Namely, a
portfolio consisting of one risky and one risk-free asset. Applying our method,
the empirical findings indicate that the weight invested in the risky asset
varies substantially even among utility functions with similar characteristics. </font><br> Link: <a href='http://arxiv.org/pdf/2211.03604v1' target="_blank">http://arxiv.org/pdf/2211.03604v1</a><br> <br> <br> <font size='5'> 468 </font> <div style="text-align: right"> 2022-11-07 10:17:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prospects and Challenges for Sustainable Tourism: Evidence from South Asian Countries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Tourism is one of the world's fastest expanding businesses, as well as a
significant source of foreign exchange profits and jobs. The research is based
on secondary sources. The facts and information were primarily gathered and
analyzed from various published papers and articles. The study goals are to
illustrate the current scenario of tourism industry in south Asia, classifies
the restraints and recommends helpful key developments to achieve sustainable
tourism consequently. The study revealed that major challenges of sustainable
tourism in south Asian region are lack of infrastructure facilities, modern and
sufficient recreation facilities, security and safety, proper training and HR,
proper planning from government, marketing and information, product
development, tourism awareness, security and safety, and political instability
etc. The study also provides some suggestive measures that for the long-term
growth of regional tourism, the government should establish and implement
policies involving public and private investment and collaboration. </font><br> Link: <a href='http://arxiv.org/pdf/2211.03411v1' target="_blank">http://arxiv.org/pdf/2211.03411v1</a><br> <br> <br> <font size='5'> 469 </font> <div style="text-align: right"> 2022-11-07 05:14:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DeepFlow: A Cross-Stack Pathfinding Framework for Distributed AI Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past decade, machine learning model complexity has grown at an
extraordinary rate, as has the scale of the systems training such large models.
However there is an alarmingly low hardware utilization (5-20%) in large scale
AI systems. The low system utilization is a cumulative effect of minor losses
across different layers of the stack, exacerbated by the disconnect between
engineers designing different layers spanning across different industries. We
propose CrossFlow, a novel framework that enables cross-layer analysis all the
way from the technology layer to the algorithmic layer. We also propose
DeepFlow (built on top of CrossFlow using machine learning techniques) to
automate the design space exploration and co-optimization across different
layers of the stack. We have validated CrossFlow accuracy with distributed
training on real commercial hardware and showcase several DeepFlow case studies
demonstrating pitfalls of not optimizing across the
technology-hardware-software stack for what is likely, the most important
workload driving large development investments in all aspects of computing
stack. </font><br> Link: <a href='http://arxiv.org/pdf/2211.03309v2' target="_blank">http://arxiv.org/pdf/2211.03309v2</a><br> <br> <br> <font size='5'> 470 </font> <div style="text-align: right"> 2022-11-07 03:48:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Institutional ownership and liquidity commonality: evidence from Australia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the liquidity commonality impact of local and foreign institutional
investment in the Australian equity market in the cross-section and over time.
We find that commonality in liquidity is higher for large stocks compared to
small stocks in the cross-section of stocks, and the spread between the two has
increased over the past two decades. We show that this divergence can be
explained by foreign institutional ownership. This finding suggests that
foreign institutional investment contributes to an increase in the exposure of
large stocks to unexpected liquidity events in the local market. We find a
positive association between foreign institutional ownership and commonality in
liquidity across all stocks, particularly in large and mid-cap stocks.
Correlated trading by foreign institutions explains this association. However,
local institutional ownership is positively related to the commonality in
liquidity for large-cap stocks only. </font><br> Link: <a href='http://arxiv.org/pdf/2211.03287v1' target="_blank">http://arxiv.org/pdf/2211.03287v1</a><br> <br> <br> <font size='5'> 471 </font> <div style="text-align: right"> 2022-11-06 19:01:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "Seeing Sound": Audio Classification with the Wigner-Wille Distribution and Convolutional Neural Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With big data becoming increasingly available, IoT hardware becoming widely
adopted, and AI capabilities becoming more powerful, organizations are
continuously investing in sensing. Data coming from sensor networks are
currently combined with sensor fusion and AI algorithms to drive innovation in
fields such as self-driving cars. Data from these sensors can be utilized in
numerous use cases, including alerts in safety systems of urban settings, for
events such as gun shots and explosions. Moreover, diverse types of sensors,
such as sound sensors, can be utilized in low-light conditions or at locations
where a camera is not available. This paper investigates the potential of the
utilization of sound-sensor data in an urban context. Technically, we propose a
novel approach of classifying sound data using the Wigner-Ville distribution
and Convolutional Neural Networks. In this paper, we report on the performance
of the approach on open-source datasets. The concept and work presented is
based on my doctoral thesis, which was performed as part of the Engineering
Doctorate program in Data Science at the University of Eindhoven, in
collaboration with the Dutch National Police. Additional work on real-world
datasets was performed during the thesis, which are not presented here due to
confidentiality. </font><br> Link: <a href='http://arxiv.org/pdf/2211.03202v1' target="_blank">http://arxiv.org/pdf/2211.03202v1</a><br> <br> <br> <font size='5'> 472 </font> <div style="text-align: right"> 2022-11-04 20:36:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bridging HPC Communities through the Julia Programming Language</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Julia programming language has evolved into a modern alternative to fill
existing gaps in scientific computing and data science applications. Julia
leverages a unified and coordinated single-language and ecosystem paradigm and
has a proven track record of achieving high performance without sacrificing
user productivity. These aspects make Julia a viable alternative to
high-performance computing's (HPC's) existing and increasingly costly many-body
workflow composition strategy in which traditional HPC languages (e.g.,
Fortran, C, C++) are used for simulations, and higher-level languages (e.g.,
Python, R, MATLAB) are used for data analysis and interactive computing.
Julia's rapid growth in language capabilities, package ecosystem, and community
make it a promising universal language for HPC. This paper presents the views
of a multidisciplinary group of researchers from academia, government, and
industry that advocate for an HPC software development paradigm that emphasizes
developer productivity, workflow portability, and low barriers for entry. We
believe that the Julia programming language, its ecosystem, and its community
provide modern and powerful capabilities that enable this group's objectives.
Crucially, we believe that Julia can provide a feasible and less costly
approach to programming scientific applications and workflows that target HPC
facilities. In this work, we examine the current practice and role of Julia as
a common, end-to-end programming model to address major challenges in
scientific reproducibility, data-driven AI/machine learning, co-design and
workflows, scalability and performance portability in heterogeneous computing,
network communication, data management, and community education. As a result,
the diversification of current investments to fulfill the needs of the upcoming
decade is crucial as more supercomputing centers prepare for the exascale era. </font><br> Link: <a href='http://arxiv.org/pdf/2211.02740v2' target="_blank">http://arxiv.org/pdf/2211.02740v2</a><br> <br> <br> <font size='5'> 473 </font> <div style="text-align: right"> 2022-11-04 13:37:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Collaborative Multiobjective Evolutionary Algorithms in search of better Pareto Fronts. An application to trading systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Technical indicators use graphic representations of data sets by applying
various mathematical formulas to financial time series of prices. These
formulas comprise a set of rules and parameters whose values are not
necessarily known and depend on many factors: the market in which it operates,
the size of the time window, and others. This paper focuses on the real-time
optimization of the parameters applied for analyzing time series of data. In
particular, we optimize the parameters of technical and financial indicators
and propose other applications, such as glucose time series. We propose the
combination of several Multi-objective Evolutionary Algorithms (MOEAs). Unlike
other approaches, this paper applies a set of different MOEAs, collaborating to
construct a global Pareto Set of solutions. Solutions for financial problems
seek high returns with minimal risk. The optimization process is continuous and
occurs at the same frequency as the investment time interval. This technique
permits the application of non-dominated solutions obtained with different
MOEAs simultaneously. Experimental results show that this technique increases
the returns of the commonly used Buy \& Hold strategy and other multi-objective
strategies, even for daily operations. </font><br> Link: <a href='http://arxiv.org/pdf/2211.02451v1' target="_blank">http://arxiv.org/pdf/2211.02451v1</a><br> <br> <br> <font size='5'> 474 </font> <div style="text-align: right"> 2022-11-04 00:33:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: (Machine) Learning from the COVID-19 Lockdown about Electricity Market Performance with a Large Share of Renewables</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The negative demand shock due to the COVID-19 lockdown has reduced net demand
for electricity -- system demand less amount of energy produced by intermittent
renewables, hydroelectric units, and net imports -- that must be served by
controllable generation units. Under normal demand conditions, introducing
additional renewable generation capacity reduces net demand. Consequently, the
lockdown can provide insights about electricity market performance with a large
share of renewables. We find that although the lockdown reduced average
day-ahead prices in Italy by 45%, re-dispatch costs increased by 73%, both
relative to the average of the same magnitude for the same period in previous
years. We estimate a deep-learning model using data from 2017--2019 and find
that predicted re-dispatch costs during the lockdown period are only 26% higher
than the same period in previous years. We argue that the difference between
actual and predicted lockdown period re-dispatch costs is the result of
increased opportunities for suppliers with controllable units to exercise
market power in the re-dispatch market in these persistently low net demand
conditions. Our results imply that without grid investments and other
technologies to manage low net demand conditions, an increased share of
intermittent renewables is likely to increase costs of maintaining a reliable
grid. </font><br> Link: <a href='http://arxiv.org/pdf/2211.02196v1' target="_blank">http://arxiv.org/pdf/2211.02196v1</a><br> <br> <br> <font size='5'> 475 </font> <div style="text-align: right"> 2022-11-03 10:00:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment with insider information using Skorokhod & Russo-Vallois integration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the maximization of the logarithmic utility of an insider with
different anticipating techniques. Our aim is to compare the usage of the
forward and Skorokhod integrals in this context with multiple assets. We show
theoretically and with simulations that the Skorokhod insider always overcomes
the forward insider, just the opposite of what happens in the case of
risk-neutral traders. Moreover, an ordinary trader might overcome both insiders
if there is a large enough negative fluctuation in the driving stochastic
process that leads to a negative enough final value. Our results point to the
fact that the interplay between anticipating stochastic calculus and nonlinear
utilities might yield non-intuitive results from the financial viewpoint. </font><br> Link: <a href='http://arxiv.org/pdf/2211.07471v1' target="_blank">http://arxiv.org/pdf/2211.07471v1</a><br> <br> <br> <font size='5'> 476 </font> <div style="text-align: right"> 2022-11-02 09:17:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Topology-based Approximations for $\mathcal{N}-1$ Contingency Constraints in Power Transmission Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It is crucial for maintaining the security of supply that transmission
networks continue to operate even if a single line fails. Modeling $\mathcal{N}
- 1$ security in power system capacity expansion problems introduces many extra
constraints if all possible outages are accounted for, which leads to a high
computational burden. Typical approaches to avoid this burden consider only a
subset of possible outages relevant to a given dispatch situation. However,
this relies on knowing the dispatch situation beforehand, and it is not
suitable for investment optimization problems where the generation fleet is not
known in advance. In this paper, we introduce a heuristic approach to model the
fully secured $\mathcal{N}-1$ feasible space using a smaller number of
constraints in a way that only depends on the topology of transmission
networks. In our proposed approach, the network's security is modelled by
comparing the polytope of the feasible space of nodal net power obtained from
the security-constrained linearized AC optimal power flow problem. To
approximate this polytope, a buffer capacity factor is defined for transmission
lines in the $\mathcal{N}-0$ secure case, thereby avoiding the introduction of
many additional constraints. In this way, three approaches are introduced for
obtaining a buffer capacity factor consisting of approximate, robust and
line-specific approaches. Finally, the performance of our proposed approaches
is assessed in different scales of transmission networks for determining the
proposed buffer capacity factors, contingency analysis and economic evaluation.
Moreover, we find that our proposed heuristics provide excellent approximations
of the fully secured $\mathcal{N}-1$ solutions with a much lower computational
burden. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00970v1' target="_blank">http://arxiv.org/pdf/2211.00970v1</a><br> <br> <br> <font size='5'> 477 </font> <div style="text-align: right"> 2022-11-02 04:42:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Effects of syndication network on specialisation and performance of venture capital firms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Chinese venture capital (VC) market is a young and rapidly expanding
financial subsector. Gaining a deeper understanding of the investment
behaviours of VC firms is crucial for the development of a more sustainable and
healthier market and economy. Contrasting evidence supports that either
specialisation or diversification helps to achieve a better investment
performance. However, the impact of the syndication network is overlooked.
Syndication network has a great influence on the propagation of information and
trust. By exploiting an authoritative VC dataset of thirty-five-year investment
information in China, we construct a joint-investment network of VC firms and
analyse the effects of syndication and diversification on specialisation and
investment performance. There is a clear correlation between the syndication
network degree and specialisation level of VC firms, which implies that the
well-connected VC firms are diversified. More connections generally bring about
more information or other resources, and VC firms are more likely to enter a
new stage or industry with some new co-investing VC firms when compared to a
randomised null model. Moreover, autocorrelation analysis of both
specialisation and success rate on the syndication network indicates that
clustering of similar VC firms is roughly limited to the secondary
neighbourhood. When analysing local clustering patterns, we discover that,
contrary to popular beliefs, there is no apparent successful club of investors.
In contrast, investors with low success rates are more likely to cluster. Our
discoveries enrich the understanding of VC investment behaviours and can assist
policymakers in designing better strategies to promote the development of the
VC industry. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00873v1' target="_blank">http://arxiv.org/pdf/2211.00873v1</a><br> <br> <br> <font size='5'> 478 </font> <div style="text-align: right"> 2022-11-02 04:41:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: State-dependent Asset Allocation Using Neural Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Changes in market conditions present challenges for investors as they cause
performance to deviate from the ranges predicted by long-term averages of means
and covariances. The aim of conditional asset allocation strategies is to
overcome this issue by adjusting portfolio allocations to hedge changes in the
investment opportunity set. This paper proposes a new approach to conditional
asset allocation that is based on machine learning; it analyzes historical
market states and asset returns and identifies the optimal portfolio choice in
a new period when new observations become available. In this approach, we
directly relate state variables to portfolio weights, rather than firstly
modeling the return distribution and subsequently estimating the portfolio
choice. The method captures nonlinearity among the state (predicting) variables
and portfolio weights without assuming any particular distribution of returns
and other data, without fitting a model with a fixed number of predicting
variables to data and without estimating any parameters. The empirical results
for a portfolio of stock and bond indices show the proposed approach generates
a more efficient outcome compared to traditional methods and is robust in using
different objective functions across different sample periods. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00871v1' target="_blank">http://arxiv.org/pdf/2211.00871v1</a><br> <br> <br> <font size='5'> 479 </font> <div style="text-align: right"> 2022-11-01 15:31:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A novel approach to quantify volatility prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Volatility prediction in the financial market helps to understand the profit
and involved risks in investment. However, due to irregularities, high
fluctuations, and noise in the time series, predicting volatility poses a
challenging task. In the recent Covid-19 pandemic situation, volatility
prediction using complex intelligence techniques has attracted enormous
attention from researchers worldwide. In this paper, a novel and simple
approach based on the robust least squares method in two approaches a) with
least absolute residuals (LAR) and b) without LAR, have been applied to the
Chicago Board Options Exchange (CBOE) Volatility Index (VIX) for a period of
ten years. For a deeper analysis, the volatility time series has been
decomposed into long-term trends, and seasonal, and random fluctuations. The
data sets have been divided into parts viz. training data set and testing data
set. The validation results have been achieved using root mean square error
(RMSE) values. It has been found that robust least squares method with LAR
approach gives better results for volatility (RMSE = 0.01366) and its
components viz. long term trend (RMSE = 0.10087), seasonal (RMSE = 0.010343)
and remainder fluctuations (RMSE = 0.014783), respectively. For the first time,
generalized prediction equations for volatility and its three components have
been presented. Young researchers working in this domain can directly use the
presented prediction equations to understand their data sets. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00528v1' target="_blank">http://arxiv.org/pdf/2211.00528v1</a><br> <br> <br> <font size='5'> 480 </font> <div style="text-align: right"> 2022-11-01 08:39:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Decision support for the Technician Routing and Scheduling Problem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The technician routing and scheduling problem (TRSP) consists of technicians
serving tasks subject to qualifications, time constraints and routing costs. In
the literature, the TRSP is solved either to provide actual technician plans or
for performing what-if analyses on different TRSP scenarios. We present a
method for building optimal TRSP scenarios, e.g., how many technicians to
employ, which technician qualifications to upgrade, etc. The scenarios are
built such that the combined TRSP costs (OPEX) and investment costs (CAPEX) are
minimized. Using a holistic approach we can generate scenarios that would not
have been found by studying the investments individually. The proposed method
consists of a matheuristic based on column generation. To reduce computational
time, the routing costs of a technician are approximated. The proposed method
is evaluated on data from the literature and on real-life data from a
telecommunication company. The evaluation shows that the proposed method
successfully suggests attractive scenarios. The method especially excels in
ensuring that more tasks are serviced but also reduces travel time with around
16% in the real-life instance. We believe that the proposed method could
constitute an important strategic tool in field service companies and we
propose future research directions to further its applicability. </font><br> Link: <a href='http://arxiv.org/pdf/2211.16968v1' target="_blank">http://arxiv.org/pdf/2211.16968v1</a><br> <br> <br> <font size='5'> 481 </font> <div style="text-align: right"> 2022-11-01 08:33:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Weak Identification in Low-Dimensional Factor Models with One or Two Factors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper describes how to reparameterize low-dimensional factor models to
fit the weak identification theory developed for generalized method of moments
(GMM) models. Identification conditions in low-dimensional factor models can be
close to failing in a similar way to identification conditions in instrumental
variables or GMM models. Weak identification estimation theory requires a
reparameterization to separate the weakly identified parameters from the
strongly identified parameters. Furthermore, identification-robust hypothesis
tests benefit from a reparameterization that makes the nuisance parameters
strongly identified. We describe such a reparameterization in low-dimensional
factor models with one or two factors. Simulations show that
identification-robust hypothesis tests that require the reparameterization are
less conservative than identification-robust hypothesis tests that use the
original parameterization. The simulations also show that estimates of the
number of factors frequently include weakly identified factors. An empirical
application to a factor model of parental investments in children is included. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00329v1' target="_blank">http://arxiv.org/pdf/2211.00329v1</a><br> <br> <br> <font size='5'> 482 </font> <div style="text-align: right"> 2022-10-31 17:39:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cost-aware Generalized $$-investing for Multiple Hypothesis Testing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the problem of sequential multiple hypothesis testing with
nontrivial data collection cost. This problem appears, for example, when
conducting biological experiments to identify differentially expressed genes in
a disease process. This work builds on the generalized $\alpha$-investing
framework that enables control of the false discovery rate in a sequential
testing setting. We make a theoretical analysis of the long term asymptotic
behavior of $\alpha$-wealth which motivates a consideration of sample size in
the $\alpha$-investing decision rule. Posing the testing process as a game with
nature, we construct a decision rule that optimizes the expected return (ERO)
of $\alpha$-wealth and provides an optimal sample size for the test. Empirical
results show that a cost-aware ERO decision rule correctly rejects more false
null hypotheses than other methods. We extend cost-aware ERO investing to
finite-horizon testing which enables the decision rule to allocate samples
across many tests. Finally, empirical tests on real data sets from biological
experiments show that cost-aware ERO produces actionable decisions to conduct
tests at optimal sample sizes. </font><br> Link: <a href='http://arxiv.org/pdf/2210.17514v2' target="_blank">http://arxiv.org/pdf/2210.17514v2</a><br> <br> <br> <font size='5'> 483 </font> <div style="text-align: right"> 2022-10-31 03:07:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uncertainty Aware Trader-Company Method: Interpretable Stock Price Prediction Capturing Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine learning is an increasingly popular tool with some success in
predicting stock prices. One promising method is the Trader-Company~(TC)
method, which takes into account the dynamism of the stock market and has both
high predictive power and interpretability. Machine learning-based stock
prediction methods including the TC method have been concentrating on point
prediction. However, point prediction in the absence of uncertainty estimates
lacks credibility quantification and raises concerns about safety. The
challenge in this paper is to make an investment strategy that combines high
predictive power and the ability to quantify uncertainty. We propose a novel
approach called Uncertainty Aware Trader-Company Method~(UTC) method. The core
idea of this approach is to combine the strengths of both frameworks by merging
the TC method with the probabilistic modeling, which provides probabilistic
predictions and uncertainty estimations. We expect this to retain the
predictive power and interpretability of the TC method while capturing the
uncertainty. We theoretically prove that the proposed method estimates the
posterior variance and does not introduce additional biases from the original
TC method. We conduct a comprehensive evaluation of our approach based on the
synthetic and real market datasets. We confirm with synthetic data that the UTC
method can detect situations where the uncertainty increases and the prediction
is difficult. We also confirmed that the UTC method can detect abrupt changes
in data generating distributions. We demonstrate with real market data that the
UTC method can achieve higher returns and lower risks than baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2210.17030v2' target="_blank">http://arxiv.org/pdf/2210.17030v2</a><br> <br> <br> <font size='5'> 484 </font> <div style="text-align: right"> 2022-10-28 15:59:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Empirical Macroeconomics and DSGE Modeling in Statistical Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Dynamic stochastic general equilibrium (DSGE) models have been an ubiquitous,
and controversial, part of macroeconomics for decades. In this paper, we
approach DSGEs purely as statstical models. We do this by applying two common
model validation checks to the canonical Smets and Wouters 2007 DSGE: (1) we
simulate the model and see how well it can be estimated from its own simulation
output, and (2) we see how well it can seem to fit nonsense data. We find that
(1) even with centuries' worth of data, the model remains poorly estimated, and
(2) when we swap series at random, so that (e.g.) what the model gets as the
inflation rate is really hours worked, what it gets as hours worked is really
investment, etc., the fit is often only slightly impaired, and in a large
percentage of cases actually improves (even out of sample). Taken together,
these findings cast serious doubt on the meaningfulness of parameter estimates
for this DSGE, and on whether this specification represents anything structural
about the economy. Constructively, our approaches can be used for model
validation by anyone working with macroeconomic time series. </font><br> Link: <a href='http://arxiv.org/pdf/2210.16224v2' target="_blank">http://arxiv.org/pdf/2210.16224v2</a><br> <br> <br> <font size='5'> 485 </font> <div style="text-align: right"> 2022-10-28 13:17:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Intelligence and Global Bias in the Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Trade is one of the essential feature of human intelligence. The securities
market is the ultimate expression of it. The fundamental indicators of stocks
include information as well as the effects of noise and bias on the stock
prices; however, identifying the effects of noise and bias is generally
difficult. In this article, I present the true fundamentals hypothesis based on
rational expectations and detect the global bias components from the actual
fundamental indicators by using a log-normal distribution model based on the
true fundamentals hypothesis. The analysis results show that biases generally
exhibit the same characteristics, strongly supporting the true fundamentals
hypothesis. Notably, the positive price-to-cash flows from the investing
activities ratio is a proxy for the true fundamentals. Where do these biases
come from? The answer is extremely simple: ``Cash is a fact, profit is an
opinion.'' Namely, opinions of management and accounting are added to true
fundamentals. As a result, Kesten process is realized and the Pareto
distribution is to be obtained. This means that the market knows it and
represents as a stable global bias in the stock market. </font><br> Link: <a href='http://arxiv.org/pdf/2210.16113v1' target="_blank">http://arxiv.org/pdf/2210.16113v1</a><br> <br> <br> <font size='5'> 486 </font> <div style="text-align: right"> 2022-10-28 06:14:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Incorporating Interactive Facts for Stock Selection via Neural Recursive ODEs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stock selection attempts to rank a list of stocks for optimizing investment
decision making, aiming at minimizing investment risks while maximizing profit
returns. Recently, researchers have developed various (recurrent) neural
network-based methods to tackle this problem. Without exceptions, they
primarily leverage historical market volatility to enhance the selection
performance. However, these approaches greatly rely on discrete sampled market
observations, which either fail to consider the uncertainty of stock
fluctuations or predict continuous stock dynamics in the future. Besides, some
studies have considered the explicit stock interdependence derived from
multiple domains (e.g., industry and shareholder). Nevertheless, the implicit
cross-dependencies among different domains are under-explored. To address such
limitations, we present a novel stock selection solution -- StockODE, a latent
variable model with Gaussian prior. Specifically, we devise a Movement Trend
Correlation module to expose the time-varying relationships regarding stock
movements. We design Neural Recursive Ordinary Differential Equation Networks
(NRODEs) to capture the temporal evolution of stock volatility in a continuous
dynamic manner. Moreover, we build a hierarchical hypergraph to incorporate the
domain-aware dependencies among the stocks. Experiments conducted on two
real-world stock market datasets demonstrate that StockODE significantly
outperforms several baselines, such as up to 18.57% average improvement
regarding Sharpe Ratio. </font><br> Link: <a href='http://arxiv.org/pdf/2210.15925v1' target="_blank">http://arxiv.org/pdf/2210.15925v1</a><br> <br> <br> <font size='5'> 487 </font> <div style="text-align: right"> 2022-10-27 11:12:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Measuring Transition Risk in Investment Funds</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a comprehensive framework to measure the impact of the climate
transition on investment portfolios. Our analysis is enriched by including
geographical, sectoral, company and ISIN-level data to assess transition risk.
We find that investment funds suffer a moderate 5.7% loss upon materialization
of a high transition risk scenario. However, the risk distribution is
significantly left-skewed, with the worst 1% funds experiencing an average loss
of 21.3%. In terms of asset classes, equities are the worst performers
(-12.7%), followed by corporate bonds (-5.6%) and government bonds (-4.8%). We
discriminate among financial instruments by considering the carbon footprint of
specific counterparties and the credit rating, duration, convexity and
volatility of individual exposures. We find that sustainable funds are less
exposed to transition risk and perform better than the overall fund sector in
the low-carbon transition, validating their choice as green investments. </font><br> Link: <a href='http://arxiv.org/pdf/2210.15329v3' target="_blank">http://arxiv.org/pdf/2210.15329v3</a><br> <br> <br> <font size='5'> 488 </font> <div style="text-align: right"> 2022-10-26 20:07:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Role of Risk Perceptions in Cyber Insurance Contracts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Risk perceptions are essential in cyber insurance contracts. With the recent
surge of information, human risk perceptions are exposed to the influences from
both beneficial knowledge and fake news. In this paper, we study the role of
the risk perceptions of the insurer and the user in cyber insurance contracts.
We formulate the cyber insurance problem into a principal-agent problem where
the insurer designs the contract containing a premium payment and a coverage
plan. The risk perceptions of the insurer and the user are captured by coherent
risk measures. Our framework extends the cyber insurance problem containing a
risk-neutral insurer and a possibly risk-averse user, which is often considered
in the literature. The explicit characterizations of both the insurer's and the
user's risk perceptions allow us to show that cyber insurance has the potential
to incentivize the user to invest more on system protection. This possibility
to increase cyber security relies on the facts that the insurer is more
risk-averse than the user (in a minimization setting) and that the insurer's
risk perception is more sensitive to the changes in the user's actions than the
user himself. We investigate the properties of feasible contracts in a case
study on the insurance of a computer system against ransomware. </font><br> Link: <a href='http://arxiv.org/pdf/2210.15010v1' target="_blank">http://arxiv.org/pdf/2210.15010v1</a><br> <br> <br> <font size='5'> 489 </font> <div style="text-align: right"> 2022-10-26 14:38:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Accelerating Progress Towards Practical Quantum Advantage: The Quantum Technology Demonstration Project Roadmap</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum information science and technology (QIST) is a critical and emerging
technology with the potential for enormous world impact and is currently
invested in by over 40 nations. To bring these large-scale investments to
fruition and bridge the lower technology readiness levels (TRLs) of fundamental
research at universities to the high TRLs necessary to realize the promise of
practical quantum advantage accessible to industry and the public, we present a
roadmap for Quantum Technology Demonstration Projects (QTDPs). Such QTDPs,
focused on intermediate TRLs, are large-scale public-private partnerships with
a high probability of translation from laboratory to practice. They create
technology demonstrating a clear 'quantum advantage' for science breakthroughs
that are user-motivated and will provide access to a broad and diverse
community of scientific users. Successful implementation of a program of QTDPs
will have large positive economic impacts. </font><br> Link: <a href='http://arxiv.org/pdf/2210.14757v3' target="_blank">http://arxiv.org/pdf/2210.14757v3</a><br> <br> <br> <font size='5'> 490 </font> <div style="text-align: right"> 2022-10-26 07:09:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Which Factors Matter Most? Can Startup Valuation be Micro-Targeted?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While startup valuations are influenced by revenues, risks, age, and
macroeconomic conditions, specific causality is traditionally a black box.
Because valuations are not disclosed, roles played by other factors (industry,
geography, and intellectual property) can often only be guessed at. VC
valuation research indicates the importance of establishing a factor-hierarchy
to better understand startup valuations and their dynamics, suggesting the
wisdom of hiring data-scientists for this purpose. Bespoke understanding can be
established via construction of hierarchical prediction models based on
decision trees and random forests. These have the advantage of understanding
which factors matter most. In combination with OLS, the also tell us the
circumstances of when specific causalities apply. This study explores the
deterministic role of categorical variables on the valuation of start-ups (i.e.
the joint-combination geographic, urban, and sectoral denomination-variables),
in order to be able to build a generalized valuation scorecard approach. Using
a dataset of 1,091 venture-capital investments, containing 1,044 unique EU and
EEA, this study examines microeconomic, sectoral, and local-level impacts on
startup valuation. In principle, the study relies on Fixedeffects and
Joint-fixed-effects regressions as well as the analysis and exploration of
divergent micropopulations and fault-lines by means of non-parametric
approaches combining econometric and machinelearning techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2210.14518v1' target="_blank">http://arxiv.org/pdf/2210.14518v1</a><br> <br> <br> <font size='5'> 491 </font> <div style="text-align: right"> 2022-10-25 13:40:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Three innovations of next-generation antibiotics: evolvability, specificity, and non-immunogenicity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Antimicrobial resistance is a silent pandemic that is being exacerbated by
the uncontrolled use of antibiotics. Since the discovery of penicillin, we have
been largely dependent on microbe-derived small molecules to treat bacterial
infections. However, the golden era of antibiotics is coming to an end as the
emergence and spread of antimicrobial resistance against these antibacterial
compounds is outpacing the discovery and development of new antibiotics. The
current antibiotic market suffers from various shortcomings, including the
absence of profitability and investment. However, the most important underlying
issue of traditional antibiotics arises from the inherent properties of these
small molecules being mostly broad-spectrum and non-programmable. As the
scientific knowledge of microbes progresses, the scientific community is
starting to explore entirely novel approaches to tackling antimicrobial
resistance. One of the most prominent approaches is to develop next-generation
antibiotics. In this review, we discuss three innovations of next-generation
antibiotics compared to traditional antibiotics as specificity, evolvability,
and non-immunogenicity. We present a number of potential antimicrobial agents,
including bacteriophage-based therapy, CRISPR-Cas-based antimicrobials, and
microbiome-derived antimicrobial compounds. These alternative antimicrobial
agents possess innovative properties that may overcome the inherent
shortcomings of traditional antibiotics, and some of these next-generation
antibiotics are not merely far-fetched ideas but are currently in clinical
development. We further discuss some related issues and challenges such as
infection diagnostics and regulatory frameworks that still need to be addressed
to bring these next-generation antibiotics to the antibiotic market as viable
products to combat antimicrobial resistance using a diversified set of
strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2210.14017v1' target="_blank">http://arxiv.org/pdf/2210.14017v1</a><br> <br> <br> <font size='5'> 492 </font> <div style="text-align: right"> 2022-10-24 19:32:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Flowers of immortality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There has been a recent surge of interest in what causes aging. This has been
matched by unprecedented research investment in the field from tech companies.
But, despite considerable effort from a broad range of researchers, we do not
have a rigorous mathematical theory of programmed aging. To address this, we
recently derived a mortality equation that governs the transition matrix of an
evolving population with a given maximum age. Here, we characterize the
spectrum of eigenvalues of the solution to this equation. The eigenvalues fall
into two classes. The complex and negative real eigenvalues, which we call the
flower, are always contained in the unit circle in the complex plane. They play
a negligible role in controlling the dynamics of an aging population. The
positive real eigenvalues, which we call the stem, are the only eigenvalues
which can exceed the unit circle. They control the most important properties of
the dynamics. In particular, the spectral radius increases with the maximum
allowed age. This suggests that programmed aging confers no advantage in a
constant environment. However, the spectral gap, which governs the rate of
convergence to equilibrium, decreases with the maximum allowed age. This opens
the door to an evolutionary advantage in a changing environment. </font><br> Link: <a href='http://arxiv.org/pdf/2210.13561v1' target="_blank">http://arxiv.org/pdf/2210.13561v1</a><br> <br> <br> <font size='5'> 493 </font> <div style="text-align: right"> 2022-10-24 08:01:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Understanding Player Behavior in Blockchain Games: A Case Study of Aavegotchi</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Blockchain games introduce unique gameplay and incentive mechanisms by
allowing players to be rewarded with in-game assets or tokens through financial
activities. However, most blockchain games are not comparable to traditional
games in terms of lifespan and player engagement. In this paper, we try to see
the big picture in a small way to explore and determine the impact of gameplay
and financial factors on player behavior in blockchain games. Taking Aavegotchi
as an example, we collect one year of operation data to build player profiles.
We perform an in-depth analysis of player behavior from the macroscopic data
and apply an unsupervised clustering method to distinguish the attraction of
the gameplay and incentives. Our results reveal that the whole game is held up
by a small number of players with high-frequent interaction or vast amounts of
funds invested. Financial incentives are indispensable for blockchain games for
they provide attraction and optional ways for players to engage with the game.
However, financial services are tightly linked to the free market. The game
will face an irreversible loss of players when the market experiences
depression. For blockchain games, well-designed gameplay should be the
fundamental basis for the long-lasting retention of players. </font><br> Link: <a href='http://arxiv.org/pdf/2210.13013v1' target="_blank">http://arxiv.org/pdf/2210.13013v1</a><br> <br> <br> <font size='5'> 494 </font> <div style="text-align: right"> 2022-10-22 14:47:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Factor Investing with a Deep Multi-Factor Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Modeling and characterizing multiple factors is perhaps the most important
step in achieving excess returns over market benchmarks. Both academia and
industry are striving to find new factors that have good explanatory power for
future stock returns and good stability of their predictive power. In practice,
factor investing is still largely based on linear multi-factor models, although
many deep learning methods show promising results compared to traditional
methods in stock trend prediction and portfolio risk management. However, the
existing non-linear methods have two drawbacks: 1) there is a lack of
interpretation of the newly discovered factors, 2) the financial insights
behind the mining process are unclear, making practitioners reluctant to apply
the existing methods to factor investing. To address these two shortcomings, we
develop a novel deep multi-factor model that adopts industry neutralization and
market neutralization modules with clear financial insights, which help us
easily build a dynamic and multi-relational stock graph in a hierarchical
structure to learn the graph representation of stock relationships at different
levels, e.g., industry level and universal level. Subsequently, graph attention
modules are adopted to estimate a series of deep factors that maximize the
cumulative factor returns. And a factor-attention module is developed to
approximately compose the estimated deep factors from the input factors, as a
way to interpret the deep factors explicitly. Extensive experiments on
real-world stock market data demonstrate the effectiveness of our deep
multi-factor model in the task of factor investing. </font><br> Link: <a href='http://arxiv.org/pdf/2210.12462v1' target="_blank">http://arxiv.org/pdf/2210.12462v1</a><br> <br> <br> <font size='5'> 495 </font> <div style="text-align: right"> 2022-10-21 13:02:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rogue Protocol: A Framework For NFT Royalties Tokenisation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The crypto ecosystem has evolved into a formidable channel for raising
venture capital. Each new wave of capital inflows has been epitomized by a new
type of investment vehicle, may it be ICOs, DAOs, or NFTs. Regrettably, none of
these paradigms tried to address the issue of investor protection, a pillar of
efficient capital markets. Moreover, very few projects tried to generate
economic revenue, focusing instead on marketing alone to attract new investors.
Without revenues, price discovery was impossible, while investors were left
without any protection against rug pulls. This has forced regulators to take a
hard-line approach to the ecosystem, and rule that certain tokens are
securities when they are not intended to be. Regulators have left the door open
to cryptocurrencies with truly decentralised activity like Ethereum, most
notably the SEC in its interpretation of the Howey test for digital assets. We
believe that a great number of decentralised projects could benefit from this
regulatory exception. A system where project revenue is automatically directed
to a treasury pool, and the price of tokens is computed following a
predetermined bonding curve, would allow to efficiently raise capital, while
investors would have automatic guarantees of fair participation in the success
of the project. Such a framework would incentivise founders to design
decentralised projects that create value instead of hype, while making the
application of securities laws less stringent or even needed. NFT royalties in
particular are an example of decentralised economic activity that generates
cash flows, used to back the value of associated tokens. We propose a
cryptographic system that ties the price of tokens to the success of a
decentralised activity, guarantees the fair distribution of tokens, and rewards
founders and participants in the system in line with the amount of risk they
are taking. </font><br> Link: <a href='http://arxiv.org/pdf/2211.00063v1' target="_blank">http://arxiv.org/pdf/2211.00063v1</a><br> <br> <br> <font size='5'> 496 </font> <div style="text-align: right"> 2022-10-21 03:58:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey of Data Optimization for Problems in Computer Vision Datasets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent years have witnessed remarkable progress in artificial intelligence
(AI) thanks to refined deep network structures, powerful computing devices, and
large-scale labeled datasets. However, researchers have mainly invested in the
optimization of models and computational devices, leading to the fact that good
models and powerful computing devices are currently readily available, while
datasets are still stuck at the initial stage of large-scale but low quality.
Data becomes a major obstacle to AI development. Taking note of this, we dig
deeper and find that there has been some but unstructured work on data
optimization. They focus on various problems in datasets and attempt to improve
dataset quality by optimizing its structure to facilitate AI development. In
this paper, we present the first review of recent advances in this area. First,
we summarize and analyze various problems that exist in large-scale computer
vision datasets. We then define data optimization and classify data
optimization algorithms into three directions according to the optimization
form: data sampling, data subset selection, and active learning. Next, we
organize these data optimization works according to data problems addressed,
and provide a systematic and comparative description. Finally, we summarize the
existing literature and propose some potential future research topics. </font><br> Link: <a href='http://arxiv.org/pdf/2210.11717v1' target="_blank">http://arxiv.org/pdf/2210.11717v1</a><br> <br> <br> <font size='5'> 497 </font> <div style="text-align: right"> 2022-10-20 22:23:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generalized Reciprocal Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Across many domains, real-world problems can be represented as a network.
Nodes represent domain-specific elements and edges capture the relationship
between elements. Leveraging high-performance computing and optimized link
prediction algorithms, it is increasingly possible to evaluate every possible
combination of nodal pairs enabling the generation of a comprehensive
prediction matrix (CPM) that places an individual link prediction score in the
context of all possible links involving either node (providing data-driven
context). Historically, this contextual information has been ignored given
exponentially growing problem sizes resulting in computational intractability;
however, we demonstrate that expending high-performance compute resources to
generate CPMs is a worthwhile investment given the improvement in predictive
performance. In this work, we generalize for all pairwise link-prediction tasks
our novel semi-supervised machine learning method, denoted Reciprocal
Perspective (RP). We demonstrate that RP significantly improves link prediction
accuracy by leveraging the wealth of information in a CPM. Context-based
features are extracted from the CPM for use in a stacked classifier and we
demonstrate that the application of RP in a cascade almost always results in
significantly (p < 0.05) improved predictions. These results on RS-type
problems suggest that RP is applicable to a broad range of link prediction
problems. </font><br> Link: <a href='http://arxiv.org/pdf/2210.11616v1' target="_blank">http://arxiv.org/pdf/2210.11616v1</a><br> <br> <br> <font size='5'> 498 </font> <div style="text-align: right"> 2022-10-20 15:30:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Practical Forecasting of AC Losses in Multi-layer 2G-HTS Cold Dielectric Conductors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the recent progresses on the designing and manufacturing of lightweight
and high engineering current density superconducting cables, the need for an
established, fast, and sufficiently accurate computational model for the
forecasting of AC-losses in cold-dielectric conductors, is pivotal for
increasing the investment confidence of power grid operators. However,
validating such models is not an easy task, this because on the one hand, there
is a low availability of experimental results for large scale power cables and,
on the other hand, there is a large number of 2G-HTS tapes involved whose
cross-sectional aspect ratio hinders the numerical convergence of the models
within reasonable delivery times. Thus, aiming to overcome this challenge, we
present a detailed two-dimensional H-model capable to reproduce the
experimentally measured AC-losses of multi-layer power cables made of tens of
2G-HTS tapes. Two cable designs with very high critical currents have been
considered, the first rated at 1.7 kA critical current, consisting of fifty 4
mm width 2G-HTS tapes, these split in 5 concentric layers wound over a
cylindrical former, with the three inner layers forming an arrangement of 24
tapes shielded by two further layers with 13 tapes each. This cable is
contrasted with a size wise equivalent cable with 67 superconducting tapes
rated at 3.2 kA critical current, whose design implies the use of 40 tapes of 3
mm width split within four core layers, and 27 tapes of 4 mm width distributed
in two shielding layers. In both situations a remarkable resemblance between
the simulations and experiments has been found, rendering to acceptable
estimates of the AC-losses for cold dielectric conductors, and offering a
unique view of the local electrodynamics of the wound tapes where the
mechanisms of shielding, magnetization, and transport currents can coexist
within the hysteretic process. </font><br> Link: <a href='http://arxiv.org/pdf/2210.11345v1' target="_blank">http://arxiv.org/pdf/2210.11345v1</a><br> <br> <br> <font size='5'> 499 </font> <div style="text-align: right"> 2022-10-20 15:30:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Evology: a Market Ecology Agent-Based Model of US Equity Mutual Funds</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The profitability of various investment styles in investment funds depends on
macroeconomic conditions. Market ecology, which views financial markets as
ecosystems of diverse, interacting and evolving trading strategies, has shown
that endogenous interactions between strategies determine market behaviour and
styles' performance. We present Evology: a heterogeneous, empirically
calibrated multi-agent market ecology agent-based model to quantify endogenous
interactions between US equity mutual funds, particularly Value and Growth
investment styles. We outline the model design, validation and calibration
approach and its potential for optimising investment strategies using machine
learning algorithms. </font><br> Link: <a href='http://arxiv.org/pdf/2210.11344v2' target="_blank">http://arxiv.org/pdf/2210.11344v2</a><br> <br> <br> <font size='5'> 500 </font> <div style="text-align: right"> 2022-10-19 20:30:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantifying $T$-gate-count improvements for ground-state-energy estimation with near-optimal state preparation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the question of when investing additional quantum resources in
preparing a ground state will improve the aggregate runtime associated with
estimating its energy. We analyze Lin and Tong's near-optimal state preparation
algorithm and show that it can reduce a proxy for the runtime, the $T$-gate
count, of ground state energy estimation near quadratically. Resource estimates
are provided that specify the conditions under which the added cost of state
preparation is worthwhile. </font><br> Link: <a href='http://arxiv.org/pdf/2210.10872v3' target="_blank">http://arxiv.org/pdf/2210.10872v3</a><br> <br> <br> <font size='5'> 501 </font> <div style="text-align: right"> 2022-10-19 19:43:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Beyond capacity: contractual form in electricity reliability obligations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Liberalized electricity markets often include resource adequacy mechanisms
that require consumers to contract with generation resources well in advance of
real-time operations. While administratively defined mechanisms have most
commonly taken the form of a capacity obligation, efficient markets would
feature a broad array of arrangements adapted to the risk profiles and
appetites of market participants. This article considers how the financial
hedge embedded in alternative resource adequacy contract designs can induce
different responses from risk-averse investors, with consequences for the
resource mix and market structure. We construct a stochastic equilibrium model
describing a competitive market with incomplete risk trading and compute
investment equilibria under different contracting regimes. Two policy
recommendations result. First, to avoid creating inefficiency by crowding out
other forms of risk sharing, system operators should allow resources contracted
through other means to opt out of mandatory capacity mechanisms, with their
contribution to those requirements subtracted from administratively defined
demand curves. Second, if they wish to promote a single contractual form,
regulators should consider replacing existing option-like capacity mechanisms
with a shaped forward contract for energy. Beyond these recommendations, we
discuss the tension that liberalized systems face in seeking to promote both
reliability and competitive outcomes. </font><br> Link: <a href='http://arxiv.org/pdf/2210.10858v1' target="_blank">http://arxiv.org/pdf/2210.10858v1</a><br> <br> <br> <font size='5'> 502 </font> <div style="text-align: right"> 2022-10-19 14:49:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Diamonds are Forever, Loss-Versus-Rebalancing is Not</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The always-available liquidity of automated market makers (AMMs) has been one
of the most important catalysts in early cryptocurrency adoption. However, it
has become increasingly evident that AMMs in their current form are not viable
investment options for passive liquidity providers. This is because of the cost
incurred by AMMs providing stale prices to arbitrageurs against external market
prices, formalized as loss-versus-rebalancing (LVR) [Milionis et al., 2022].
  In this paper, we present Diamond, an automated market making protocol that
aligns the incentives of liquidity providers and block producers in the
protocol-level retention of LVR. In Diamond, block producers effectively
auction the right to capture any arbitrage that exists between the external
market price of a Diamond pool, and the price of the pool itself. The proceeds
of these auctions are shared by the Diamond pool and block producer in a way
that is proven to remain incentive compatible for the block producer. Given the
participation of competing arbitrageurs, LVR is effectively prevented in
Diamond.
  We formally prove this result, and detail an implementation of Diamond. We
also provide comparative simulations of Diamond to relevant benchmarks, further
evidencing the LVR-protection capabilities of Diamond.
  With this new protection, passive liquidity provision on blockchains becomes
rationally viable, beckoning a new age for decentralized finance. </font><br> Link: <a href='http://arxiv.org/pdf/2210.10601v1' target="_blank">http://arxiv.org/pdf/2210.10601v1</a><br> <br> <br> <font size='5'> 503 </font> <div style="text-align: right"> 2022-10-19 09:45:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment and reinsurance under exponential forward preferences</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the optimal investment and proportional reinsurance problem of an
insurance company, whose investment preferences are described via a forward
dynamic utility of exponential type in a stochastic factor model allowing for a
possible dependence between the financial and insurance markets. Specifically,
we assume that the asset price process dynamics and the claim arrival intensity
are both affected by a common stochastic process and we account for a possible
environmental contagion effect through the non-zero correlation parameter
between the underlying Brownian motions driving the asset price process and the
stochastic factor dynamics. By stochastic control techniques, we construct a
forward dynamic exponential utility, and we characterize the optimal investment
and reinsurance strategy. Moreover, we investigate in detail the
zero-volatility case and provide a comparison analysis with classical results
in an analogous setting under backward utility preferences. We also discuss an
extension of the conditional certainty equivalent. Finally, we perform a
numerical analysis to highlight some features of the optimal strategy. </font><br> Link: <a href='http://arxiv.org/pdf/2210.10425v1' target="_blank">http://arxiv.org/pdf/2210.10425v1</a><br> <br> <br> <font size='5'> 504 </font> <div style="text-align: right"> 2022-10-18 18:38:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cryptocurrency, Sanctions and Agricultural Prices: An empirical study on the negative implications of sanctions and how decentralized technologies affect the agriculture futures market in developing countries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The 2022 Russia Ukraine War has led to many sanctions being placed on Russia
and Ukraine. The paper will discuss the impact the 2022 Russian Sanctions have
on agricultural food prices and hunger. The paper also uses Instrumental
Variable Analysis to find how Cryptocurrency and Bitcoin can be used to hedge
against the impact of sanctions. The 6 different countries analyzed in this
study including Bangladesh, El Salvador, Iran, Nigeria, Philippines, and South
Africa, all of which are heavy importers of wheat and corn. The paper shows
that although Bitcoin may be volatile compared to other local currencies, it
might be a good investment to safeguard assets since it is not correlated with
commodity prices.Furthermore, the study demonstrates that transaction volume
has a strong relationship with prices. </font><br> Link: <a href='http://arxiv.org/pdf/2210.10087v1' target="_blank">http://arxiv.org/pdf/2210.10087v1</a><br> <br> <br> <font size='5'> 505 </font> <div style="text-align: right"> 2022-10-18 15:15:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Research of an optimization model for servicing a network of ATMs and information payment terminals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The steadily high demand for cash contributes to the expansion of the network
of Bank payment terminals. To optimize the amount of cash in payment terminals,
it is necessary to minimize the cost of servicing them and ensure that there
are no excess funds in the network. The purpose of this work is to create a
cash management system in the network of payment terminals. The article
discusses the solution to the problem of determining the optimal amount of
funds to be loaded into the terminals, and the effective frequency of
collection, which allows to get additional income by investing the released
funds. The paper presents the results of predicting daily cash withdrawals at
ATMs using a triple exponential smoothing model, a recurrent neural network
with long short-term memory, and a model of singular spectrum analysis. These
forecasting models allowed us to obtain a sufficient level of correct forecasts
with good accuracy and completeness. The results of forecasting cash
withdrawals were used to build a discrete optimal control model, which was used
to develop an optimal schedule for adding funds to the payment terminal. It is
proved that the efficiency and reliability of the proposed model is higher than
that of the classical Baumol-Tobin inventory management model: when tested on
the time series of three ATMs, the discrete optimal control model did not allow
exhaustion of funds and allowed to earn on average 30% more than the classical
model. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09927v1' target="_blank">http://arxiv.org/pdf/2210.09927v1</a><br> <br> <br> <font size='5'> 506 </font> <div style="text-align: right"> 2022-10-18 14:20:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is Dogecoin a Viable Investment? Insights from Network and Bubble Effects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We find that three factors: Dogecoin network externalities, momentum, and
tweet sentiment that capture the time-series expected Dogecoin returns.
Dogecoin returns are exposed to Dogecoin network factors. We construct the
network factors to capture the user adoption of Dogecoin. Moreover, there is a
strong time-series momentum effect, and proxies for investor attention strongly
forecast future Dogecoin returns. Lastly, we examine potential underlying
mechanisms of the Dogecoin price bubble. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09884v2' target="_blank">http://arxiv.org/pdf/2210.09884v2</a><br> <br> <br> <font size='5'> 507 </font> <div style="text-align: right"> 2022-10-18 13:11:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Deep Learning to Find the Next Unicorn: A Practical Synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Startups often represent newly established business models associated with
disruptive innovation and high scalability. They are commonly regarded as
powerful engines for economic and social development. Meanwhile, startups are
heavily constrained by many factors such as limited financial funding and human
resources. Therefore the chance for a startup to eventually succeed is as rare
as ``spotting a unicorn in the wild''. Venture Capital (VC) strives to identify
and invest in unicorn startups during their early stages, hoping to gain a high
return. To avoid entirely relying on human domain expertise and intuition,
investors usually employ data-driven approaches to forecast the success
probability of startups. Over the past two decades, the industry has gone
through a paradigm shift moving from conventional statistical approaches
towards becoming machine-learning (ML) based. Notably, the rapid growth of data
volume and variety is quickly ushering in deep learning (DL), a subset of ML,
as a potentially superior approach in terms capacity and expressivity. In this
work, we carry out a literature review and synthesis on DL-based approaches,
covering the entire DL life cycle. The objective is a) to obtain a thorough and
in-depth understanding of the methodologies for startup evaluation using DL,
and b) to distil valuable and actionable learning for practitioners. To the
best of our knowledge, our work is the first of this kind. </font><br> Link: <a href='http://arxiv.org/pdf/2210.14195v1' target="_blank">http://arxiv.org/pdf/2210.14195v1</a><br> <br> <br> <font size='5'> 508 </font> <div style="text-align: right"> 2022-10-18 06:25:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sector-wise analysis of Indian stock market: Long and short-term risk and stability analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper, for the first time, focuses on the sector-wise analysis of a
stock market through multifractal analysis. We have considered Bombay Stock
Exchange, India, and identified two time scales, short ($<200$ days) and long
time-scale ($>200$ days) for investment. We infer that long-term investment
will be more profitable. For long time scale, sectors can be separated into two
categories based on the Hurst exponent values; one corresponds to stable
sectors with small fluctuations, and the other with dominance of large
fluctuations leading to possible downturns in those sectors. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09619v1' target="_blank">http://arxiv.org/pdf/2210.09619v1</a><br> <br> <br> <font size='5'> 509 </font> <div style="text-align: right"> 2022-10-17 20:43:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Party On: The Labor Market Returns to Social Networks in Adolescence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the returns to adolescent friendships on earnings in
adulthood. Using data from the National Longitudinal Study of Adolescent to
Adult Health, we document that individuals make investments to accumulate
friends in addition to educational investments. Because both education and
friendships are jointly determined in adolescence, OLS estimates of their
returns are biased. To estimate the causal returns to friendships, we implement
a novel procedure that assumes the returns to schooling range from 5 to 15% (as
the literature has documented), and instrument for friendships using homophily
(similarity) measures among peers to obtain bounds on the returns to
friendships. We find that having one more friend in adolescence increases
earnings between 7 and 14%, which is substantially larger than the OLS
estimates: measurement error and omitted variables lead to significant downward
bias. We also find suggestive evidence that although some friendships generate
larger returns in the labor market (close friendships, friendships with high
SES individuals), most friendships appear to have positive returns. Individuals
with more friendships in adolescence obtain more schooling and become more
social and connected adults, in turn making them more likely to work and to
work in higher paying occupations where social skills are rewarded. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09426v2' target="_blank">http://arxiv.org/pdf/2210.09426v2</a><br> <br> <br> <font size='5'> 510 </font> <div style="text-align: right"> 2022-10-17 17:55:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The impact of big winners on passive and active equity investment strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the impact of big winner stocks on the performance of active
and passive investment strategies using a combination of numerical and
analytical techniques. Our analysis is based on historical stock price data
from 2006 to 2021 for a large variety of global indexes. We show that the
log-normal distribution provides a reasonable fit for total returns for the
majority of world stock indexes but highlight the limitations of this model.
Using an analytical expression for a finite sum of log-normal random variables,
we show that the typical return of a small portfolio is smaller than that of an
equally weighted index. This finding indicates that active managers face a
significant risk of underperforming due to the potential for missing out on the
substantial returns generated by big winner stocks. Our results suggest that
passive investing strategies, that do not involve the selection of individual
stocks, are likely to be more effective in achieving long-term financial goals. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09302v4' target="_blank">http://arxiv.org/pdf/2210.09302v4</a><br> <br> <br> <font size='5'> 511 </font> <div style="text-align: right"> 2022-10-16 03:04:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Duality in optimal consumption--investment problems with alternative data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we investigate an optimal consumption--investment problem in
which the unobserved stock trend is modulated by a hidden Markov chain,
representing different economic regimes. In the classical approach, the hidden
state is estimated from historical asset prices, but recent technology enables
investors to consider alternative data when making decisions. These include
social media commentary, expert opinions, pandemic data, and GPS data, which
originate outside of the standard repertoire of market data but are considered
useful for predicting stock trends. We model the asset price and alternative
data series as a diffusion process and a jump-diffusion process, respectively.
By incorporating the alternative data into the filtering process, we extend the
Wonham filter to a degenerate jump diffusion with L\'{e}vy-type jumps. This
introduces major analytical challenge to the corresponding stochastic control
problem. We address this by taking a novel duality approach. We link the dual
problem to an optimization problem over a set of equivalent local martingale
measures and devise a methodology to obtain the optimal solution using a
filtering technique with the alternative data. We show that the dual problem
provides a unique smooth solution for constant relative risk aversion (CRRA)
utility functions. In addition, we obtain an explicit feedback optimal
consumption--investment strategy through the advantages provided by the
alternative data. </font><br> Link: <a href='http://arxiv.org/pdf/2210.08422v1' target="_blank">http://arxiv.org/pdf/2210.08422v1</a><br> <br> <br> <font size='5'> 512 </font> <div style="text-align: right"> 2022-10-15 17:18:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Neuroscience has long been an essential driver of progress in artificial
intelligence (AI). We propose that to accelerate progress in AI, we must invest
in fundamental research in NeuroAI. A core component of this is the embodied
Turing test, which challenges AI animal models to interact with the
sensorimotor world at skill levels akin to their living counterparts. The
embodied Turing test shifts the focus from those capabilities like game playing
and language that are especially well-developed or uniquely human to those
capabilities, inherited from over 500 million years of evolution, that are
shared with all animals. Building models that can pass the embodied Turing test
will provide a roadmap for the next generation of AI. </font><br> Link: <a href='http://arxiv.org/pdf/2210.08340v3' target="_blank">http://arxiv.org/pdf/2210.08340v3</a><br> <br> <br> <font size='5'> 513 </font> <div style="text-align: right"> 2022-10-14 22:53:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pseudo AI Bias</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Pseudo Artificial Intelligence bias (PAIB) is broadly disseminated in the
literature, which can result in unnecessary AI fear in society, exacerbate the
enduring inequities and disparities in access to and sharing the benefits of AI
applications, and waste social capital invested in AI research. This study
systematically reviews publications in the literature to present three types of
PAIBs identified due to: a) misunderstandings, b) pseudo mechanical bias, and
c) over-expectations. We discussed the consequences of and solutions to PAIBs,
including certifying users for AI applications to mitigate AI fears, providing
customized user guidance for AI applications, and developing systematic
approaches to monitor bias. We concluded that PAIB due to misunderstandings,
pseudo mechanical bias, and over-expectations of algorithmic predictions is
socially harmful. </font><br> Link: <a href='http://arxiv.org/pdf/2210.08141v2' target="_blank">http://arxiv.org/pdf/2210.08141v2</a><br> <br> <br> <font size='5'> 514 </font> <div style="text-align: right"> 2022-10-14 16:58:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Perspectives for self-driving labs in synthetic biology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Self-driving labs (SDLs) combine fully automated experiments with artificial
intelligence (AI) that decides the next set of experiments. Taken to their
ultimate expression, SDLs could usher a new paradigm of scientific research,
where the world is probed, interpreted, and explained by machines for human
benefit. While there are functioning SDLs in the fields of chemistry and
materials science, we contend that synthetic biology provides a unique
opportunity since the genome provides a single target for affecting the
incredibly wide repertoire of biological cell behavior. However, the level of
investment required for the creation of biological SDLs is only warranted if
directed towards solving difficult and enabling biological questions. Here, we
discuss challenges and opportunities in creating SDLs for synthetic biology. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09085v2' target="_blank">http://arxiv.org/pdf/2210.09085v2</a><br> <br> <br> <font size='5'> 515 </font> <div style="text-align: right"> 2022-10-14 13:50:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TechRank</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce TechRank, a recursive algorithm based on a bi-partite graph with
weighted nodes. We develop TechRank to link companies and technologies based on
the method of reflection. We allow the algorithm to incorporate exogenous
variables that reflect an investor's preferences. We calibrate the algorithm in
the cybersecurity sector. First, our results help estimate each entity's
influence and explain companies' and technologies' ranking. Second, they
provide investors with a quantitative optimal ranking of technologies and thus,
help them design their optimal portfolio. We propose this method as an
alternative to traditional portfolio management and, in the case of private
equity investments, as a new way to price assets for which cash flows are not
observable. </font><br> Link: <a href='http://arxiv.org/pdf/2210.07824v1' target="_blank">http://arxiv.org/pdf/2210.07824v1</a><br> <br> <br> <font size='5'> 516 </font> <div style="text-align: right"> 2022-10-14 06:44:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey of Parameters Associated with the Quality of Benchmarks in NLP</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Several benchmarks have been built with heavy investment in resources to
track our progress in NLP. Thousands of papers published in response to those
benchmarks have competed to top leaderboards, with models often surpassing
human performance. However, recent studies have shown that models triumph over
several popular benchmarks just by overfitting on spurious biases, without
truly learning the desired task. Despite this finding, benchmarking, while
trying to tackle bias, still relies on workarounds, which do not fully utilize
the resources invested in benchmark creation, due to the discarding of low
quality data, and cover limited sets of bias. A potential solution to these
issues -- a metric quantifying quality -- remains underexplored. Inspired by
successful quality indices in several domains such as power, food, and water,
we take the first step towards a metric by identifying certain language
properties that can represent various possible interactions leading to biases
in a benchmark. We look for bias related parameters which can potentially help
pave our way towards the metric. We survey existing works and identify
parameters capturing various properties of bias, their origins, types and
impact on performance, generalization, and robustness. Our analysis spans over
datasets and a hierarchy of tasks ranging from NLI to Summarization, ensuring
that our parameters are generic and are not overfitted towards a specific task
or dataset. We also develop certain parameters in this process. </font><br> Link: <a href='http://arxiv.org/pdf/2210.07566v1' target="_blank">http://arxiv.org/pdf/2210.07566v1</a><br> <br> <br> <font size='5'> 517 </font> <div style="text-align: right"> 2022-10-13 22:20:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Non-fungible token transactions: data and challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Non-fungible tokens (NFT) have recently emerged as a novel blockchain hosted
financial asset class that has attracted major transaction volumes. Investment
decisions rely on data and adequate preprocessing and application of analytics
to them. Both owing to the non-fungible nature of the tokens and to a
blockchain being the primary data source, NFT transaction data pose several
challenges not commonly encountered in traditional financial data. Using data
that consist of the transaction history of eight highly valued NFT collections,
a selection of such challenges is illustrated. These are: price differentiation
by token traits, the possible existence of lateral swaps and wash trades in the
transaction history and finally, severe volatility. While this paper merely
scratches the surface of how data analytics can be applied in this context, the
data and challenges laid out here may present opportunities for future research
on the topic. </font><br> Link: <a href='http://arxiv.org/pdf/2210.07393v1' target="_blank">http://arxiv.org/pdf/2210.07393v1</a><br> <br> <br> <font size='5'> 518 </font> <div style="text-align: right"> 2022-10-13 16:41:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: KiDS-Legacy calibration: unifying shear and redshift calibration with the SKiLLS multi-band image simulations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present SKiLLS, a suite of multi-band image simulations for the weak
lensing analysis of the complete Kilo-Degree Survey (KiDS), dubbed KiDS-Legacy
analysis. The resulting catalogues enable joint shear and redshift calibration,
enhancing the realism and hence accuracy over previous efforts. To create a
large volume of simulated galaxies with faithful properties and to a sufficient
depth, we integrated cosmological simulations with high-quality imaging
observations. We also improved the realism of simulated images by allowing the
point spread function (PSF) to differ between CCD images, including stellar
density variations and varying noise levels between pointings. Using realistic
variable shear fields, we accounted for the impact of blended systems at
different redshifts. Although the overall correction is minor, we found a clear
redshift-bias correlation in the blending-only variable shear simulations,
indicating the non-trivial impact of this higher-order blending effect. We also
explored the impact of the PSF modelling errors and found a small yet
noticeable effect on the shear bias. Finally, we conducted a series of
sensitivity tests, including changing the input galaxy properties. We conclude
that our fiducial shape measurement algorithm, lensfit, is robust within the
requirements of lensing analyses with KiDS. As for future weak lensing surveys
with tighter requirements, we suggest further investments in understanding the
impact of blends at different redshifts, improving the PSF modelling algorithm
and developing the shape measurement method to be less sensitive to the galaxy
properties. </font><br> Link: <a href='http://arxiv.org/pdf/2210.07163v2' target="_blank">http://arxiv.org/pdf/2210.07163v2</a><br> <br> <br> <font size='5'> 519 </font> <div style="text-align: right"> 2022-10-13 08:23:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ensemble Creation via Anchored Regularization for Unsupervised Aspect Extraction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Aspect Based Sentiment Analysis is the most granular form of sentiment
analysis that can be performed on the documents / sentences. Besides delivering
the most insights at a finer grain, it also poses equally daunting challenges.
One of them being the shortage of labelled data. To bring in value right out of
the box for the text data being generated at a very fast pace in today's world,
unsupervised aspect-based sentiment analysis allows us to generate insights
without investing time or money in generating labels. From topic modelling
approaches to recent deep learning-based aspect extraction models, this domain
has seen a lot of development. One of the models that we improve upon is ABAE
that reconstructs the sentences as a linear combination of aspect terms present
in it, In this research we explore how we can use information from another
unsupervised model to regularize ABAE, leading to better performance. We
contrast it with baseline rule based ensemble and show that the ensemble
methods work better than the individual models and the regularization based
ensemble performs better than the rule-based one. </font><br> Link: <a href='http://arxiv.org/pdf/2210.06829v1' target="_blank">http://arxiv.org/pdf/2210.06829v1</a><br> <br> <br> <font size='5'> 520 </font> <div style="text-align: right"> 2022-10-12 15:47:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Betting the system: Using lineups to predict football scores</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper aims to reduce randomness in football by analysing the role of
lineups in final scores using machine learning prediction models we have
developed. Football clubs invest millions of dollars on lineups and knowing how
individual statistics translate to better outcomes can optimise investments.
Moreover, sports betting is growing exponentially and being able to predict the
future is profitable and desirable. We use machine learning models and
historical player data from English Premier League (2020-2022) to predict
scores and to understand how individual performance can improve the outcome of
a match. We compared different prediction techniques to maximise the
possibility of finding useful models. We created heuristic and machine learning
models predicting football scores to compare different techniques. We used
different sets of features and shown goalkeepers stats are more important than
attackers stats to predict goals scored. We applied a broad evaluation process
to assess the efficacy of the models in real world applications. We managed to
predict correctly all relegated teams after forecast 100 consecutive matches.
We show that Support Vector Regression outperformed other techniques predicting
final scores and that lineups do not improve predictions. Finally, our model
was profitable (42% return) when emulating a betting system using real world
odds data. </font><br> Link: <a href='http://arxiv.org/pdf/2210.06327v3' target="_blank">http://arxiv.org/pdf/2210.06327v3</a><br> <br> <br> <font size='5'> 521 </font> <div style="text-align: right"> 2022-10-12 12:47:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-Knowledge Optimal Monetary Policy under Stochastic Dominance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Optimal simple rules for the monetary policy of the first stochastically
dominant crypto-currency are derived in a Dynamic Stochastic General
Equilibrium (DSGE) model, in order to provide optimal responses to changes in
inflation, output, and other sources of uncertainty. The optimal monetary
policy stochastically dominates all the previous crypto-currencies, thus the
efficient portfolio is to go long on the stochastically dominant
crypto-currency: a strategy-proof arbitrage featuring a higher Omega ratio with
higher expected returns, inducing an investment-efficient Nash equilibrium over
the crypto-market. Zero-knowledge proofs of the monetary policy are committed
on the blockchain: an implementation is provided. </font><br> Link: <a href='http://arxiv.org/pdf/2210.06139v1' target="_blank">http://arxiv.org/pdf/2210.06139v1</a><br> <br> <br> <font size='5'> 522 </font> <div style="text-align: right"> 2022-10-12 12:34:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detection of fraudulent financial papers by picking a collection of characteristics using optimization algorithms and classification techniques based on squirrels</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: To produce important investment decisions, investors require financial
records and economic information. However, most companies manipulate investors
and financial institutions by inflating their financial statements. Fraudulent
Financial Activities exist in any monetary or financial transaction scenario,
whether physical or electronic. A challenging problem that arises in this
domain is the issue that affects and troubles individuals and institutions.
This problem has attracted more attention in the field in part owing to the
prevalence of financial fraud and the paucity of previous research. For this
purpose, in this study, the main approach to solve this problem, an anomaly
detection-based approach based on a combination of feature selection based on
squirrel optimization pattern and classification methods have been used. The
aim is to develop this method to provide a model for detecting anomalies in
financial statements using a combination of selected features with the nearest
neighbor classifications, neural networks, support vector machine, and
Bayesian. Anomaly samples are then analyzed and compared to recommended
techniques using assessment criteria. Squirrel optimization's meta-exploratory
capability, along with the approach's ability to identify abnormalities in
financial data, has been shown to be effective in implementing the suggested
strategy. They discovered fake financial statements because of their expertise. </font><br> Link: <a href='http://arxiv.org/pdf/2211.07747v1' target="_blank">http://arxiv.org/pdf/2211.07747v1</a><br> <br> <br> <font size='5'> 523 </font> <div style="text-align: right"> 2022-10-12 11:54:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Auto-bidding Equilibrium in ROI-Constrained Online Advertising Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Most of the work in auction design literature assumes that bidders behave
rationally based on the information available for each individual auction.
However, in today's online advertising markets, one of the most important
real-life applications of auction design, the data and computational power
required to bid optimally are only available to the auction designer, and an
advertiser can only participate by setting performance objectives (clicks,
conversions, etc.) for the campaign.
  In this paper, we focus on value-maximizing campaigns with
return-on-investment (ROI) constraints, which is widely adopted in many
global-scale auto-bidding platforms. Through theoretical analysis and empirical
experiments on both synthetic and realistic data, we find that second price
auction exhibits many undesirable properties and loses its dominant theoretical
advantages in single-item scenarios. In particular, second price auction brings
equilibrium multiplicity, non-monotonicity, vulnerability to exploitation by
both bidders and even auctioneers, and PPAD-hardness for the system to reach a
steady-state. We also explore the broader impacts of the auto-bidding mechanism
beyond efficiency and strategyproofness. In particular, the multiplicity of
equilibria and the input sensitivity make advertisers' utilities unstable. In
addition, the interference among both bidders and advertising slots introduces
bias into A/B testing, which hinders the development of even non-bidding
components of the platform. The aforementioned phenomena have been widely
observed in practice, and our results indicate that one of the reasons might be
intrinsic to the underlying auto-bidding mechanism. To deal with these
challenges, we provide suggestions and candidate solutions for practitioners. </font><br> Link: <a href='http://arxiv.org/pdf/2210.06107v4' target="_blank">http://arxiv.org/pdf/2210.06107v4</a><br> <br> <br> <font size='5'> 524 </font> <div style="text-align: right"> 2022-10-12 05:25:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Nonlocal Reconfigurable Intelligent Surfaces for Wireless Communication: Modeling and Physical Layer Aspects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Delivering wireless ultrahigh-speed access at wider coverage is becoming
considerably challenging due to the prohibitive investment costs per user and
the necessary shift to range-limited millimeter-wave (mmWave) transmissions.
Reconfigurable intelligent surfaces (RIS) are expected to extend the reach of
mmWave and TeraHz signals more cost-effectively in situations where fiber
backhaul and fronthaul are not accessible or infrastructure densification is
costly. This paper investigates some challenges facing this technology,
particularly in terms of scalability and the question of what type of RIS
configurations would be appropriate for mmWave networks and what design
strategies can be adopted to optimize the performance and minimize the
signaling overhead. We conclude that RIS configurations for the wireless
infrastructure likely need to be nonlocal (i.e., redirective,
wavefront-selective) rather than local (i.e., reflective) to support
communications and networking tasks such as integrated fronthaul and access
(IFA) most efficiently. </font><br> Link: <a href='http://arxiv.org/pdf/2210.05928v1' target="_blank">http://arxiv.org/pdf/2210.05928v1</a><br> <br> <br> <font size='5'> 525 </font> <div style="text-align: right"> 2022-10-11 19:29:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Design of Novel Optical Cavities for Strong Shock Compression</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Laser-induced strong shock waves with high efficiency remain a technical
challenge, evidenced by the effort invested at large-scale national
laboratories to optimize the laser-induced shock compression of pellets of
light elements. Here, we present theoretical work on designing novel optical
cavities for strong shock generation at a tabletop scale. The key idea is to
utilize multiple laser pulses spatially and temporally shaped to form
concentric laser rings on condensed matter samples. Each laser ring launches a
2D focusing pressure wave that converges at a common central point. The pulses
are delayed in the nanosecond range and spaced by microns, matching the laser
scanning speed to the shock wave speed, typically several \textmu m/ns in
condensed matter, allowing amplification of the pressure waves through
superposition. The herein-described optical cavities are expected to maintain
or even exceed the shock excitation efficiency of $10^4$ GPa/J reported in our
previous work using a single laser ring, where the limiting factor in
generating stronger shocks was the saturation of input laser energy. The
current design for multiple laser rings bypasses the saturation due to much
larger excitation areas; thus, the total input laser energy can be dramatically
increased. We further experimentally show that dielectric metasurfaces can
combine all optics for forming the required clean high-fluence laser rings in a
single optics. Our work provides a viable pathway toward applying laser-induced
strong shock compression of condensed matter. The current tabletop scheme
caters to the need of the shock compression community by providing the
flexibility to test new shock compression strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2210.05750v2' target="_blank">http://arxiv.org/pdf/2210.05750v2</a><br> <br> <br> <font size='5'> 526 </font> <div style="text-align: right"> 2022-10-08 07:18:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Design and Analysis of Optimized Portfolios for Selected Sectors of the Indian Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Portfolio optimization is a challenging problem that has attracted
considerable attention and effort from researchers. The optimization of stock
portfolios is a particularly hard problem since the stock prices are volatile
and estimation of their future volatilities and values, in most cases, is very
difficult, if not impossible. This work uses three ratios, the Sharpe ratio,
the Sortino ratio, and the Calmar ratio, for designing the mean-variance
optimized portfolios for six important sectors listed in the National Stock
Exchange (NSE) of India. Three portfolios are designed for each sector
maximizing the ratios based on the historical prices of the ten most important
stocks of each sector from Jan 1, 2017, to Dec 31, 2020. The evaluation of the
portfolios is done based on their cumulative returns over the test period from
Jan 1, 2021, to Dec 31, 2021. The ratio that yields the maximum cumulative
returns for both the training and the test periods for the majority of the
sectors is identified. The sectors that exhibit the maximum cumulative returns
for the same ratio are also identified. The results provide useful insights for
investors in the stock market in making their investment decisions based on the
current return and risks associated with the six sectors and their stocks. </font><br> Link: <a href='http://arxiv.org/pdf/2210.03943v1' target="_blank">http://arxiv.org/pdf/2210.03943v1</a><br> <br> <br> <font size='5'> 527 </font> <div style="text-align: right"> 2022-10-08 04:18:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Higher Purpose: Measuring Electricity Access Using High-Resolution Daytime Satellite Imagery</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Governments and international organizations the world over are investing
towards the goal of achieving universal energy access for improving
socio-economic development. However, in developing settings, monitoring
electrification efforts is typically inaccurate, infrequent, and expensive. In
this work, we develop and present techniques for high-resolution monitoring of
electrification progress at scale. Specifically, our 3 unique contributions
are: (i) identifying areas with(out) electricity access, (ii) quantifying the
extent of electrification in electrified areas (percentage/number of
electrified structures), and (iii) differentiating between customer types in
electrified regions (estimating the percentage/number of
residential/non-residential electrified structures). We combine high-resolution
50 cm daytime satellite images with Convolutional Neural Networks (CNNs) to
train a series of classification and regression models. We evaluate our models
using unique ground truth datasets on building locations, building types
(residential/non-residential), and building electrification status. Our
classification models show a 92% accuracy in identifying electrified regions,
85% accuracy in estimating percent of (low/high) electrified buildings within
the region, and 69% accuracy in differentiating between (low/high) percentage
of electrified residential buildings. Our regressions show $R^2$ scores of 78%
and 80% in estimating the number of electrified buildings and number of
residential electrified building in images respectively. We also demonstrate
the generalizability of our models in never-before-seen regions to assess their
potential for consistent and high-resolution measurements of electrification in
emerging economies, and conclude by highlighting opportunities for improvement. </font><br> Link: <a href='http://arxiv.org/pdf/2210.03909v1' target="_blank">http://arxiv.org/pdf/2210.03909v1</a><br> <br> <br> <font size='5'> 528 </font> <div style="text-align: right"> 2022-10-07 13:40:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Energy transition under scenario uncertainty: a mean-field game approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the impact of transition scenario uncertainty, and in particular,
the uncertainty about future carbon price and electricity demand, on the pace
of decarbonization of the electricity industry. To this end, we build a
discrete time mean-field game model for the long-term dynamics of the
electricity market subject to common random shocks affecting the carbon price
and the electricity demand. These shocks depend on a macroeconomic scenario,
which is not observed by the agents, but can be partially deduced from the
frequency of the shocks. Due to this partial observation feature, the common
noise is non-Markovian. We consider two classes of agents: conventional
producers and renewable producers. The former choose an optimal moment to exit
the market and the latter choose an optimal moment to enter the market by
investing into renewable generation. The agents interact through the market
price determined by a merit order mechanism with an exogenous stochastic
demand. We prove the existence of Nash equilibria in the resulting mean-field
game of optimal stopping with common noise, developing a novel linear
programming approach for these problems. We illustrate our model by an example
inspired by the UK electricity market, and show that scenario uncertainty leads
to significant changes in the speed of replacement of conventional generators
by renewable production. </font><br> Link: <a href='http://arxiv.org/pdf/2210.03554v1' target="_blank">http://arxiv.org/pdf/2210.03554v1</a><br> <br> <br> <font size='5'> 529 </font> <div style="text-align: right"> 2022-10-06 20:57:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-Supervised Monocular Depth Underwater</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Depth estimation is critical for any robotic system. In the past years
estimation of depth from monocular images have shown great improvement,
however, in the underwater environment results are still lagging behind due to
appearance changes caused by the medium. So far little effort has been invested
on overcoming this. Moreover, underwater, there are more limitations for using
high resolution depth sensors, this makes generating ground truth for learning
methods another enormous obstacle. So far unsupervised methods that tried to
solve this have achieved very limited success as they relied on domain transfer
from dataset in air. We suggest training using subsequent frames
self-supervised by a reprojection loss, as was demonstrated successfully above
water. We suggest several additions to the self-supervised framework to cope
with the underwater environment and achieve state-of-the-art results on a
challenging forward-looking underwater dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2210.03206v1' target="_blank">http://arxiv.org/pdf/2210.03206v1</a><br> <br> <br> <font size='5'> 530 </font> <div style="text-align: right"> 2022-10-06 08:01:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Green transition as a driver of technical efficiency. An empirical study on Italian airports</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The transition to more environmentally sustainable production processes and
managerial practices is an increasingly important topic. Many industries need
to undergo radical change to meet environmental sustainability requirements;
the tourism industry is no exception. In this respect, a particular aspect that
needs further attention is the relationship between airport performances and
investments in environmental sustainability policies. This work represents a
first attempt to provide empirical evidences about this relationship. Through
the application of a non-parametrical method, we first assess the efficiency of
the Italian airports industry. Secondly, we investigated the relationship
between airports performance and management commitment toward the ecological
transition using a Tobit regression model. The results show that airports'
adherence to formal multi-year ecological transition programs has a positive
and consistent impact on their performance. </font><br> Link: <a href='http://arxiv.org/pdf/2210.02736v1' target="_blank">http://arxiv.org/pdf/2210.02736v1</a><br> <br> <br> <font size='5'> 531 </font> <div style="text-align: right"> 2022-10-05 22:40:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TF07 Snowmass Report: Theory of Collider Phenomena</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Theoretical research has long played an essential role in interpreting data
from high-energy particle colliders and motivating new accelerators to advance
the energy and precision frontiers. Collider phenomenology is an essential
interface between theoretical models and experimental observations, since
theoretical studies inspire experimental analyses while experimental results
sharpen theoretical ideas. This report -- from the Snowmass 2021 Theory
Frontier topical group for Collider Phenomenology (TF07) -- showcases the
dynamism, engagement, and motivations of collider phenomenologists by exposing
selected exciting new directions and establishing key connections between
cutting-edge theoretical advances and current and future experimental
opportunities. By investing in collider phenomenology, the high-energy physics
community can help ensure that theoretical advances are translated into
concrete tools that enable and enhance current and future experiments, and in
turn, experimental results feed into a more complete theoretical understanding
and motivate new questions and explorations. </font><br> Link: <a href='http://arxiv.org/pdf/2210.02591v2' target="_blank">http://arxiv.org/pdf/2210.02591v2</a><br> <br> <br> <font size='5'> 532 </font> <div style="text-align: right"> 2022-10-05 13:26:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sincov's and other functional equations and negative interest rates</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investigating the future value $F(K,s,t)$ of a capital $K$ invested at date
$S$ at date $t$ the "natural" condition $F(K,s,t)\geq K$ has lost its
naturality because of the strange fact of negative interest rates. This leads
to the task of describing the possible solutions of the multiplicative Sincov
equation $f(s,u)=f(s,t)f(t,u)$ for $s\leq t\leq u$, where $f(s,t)=0$ may
happen. In this paper we solve this task and discuss connections to the theory
of investments. </font><br> Link: <a href='http://arxiv.org/pdf/2210.02250v2' target="_blank">http://arxiv.org/pdf/2210.02250v2</a><br> <br> <br> <font size='5'> 533 </font> <div style="text-align: right"> 2022-10-04 20:46:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: When would online platforms pay data dividends</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Online platforms, including social media and search platforms, have routinely
used their users' data for targeted ads, to improve their services, and to sell
to third-party buyers. But an increasing awareness of the importance of users'
data privacy has led to new laws that regulate data-sharing by platforms.
Further, there have been political discussions on introducing data dividends,
that is paying users for their data. Three interesting questions are then: When
would these online platforms be incentivized to pay data dividends? How does
their decision depend on whether users value their privacy more than the
platform's free services? And should platforms invest in protecting users'
data? This paper considers various factors affecting the users' and platform's
decisions through utility functions. We construct a principal-agent model using
a Stackelberg game to calculate their optimal decisions and qualitatively
discuss the implications. Our results could inform a policymaker trying to
understand the consequences of mandating data dividends. </font><br> Link: <a href='http://arxiv.org/pdf/2210.01900v1' target="_blank">http://arxiv.org/pdf/2210.01900v1</a><br> <br> <br> <font size='5'> 534 </font> <div style="text-align: right"> 2022-10-03 15:30:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Smoothness of the Value Function for Optimal Consumption Model with Consumption-Wealth Utility and Borrowing Constraint</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper considers an optimal consumption-investment problem for an
investor whose instantaneous utility depends on both consumption and wealth.
The investor faces a constraint that the investment amount in the risky asset
does not exceed an exogenous function of the wealth. We prove that the value
function is second-order smooth, and the optimal consumption-investment policy
is provided in a feedback form. Moreover, when the risky investment amount is
bounded by a fixed constant, we show that under certain conditions, the
constraint is binding if and only if an endogenous threshold bounds the
portfolio wealth. Our results encompass many well-developed portfolio choice
models and imply new applications. </font><br> Link: <a href='http://arxiv.org/pdf/2210.01016v2' target="_blank">http://arxiv.org/pdf/2210.01016v2</a><br> <br> <br> <font size='5'> 535 </font> <div style="text-align: right"> 2022-10-03 14:53:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Data Budgeting for Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Data is the fuel powering AI and creates tremendous value for many domains.
However, collecting datasets for AI is a time-consuming, expensive, and
complicated endeavor. For practitioners, data investment remains to be a leap
of faith in practice. In this work, we study the data budgeting problem and
formulate it as two sub-problems: predicting (1) what is the saturating
performance if given enough data, and (2) how many data points are needed to
reach near the saturating performance. Different from traditional
dataset-independent methods like PowerLaw, we proposed a learning method to
solve data budgeting problems. To support and systematically evaluate the
learning-based method for data budgeting, we curate a large collection of 383
tabular ML datasets, along with their data vs performance curves. Our empirical
evaluation shows that it is possible to perform data budgeting given a small
pilot study dataset with as few as $50$ data points. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00987v1' target="_blank">http://arxiv.org/pdf/2210.00987v1</a><br> <br> <br> <font size='5'> 536 </font> <div style="text-align: right"> 2022-10-03 14:07:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal consumption-investment choices under wealth-driven risk aversion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: CRRA utility where the risk aversion coefficient is a constant is commonly
seen in various economics models. But wealth-driven risk aversion rarely shows
up in investor's investment problems. This paper mainly focus on numerical
solutions to the optimal consumption-investment choices under wealth-driven
aversion done by neural network. A jump-diffusion model is used to simulate the
artificial data that is needed for the neural network training. The WDRA Model
is set up for describing the investment problem and there are two parameters
that require to be optimized, which are the investment rate of the wealth on
the risky assets and the consumption during the investment time horizon. Under
this model, neural network LSTM with one objective function is implemented and
shows promising results. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00950v1' target="_blank">http://arxiv.org/pdf/2210.00950v1</a><br> <br> <br> <font size='5'> 537 </font> <div style="text-align: right"> 2022-10-03 06:24:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sentiment Analysis of ESG disclosures on Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we look at the impact of Environment, Social and Governance
related news articles and social media data on the stock market performance. We
pick four stocks of companies which are widely known in their domain to
understand the complete effect of ESG as the newly opted investment style
remains restricted to only the stocks with widespread information. We summarise
live data of both twitter tweets and newspaper articles and create a sentiment
index using a dictionary technique based on online information for the month of
July, 2022. We look at the stock price data for all the four companies and
calculate the percentage change in each of them. We also compare the overall
sentiment of the company to its percentage change over a specific historical
period. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00731v1' target="_blank">http://arxiv.org/pdf/2210.00731v1</a><br> <br> <br> <font size='5'> 538 </font> <div style="text-align: right"> 2022-10-02 08:45:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large-Scale Allocation of Personalized Incentives</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider a regulator willing to drive individual choices towards
increasing social welfare by providing incentives to a large population of
individuals.
  For that purpose, we formalize and solve the problem of finding an optimal
personalized-incentive policy: optimal in the sense that it maximizes social
welfare under an incentive budget constraint, personalized in the sense that
the incentives proposed depend on the alternatives available to each
individual, as well as her preferences.
  We propose a polynomial time approximation algorithm that computes a policy
within few seconds and we analytically prove that it is boundedly close to the
optimum.
  We then extend the problem to efficiently calculate the Maximum Social
Welfare Curve, which gives the maximum social welfare achievable for a range of
incentive budgets (not just one value).
  This curve is a valuable practical tool for the regulator to determine the
right incentive budget to invest.
  Finally, we simulate a large-scale application to mode choice in a French
department (about 200 thousands individuals) and illustrate the effectiveness
of the proposed personalized-incentive policy in reducing CO2 emissions. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00463v1' target="_blank">http://arxiv.org/pdf/2210.00463v1</a><br> <br> <br> <font size='5'> 539 </font> <div style="text-align: right"> 2022-10-02 07:43:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Citation Trajectory Prediction via Publication Influence Representation Using Temporal Knowledge Graph</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Predicting the impact of publications in science and technology has become an
important research area, which is useful in various real world scenarios such
as technology investment, research direction selection, and technology
policymaking. Citation trajectory prediction is one of the most popular tasks
in this area. Existing approaches mainly rely on mining temporal and graph data
from academic articles. Some recent methods are capable of handling cold-start
prediction by aggregating metadata features of new publications. However, the
implicit factors causing citations and the richer information from handling
temporal and attribute features still need to be explored. In this paper, we
propose CTPIR, a new citation trajectory prediction framework that is able to
represent the influence (the momentum of citation) of either new or existing
publications using the history information of all their attributes. Our
framework is composed of three modules: difference-preserved graph embedding,
fine-grained influence representation, and learning-based trajectory
calculation. To test the effectiveness of our framework in more situations, we
collect and construct a new temporal knowledge graph dataset from the real
world, named AIPatent, which stems from global patents in the field of
artificial intelligence. Experiments are conducted on both the APS academic
dataset and our contributed AIPatent dataset. The results demonstrate the
strengths of our approach in the citation trajectory prediction task. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00450v1' target="_blank">http://arxiv.org/pdf/2210.00450v1</a><br> <br> <br> <font size='5'> 540 </font> <div style="text-align: right"> 2022-09-30 23:58:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Coalitional Game-Theoretical Approach to Coinvestment with Application to Edge Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose in this paper a coinvestment plan between several stakeholders of
different types, namely a physical network owner, operating network nodes, e.g.
a network operator or a tower company, and a set of service providers willing
to use these resources to provide services as video streaming, augmented
reality, autonomous driving assistance, etc. One such scenario is that of
deployment of Edge Computing resources.
  Indeed, although the latter technology is ready, the high Capital Expenditure
(CAPEX) cost of such resources is the barrier to its deployment. For this
reason, a solid economical framework to guide the investment and the returns of
the stakeholders is key to solve this issue. We formalize the coinvestment
framework using coalitional game theory. We provide a solution to calculate how
to divide the profits and costs among the stakeholders, taking into account
their characteristics: traffic load, revenues, utility function. We prove that
it is always possible to form the grand coalition composed of all the
stakeholders, by showing that our game is convex. We derive the payoff of the
stakeholders using the Shapley value concept, and elaborate on some properties
of our game. We show our solution in simulation. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00145v1' target="_blank">http://arxiv.org/pdf/2210.00145v1</a><br> <br> <br> <font size='5'> 541 </font> <div style="text-align: right"> 2022-09-30 22:58:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Equity Scores for Public Transit Lines from Open-Data and Accessibility Measures</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Current transit suffers from an evident inequity: the level of service of
transit in suburbs is much less satisfying than in city centers. As a
consequence, private cars are still the dominant transportation mode for
suburban people, which results in congestion and pollution. To achieve
sustainability goals and reduce car-dependency, transit should be (re)designed
around equity. To this aim, it is necessary to (i) quantify the "level of
equity" of the transit system and (ii) provide an indicator that scores the
transit lines that contribute the most to keep transit equitable. This
indicator could suggest on which lines the transit operator must invest to
increase the service level (frequency or coverage) in order to reduce inequity
in the system.
  To the best of our knowledge, this paper is the first to tackle (ii). To this
aim, we propose efficient scoring methods that rely solely on open data, which
allows us to perform the analysis on multiple cities (7 in this paper). Our
method can be used to guide large-scale iterative optimization algorithms to
improve accessibility equity. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00128v1' target="_blank">http://arxiv.org/pdf/2210.00128v1</a><br> <br> <br> <font size='5'> 542 </font> <div style="text-align: right"> 2022-09-30 08:27:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Proportionally Fair Online Allocation of Public Goods with Predictions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We design online algorithms for the fair allocation of public goods to a set
of $N$ agents over a sequence of $T$ rounds and focus on improving their
performance using predictions. In the basic model, a public good arrives in
each round, the algorithm learns every agent's value for the good, and must
irrevocably decide the amount of investment in the good without exceeding a
total budget of $B$ across all rounds. The algorithm can utilize (potentially
inaccurate) predictions of each agent's total value for all the goods to
arrive. We measure the performance of the algorithm using a proportional
fairness objective, which informally demands that every group of agents be
rewarded in proportion to its size and the cohesiveness of its preferences.
  In the special case of binary agent preferences and a unit budget, we show
that $O(\log N)$ proportional fairness can be achieved without using any
predictions, and that this is optimal even if perfectly accurate predictions
were available. However, for general preferences and budget no algorithm can
achieve better than $\Theta(T/B)$ proportional fairness without predictions. We
show that algorithms with (reasonably accurate) predictions can do much better,
achieving $\Theta(\log (T/B))$ proportional fairness. We also extend this
result to a general model in which a batch of $L$ public goods arrive in each
round and achieve $O(\log (\min(N,L) \cdot T/B))$ proportional fairness. Our
exact bounds are parametrized as a function of the error in the predictions and
the performance degrades gracefully with increasing errors. </font><br> Link: <a href='http://arxiv.org/pdf/2209.15305v1' target="_blank">http://arxiv.org/pdf/2209.15305v1</a><br> <br> <br> <font size='5'> 543 </font> <div style="text-align: right"> 2022-09-28 22:57:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Scalable Approach to Large Scale Risk-Averse Distribution Grid Expansion Planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Distribution grid reliability and resilience has become a major topic of
concern for utilities and their regulators. In particular, with the increase in
severity of extreme events, utilities are considering major investments in
distribution grid assets to mitigate the damage of highly impactful outages.
Communicating the overall economic and risk-mitigation benefits of these
investments to regulators is an important element of the approval process.
Today, industry reliability and resilience planning practices are based largely
on methods that do not take explicit account of risk. This paper proposes a
practical method for identifying optimal combinations of investments in new
line segments and storage devices while considering the balance between the
risk associated with high impact low probability events and the reliability
related to routine failures. We show that this method can be scaled to address
large scale networks and demonstrate its benefits using a Target Feeder from
the Commonwealth Edison Reliability Program. </font><br> Link: <a href='http://arxiv.org/pdf/2209.14460v1' target="_blank">http://arxiv.org/pdf/2209.14460v1</a><br> <br> <br> <font size='5'> 544 </font> <div style="text-align: right"> 2022-09-28 21:57:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Government Spending and Money Supply Roles in Alleviating Poverty in Africa</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study examines the roles of government spending and money supply on
alleviating poverty in Africa. The study used 48 Sub-Saharan Africa countries
from 2001 to 2017. The study employed one step and two-step system GMM and
found that both the procedures have similar results. Different specifications
were employed and the model selected was robust, with valid instruments and
absence of autocorrelation at the second order. The study revealed that
government spending and foreign direct investment have significant negative
influence on reducing poverty while money supply has positive influence on the
level of poverty in the region. The implication of the finding is that monetary
policy tool of money supply has no strong influence in combating the menace of
poverty. The study therefore recommends that emphasis should be placed on
increasing more of government spending that would impact on the quality of life
of the people in the region through multiplier effect, improving the financial
system for effective monetary policy and attracting foreign direct inflows
through enabling business environment in Africa. </font><br> Link: <a href='http://arxiv.org/pdf/2209.14443v1' target="_blank">http://arxiv.org/pdf/2209.14443v1</a><br> <br> <br> <font size='5'> 545 </font> <div style="text-align: right"> 2022-09-28 18:36:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Discrete Optimal Designs for Distributed Energy Systems with Nonconvex Multiphase Optimal Power Flow</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The optimal selection, sizing, and location of small-scale technologies
within a grid-connected distributed energy system (DES) can contribute to
reducing carbon emissions, consumer costs, and network imbalances. This is the
first study to present an optimisation framework for obtaining discrete
technology sizing and selection for grid-connected DES design, while
simultaneously considering multiphase optimal power flow (MOPF) constraints to
accurately represent unbalanced low-voltage distribution networks. An algorithm
is developed to solve the resulting Mixed-Integer Nonlinear Programming (MINLP)
formulation. It employs a decomposition based on Mixed-Integer Linear
Programming (MILP) and Nonlinear Programming (NLP), and utilises integer cuts
and complementarity reformulations to obtain discrete designs that are also
feasible with respect to the network constraints. A heuristic modification to
the original algorithm is also proposed to improve computational speed.
Improved formulations for selecting feasible combinations of air source heat
pumps (ASHPs) and hot water storage tanks are also presented. The algorithms
outperform the existing state-of-the-art commercial MINLP solver, which fails
to find any solutions in two instances. While feasible solutions were obtained
for all cases, convergence was not achieved for all, especially for those
involving the larger network. Where converged, the algorithm with the heuristic
modification has achieved results up to 70% faster than the original algorithm.
Results for case studies suggest that including ASHPs can support up to 16%
higher renewable generation capacity compared to gas boilers, albeit with
higher ASHP investment costs. The optimisation framework and results can be
used to inform stakeholders such as policy-makers and network operators, to
increase renewable energy capacity and aid the decarbonisation of domestic
heating systems. </font><br> Link: <a href='http://arxiv.org/pdf/2209.14354v1' target="_blank">http://arxiv.org/pdf/2209.14354v1</a><br> <br> <br> <font size='5'> 546 </font> <div style="text-align: right"> 2022-09-28 18:09:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Establishing trust with readers is an important first step in visual data
communication. But what makes a visualization trustworthy? Psychology and
behavioral economics research has found processing fluency (i.e., speed and
accuracy of perceiving and processing a stimulus) is central to perceived
trust. We examine the association between processing fluency and trust in
visualizations through two empirical studies. In Experiment 1, we tested the
effect of camouflaging a visualization on processing fluency. Participants
estimated the proportion of data values within a specified range for six
camouflaged visualizations and one non-camouflaged control; they also reported
their perceived difficulty for each of the visualizations. Camouflaged
visualizations produced less accurate estimations compared to the control. In
Experiment 2, we created a decision task based on trust games adapted from
behavioral economics. We asked participants to invest money in two hypothetical
companies and report how much they trust each company. One company communicates
its strategy with a camouflaged visualization, the other with a controlled
visualization. Participants tended to invest less money in the company
presenting a camouflaged visualization. Hence, we found support for the
hypothesis that processing fluency is key to the perception of trust in visual
data communication. </font><br> Link: <a href='http://arxiv.org/pdf/2209.14340v1' target="_blank">http://arxiv.org/pdf/2209.14340v1</a><br> <br> <br> <font size='5'> 547 </font> <div style="text-align: right"> 2022-09-27 09:50:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding Braess' Paradox in power grids</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ongoing energy transition requires power grid extensions to connect
renewable generators to consumers and to transfer power among distant areas.
The process of grid extension requires a large investment of resources and is
supposed to make grid operation more robust. Yet, counter-intuitively,
increasing the capacity of existing lines or adding new lines may also reduce
the overall system performance and even promote blackouts due to Braess'
paradox. Braess' paradox was theoretically modeled but not yet proven in
realistically scaled power grids. Here, we present an experimental setup
demonstrating Braess' paradox in an AC power grid and show how it constrains
ongoing large-scale grid extension projects. We present a topological theory
that reveals the key mechanism and predicts Braessian grid extensions from the
network structure. These results offer a theoretical method to understand and
practical guidelines in support of preventing unsuitable infrastructures and
the systemic planning of grid extensions. </font><br> Link: <a href='http://arxiv.org/pdf/2209.13278v1' target="_blank">http://arxiv.org/pdf/2209.13278v1</a><br> <br> <br> <font size='5'> 548 </font> <div style="text-align: right"> 2022-09-27 01:29:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Discount Puzzle Of Closed-End Mutual Funds: A Case Of Bangladesh</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper intends to perform a relevant study on the closed-end fund puzzle
in the perspective of an emerging market. Quarterly data of 36 closed-end
mutual funds traded in Dhaka Stock Exchange are collected over the sample
period of 2016 to 2019. Dependent and independent variables are mapped down by
exploring previous researches. Weight of top 10 investments, fund size, fund
age, fund maturity, turnover and dividend yield are taken as explanatory
variable to analyze the impact on CEF discount. A fixed effects panel
regression is performed on the data set with few diagnostic tests to ensure the
reliability of the analysis conducted. The results show that, the variable fund
size and fund maturity have a significant positive and turnover has a
significant negative impact on CEF discount while the impact of weight of top
10 investments, dividend yield and fund age are found insignificant. </font><br> Link: <a href='http://arxiv.org/pdf/2209.13102v1' target="_blank">http://arxiv.org/pdf/2209.13102v1</a><br> <br> <br> <font size='5'> 549 </font> <div style="text-align: right"> 2022-09-26 18:52:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FaRO 2: an Open Source, Configurable Smart City Framework for Real-Time Distributed Vision and Biometric Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent global growth in the interest of smart cities has led to trillions of
dollars of investment toward research and development. These connected cities
have the potential to create a symbiosis of technology and society and
revolutionize the cost of living, safety, ecological sustainability, and
quality of life of societies on a world-wide scale. Some key components of the
smart city construct are connected smart grids, self-driving cars, federated
learning systems, smart utilities, large-scale public transit, and proactive
surveillance systems. While exciting in prospect, these technologies and their
subsequent integration cannot be attempted without addressing the potential
societal impacts of such a high degree of automation and data sharing.
Additionally, the feasibility of coordinating so many disparate tasks will
require a fast, extensible, unifying framework. To that end, we propose FaRO2,
a completely reimagined successor to FaRO1, built from the ground up. FaRO2
affords all of the same functionality as its predecessor, serving as a unified
biometric API harness that allows for seamless evaluation, deployment, and
simple pipeline creation for heterogeneous biometric software. FaRO2
additionally provides a fully declarative capability for defining and
coordinating custom machine learning and sensor pipelines, allowing the
distribution of processes across otherwise incompatible hardware and networks.
FaRO2 ultimately provides a way to quickly configure, hot-swap, and expand
large coordinated or federated systems online without interruptions for
maintenance. Because much of the data collected in a smart city contains
Personally Identifying Information (PII), FaRO2 also provides built-in tools
and layers to ensure secure and encrypted streaming, storage, and access of PII
data across distributed systems. </font><br> Link: <a href='http://arxiv.org/pdf/2209.12962v1' target="_blank">http://arxiv.org/pdf/2209.12962v1</a><br> <br> <br> <font size='5'> 550 </font> <div style="text-align: right"> 2022-09-26 17:20:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Embedding-based neural network for investment return prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In addition to being familiar with policies, high investment returns also
require extensive knowledge of relevant industry knowledge and news. In
addition, it is necessary to leverage relevant theories for investment to make
decisions, thereby amplifying investment returns. A effective investment return
estimate can feedback the future rate of return of investment behavior. In
recent years, deep learning are developing rapidly, and investment return
prediction based on deep learning has become an emerging research topic. This
paper proposes an embedding-based dual branch approach to predict an
investment's return. This approach leverages embedding to encode the investment
id into a low-dimensional dense vector, thereby mapping high-dimensional data
to a low-dimensional manifold, so that highdimensional features can be
represented competitively. In addition, the dual branch model realizes the
decoupling of features by separately encoding different information in the two
branches. In addition, the swish activation function further improves the model
performance. Our approach are validated on the Ubiquant Market Prediction
dataset. The results demonstrate the superiority of our approach compared to
Xgboost, Lightgbm and Catboost. </font><br> Link: <a href='http://arxiv.org/pdf/2210.00876v1' target="_blank">http://arxiv.org/pdf/2210.00876v1</a><br> <br> <br> <font size='5'> 551 </font> <div style="text-align: right"> 2022-09-26 12:33:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does limited liability reduce leveraged risk?: The case of loan portfolio management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Return-risk models are the two pillars of modern portfolio theory, which are
widely used to make decisions in choosing the loan portfolio of a bank. Banks
and other financial institutions are subjected to limited liability protection.
However, in most of the model formulation, limited liability is not taken into
consideration. Accordingly, to address this, we have, in this article, analyzed
the effect of including it in the model formulation. We formulate four models,
two of them are maximizing the expected return with risk constraint, including
and excluding limited-liability, and other two are minimization of risk with
threshold level of return with and without limited-liability. Our theoretical
results show that the solutions of the models with limited-liability produce
better results than the others, in both minimizing risk and maximizing expected
return. It has less risky investment than the other portfolio that solves the
other model. Finally, an illustrative example is presented to support the
theoretical results obtained. </font><br> Link: <a href='http://arxiv.org/pdf/2209.12636v1' target="_blank">http://arxiv.org/pdf/2209.12636v1</a><br> <br> <br> <font size='5'> 552 </font> <div style="text-align: right"> 2022-09-26 09:17:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning to simulate realistic limit order book markets from data as a World Agent</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Multi-agent market simulators usually require careful calibration to emulate
real markets, which includes the number and the type of agents. Poorly
calibrated simulators can lead to misleading conclusions, potentially causing
severe loss when employed by investment banks, hedge funds, and traders to
study and evaluate trading strategies. In this paper, we propose a world model
simulator that accurately emulates a limit order book market -- it requires no
agent calibration but rather learns the simulated market behavior directly from
historical data. Traditional approaches fail short to learn and calibrate
trader population, as historical labeled data with details on each individual
trader strategy is not publicly available. Our approach proposes to learn a
unique "world" agent from historical data. It is intended to emulate the
overall trader population, without the need of making assumptions about
individual market agent strategies. We implement our world agent simulator
models as a Conditional Generative Adversarial Network (CGAN), as well as a
mixture of parametric distributions, and we compare our models against previous
work. Qualitatively and quantitatively, we show that the proposed approaches
consistently outperform previous work, providing more realism and
responsiveness. </font><br> Link: <a href='http://arxiv.org/pdf/2210.09897v1' target="_blank">http://arxiv.org/pdf/2210.09897v1</a><br> <br> <br> <font size='5'> 553 </font> <div style="text-align: right"> 2022-09-25 04:41:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Learning and Artificial Intelligence-Driven Multi-Scale Modeling for High Burnup Accident-Tolerant Fuels for Light Water-Based SMR Applications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The concept of small modular reactor has changed the outlook for tackling
future energy crises. This new reactor technology is very promising considering
its lower investment requirements, modularity, design simplicity, and enhanced
safety features. The application of artificial intelligence-driven multi-scale
modeling (neutronics, thermal hydraulics, fuel performance, etc.) incorporating
Digital Twin and associated uncertainties in the research of small modular
reactors is a recent concept. In this work, a comprehensive study is conducted
on the multiscale modeling of accident-tolerant fuels. The application of these
fuels in the light water-based small modular reactors is explored. This chapter
also focuses on the application of machine learning and artificial intelligence
in the design optimization, control, and monitoring of small modular reactors.
Finally, a brief assessment of the research gap on the application of
artificial intelligence to the development of high burnup composite
accident-tolerant fuels is provided. Necessary actions to fulfill these gaps
are also discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2209.12146v1' target="_blank">http://arxiv.org/pdf/2209.12146v1</a><br> <br> <br> <font size='5'> 554 </font> <div style="text-align: right"> 2022-09-22 21:19:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockchain-Oriented Services Computing in Action: Insights from a User Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Blockchain architectures promise disruptive innovation but factually they
pose many architectural restrictions to classical service-based applications
and show considerable design, implementation, and operations overhead.
Furthermore, the relation between such overheads and user benefits is not clear
yet. To shed light on the aforementioned relations, a service-based blockchain
architecture was designed and deployed as part of a field study in real-life
experimentation. An observational approach was then performed to elaborate on
the technology-acceptance of the service-based blockchain architecture in
question. Evidence shows that the resulting architecture is, in principle, not
different than other less complex equivalents; furthermore, the architectural
limitations posed by the blockchain-oriented design demand a significant
additional effort to be put onto even the simplest of functionalities. We
conclude that further research shall be invested in clarifying further the
design principles we learned as part of this study as well as any trade-offs
posed by blockchain-oriented service design and operation. </font><br> Link: <a href='http://arxiv.org/pdf/2209.11320v1' target="_blank">http://arxiv.org/pdf/2209.11320v1</a><br> <br> <br> <font size='5'> 555 </font> <div style="text-align: right"> 2022-09-22 16:51:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Transmission of US Monetary Policy Shocks: The Role of Investment & Financial Heterogeneity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the transmission of US monetary policy shocks into
Emerging Markets emphasizing the role of investment and financial
heterogeneity. First, we use a panel SVAR model to show that a US interest
tightening leads to a persistent recession in Emerging Markets driven by a
sharp reduction in aggregate investment. Second, we study the role of firms'
financial heterogeneity in the transmission of US interest rate shocks by
exploiting detailed balance sheet dataset from Chile. We find that more
indebted firms experience greater drops in investment in response to a US
tightening shock than less indebted firms. This result is at odds with recent
evidence from US firms, even when using the same identification strategy and
econometric methods. Third, we rationalize this finding using a stylized model
of heterogeneous firms subject to a tightening leverage constraint. Finally, we
present evidence in support of this hypothesis as well as robustness checks to
our main results. Overall, our results suggests that the transmission channel
of US monetary policy shocks within and outside the US differ, a result novel
to the literature. </font><br> Link: <a href='http://arxiv.org/pdf/2209.11150v1' target="_blank">http://arxiv.org/pdf/2209.11150v1</a><br> <br> <br> <font size='5'> 556 </font> <div style="text-align: right"> 2022-09-22 01:19:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Real Data-Driven Analytical Model to Predict Information Technology Sector Index Price of S&P 500</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: S&P 500 Index is one of the most sought after stock indices in the world. In
particular, Information Technology Sector of S&P 500 is the number one business
segment of the S&P 500 in terms of market capital, annual revenue and the
number of companies (75) associated with it, and is one of the most attracting
areas for many investors due to high percentage annual returns on investment
over the years. A non-linear real data-driven analytical model is built to
predict the Weekly Closing Price (WCP) of the Information Technology Sector
Index of S&P 500 using six financial, four economic indicators and their two
way interactions as the attributable entities that drive the stock returns. We
rank the statistically significant indicators and their interactions based on
the percentage of contribution to the $WCP$ of the Information Technology
Sector Index of the S&P 500 that provides significant information for the
beneficiary of the proposed predictive model. The model has the predictive
accuracy of 99.4%, and the paper presents some intriguing findings and the
model's usefulness. </font><br> Link: <a href='http://arxiv.org/pdf/2209.10720v1' target="_blank">http://arxiv.org/pdf/2209.10720v1</a><br> <br> <br> <font size='5'> 557 </font> <div style="text-align: right"> 2022-09-21 14:33:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Powering Europe with North Sea Offshore Wind: The Impact of Hydrogen Investments on Grid Infrastructure and Power Prices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Hydrogen will be a central cross-sectoral energy carrier in the
decarbonization of the European energy system. This paper investigates how a
large-scale deployment of green hydrogen production affects the investments in
transmission and generation towards 2060, analyzes the North Sea area with the
main offshore wind projects, and assesses the development of an offshore energy
hub. Results indicate that the hydrogen deployment has a tremendous impact on
the grid development in Europe and in the North Sea. Findings indicate that
total power generation capacity increases around 50%. The offshore energy hub
acts mainly as a power transmission asset, leads to a reduction in total
generation capacity, and is central to unlock the offshore wind potential in
the North Sea. The effect of hydrogen deployment on power prices is
multifaceted. In regions where power prices have typically been lower than
elsewhere in Europe, it is observed that hydrogen increases the power price
considerably. However, as hydrogen flexibility relieves stress in high-demand
periods for the grid, power prices decrease in average for some countries. This
suggests that while the deployment of green hydrogen will lead to a significant
increase in power demand, power prices will not necessarily experience a large
increase. </font><br> Link: <a href='http://arxiv.org/pdf/2209.10389v1' target="_blank">http://arxiv.org/pdf/2209.10389v1</a><br> <br> <br> <font size='5'> 558 </font> <div style="text-align: right"> 2022-09-21 14:12:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Incremental Updates of Generalized Hypertree Decompositions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Structural decomposition methods, such as generalized hypertree
decompositions, have been successfully used for solving constraint satisfaction
problems (CSPs). As decompositions can be reused to solve CSPs with the same
constraint scopes, investing resources in computing good decompositions is
beneficial, even though the computation itself is hard. Unfortunately, current
methods need to compute a completely new decomposition even if the scopes
change only slightly. In this paper, we make the first steps toward solving the
problem of updating the decomposition of a CSP $P$ so that it becomes a valid
decomposition of a new CSP $P'$ produced by some modification of $P$. Even
though the problem is hard in theory, we propose and implement a framework for
effectively updating GHDs. The experimental evaluation of our algorithm
strongly suggests practical applicability. </font><br> Link: <a href='http://arxiv.org/pdf/2209.10375v1' target="_blank">http://arxiv.org/pdf/2209.10375v1</a><br> <br> <br> <font size='5'> 559 </font> <div style="text-align: right"> 2022-09-21 13:57:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Insurance Contract for High Renewable Energy Integration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The increasing penetration of renewable energy poses significant challenges
to power grid reliability. There have been increasing interests in utilizing
financial tools, such as insurance, to help end-users hedge the potential risk
of lost load due to renewable energy variability. With insurance, a user pays a
premium fee to the utility, so that he will get compensated in case his demand
is not fully satisfied. A proper insurance design needs to resolve the
following two challenges: (i) users' reliability preference is private
information; and (ii) the insurance design is tightly coupled with the
renewable energy investment decision. To address these challenges, we adopt the
contract theory to elicit users' private reliability preferences, and we study
how the utility can jointly optimize the insurance contract and the planning of
renewable energy. A key analytical challenge is that the joint optimization of
the insurance design and the planning of renewables is non-convex. We resolve
this difficulty by revealing important structural properties of the optimal
solution, using the help of two benchmark problems: the no-insurance benchmark
and the social-optimum benchmark. Compared with the no-insurance benchmark, we
prove that the social cost and users' total energy cost are always no larger
under the optimal contract. Simulation results show that the largest benefit of
the insurance contract is achieved at a medium electricity-bill price together
with a low type heterogeneity and a high renewable uncertainty. </font><br> Link: <a href='http://arxiv.org/pdf/2209.10363v1' target="_blank">http://arxiv.org/pdf/2209.10363v1</a><br> <br> <br> <font size='5'> 560 </font> <div style="text-align: right"> 2022-09-21 06:58:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detecting Crop Burning in India using Satellite Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Crop residue burning is a major source of air pollution in many parts of the
world, notably South Asia. Policymakers, practitioners and researchers have
invested in both measuring impacts and developing interventions to reduce
burning. However, measuring the impacts of burning or the effectiveness of
interventions to reduce burning requires data on where burning occurred. These
data are challenging to collect in the field, both in terms of cost and
feasibility. We take advantage of data from ground-based monitoring of crop
residue burning in Punjab, India to explore whether burning can be detected
more effectively using accessible satellite imagery. Specifically, we used 3m
PlanetScope data with high temporal resolution (up to daily) as well as
publicly-available Sentinel-2 data with weekly temporal resolution but greater
depth of spectral information. Following an analysis of the ability of
different spectral bands and burn indices to separate burned and unburned plots
individually, we built a Random Forest model with those determined to provide
the greatest separability and evaluated model performance with ground-verified
data. Our overall model accuracy of 82-percent is favorable given the
challenges presented by the measurement. Based on insights from this process,
we discuss technical challenges of detecting crop residue burning from
satellite imagery as well as challenges to measuring impacts, both of burning
and of policy interventions. </font><br> Link: <a href='http://arxiv.org/pdf/2209.10148v1' target="_blank">http://arxiv.org/pdf/2209.10148v1</a><br> <br> <br> <font size='5'> 561 </font> <div style="text-align: right"> 2022-09-21 06:52:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Artificial Intelligence and Innovation to Reduce the Impact of Extreme Weather Events on Sustainable Production</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Frequent occurrences of extreme weather events substantially impact the lives
of the less privileged in our societies, particularly in agriculture-inclined
economies. The unpredictability of extreme fires, floods, drought, cyclones,
and others endangers sustainable production and life on land (SDG goal 15),
which translates into food insecurity and poorer populations. Fortunately,
modern technologies such as Artificial Intelligent (AI), the Internet of Things
(IoT), blockchain, 3D printing, and virtual and augmented reality (VR and AR)
are promising to reduce the risk and impact of extreme weather in our
societies. However, research directions on how these technologies could help
reduce the impact of extreme weather are unclear. This makes it challenging to
emploring digital technologies within the spheres of extreme weather. In this
paper, we employed the Delphi Best Worst method and Machine learning approaches
to identify and assess the push factors of technology. The BWM evaluation
revealed that predictive nature was AI's most important criterion and role,
while the mass-market potential was the less important criterion. Based on this
outcome, we tested the predictive ability of machine elarning on a publilcly
available dataset to affrm the predictive rols of AI. We presented the
managerial and methodological implications of the study, which are crucial for
research and practice. The methodology utilized in this study could aid
decision-makers in devising strategies and interventions to safeguard
sustainable production. This will also facilitate allocating scarce resources
and investment in improving AI techniques to reduce the adverse impacts of
extreme events. Correspondingly, we put forward the limitations of this, which
necessitate future research. </font><br> Link: <a href='http://arxiv.org/pdf/2210.08962v1' target="_blank">http://arxiv.org/pdf/2210.08962v1</a><br> <br> <br> <font size='5'> 562 </font> <div style="text-align: right"> 2022-09-20 17:33:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A unifying view on the irreversible investment exercise boundary in a stochastic, time-inhomogeneous capacity expansion problem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the investment exercise boundary erasing in a stochastic,
continuous time capacity expansion problem with irreversible investment on the
finite time interval $[0, T]$ and a state dependent scrap value associated with
the production facility at the finite horizon $T$. The capacity process is a
time-inhomogeneous diffusion in which a monotone nondecreasing, possibly
singular, process representing the cumulative investment enters additively. The
levels of capacity, employment and operating capital contribute to the firm's
production and are optimally chosen in order to maximize the expected total
discounted profits. Two different approaches are employed to study and
characterize the boundary. From one side, some first order condition are solved
by using the Bank and El Karoui Representation Theorem, and that sheds further
light on the connection between the threshold which the optimal policy of the
singular stochastic control problem activates at and the optional solution of
Representation Theorem. Its application in the presence of the scrap value is
new. It is accomplished by a suitable devise to overcome the difficulties due
to the presence of a non integral term in the maximizing functional. The
optimal investment process is shown to become active at the so-called "base
capacity" level, given as the unique solution of an integral equation. On the
other hand, when the coefficients of the uncontrolled capacity process are
deterministic, the optimal stopping problem classically associated to the
original capacity problem is resumed and some essential properties of the
investment exercise boundary are obtained. The optimal investment process is
proved to be continuous. Unifying approaches and views, the exercise boundary
is shown to coincide with the base capacity, hence it is characterized by an
integral equation not requiring any a priori regularity. </font><br> Link: <a href='http://arxiv.org/pdf/2209.09878v1' target="_blank">http://arxiv.org/pdf/2209.09878v1</a><br> <br> <br> <font size='5'> 563 </font> <div style="text-align: right"> 2022-09-19 21:44:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analyzing Machine Learning Models for Credit Scoring with Explainable AI and Optimizing Investment Decisions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper examines two different yet related questions related to
explainable AI (XAI) practices. Machine learning (ML) is increasingly important
in financial services, such as pre-approval, credit underwriting, investments,
and various front-end and back-end activities. Machine Learning can
automatically detect non-linearities and interactions in training data,
facilitating faster and more accurate credit decisions. However, machine
learning models are opaque and hard to explain, which are critical elements
needed for establishing a reliable technology. The study compares various
machine learning models, including single classifiers (logistic regression,
decision trees, LDA, QDA), heterogeneous ensembles (AdaBoost, Random Forest),
and sequential neural networks. The results indicate that ensemble classifiers
and neural networks outperform. In addition, two advanced post-hoc model
agnostic explainability techniques - LIME and SHAP are utilized to assess
ML-based credit scoring models using the open-access datasets offered by
US-based P2P Lending Platform, Lending Club. For this study, we are also using
machine learning algorithms to develop new investment models and explore
portfolio strategies that can maximize profitability while minimizing risk. </font><br> Link: <a href='http://arxiv.org/pdf/2209.09362v1' target="_blank">http://arxiv.org/pdf/2209.09362v1</a><br> <br> <br> <font size='5'> 564 </font> <div style="text-align: right"> 2022-09-19 21:21:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Wireless-Assisted Hierarchical Framework to Accommodate Mobile Energy Resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The societal decarbonisation fosters the installation of massive renewable
inverter-based resources (IBRs) in replacing fossil fuel based traditional
energy supply. The efficient and reliable operation of distributed IBRs
requires advanced Information and Communication Technologies (ICT) , which may
lead to a huge infrastructure investment and long construction time for remote
communities. Therefore, to efficiently coordinate IBRs, we propose a low-cost
hierarchical structure, especially for remote communities without existing
strong ICT connections, that combines the advantages of centralised and
distributed frameworks via advanced wireless communication technologies. More
specifically, in each region covered by a single cellular network, dispatchable
resources are controlled via a regional aggregated controller, and the
corresponding regional information flow is enabled by a device-to-device (D2D)
communication assisted wireless network. The wireless network can fully reuse
the bandwidth to improve data flow efficiency, leading to a flexible
information structure that can accommodate the plug-and-play operation of
mobile IBRs. Simulation results demonstrate that the proposed wireless
communication scheme significantly improves the utilization of existing
bandwidth, and the dynamically allocated wireless system ensures the flexible
operation of mobile IBRs. </font><br> Link: <a href='http://arxiv.org/pdf/2209.09353v1' target="_blank">http://arxiv.org/pdf/2209.09353v1</a><br> <br> <br> <font size='5'> 565 </font> <div style="text-align: right"> 2022-09-18 14:24:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Skills and Liquidity Barriers to Youth Employment: Medium-term Evidence from a Cash Benchmarking Experiment in Rwanda</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present results of an experiment benchmarking a workforce training program
against cash transfers for underemployed young adults in Rwanda. 3.5 years
after treatment, the training program enhances productive time use and asset
investment, while the cash transfers drive productive assets, livestock values,
savings, and subjective well-being. Both interventions have powerful effects on
entrepreneurship. But while labor, sales, and profits all go up, the implied
wage rate in these businesses is low. Our results suggest that credit is a
major barrier to self-employment, but deeper reforms may be required to enable
entrepreneurship to provide a transformative pathway out of poverty. </font><br> Link: <a href='http://arxiv.org/pdf/2209.08574v1' target="_blank">http://arxiv.org/pdf/2209.08574v1</a><br> <br> <br> <font size='5'> 566 </font> <div style="text-align: right"> 2022-09-18 13:55:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting Mutual Funds' Performance using Deep Learning and Ensemble Techniques</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Predicting fund performance is beneficial to both investors and fund
managers, and yet is a challenging task. In this paper, we have tested whether
deep learning models can predict fund performance more accurately than
traditional statistical techniques. Fund performance is typically evaluated by
the Sharpe ratio, which represents the risk-adjusted performance to ensure
meaningful comparability across funds. We calculated the annualised Sharpe
ratios based on the monthly returns time series data for more than 600 open-end
mutual funds investing in listed large-cap equities in the United States. We
find that long short-term memory (LSTM) and gated recurrent units (GRUs) deep
learning methods, both trained with modern Bayesian optimization, provide
higher accuracy in forecasting funds' Sharpe ratios than traditional
statistical ones. An ensemble method, which combines forecasts from LSTM and
GRUs, achieves the best performance of all models. There is evidence to say
that deep learning and ensembling offer promising solutions in addressing the
challenge of fund performance forecasting. </font><br> Link: <a href='http://arxiv.org/pdf/2209.09649v2' target="_blank">http://arxiv.org/pdf/2209.09649v2</a><br> <br> <br> <font size='5'> 567 </font> <div style="text-align: right"> 2022-09-17 07:02:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Report of the Topical Group on Cosmic Frontier 5 Dark Energy and Cosmic Acceleration: Cosmic Dawn and Before for Snowmass 2021</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This report summarizes the envisioned research activities as gathered from
the Snowmass 2021 CF5 working group concerning Dark Energy and Cosmic
Acceleration: Cosmic Dawn and Before. The scientific goals are to study
inflation and to search for new physics through precision measurements of relic
radiation from the early universe. The envisioned research activities for this
decade (2025-35) are constructing and operating major facilities and developing
critical enabling capabilities. The major facilities for this decade are the
CMB-S4 project, a new Stage-V spectroscopic survey facility, and existing
gravitational wave observatories. Enabling capabilities include aligning and
investing in theory, computation and model building, and investing in new
technologies needed for early universe studies in the following decade (2035+). </font><br> Link: <a href='http://arxiv.org/pdf/2209.08265v1' target="_blank">http://arxiv.org/pdf/2209.08265v1</a><br> <br> <br> <font size='5'> 568 </font> <div style="text-align: right"> 2022-09-16 17:11:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reinterpretation and Long-Term Preservation of Data and Code</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Careful preservation of experimental data, simulations, analysis products,
and theoretical work maximizes their long-term scientific return on investment
by enabling new analyses and reinterpretation of the results in the future. Key
infrastructure and technical developments needed for some high-value science
targets are not in scope for the operations program of the large experiments
and are often not effectively funded. Increasingly, the science goals of our
projects require contributions that span the boundaries between individual
experiments and surveys, and between the theoretical and experimental
communities. Furthermore, the computational requirements and technical
sophistication of this work is increasing. As a result, it is imperative that
the funding agencies create programs that can devote significant resources to
these efforts outside of the context of the operations of individual major
experiments, including smaller experiments and theory/simulation work. In this
Snowmass 2021 Computational Frontier topical group report (CompF7:
Reinterpretation and long-term preservation of data and code), we summarize the
current state of the field and make recommendations for the future. </font><br> Link: <a href='http://arxiv.org/pdf/2209.08054v1' target="_blank">http://arxiv.org/pdf/2209.08054v1</a><br> <br> <br> <font size='5'> 569 </font> <div style="text-align: right"> 2022-09-16 17:04:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021 Cosmic Frontier: Report of the CF04 Topical Group on Dark Energy and Cosmic Acceleration in the Modern Universe</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cosmological observations in the new millennium have dramatically increased
our understanding of the Universe, but several fundamental questions remain
unanswered. This topical group report describes the best opportunities to
address these questions over the coming decades by extending observations to
the $z<6$ universe. The greatest opportunity to revolutionize our understanding
of cosmic acceleration both in the modern universe and the inflationary epoch
would be provided by a new Stage V Spectroscopic Facility (Spec-S5) which would
combine a large telescope aperture, wide field of view, and high multiplexing.
Such a facility could simultaneously provide a dense sample of galaxies at
lower redshifts to provide robust measurements of the growth of structure at
small scales, as well as a sample at redshifts $2<z<5$ to measure cosmic
structure at the largest scales, spanning a sufficient volume to probe
primordial non-Gaussianity from inflation, to search for features in the
inflationary power spectrum on a broad range of scales, to test dark energy
models in poorly-explored regimes, and to determine the total neutrino mass and
effective number of light relics. A number of compelling opportunities at
smaller scales should also be pursued alongside Spec-S5. The science
collaborations analyzing DESI and LSST data will need funding for a variety of
activities, including cross-survey simulations and combined analyses. The
results from these experiments can be greatly improved by smaller programs to
obtain complementary data, including follow-up studies of supernovae and
spectroscopy to improve photometric redshift measurements. The best future use
of the Vera C. Rubin Observatory should be evaluated later this decade after
the first LSST analyses have been done. Finally, investments in pathfinder
projects could enable powerful new probes of cosmology to come online in future
decades. </font><br> Link: <a href='http://arxiv.org/pdf/2209.08049v1' target="_blank">http://arxiv.org/pdf/2209.08049v1</a><br> <br> <br> <font size='5'> 570 </font> <div style="text-align: right"> 2022-09-16 13:52:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Structure Preserving Transformations for Practical Model-based Systems Engineering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this third decade of systems engineering in the twenty-first century, it
is important to develop and demonstrate practical methods to exploit
machine-readable models in the engineering of systems. Substantial investment
has been made in languages and modelling tools for developing models. A key
problem is that system architects and engineers work in a multidisciplinary
environment in which models are not the product of any one individual. This
paper provides preliminary results of a formal approach to specify models and
structure preserving transformations between them that support model
synchronization. This is an important area of research and practice in software
engineering. However, it is limited to synchronization at the code level of
systems. This paper leverages previous research of the authors to define a core
fractal for interpretation of concepts into model specifications and
transformation between models. This fractal is used to extend the concept of
synchronization of models to the system level and is demonstrated through a
practical engineering example for an advanced driver assistance system. </font><br> Link: <a href='http://arxiv.org/pdf/2209.07935v1' target="_blank">http://arxiv.org/pdf/2209.07935v1</a><br> <br> <br> <font size='5'> 571 </font> <div style="text-align: right"> 2022-09-15 18:46:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapidly-developing intersection of machine learning (ML) with high-energy
physics (HEP) presents both opportunities and challenges to our community. Far
beyond applications of standard ML tools to HEP problems, genuinely new and
potentially revolutionary approaches are being developed by a generation of
talent literate in both fields. There is an urgent need to support the needs of
the interdisciplinary community driving these developments, including funding
dedicated research at the intersection of the two fields, investing in
high-performance computing at universities and tailoring allocation policies to
support this work, developing of community tools and standards, and providing
education and career paths for young researchers attracted by the intellectual
vitality of machine learning for high energy physics. </font><br> Link: <a href='http://arxiv.org/pdf/2209.07559v1' target="_blank">http://arxiv.org/pdf/2209.07559v1</a><br> <br> <br> <font size='5'> 572 </font> <div style="text-align: right"> 2022-09-15 15:11:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass Computational Frontier: Topical Group Report on Experimental Algorithm Parallelization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The substantial increase in data volume and complexity expected from future
experiments will require significant investment to prepare experimental
algorithms. These algorithms include physics object reconstruction,
calibrations, and processing of observational data. In addition, the changing
computing architecture landscape, which will be primarily composed of
heterogeneous resources, will continue to pose major challenges with regard to
algorithmic migration. Portable tools need to be developed that can be shared
among the frontiers (e.g., for code execution on different platforms) and
opportunities, such as forums or cross-experimental working groups, need to be
provided where experiences and lessons learned can be shared between
experiments and frontiers. At the same time, individual experiments also need
to invest considerable resources to develop algorithms unique to their needs
(e.g., for facilities dedicated to the experiment), and ensure that their
specific algorithms will be able to efficiently exploit external heterogeneous
computing facilities. Common software tools represent a cost-effective
solution, providing ready-to-use software solutions as well as a platform for
R\&D work. These are particularly important for small experiments which
typically do not have dedicated resources needed to face the challenges imposed
by the evolving computing technologies. Workforce development is a key concern
across frontiers and experiments, and additional support is needed to provide
career opportunities for researchers working in the field of experimental
algorithm development. Finally, cross-discipline collaborations going beyond
high-energy physics are a key ingredient to address the challenges ahead and
more support for such collaborations needs to be created. This report targets
future experiments, observations and experimental algorithm development for the
next 10-15 years. </font><br> Link: <a href='http://arxiv.org/pdf/2209.07356v1' target="_blank">http://arxiv.org/pdf/2209.07356v1</a><br> <br> <br> <font size='5'> 573 </font> <div style="text-align: right"> 2022-09-15 07:27:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Thermal transport in nanoelectronic devices cooled by on-chip magnetic refrigeration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: On-chip demagnetization refrigeration has recently emerged as a powerful tool
for reaching microkelvin electron temperatures in nanoscale structures. The
relative importance of cooling on-chip and off-chip components and the thermal
subsystem dynamics are yet to be analyzed. We study a Coulomb blockade
thermometer with on-chip copper refrigerant both experimentally and
numerically, showing that dynamics in this device are captured by a
first-principles model. Our work shows how to simulate thermal dynamics in
devices down to microkelvin temperatures, and outlines a recipe for a
low-investment platform for quantum technologies and fundamental nanoscience in
this novel temperature range. </font><br> Link: <a href='http://arxiv.org/pdf/2209.07099v2' target="_blank">http://arxiv.org/pdf/2209.07099v2</a><br> <br> <br> <font size='5'> 574 </font> <div style="text-align: right"> 2022-09-14 17:10:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass Computational Frontier: Topical Group Report on Quantum Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum computing will play a pivotal role in the High Energy Physics (HEP)
science program over the early parts of the 21$^{st}$ Century, both as a major
expansion of our capabilities across the Computational Frontier, and in
synthesis with quantum sensing and quantum networks. This report outlines how
Quantum Information Science (QIS) and HEP are deeply intertwined endeavors that
benefit enormously from a strong engagement together. Quantum computers do not
represent a detour for HEP, rather they are set to become an integral part of
our discovery toolkit. Problems ranging from simulating quantum field theories,
to fully leveraging the most sensitive sensor suites for new particle searches,
and even data analysis will run into limiting bottlenecks if constrained to our
current computing paradigms. Easy access to quantum computers is needed to
build a deeper understanding of these opportunities. In turn, HEP brings
crucial expertise to the national quantum ecosystem in quantum domain
knowledge, superconducting technology, cryogenic and fast microelectronics, and
massive-scale project management. The role of quantum technologies across the
entire economy is expected to grow rapidly over the next decade, so it is
important to establish the role of HEP in the efforts surrounding QIS. Fully
delivering on the promise of quantum technologies in the HEP science program
requires robust support. It is important to both invest in the co-design
opportunities afforded by the broader quantum computing ecosystem and leverage
HEP strengths with the goal of designing quantum computers tailored to HEP
science. </font><br> Link: <a href='http://arxiv.org/pdf/2209.06786v1' target="_blank">http://arxiv.org/pdf/2209.06786v1</a><br> <br> <br> <font size='5'> 575 </font> <div style="text-align: right"> 2022-09-13 23:00:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Decarbonizing OCP</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present our collaboration with the OCP Group, one of the world's largest
producers of phosphate and phosphate-based products, to reduce OCP's carbon
emissions significantly. We study the problem of decarbonizing OCP's
electricity supply by installing a mixture of solar panels and batteries to
minimize its time-discounted investment cost plus the cost of satisfying its
remaining demand via the national grid. OCP is currently designing its
renewable investment strategy, using insights gleaned from our optimization
model, and has pledged to invest 130 billion MAD (approx. 13 billion USD) in a
green initiative by 2027, a subset of which involves decarbonization. We
immunize our model against deviations between forecast and realized solar
generation output via a combination of robust and distributionally robust
optimization. To account for variability in daily solar generation, we propose
a data-driven robust optimization approach that prevents excessive conservatism
by averaging across uncertainty sets. To protect against variability in
seasonal weather patterns induced by climate change, we invoke distributionally
robust optimization techniques. Under a 10 billion MAD investment by OCP, the
proposed methodology reduces the carbon emissions which arise from OCP's energy
needs by more than 70% while generating a net present value (NPV) of 5 billion
MAD over twenty years. Moreover, a 20 billion MAD investment induces a 95%
reduction in carbon emissions and generates an NPV of around 2 billion MAD. To
fulfill the Paris climate agreement, rapidly decarbonizing the global economy
in a financially sustainable fashion is imperative. Accordingly, this work
develops a robust optimization methodology that enables OCP to decarbonize at a
profit by purchasing solar panels and batteries. Moreover, the methodology
could be applied to decarbonize other industrial consumers. </font><br> Link: <a href='http://arxiv.org/pdf/2209.06341v2' target="_blank">http://arxiv.org/pdf/2209.06341v2</a><br> <br> <br> <font size='5'> 576 </font> <div style="text-align: right"> 2022-09-13 15:39:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Strategic investments in multi-stage General Lotto games</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In adversarial interactions, one is often required to make strategic
decisions over multiple periods of time, wherein decisions made earlier impact
a player's competitive standing as well as how choices are made in later
stages. In this paper, we study such scenarios in the context of General Lotto
games, which models the competitive allocation of resources over multiple
battlefields between two players. We propose a two-stage formulation where one
of the players has reserved resources that can be strategically pre-allocated
across the battlefields in the first stage. The pre-allocation then becomes
binding and is revealed to the other player. In the second stage, the players
engage by simultaneously allocating their real-time resources against each
other. The main contribution in this paper provides complete characterizations
of equilibrium payoffs in the two-stage game, revealing the interplay between
performance and the amount of resources expended in each stage of the game. We
find that real-time resources are at least twice as effective as pre-allocated
resources. We then determine the player's optimal investment when there are
linear costs associated with purchasing each type of resource before play
begins, and there is a limited monetary budget. </font><br> Link: <a href='http://arxiv.org/pdf/2209.06090v1' target="_blank">http://arxiv.org/pdf/2209.06090v1</a><br> <br> <br> <font size='5'> 577 </font> <div style="text-align: right"> 2022-09-13 04:58:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Many-ported and Shared Memory Architecture for High-Performance ADAS SoCs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Increasing investment in computing technologies and the advancements in
silicon technology has fueled rapid growth in advanced driver assistance
systems (ADAS) and corresponding SoC developments. An ADAS SoC represents a
heterogeneous architecture that consists of CPUs, GPUs and artificial
intelligence (AI) accelerators. In order to guarantee its safety and
reliability, it must process massive amount of raw data collected from multiple
redundant sources such as high-definition video cameras, Radars, and Lidars to
recognize objects correctly and to make the right decisions promptly. A domain
specific memory architecture is essential to achieve the above goals. We
present a shared memory architecture that enables high data throughput among
multiple parallel accesses native to the ADAS applications. It also provides
deterministic access latency with proper isolation under the stringent
real-time QoS constraints. A prototype is built and analyzed. The results
validate that the proposed architecture provides close to 100\% throughput for
both read and write accesses generated simultaneously by many accessing masters
with full injection rate. It can also provide consistent QoS to the domain
specific payloads while enabling the scalability and modularity of the design. </font><br> Link: <a href='http://arxiv.org/pdf/2209.05731v1' target="_blank">http://arxiv.org/pdf/2209.05731v1</a><br> <br> <br> <font size='5'> 578 </font> <div style="text-align: right"> 2022-09-13 01:17:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sustainable Venture Capital</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sustainability initiatives are set to benefit greatly from the growing
involvement of venture capital, in the same way that other technological
endeavours have been enabled and accelerated in the post-war period. With the
spoils increasingly being shared between shareholders and other stakeholders,
this requires a more nuanced view than the finance-first methodologies deployed
to date. Indeed, it is possible for a venture-backed sustainability startup to
deliver outstanding results to society in general without returning a cent to
investors, though the most promising outcomes deliver profit with purpose,
satisfying all stakeholders in ways that make existing 'extractive' venture
capital seem hollow.
  To explore this nascent area, a review of related research was conducted and
social entrepreneurs & investors interviewed to construct a questionnaire
assessing the interests and intentions of current & future ecosystem
participants. Analysis of 114 responses received via several sampling methods
revealed statistically significant relationships between investing preferences
and genders, generations, sophistication, and other variables, all the way down
to the level of individual UN Sustainable Development Goals (SDGs). </font><br> Link: <a href='http://arxiv.org/pdf/2209.10518v1' target="_blank">http://arxiv.org/pdf/2209.10518v1</a><br> <br> <br> <font size='5'> 579 </font> <div style="text-align: right"> 2022-09-12 15:15:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For industrial-scale advertising systems, prediction of ad click-through rate
(CTR) is a central problem. Ad clicks constitute a significant class of user
engagements and are often used as the primary signal for the usefulness of ads
to users. Additionally, in cost-per-click advertising systems where advertisers
are charged per click, click rate expectations feed directly into value
estimation. Accordingly, CTR model development is a significant investment for
most Internet advertising companies. Engineering for such problems requires
many machine learning (ML) techniques suited to online learning that go well
beyond traditional accuracy improvements, especially concerning efficiency,
reproducibility, calibration, credit attribution. We present a case study of
practical techniques deployed in Google's search ads CTR model. This paper
provides an industry case study highlighting important areas of current ML
research and illustrating how impactful new ML methods are evaluated and made
useful in a large-scale industrial setting. </font><br> Link: <a href='http://arxiv.org/pdf/2209.05310v1' target="_blank">http://arxiv.org/pdf/2209.05310v1</a><br> <br> <br> <font size='5'> 580 </font> <div style="text-align: right"> 2022-09-12 14:38:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Leveraging Artificial Intelligence Techniques for Smart Palm Tree Detection: A Decade Systematic Review</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past few years, total financial investment in the agricultural
sector has increased substantially. Palm tree is important for many countries'
economies, particularly in northern Africa and the Middle East. Monitoring in
terms of detection and counting palm trees provides useful information for
various stakeholders; it helps in yield estimation and examination to ensure
better crop quality and prevent pests, diseases, better irrigation, and other
potential threats. Despite their importance, this information is still
challenging to obtain. This study systematically reviews research articles
between 2011 and 2021 on artificial intelligence (AI) technology for smart palm
tree detection. A systematic review (SR) was performed using the PRISMA
approach based on a four-stage selection process. Twenty-two articles were
included for the synthesis activity reached from the search strategy alongside
the inclusion criteria in order to answer to two main research questions. The
study's findings reveal patterns, relationships, networks, and trends in
applying artificial intelligence in palm tree detection over the last decade.
Despite the good results in most of the studies, the effective and efficient
management of large-scale palm plantations is still a challenge. In addition,
countries whose economies strongly related to intelligent palm services,
especially in North Africa, should give more attention to this kind of study.
The results of this research could benefit both the research community and
stakeholders. </font><br> Link: <a href='http://arxiv.org/pdf/2209.05282v1' target="_blank">http://arxiv.org/pdf/2209.05282v1</a><br> <br> <br> <font size='5'> 581 </font> <div style="text-align: right"> 2022-09-09 14:25:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The MegaMapper: A Stage-5 Spectroscopic Instrument Concept for the Study of Inflation and Dark Energy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this white paper, we present the MegaMapper concept. The MegaMapper is a
proposed ground-based experiment to measure Inflation parameters and Dark
Energy from galaxy redshifts at $2<z<5$. In order to achieve path-breaking
results with a mid-scale investment, the MegaMapper combines existing
technologies for critical path elements and pushes innovative development in
other design areas. To this aim, we envision a 6.5-m Magellan-like telescope,
with a newly designed wide field, coupled with DESI spectrographs, and
small-pitch robots to achieve multiplexing of at least 26,000. This will match
the expected achievable target density in the redshift range of interest and
provide a 10x capability over the existing state-of the art, without a 10x
increase in project budget. </font><br> Link: <a href='http://arxiv.org/pdf/2209.04322v1' target="_blank">http://arxiv.org/pdf/2209.04322v1</a><br> <br> <br> <font size='5'> 582 </font> <div style="text-align: right"> 2022-09-08 22:15:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dr. Neurosymbolic, or: How I Learned to Stop Worrying and Accept Statistics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The symbolic AI community is increasingly trying to embrace machine learning
in neuro-symbolic architectures, yet is still struggling due to cultural
barriers. To break the barrier, this rather opinionated personal memo attempts
to explain and rectify the conventions in Statistics, Machine Learning, and
Deep Learning from the viewpoint of outsiders. It provides a step-by-step
protocol for designing a machine learning system that satisfies a minimum
theoretical guarantee necessary for being taken seriously by the symbolic AI
community, i.e., it discusses "in what condition we can stop worrying and
accept statistical machine learning." Unlike most textbooks which are written
for students trying to specialize in Stat/ML/DL and willing to accept jargons,
this memo is written for experienced symbolic researchers that hear a lot of
buzz but are still uncertain and skeptical. Information on Stat/ML/DL is
currently too scattered or too noisy to invest in. This memo prioritizes
compactness, citations to old papers (many in early 20th century), and concepts
that resonate well with symbolic paradigms in order to offer time savings. It
prioritizes general mathematical modeling and does not discuss any specific
function approximator, such as neural networks (NNs), SVMs, decision trees,
etc. Finally, it is open to corrections. Consider this memo as something
similar to a blog post taking the form of a paper on Arxiv. </font><br> Link: <a href='http://arxiv.org/pdf/2209.04049v8' target="_blank">http://arxiv.org/pdf/2209.04049v8</a><br> <br> <br> <font size='5'> 583 </font> <div style="text-align: right"> 2022-09-08 15:55:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What You See is What You Get: Local Labor Markets and Skill Acquisition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper highlights the potential for negative dynamic consequences of
recent trends towards the formation of "skill-hubs". I first show evidence that
skill acquisition is biased towards skills which are in demand in local labor
markets. This fact along with large heterogeneity in outcomes by major and
recent reductions in migration rates implies a significant potential for
inefficient skill upgrading over time. To evaluate the impact of local bias in
education in the context of standard models which focus on agglomeration
effects, I develop a structural spatial model which includes educational
investment. The model focuses on two sources of externalities: productivity
through agglomeration and signaling. Both of these affect educational decisions
tilting the balance of aggregate skill composition. Signaling externalities can
provide a substantial wedge in the response to changes in skill demand and
skill concentration with the potential for substantial welfare gains from a
more equal distribution of skills. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03892v1' target="_blank">http://arxiv.org/pdf/2209.03892v1</a><br> <br> <br> <font size='5'> 584 </font> <div style="text-align: right"> 2022-09-07 21:08:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A stabilised Benders decomposition with adaptive oracles applied to investment planning of multi-region power systems with short-term and long-term uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Benders decomposition with adaptive oracles was proposed to solve large-scale
optimisation problems with a column bounded block-diagonal structure, where
subproblems differ on the right-hand side and cost coefficients. Adaptive
Benders reduces computational effort significantly by iteratively building
inexact cutting planes and valid upper and lower bounds. However, Adaptive
Benders and standard Benders may suffer severe oscillation when solving a
multi-region investment planning problem. Therefore, we propose stabilising
Adaptive Benders with the level set method and adaptively selecting the
subproblems to solve per iteration for more accurate information. Furthermore,
we propose a dynamic level set method to improve the robustness of stabilised
Adaptive Benders by adjusting the level set per iteration. We compare
stabilised Adaptive Benders with the unstabilised versions of Adaptive Benders
with one subproblem solved per iteration and standard Benders on a multi-region
long-term power system investment planning problem with short-term and
long-term uncertainty. The problem is formulated as multi-horizon stochastic
programming. Four algorithms were implemented to solve linear programming with
up to 1 billion variables and 4.5 billion constraints. The computational
results show that: a) for a 1.00% convergence tolerance, the proposed
stabilised method is up to 113.7 times faster than standard Benders and 2.14
times faster than unstabilised Adaptive Benders; b) for a 0.10% convergence
tolerance, the proposed stabilised method is up to 45.5 times faster than
standard Benders and unstabilised Adaptive Benders cannot solve the largest
instance to convergence tolerance due to severe oscillation and c) dynamic
level set method makes stabilisation more robust. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03471v1' target="_blank">http://arxiv.org/pdf/2209.03471v1</a><br> <br> <br> <font size='5'> 585 </font> <div style="text-align: right"> 2022-09-07 16:52:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trading Strategies: Earning More in Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Gold and bitcoin are not new to us, but with limited cash and time, given
only the past stream of the daily price of gold and bitcoin, it is a kind of
new problem for us to develop a certain model and determine the best strategy
to get the most return. Here, our team members analyzed the data provided and
finally made a unified system of models to predict the price and evaluate the
risk and return in our act of investment, and we name this series of models and
measurements as CTP Model. This is a model which can determine and describe
what transaction should the trader make each day and what is the certain
maximum return he will get under different risk levels. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03294v1' target="_blank">http://arxiv.org/pdf/2209.03294v1</a><br> <br> <br> <font size='5'> 586 </font> <div style="text-align: right"> 2022-09-07 11:06:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Review on the Process of Automated Software Testing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The requirements in automation, digitalization, and fast computations have
loaded the IT sector with expectations of highly reliable, efficient, and
cost-effective software. Given that the process of testing, verification, and
validation of software products consumes 50-75% of the total revenue if the
testing process is ineffective, "n" times the expenditure must be invested to
mend the havoc caused. A delay in project completion is often attributed to the
testing phase because of the numerous cycles of debugging process. The software
testing process determines the face of the product released to the user. It
sets the standard and reliability of a company's outputs. As the complexity
increases, testing gets intense so as to examine all the outliers and various
branches of the processing flow. The testing process is automated using
software tools to avoid the tedious manual process of test input generation and
validation criteria, which certifies the program only to a certain confidence
level in the presence of outliers. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03069v1' target="_blank">http://arxiv.org/pdf/2209.03069v1</a><br> <br> <br> <font size='5'> 587 </font> <div style="text-align: right"> 2022-09-07 10:29:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How reliable are unsupervised author disambiguation algorithms in the assessment of research organization performance?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper examines extent of bias in the performance rankings of research
organisations when the assessments are based on unsupervised author-name
disambiguation algorithms. It compares the outcomes of a research performance
evaluation exercise of Italian universities using the unsupervised approach by
Caron and van Eck (2014) for derivation of the universities' research staff,
with those of a benchmark using the supervised algorithm of D'Angelo,
Giuffrida, and Abramo (2011), which avails of input data. The methodology
developed could be replicated for comparative analyses in other frameworks of
national or international interest, meaning that practitioners would have a
precise measure of the extent of distortions inherent in any evaluation
exercises using unsupervised algorithms. This could in turn be useful in
informing policy-makers' decisions on whether to invest in building national
research staff databases, instead of settling for the unsupervised approaches
with their measurement biases. </font><br> Link: <a href='http://arxiv.org/pdf/2210.03499v1' target="_blank">http://arxiv.org/pdf/2210.03499v1</a><br> <br> <br> <font size='5'> 588 </font> <div style="text-align: right"> 2022-09-07 08:27:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey on Automated Diagnosis of Alzheimer's Disease Using Optical Coherence Tomography and Angiography</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Retinal optical coherence tomography (OCT) and optical coherence tomography
angiography (OCTA) are promising tools for the (early) diagnosis of Alzheimer's
disease (AD). These non-invasive imaging techniques are cost-effective and more
accessible than alternative neuroimaging tools. However, interpreting and
classifying multi-slice scans produced by OCT devices is time-consuming and
challenging even for trained practitioners.
  There are surveys on machine learning and deep learning approaches concerning
the automated analysis of OCT scans for various diseases such as glaucoma.
However, the current literature lacks an extensive survey on the diagnosis of
Alzheimer's disease or cognitive impairment using OCT or OCTA. This has
motivated us to do a comprehensive survey aimed at machine/deep learning
scientists or practitioners who require an introduction to the problem. The
paper contains 1) an introduction to the medical background of Alzheimer's
Disease and Cognitive Impairment and their diagnosis using OCT and OCTA imaging
modalities, 2) a review of various technical proposals for the problem and the
sub-problems from an automated analysis perspective, 3) a systematic review of
the recent deep learning studies and available OCT/OCTA datasets directly aimed
at the diagnosis of Alzheimer's Disease and Cognitive Impairment. For the
latter, we used Publish or Perish Software to search for the relevant studies
from various sources such as Scopus, PubMed, and Web of Science. We followed
the PRISMA approach to screen an initial pool of 3073 references and determined
ten relevant studies (N=10, out of 3073) that directly targeted AD diagnosis.
We identified the lack of open OCT/OCTA datasets (about Alzheimer's disease) as
the main issue that is impeding the progress in the field. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03354v1' target="_blank">http://arxiv.org/pdf/2209.03354v1</a><br> <br> <br> <font size='5'> 589 </font> <div style="text-align: right"> 2022-09-07 03:36:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Social Media Engagement and Cryptocurrency Performance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the problem of predicting the future performance of cryptocurrencies
using social media data. We propose a new model to measure the engagement of
users with topics discussed on social media based on interactions with social
media posts. This model overcomes the limitations of previous volume and
sentiment based approaches. We use this model to estimate engagement
coefficients for 48 cryptocurrencies created between 2019 and 2021 using data
from Twitter from the first month of the cryptocurrencies' existence. We find
that the future returns of the cryptocurrencies are dependent on the engagement
coefficients. Cryptocurrencies whose engagement coefficients are too low or too
high have lower returns. Low engagement coefficients signal a lack of interest,
while high engagement coefficients signal artificial activity which is likely
from automated accounts known as bots. We measure the amount of bot posts for
the cryptocurrencies and find that generally, cryptocurrencies with more bot
posts have lower future returns. While future returns are dependent on both the
bot activity and engagement coefficient, the dependence is strongest for the
engagement coefficient, especially for short-term returns. We show that simple
investment strategies which select cryptocurrencies with engagement
coefficients exceeding a fixed threshold perform well for holding times of a
few months. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02911v1' target="_blank">http://arxiv.org/pdf/2209.02911v1</a><br> <br> <br> <font size='5'> 590 </font> <div style="text-align: right"> 2022-09-06 18:21:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A low-cost alternating projection approach for a continuous formulation of convex and cardinality constrained optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider convex constrained optimization problems that also include a
cardinality constraint. In general, optimization problems with cardinality
constraints are difficult mathematical programs which are usually solved by
global techniques from discrete optimization. We assume that the region defined
by the convex constraints can be written as the intersection of a finite
collection of convex sets, such that it is easy and inexpensive to project onto
each one of them (e.g., boxes, hyper-planes, or half-spaces).
  Taking advantage of a recently developed continuous reformulation that
relaxes the cardinality constraint, we propose a specialized penalty gradient
projection scheme combined with alternating projection ideas to solve these
problems. To illustrate the combined scheme, we focus on the standard
mean-variance portfolio optimization problem for which we can only invest in a
preestablished limited number of assets.
  For these portfolio problems with cardinality constraints we present a
numerical study on a variety of data sets involving real-world capital market
indices from major stock markets. On those data sets we illustrate the
computational performance of the proposed scheme to produce the effective
frontiers for different values of the limited number of allowed assets. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02756v1' target="_blank">http://arxiv.org/pdf/2209.02756v1</a><br> <br> <br> <font size='5'> 591 </font> <div style="text-align: right"> 2022-09-06 16:45:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do Unions Shape Political Ideologies at Work?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Labor unions' greatest potential for political influence likely arises from
their direct connection to millions of individuals at the workplace. There,
they may change the ideological positions of both unionizing workers and their
non-unionizing management. In this paper, we analyze the workplace-level impact
of unionization on workers' and managers' political campaign contributions over
the 1980-2016 period in the United States. To do so, we link
establishment-level union election data with transaction-level campaign
contributions to federal and local candidates. In a difference-in-differences
design that we validate with regression discontinuity tests and a novel
instrumental variables approach, we find that unionization leads to a leftward
shift of campaign contributions. Unionization increases the support for
Democrats relative to Republicans not only among workers but also among
managers, which speaks against an increase in political cleavages between the
two groups. We provide evidence that our results are not driven by
compositional changes of the workforce and are weaker in states with
Right-to-Work laws where unions can invest fewer resources in political
activities. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02637v2' target="_blank">http://arxiv.org/pdf/2209.02637v2</a><br> <br> <br> <font size='5'> 592 </font> <div style="text-align: right"> 2022-09-06 16:04:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A perspective to navigate the National Laboratory environment for RSE career growth</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper shares a perspective for the research software engineering (RSE)
community to navigate the National Laboratory landscape. The RSE role is a
recent concept that led to organizational challenges to place and evaluate
their impact, costs and benefits. The premise is that RSEs are a natural fit
into the current landscape and can use traditional career growth strategies in
science: publications, community engagements and proposals. Projects funding
RSEs can benefit from this synergy and be inclusive on traditional activities.
Still, a great deal of introspection is needed to close gaps between the
rapidly evolving RSE landscape and the well-established communication patterns
in science. This perspective is built upon interactions in industry, academia
and government in high-performance computing (HPC) environments. The goal is to
contribute to the conversation around RSE career growth and understand their
return on investment for scientific projects and sponsors. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02610v1' target="_blank">http://arxiv.org/pdf/2209.02610v1</a><br> <br> <br> <font size='5'> 593 </font> <div style="text-align: right"> 2022-09-06 13:22:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Avast-CTU Public CAPE Dataset</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is a limited amount of publicly available data to support research in
malware analysis technology. Particularly, there are virtually no publicly
available datasets generated from rich sandboxes such as Cuckoo/CAPE. The
benefit of using dynamic sandboxes is the realistic simulation of file
execution in the target machine and obtaining a log of such execution. The
machine can be infected by malware hence there is a good chance of capturing
the malicious behavior in the execution logs, thus allowing researchers to
study such behavior in detail. Although the subsequent analysis of log
information is extensively covered in industrial cybersecurity backends, to our
knowledge there has been only limited effort invested in academia to advance
such log analysis capabilities using cutting edge techniques. We make this
sample dataset available to support designing new machine learning methods for
malware detection, especially for automatic detection of generic malicious
behavior. The dataset has been collected in cooperation between Avast Software
and Czech Technical University - AI Center (AIC). </font><br> Link: <a href='http://arxiv.org/pdf/2209.03188v1' target="_blank">http://arxiv.org/pdf/2209.03188v1</a><br> <br> <br> <font size='5'> 594 </font> <div style="text-align: right"> 2022-09-06 13:18:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Framework for Highway Traffic Profiling using Connected Vehicle Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The connected vehicle (CV) data could potentially revolutionize the traffic
monitoring landscape as a new source of CV data that are collected exclusively
from original equipment manufactures (OEMs) have emerged in the commercial
market in recent years. Compared to existing CV data that are used by agencies,
the new-generation of CV data have certain advantages including nearly
ubiquitous coverage, high temporal resolution, high spatial accuracy, and
enriched vehicle telematics data (e.g., hard braking events). This paper
proposed a traffic profiling framework that target vehicle-level performance
indexes across mobility, safety, riding comfort, traffic flow stability, and
fuel consumption. The proof-of-concept study of a major interstate highway
(i.e., I-280 NJ), using the CV data, illustrates the feasibility of going
beyond traditional aggregated traffic metrics. Lastly, potential applications
for either historical analysis and even near real-time monitoring are
discussed. The proposed framework can be easily scaled and is particularly
valuable for agencies that wish to systemically monitoring regional or
statewide roadways without substantial investment on infrastructure-based
sensing (and the associated on-going maintenance costs) </font><br> Link: <a href='http://arxiv.org/pdf/2209.14245v1' target="_blank">http://arxiv.org/pdf/2209.14245v1</a><br> <br> <br> <font size='5'> 595 </font> <div style="text-align: right"> 2022-09-06 09:59:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bidirectional coupling of a long-term integrated assessment model REMIND v3.0.0 with an hourly power sector model DIETER v1.0.2</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Integrated assessment models (IAMs) are a central tool for the quantitative
analysis of climate change mitigation strategies. However, due to their global,
cross-sectoral and centennial scope, IAMs cannot explicitly represent the
spatio-temporal detail required to properly analyze the key role of variable
renewable electricity (VRE) for decarbonizing the power sector and end-use
electrification. In contrast, power sector models (PSMs) incorporate high
spatio-temporal resolutions, but tend to have narrower scopes and shorter time
horizons. To overcome these limitations, we present a novel methodology: an
iterative and fully automated soft-coupling framework that combines the
strengths of a IAM and a PSM. This framework uses the market values of power
generation as well as the capture prices of demand in the PSM as price signals
that change the capacity and power mix of the IAM. Hence, both models make
endogenous investment decisions, leading to a joint solution. We apply the
method to Germany in a proof-of-concept study using the IAM REMIND and the PSM
DIETER, and confirm the theoretical prediction of almost-full convergence both
in terms of decision variables and (shadow) prices. At the end of the iterative
process, the absolute model difference between the generation shares of any
generator type for any year is <5% for a simple configuration (no storage, no
flexible demand), and 6-7% for a more realistic and detailed configuration
(with storage and flexible demand). For the simple configuration, we
mathematically show that this coupling scheme corresponds uniquely to an
iterative mapping of the Lagrangians of two power sector optimization problems
of different time resolutions, which can lead to a comprehensive model
convergence of both decision variables and (shadow) prices. Since our approach
is based on fundamental economic principles, it is applicable also to other
IAM-PSM pairs. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02340v3' target="_blank">http://arxiv.org/pdf/2209.02340v3</a><br> <br> <br> <font size='5'> 596 </font> <div style="text-align: right"> 2022-09-05 12:05:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Newly Developed Flexible Grid Trading Model Combined ANN and SSO algorithm</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In modern society, the trading methods and strategies used in financial
market have gradually changed from traditional on-site trading to electronic
remote trading, and even online automatic trading performed by a pre-programmed
computer programs because the continuous development of network and computer
computing technology. The quantitative trading, which the main purpose is to
automatically formulate people's investment decisions into a fixed and
quantifiable operation logic that eliminates all emotional interference and the
influence of subjective thoughts and applies this logic to financial market
activities in order to obtain excess profits above average returns, has led a
lot of attentions in financial market. The development of self-adjustment
programming algorithms for automatically trading in financial market has
transformed a top priority for academic research and financial practice. Thus,
a new flexible grid trading model combined with the Simplified Swarm
Optimization (SSO) algorithm for optimizing parameters for various market
situations as input values and the fully connected neural network (FNN) and
Long Short-Term Memory (LSTM) model for training a quantitative trading model
to automatically calculate and adjust the optimal trading parameters for
trading after inputting the existing market situation is developed and studied
in this work. The proposed model provides a self-adjust model to reduce
investors' effort in the trading market, obtains outperformed investment return
rate and model robustness, and can properly control the balance between risk
and return. </font><br> Link: <a href='http://arxiv.org/pdf/2211.12839v1' target="_blank">http://arxiv.org/pdf/2211.12839v1</a><br> <br> <br> <font size='5'> 597 </font> <div style="text-align: right"> 2022-09-03 07:04:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How To Start a Grassroots Movement</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the influence of social messages that promote a digital public good,
a COVID-19 tracing app. We vary whether subjects receive a digital message from
another subject, and, if so, at what cost it came. Observed maximum willingness
to invest in sending varies, from 1 cent up to 20 euros. Does this affect
receivers' sending behavior? Willingness to invest in sending increases when
previously receiving the message. Yet, cost signals have no impact. Thus,
grassroots movements can be started at virtually no cost. App-support matters
normatively as non-supporters are supposed to be punished in triage. </font><br> Link: <a href='http://arxiv.org/pdf/2209.01345v1' target="_blank">http://arxiv.org/pdf/2209.01345v1</a><br> <br> <br> <font size='5'> 598 </font> <div style="text-align: right"> 2022-09-02 02:32:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Development of Integral Field Spectrographs to Revolutionize Spectroscopic Observations of Solar Flares and other Energetic Solar Eruptions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Sun's proximity offers us a unique opportunity to study in detail the
physical processes on a star's surface; however, the highly dynamic nature of
the stellar surface -- in particular, energetic eruptions such as flares and
coronal mass ejections -- presents tremendous observational challenges.
Spectroscopy probes the physical state of the solar atmosphere, but
conventional scanning spectrographs and spectrometers are unable to capture the
full evolutionary history of these dynamic events with a sufficiently wide
field of view and high spatial, spectral, and temporal resolution. Resolving
the physics of the dynamic sun requires gathering simultaneous spectra across a
contiguous area over the full duration of these events, a goal now
tantalizingly close to achievable with continued investment in developing
powerful new Integral Field Spectrographs to serve as the foundation of both
future ground- and space-based missions. This technology promises to
revolutionize our ability to study solar flares and CMEs, addressing NASA's
strategic objective to "understand the Sun, solar system, and universe." Since
such events generate electromagnetic radiation and high-energy particles that
disrupt terrestrial electric infrastructure, this investment not only advances
humanity's scientific endeavors but also enhances our space weather forecasting
capability to protect against threats to our technology-reliant civilization. </font><br> Link: <a href='http://arxiv.org/pdf/2209.00788v1' target="_blank">http://arxiv.org/pdf/2209.00788v1</a><br> <br> <br> <font size='5'> 599 </font> <div style="text-align: right"> 2022-09-01 15:32:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Shifting Attention of Political Leaders: Evidence from Two Centuries of Presidential Speeches</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We use natural-language-processing algorithms on a novel dataset of over 900
presidential speeches from ten Latin American countries spanning two centuries
to study the dynamics and determinants of presidential policy priorities. We
show that most speech content can be characterized by a compact set of policy
issues whose relative composition exhibited slow yet substantial shifts over
1819-2022. Presidential attention initially centered on military interventions
and the development of state capacity. Attention gradually evolved towards
building physical capital through investments in infrastructure and public
services and finally turned towards building human capital through investments
in education, health, and social safety nets. We characterize the way in which
president-level characteristics, like age and gender, predict the main policy
issues. Our findings offer novel insights into the dynamics of presidential
attention and the factors that shape it, expanding our understanding of
political agenda-setting. </font><br> Link: <a href='http://arxiv.org/pdf/2209.00540v3' target="_blank">http://arxiv.org/pdf/2209.00540v3</a><br> <br> <br> <font size='5'> 600 </font> <div style="text-align: right"> 2022-09-01 14:04:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Outsourcing Memory Through Niche Construction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Adaptation to changing environments is a universal feature of life and can
involve the organism modifying itself in response to the environment as well as
actively modifying the environment to control selection pressures. The latter
case couples the organism to environment. Then, how quickly should the organism
change in response to the environment? We formulate this question in terms of
how memory duration scales with environmental rate of change when there are
trade-offs in remembering vs. forgetting. We derive a universal scaling law for
optimal memory duration, taking into account memory precision as well as two
components of environmental volatility, bias and stability. We find sublinear
scaling with any amount of environmental volatility. We use a memory complexity
measure to explore the strategic conditions (game dynamics) favoring actively
reducing environmental volatility -- outsourcing memory through niche
construction -- over investing in neural tissue. We predict stabilizing niche
construction will evolve when neural tissue is costly, the environment is
variable, and it is beneficial to be able to encode a rich repertoire of
environmental states. </font><br> Link: <a href='http://arxiv.org/pdf/2209.00476v2' target="_blank">http://arxiv.org/pdf/2209.00476v2</a><br> <br> <br> <font size='5'> 601 </font> <div style="text-align: right"> 2022-09-01 06:05:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Crowd-Funded Earthquake Early-Warning System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Earthquake early warning systems has been proven to save countless lives in
Japan, Mexico, and Chile, where earthquake warnings are often broadcast live on
TV up to a minute before residents experience shaking.
  Unfortunately, traditional early warning systems require extensive capital
investment. The high cost of traditional earthquake early-warning systems and
limited budgets prevent earthquake-prone developing countries like the
Philippines, Indonesia, Afghanistan, India, Burma, Ghana, Nigeria, Columbia,
Venezuela, and Bolivia from building traditional earthquake warning systems.
  This project describes repurposing old Android smartphones into affordable
dedicated seismometers to detect tremors. These smartphones have become
disposable items and are continuously "upgraded" and replaced. Yet every one of
these devices includes everything needed to act as a dedicated seismometer:
Wi-Fi capability, GPS, and an accelerometer. The software developed for this
project converts these smartphones into dedicated seismometers and uses
existing web technologies for telemetry services. This system would also
trigger alerts to all devices that have the software installed whenever a
tremor is detected, effectively making each seismic detection station double as
an earthquake early-warning alarm.
  A large network of these seismic detection stations will effectively create
an affordable earthquake early-warning system that can be rapidly implemented
at an extremely low cost. It would provide developing nations an affordable
life-saving alternative to expensive traditional earthquake early-warning
systems. This solution is cheap, keeps old smartphones from landfills, and will
save lives. </font><br> Link: <a href='http://arxiv.org/pdf/2209.02416v1' target="_blank">http://arxiv.org/pdf/2209.02416v1</a><br> <br> <br> <font size='5'> 602 </font> <div style="text-align: right"> 2022-08-31 21:21:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: 150 Years of Return Predictability Around the World: A Holistic View</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Using new annual data of 16 developed countries across bond, equity, and
housing markets, I study the return predictability using the payout-price
ratios, i.e., coupon price, dividend price, and rent price. None of the 48
country-asset combinations shows consistent in-sample and out-of-sample
performance with positive utility gain for the mean-variance investor. Only 3
(4/2) countries show positive economic gains in their equity (housing/bond)
markets. The return predictability for the representative agents' risky asset
portfolios and wealth portfolios is even weaker, suggesting that timing the
investment return of a country using payout-price ratios will not make the
investors better off. The predictive regressions based on the VAR analysis by
Cochrane (2008, 2011) suggest that 14 (5) countries have predictable payout
growth in the equity (housing) markets, ex., the dividend price predicts the
dividend growth in the US. The VAR simulation using data from all the countries
does not reject the null that the dividend growth is predictable. This paper
presents firm evidence against the return predictability based on payout
ratios. </font><br> Link: <a href='http://arxiv.org/pdf/2209.00121v1' target="_blank">http://arxiv.org/pdf/2209.00121v1</a><br> <br> <br> <font size='5'> 603 </font> <div style="text-align: right"> 2022-08-31 09:51:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Quantum Online Portfolio Optimization Algorithm</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Portfolio optimization plays a central role in finance to obtain optimal
portfolio allocations that aim to achieve certain investment goals. Over the
years, many works have investigated different variants of portfolio
optimization. Portfolio optimization also provides a rich area to study the
application of quantum computers to obtain advantages over classical computers.
In this work, we give a sampling version of an existing classical online
portfolio optimization algorithm by Helmbold et al., for which we in turn
develop a quantum version. The quantum advantage is achieved by using
techniques such as quantum state preparation, inner product estimation and
multi-sampling. Our quantum algorithm provides a quadratic speedup in the time
complexity, in terms of $n$, where $n$ is the number of assets in the
portfolio. The transaction cost of both of our classical and quantum algorithms
is independent of $n$ which is especially useful for practical applications
with a large number of assets. </font><br> Link: <a href='http://arxiv.org/pdf/2208.14749v2' target="_blank">http://arxiv.org/pdf/2208.14749v2</a><br> <br> <br> <font size='5'> 604 </font> <div style="text-align: right"> 2022-08-30 19:05:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cost-Efficient Deployment of a Reliable Multi-UAV Unmanned Aerial System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we study the trade-off between the reliability and the
investment cost of an unmanned aerial system (UAS) consisting of a set of
unmanned aerial vehicles (UAVs) carrying radio access nodes, called portable
access points (PAPs)), deployed to serve a set of ground nodes (GNs). Using the
proposed algorithm, a given geographical region is equivalently represented as
a set of circular regions, where each circle represents the coverage region of
a PAP. Then, the steady-state availability of the UAS is analytically derived
by modelling it as a continuous time birth-death Markov decision process (MDP).
Numerical evaluations show that the investment cost to guarantee a given
steady-state availability to a set of GNs can be reduced by considering the
traffic demand and distribution of GNs. </font><br> Link: <a href='http://arxiv.org/pdf/2208.14503v1' target="_blank">http://arxiv.org/pdf/2208.14503v1</a><br> <br> <br> <font size='5'> 605 </font> <div style="text-align: right"> 2022-08-30 11:11:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Constrained portfolios in incomplete markets: a dynamic programming approach to Heston's model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We solve an expected utility-maximization problem with terminal-wealth
constraints via dynamic programming in a setting of incomplete markets due to
stochastic volatility. We demonstrate that the value function in the
constrained problem can be represented as an expected modified utility of a
vega-neutral financial derivative on the optimal unconstrained wealth. The
optimal wealth and the optimal investment strategy in the constrained problem
follow similarly. The case of a power utility and a Value-at-Risk constraint is
treated theoretically in details. In numerical studies, we substantiate the
impact of risk aversion levels, and investment horizons on the optimal
investment strategy. We find a 20% relative difference between constrained and
unconstrained allocations for average parameters in a low risk-aversion,
short-horizon setting. </font><br> Link: <a href='http://arxiv.org/pdf/2208.14152v1' target="_blank">http://arxiv.org/pdf/2208.14152v1</a><br> <br> <br> <font size='5'> 606 </font> <div style="text-align: right"> 2022-08-29 20:54:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Toward a Mathematical Vulnerability Propagation and Defense Model in Smart Grid Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For reducing threat propagation within an inter-connected network, it is
essential to distribute the defense investment optimally. Most electric power
utilities are resource constrained, yet how to account for costs while
designing threat reduction techniques is not well understood. Hence, in this
work, a vulnerability propagation and a defense model is proposed based on an
epidemic model. The new defense mechanism is then validated through sensitivity
of the propagation parameters on the optimal investment with two-node and
N-node cases. Further, the model efficacy is evaluated with implementation in
one of the communication networks of a cyber-physical power system. Topological
impact on the optimal nodal investment is also emphasized. Optimal investment
of the neighbors with less degree were found to be highly sensitive to
fluctuation in vulnerability exploitability probability. </font><br> Link: <a href='http://arxiv.org/pdf/2208.13884v1' target="_blank">http://arxiv.org/pdf/2208.13884v1</a><br> <br> <br> <font size='5'> 607 </font> <div style="text-align: right"> 2022-08-29 14:45:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021 Topical Group Report from AF07: Targets and Sources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For the next multi-megawatt accelerator generation, targets and other
beam-intercepting components will face even more severe challenges due to the
higher power densities, higher energy, and higher radiation. A comprehensive
research and development program must be implemented to address the challenges.
International collaborations such as RaDIATE or High Power Targetry Workshop
should be leveraged and expanded to accelerate knowledge gain and avoid
duplicated efforts. The next generation of high power targets will need novel
designs allowing better high heat flux cooling methods, novel materials,
advanced simulations, and better instrumentation. Four main R&D tracks were
identified and discussed for targets. For electron sources, current state of
art and challenges we are facing for key components such as cathodes, guns and
injector design for sources R&D are presented for highly polarized electrons
and unpolarized high current high brightness beams. For ion sources such as
electron cyclotron resonance ion source (ECRIS), laser ion source (LIS), charge
breeders, status of development are reviewed, and areas require investment for
future R&D are discussed separately. Facilities critical for various source
development are recommended. </font><br> Link: <a href='http://arxiv.org/pdf/2208.13641v1' target="_blank">http://arxiv.org/pdf/2208.13641v1</a><br> <br> <br> <font size='5'> 608 </font> <div style="text-align: right"> 2022-08-27 07:46:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Conditional investment strategy in evolutionary trust games with repeated group interactions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It has a long tradition to study trust behavior among humans or artificial
agents by investigating the trust game. Although previous studies based on
evolutionary game theory have revealed that trust and trustworthiness can be
promoted if network structure or reputation is considered, they often assume
that interactions among agents are one-shot and investors do not consider the
investment environment before making decisions, which collide with many
realistic situations. In this paper, we introduce the conditional investment
strategy into the repeated N-player trust game, in which conditional investors
decide to invest or not depending on their assessment of the trustworthiness
level of the group. By using the approach of the Markov decision process, we
study the evolutionary dynamics of trust in repeated group interactions with
the conditional investment strategy. We find that conditional investors can
form an effective alliance with trustworthy trustees, hence they can sweep out
untrustworthy trustees. Moreover, we verify that such alliance can exist in a
wide range of model parameters. These results may explain why trusting in
others and reciprocating them with trustworthy actions can be sustained in game
interactions among intelligent agents. </font><br> Link: <a href='http://arxiv.org/pdf/2208.12953v1' target="_blank">http://arxiv.org/pdf/2208.12953v1</a><br> <br> <br> <font size='5'> 609 </font> <div style="text-align: right"> 2022-08-26 15:09:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Federated and Privacy-Preserving Learning of Accounting Data in Financial Statement Audits</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ongoing 'digital transformation' fundamentally changes audit evidence's
nature, recording, and volume. Nowadays, the International Standards on
Auditing (ISA) requires auditors to examine vast volumes of a financial
statement's underlying digital accounting records. As a result, audit firms
also 'digitize' their analytical capabilities and invest in Deep Learning (DL),
a successful sub-discipline of Machine Learning. The application of DL offers
the ability to learn specialized audit models from data of multiple clients,
e.g., organizations operating in the same industry or jurisdiction. In general,
regulations require auditors to adhere to strict data confidentiality measures.
At the same time, recent intriguing discoveries showed that large-scale DL
models are vulnerable to leaking sensitive training data information. Today, it
often remains unclear how audit firms can apply DL models while complying with
data protection regulations. In this work, we propose a Federated Learning
framework to train DL models on auditing relevant accounting data of multiple
clients. The framework encompasses Differential Privacy and Split Learning
capabilities to mitigate data confidentiality risks at model inference. We
evaluate our approach to detect accounting anomalies in three real-world
datasets of city payments. Our results provide empirical evidence that auditors
can benefit from DL models that accumulate knowledge from multiple sources of
proprietary client data. </font><br> Link: <a href='http://arxiv.org/pdf/2208.12708v1' target="_blank">http://arxiv.org/pdf/2208.12708v1</a><br> <br> <br> <font size='5'> 610 </font> <div style="text-align: right"> 2022-08-26 12:41:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Symbolic Explanation of Affinity-Based Reinforcement Learning Agents with Markov Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The proliferation of artificial intelligence is increasingly dependent on
model understanding. Understanding demands both an interpretation - a human
reasoning about a model's behavior - and an explanation - a symbolic
representation of the functioning of the model. Notwithstanding the imperative
of transparency for safety, trust, and acceptance, the opacity of
state-of-the-art reinforcement learning algorithms conceals the rudiments of
their learned strategies. We have developed a policy regularization method that
asserts the global intrinsic affinities of learned strategies. These affinities
provide a means of reasoning about a policy's behavior, thus making it
inherently interpretable. We have demonstrated our method in personalized
prosperity management where individuals' spending behavior in time dictate
their investment strategies, i.e. distinct spending personalities may have
dissimilar associations with different investment classes. We now explain our
model by reproducing the underlying prototypical policies with discretized
Markov models. These global surrogates are symbolic representations of the
prototypical policies. </font><br> Link: <a href='http://arxiv.org/pdf/2208.12627v2' target="_blank">http://arxiv.org/pdf/2208.12627v2</a><br> <br> <br> <font size='5'> 611 </font> <div style="text-align: right"> 2022-08-26 10:36:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stock Market Prediction using Natural Language Processing -- A Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The stock market is a network which provides a platform for almost all major
economic transactions. While investing in the stock market is a good idea,
investing in individual stocks may not be, especially for the casual investor.
Smart stock-picking requires in-depth research and plenty of dedication.
Predicting this stock value offers enormous arbitrage profit opportunities.
This attractiveness of finding a solution has prompted researchers to find a
way past problems like volatility, seasonality, and dependence on time. This
paper surveys recent literature in the domain of natural language processing
and machine learning techniques used to predict stock market movements. The
main contributions of this paper include the sophisticated categorizations of
many recent articles and the illustration of the recent trends of research in
stock market prediction and its related areas. </font><br> Link: <a href='http://arxiv.org/pdf/2208.13564v1' target="_blank">http://arxiv.org/pdf/2208.13564v1</a><br> <br> <br> <font size='5'> 612 </font> <div style="text-align: right"> 2022-08-25 18:33:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Incrementality Bidding and Attribution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The causal effect of showing an ad to a potential customer versus not,
commonly referred to as "incrementality", is the fundamental question of
advertising effectiveness. In digital advertising three major puzzle pieces are
central to rigorously quantifying advertising incrementality: ad
buying/bidding/pricing, attribution, and experimentation. Building on the
foundations of machine learning and causal econometrics, we propose a
methodology that unifies these three concepts into a computationally viable
model of both bidding and attribution which spans the randomization, training,
cross validation, scoring, and conversion attribution of advertising's causal
effects. Implementation of this approach is likely to secure a significant
improvement in the return on investment of advertising. </font><br> Link: <a href='http://arxiv.org/pdf/2208.12809v1' target="_blank">http://arxiv.org/pdf/2208.12809v1</a><br> <br> <br> <font size='5'> 613 </font> <div style="text-align: right"> 2022-08-24 14:29:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving on the Markov-Switching Regression Model by the Use of an Adaptive Moving Average</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Regime detection is vital for the effective operation of trading and
investment strategies. However, the most popular means of doing this, the
two-state Markov-switching regression model (MSR), is not an optimal solution,
as two volatility states do not fully capture the complexity of the market.
Past attempts to extend this model to a multi-state MSR have proved unstable,
potentially expensive in terms of trading costs, and can only divide the market
into states with varying levels of volatility, which is not the only aspect of
market dynamics relevant to trading. We demonstrate it is possible and valuable
to instead segment the market into more than two states not on the basis of
volatility alone, but on a combined basis of volatility and trend, by combining
the two-state MSR with an adaptive moving average. A realistic trading
framework is used to demonstrate that using two selected states from the four
thus generated leads to better trading performance than traditional benchmarks,
including the two-state MSR. In addition, the proposed model could serve as a
label generator for machine learning tasks used in predicting financial regimes
ex ante. </font><br> Link: <a href='http://arxiv.org/pdf/2208.11574v1' target="_blank">http://arxiv.org/pdf/2208.11574v1</a><br> <br> <br> <font size='5'> 614 </font> <div style="text-align: right"> 2022-08-23 22:25:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Investment in the common good: free rider effect and the stability of mixed strategy equilibria</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the game of investment in the common good, the free rider problem can
delay the stakeholders' actions in the form of a mixed strategy equilibrium.
However, it has been recently shown that the mixed strategy equilibria of the
stochastic war of attrition are destabilized by even the slightest degree of
asymmetry between the players. Such extreme instability is contrary to the
widely accepted notion that a mixed strategy equilibrium is the hallmark of the
war of attrition. Motivated by this quandary, we search for a mixed strategy
equilibrium in a stochastic game of investment in the common good. Our results
show that, despite asymmetry, a mixed strategy equilibrium exists if the model
takes into account the repeated investment opportunities. The mixed strategy
equilibrium disappears only if the asymmetry is sufficiently high. Since the
mixed strategy equilibrium is less efficient than pure strategy equilibria, it
behooves policymakers to prevent it by promoting a sufficiently high degree of
asymmetry between the stakeholders through, for example, asymmetric subsidy. </font><br> Link: <a href='http://arxiv.org/pdf/2208.11217v1' target="_blank">http://arxiv.org/pdf/2208.11217v1</a><br> <br> <br> <font size='5'> 615 </font> <div style="text-align: right"> 2022-08-23 08:26:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Limit Orders and Knightian Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A range of empirical puzzles in finance has been explained as a consequence
of traders being averse to ambiguity. Ambiguity averse traders can behave in
financial portfolio problems in ways that cannot be rationalized as maximizing
subjective expected utility. However, this paper shows that when traders have
access to limit orders, all investment behavior of an ambiguity-averse
decision-maker is observationally equivalent to the behavior of a subjective
expected utility maximizer with the same risk preferences; ambiguity aversion
has no additional explanatory power. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10804v1' target="_blank">http://arxiv.org/pdf/2208.10804v1</a><br> <br> <br> <font size='5'> 616 </font> <div style="text-align: right"> 2022-08-23 05:13:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust control problems of BSDEs coupled with value functions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A robust control problem is considered in this paper, where the controlled
stochastic differential equations (SDEs) include ambiguity parameters and their
coefficients satisfy non-Lipschitz continuous and non-linear growth conditions,
the objective function is expressed as a backward stochastic differential
equation (BSDE) with the generator depending on the value function. We
establish the existence and uniqueness of the value function in a proper space
and provide a verification theorem. Moreover, we apply the results to solve two
typical optimal investment problems in the market with ambiguity, one of which
is with Heston stochastic volatility model. In particular, we establish some
sharp estimations for Heston model with ambiguity parameters. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10735v1' target="_blank">http://arxiv.org/pdf/2208.10735v1</a><br> <br> <br> <font size='5'> 617 </font> <div style="text-align: right"> 2022-08-22 17:33:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Shapelet-Based Counterfactual Explanations for Multivariate Time Series</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As machine learning and deep learning models have become highly prevalent in
a multitude of domains, the main reservation in their adoption for
decision-making processes is their black-box nature. The Explainable Artificial
Intelligence (XAI) paradigm has gained a lot of momentum lately due to its
ability to reduce models opacity. XAI methods have not only increased
stakeholders' trust in the decision process but also helped developers ensure
its fairness. Recent efforts have been invested in creating transparent models
and post-hoc explanations. However, fewer methods have been developed for time
series data, and even less when it comes to multivariate datasets. In this
work, we take advantage of the inherent interpretability of shapelets to
develop a model agnostic multivariate time series (MTS) counterfactual
explanation algorithm. Counterfactuals can have a tremendous impact on making
black-box models explainable by indicating what changes have to be performed on
the input to change the final decision. We test our approach on a real-life
solar flare prediction dataset and prove that our approach produces
high-quality counterfactuals. Moreover, a comparison to the only MTS
counterfactual generation algorithm shows that, in addition to being visually
interpretable, our explanations are superior in terms of proximity, sparsity,
and plausibility. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10462v1' target="_blank">http://arxiv.org/pdf/2208.10462v1</a><br> <br> <br> <font size='5'> 618 </font> <div style="text-align: right"> 2022-08-22 16:46:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do diverse and inclusive workplaces benefit investors? An Empirical Analysis on Europe and the United States</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As the COVID-19 pandemic restrictions slow down, employees start to return to
their offices. Hence, the discussions on optimal workplaces and issues of
diversity and inclusion have peaked. Previous research has shown that employees
and companies benefit from positive workplace changes. This research questions
whether allowing for diversity and inclusion criteria in portfolio construction
is beneficial to investors. By considering the new Diversity & Inclusion (D&I)
score by Refinitiv, I find evidence that investors might suffer lower returns
and pay for investing in responsible (i.e., more diverse and inclusive)
employers in both the US and European market. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10435v3' target="_blank">http://arxiv.org/pdf/2208.10435v3</a><br> <br> <br> <font size='5'> 619 </font> <div style="text-align: right"> 2022-08-19 12:54:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockchain-based traffic management for Advanced Air Mobility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The large public interest in Advanced Air Mobility (AAM) will soon lead to
congested skies overhead cities, analogously to what happened with other
transportation means, including commercial aviation. In the latter case, the
combination of large distances and demanded number flights is such that a
system with centralized control, with most of the decisions made by human
operators, is safe. However, for AAM, it is expected a much higher demand,
because it will be used for people's daily commutes. Thus, higher automation
levels will become a requirement for coordinating this traffic, which might not
be effectively managed by humans. The establishment of fixed air routes can
abate complexity, however at the cost of limiting capacity and decreasing
efficiency. Another alternative is the use of a powerful central system based
on Artificial Intelligence (AI), which would allow flexible trajectories and
higher efficiency. However, such system would require concentrated investment,
could contain Single-Points-of-Failure (SPoFs), would be a highly sought target
of malicious attacks, and would be subject to periods of unavailability.
  This work proposes a new technology that solves the problem of managing the
high complexity of the AAM traffic with a secure distributed approach, without
the need for a proprietary centralized automation system. This technology
enables distributed airspace allocation management and conflict resolution by
means of trusted shared data structures and associated smart contracts running
on a blockchain ecosystem. This way, it greatly reduces the risk of system
outages due to SPoFs, by allowing peer-to-peer conflict resolution, and being
more resilient to failures in the ground communication infrastructure.
Furthermore, it provides priority-based balancing mechanisms that help to
regulate fairness among participants in the utilization of the airspace. </font><br> Link: <a href='http://arxiv.org/pdf/2208.09312v1' target="_blank">http://arxiv.org/pdf/2208.09312v1</a><br> <br> <br> <font size='5'> 620 </font> <div style="text-align: right"> 2022-08-19 04:51:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding Volatility Spillover Relationship Among G7 Nations And India During Covid-19</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Purpose: In the context of a COVID pandemic in 2020-21, this paper attempts
to capture the interconnectedness and volatility transmission dynamics. The
nature of change in volatility spillover effects and time-varying conditional
correlation among the G7 countries and India is investigated. Methodology: To
assess the volatility spillover effects, the bivariate BEKK and t- DCC (1,1)
GARCH (1,1) models have been used. Our research shows how the dynamics of
volatility spillover between India and the G7 countries shift before and during
COVID-19. Findings: The findings reveal that the extent of volatility spillover
has altered during COVID compared to the pre-COVID environment. During this
pandemic, a sharp increase in conditional correlation indicates an increase in
systematic risk between countries. Originality: The study contributes to a
better understanding of the dynamics of volatility spillover between G7
countries and India. Asset managers and foreign corporations can use the
changing spillover dynamics to improve investment decisions and implement
effective hedging measures to protect their interests. Furthermore, this
research will assist financial regulators in assessing market risk in the
future owing to crises like as COVID-19. </font><br> Link: <a href='http://arxiv.org/pdf/2208.09148v1' target="_blank">http://arxiv.org/pdf/2208.09148v1</a><br> <br> <br> <font size='5'> 621 </font> <div style="text-align: right"> 2022-08-19 00:20:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Simulation-Informed Revenue Extrapolation with Confidence Estimate for Scaleup Companies Using Scarce Time-Series Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investment professionals rely on extrapolating company revenue into the
future (i.e. revenue forecast) to approximate the valuation of scaleups
(private companies in a high-growth stage) and inform their investment
decision. This task is manual and empirical, leaving the forecast quality
heavily dependent on the investment professionals' experiences and insights.
Furthermore, financial data on scaleups is typically proprietary, costly and
scarce, ruling out the wide adoption of data-driven approaches. To this end, we
propose a simulation-informed revenue extrapolation (SiRE) algorithm that
generates fine-grained long-term revenue predictions on small datasets and
short time-series. SiRE models the revenue dynamics as a linear dynamical
system (LDS), which is solved using the EM algorithm. The main innovation lies
in how the noisy revenue measurements are obtained during training and
inferencing. SiRE works for scaleups that operate in various sectors and
provides confidence estimates. The quantitative experiments on two practical
tasks show that SiRE significantly surpasses the baseline methods by a large
margin. We also observe high performance when SiRE extrapolates long-term
predictions from short time-series. The performance-efficiency balance and
result explainability of SiRE are also validated empirically. Evaluated from
the perspective of investment professionals, SiRE can precisely locate the
scaleups that have a great potential return in 2 to 5 years. Furthermore, our
qualitative inspection illustrates some advantageous attributes of the SiRE
revenue forecasts. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10375v3' target="_blank">http://arxiv.org/pdf/2208.10375v3</a><br> <br> <br> <font size='5'> 622 </font> <div style="text-align: right"> 2022-08-17 19:57:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the evolution of research in hypersonics: application of natural language processing and machine learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Research and development in hypersonics have progressed significantly in
recent years, with various military and commercial applications being
demonstrated increasingly. Public and private organizations in several
countries have been investing in hypersonics, with the aim to overtake their
competitors and secure/improve strategic advantage and deterrence. For these
organizations, being able to identify emerging technologies in a timely and
reliable manner is paramount. Recent advances in information technology have
made it possible to analyze large amounts of data, extract hidden patterns, and
provide decision-makers with new insights. In this study, we focus on
scientific publications about hypersonics within the period of 2000-2020, and
employ natural language processing and machine learning to characterize the
research landscape by identifying 12 key latent research themes and analyzing
their temporal evolution. Our publication similarity analysis revealed patterns
that are indicative of cycles during two decades of research. The study offers
a comprehensive analysis of the research field and the fact that the research
themes are algorithmically extracted removes subjectivity from the exercise and
enables consistent comparisons between topics and between time intervals. </font><br> Link: <a href='http://arxiv.org/pdf/2208.08507v1' target="_blank">http://arxiv.org/pdf/2208.08507v1</a><br> <br> <br> <font size='5'> 623 </font> <div style="text-align: right"> 2022-08-17 15:22:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ban The Box? Information, Incentives, and Statistical Discrimination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: "Banning the Box" refers to a policy campaign aimed at prohibiting employers
from soliciting applicant information that could be used to statistically
discriminate against categories of applicants (in particular, those with
criminal records). In this article, we examine how the concealing or revealing
of informative features about an applicant's identity affects hiring both
directly and, in equilibrium, by possibly changing applicants' incentives to
invest in human capital. We show that there exist situations in which an
employer and an applicant are in agreement about whether to ban the box.
Specifically, depending on the structure of the labor market, banning the box
can be (1) Pareto dominant, (2) Pareto dominated, (3) benefit the applicant
while harming the employer, or (4) benefit the employer while harming the
applicant. Our results have policy implications spanning beyond employment
decisions, including the use of credit checks by landlords and standardized
tests in college admissions. </font><br> Link: <a href='http://arxiv.org/pdf/2208.08348v1' target="_blank">http://arxiv.org/pdf/2208.08348v1</a><br> <br> <br> <font size='5'> 624 </font> <div style="text-align: right"> 2022-08-17 14:03:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Transformer-Based Deep Learning Model for Stock Price Prediction: A Case Study on Bangladesh Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In modern capital market the price of a stock is often considered to be
highly volatile and unpredictable because of various social, financial,
political and other dynamic factors. With calculated and thoughtful investment,
stock market can ensure a handsome profit with minimal capital investment,
while incorrect prediction can easily bring catastrophic financial loss to the
investors. This paper introduces the application of a recently introduced
machine learning model - the Transformer model, to predict the future price of
stocks of Dhaka Stock Exchange (DSE), the leading stock exchange in Bangladesh.
The transformer model has been widely leveraged for natural language processing
and computer vision tasks, but, to the best of our knowledge, has never been
used for stock price prediction task at DSE. Recently the introduction of
time2vec encoding to represent the time series features has made it possible to
employ the transformer model for the stock price prediction. This paper
concentrates on the application of transformer-based model to predict the price
movement of eight specific stocks listed in DSE based on their historical daily
and weekly data. Our experiments demonstrate promising results and acceptable
root mean squared error on most of the stocks. </font><br> Link: <a href='http://arxiv.org/pdf/2208.08300v1' target="_blank">http://arxiv.org/pdf/2208.08300v1</a><br> <br> <br> <font size='5'> 625 </font> <div style="text-align: right"> 2022-08-17 09:19:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generic catastrophic poverty when selfish investors exploit a degradable common resource</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The productivity of a common pool of resources may degrade when overly
exploited by a number of selfish investors, a situation known as the tragedy of
the commons (TOC). Without regulations, agents optimize the size of their
individual investments into the commons by balancing incurring costs with the
returns received. The resulting Nash equilibrium involves a self-consistency
loop between individual investment decisions and the state of the commons. As a
consequence, several non-trivial properties emerge. For $N$ investing actors we
prove rigorously that typical payoffs do not scale as $1/N$, the expected
result for cooperating agents, but as $(1/N)^2$. Payoffs are hence reduced with
regard to the functional dependence on $N$, a situation denoted catastrophic
poverty. We show that catastrophic poverty results from a fine-tuned balance
between returns and costs. Additionally, a finite number of oligarchs may be
present. Oligarchs are characterized by payoffs that are finite and not
decreasing when $N$ increases. Our results hold for generic classes of models,
including convex and moderately concave cost functions. For strongly concave
cost functions the Nash equilibrium undergoes a collective reorganization,
being characterized instead by entry barriers and sudden death forced market
exits. </font><br> Link: <a href='http://arxiv.org/pdf/2208.08171v2' target="_blank">http://arxiv.org/pdf/2208.08171v2</a><br> <br> <br> <font size='5'> 626 </font> <div style="text-align: right"> 2022-08-16 20:46:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TRoVE: Transforming Road Scene Datasets into Photorealistic Virtual Environments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: High-quality structured data with rich annotations are critical components in
intelligent vehicle systems dealing with road scenes. However, data curation
and annotation require intensive investments and yield low-diversity scenarios.
The recently growing interest in synthetic data raises questions about the
scope of improvement in such systems and the amount of manual work still
required to produce high volumes and variations of simulated data. This work
proposes a synthetic data generation pipeline that utilizes existing datasets,
like nuScenes, to address the difficulties and domain-gaps present in simulated
datasets. We show that using annotations and visual cues from existing
datasets, we can facilitate automated multi-modal data generation, mimicking
real scene properties with high-fidelity, along with mechanisms to diversify
samples in a physically meaningful way. We demonstrate improvements in mIoU
metrics by presenting qualitative and quantitative experiments with real and
synthetic data for semantic segmentation on the Cityscapes and KITTI-STEP
datasets. All relevant code and data is released on github
(https://github.com/shubham1810/trove_toolkit). </font><br> Link: <a href='http://arxiv.org/pdf/2208.07943v1' target="_blank">http://arxiv.org/pdf/2208.07943v1</a><br> <br> <br> <font size='5'> 627 </font> <div style="text-align: right"> 2022-08-16 18:52:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Introduo a otimizao de Portflio</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper we present modern portfolio theory using basic concepts of
linear algebra, differential calculus, statistics and optimization. This theory
allows us to measure the return and risk of the investment portfolio, helping
to make decisions in the financial market. As an application, we will present a
simple investment strategy that aims to minimize the risk of investing in two
assets </font><br> Link: <a href='http://arxiv.org/pdf/2208.07909v1' target="_blank">http://arxiv.org/pdf/2208.07909v1</a><br> <br> <br> <font size='5'> 628 </font> <div style="text-align: right"> 2022-08-16 09:03:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal portfolio selection of many players under relative performance criteria in the market model with random coefficients</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the optimal portfolio selection problem under relative performance
criteria in the market model with random coefficients from the perspective of
many players game theory. We consider five random coefficients which consist of
three market parameters which are used in the risky asset price modeling and
two preference parameters which are related to risk attitude and impact of
relative performance. We focus on two cases; either all agents have Constant
Absolute Risk Aversion (CARA) risk preferences or all agents have Constant
Relative Risk Aversion (CRRA) risk preferences for their investment
optimization problem. For each case, we show that the forward Nash equilibrium
and the mean field equilibrium exist for the n-agent game and the corresponding
mean field stochastic optimal control problem, respectively. To extend the
n-agent game to the continuum of players game, we introduce a measure dependent
forward relative performance process and apply an optimization over controlled
dynamics of McKean-Vlasov type. We conclude that our optimal portfolio formulas
extend the corresponding results of the market model with constant
coefficients. </font><br> Link: <a href='http://arxiv.org/pdf/2209.07411v1' target="_blank">http://arxiv.org/pdf/2209.07411v1</a><br> <br> <br> <font size='5'> 629 </font> <div style="text-align: right"> 2022-08-14 03:48:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A 100% Renewable Energy System: Enabling Zero CO2 Emission Offshore Platforms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The total electricity consumption from offshore oil/gas platforms is around
16 TWh worldwide in 2019. The majority offshore platforms are powered by the
diesel generators while the rest mainly uses gas turbines, which emits large
amounts of CO2 per year. The fast development of offshore wind turbines (WT)
can potentially replace traditional fossil fuel based resources to power
offshore loads. Thus, a novel offshore hybrid renewable energy sources (OHRES)
system is proposed to enable a zero CO2 emission offshore platform mitigating
climate change. Battery energy storage system (BESS) and hydrogen energy
storage system (HESS) are considered to mitigate the fluctuation of wind power
in the proposed OHRES system. Resilience models are designed to enhance the
resilience of the proposed OHRES system with extra energy stored in BESS and/or
HESS. Case studies demonstrate the feasibility of the proposed OHRES system to
power offshore platforms. The economic analysis reports the planning cost for
the proposed OHRES system under different resilience levels, which may benefit
the decision to balance the carbon emission and investment cost. </font><br> Link: <a href='http://arxiv.org/pdf/2208.06771v1' target="_blank">http://arxiv.org/pdf/2208.06771v1</a><br> <br> <br> <font size='5'> 630 </font> <div style="text-align: right"> 2022-08-13 19:57:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Near-Optimal Algorithm for Univariate Zeroth-Order Budget Convex Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies a natural generalization of the problem of minimizing a
univariate convex function $f$ by querying its values sequentially. At each
time-step $t$, the optimizer can invest a budget $b_t$ in a query point $X_t$
of their choice to obtain a fuzzy evaluation of $f$ at $X_t$ whose accuracy
depends on the amount of budget invested in $X_t$ across times. This setting is
motivated by the minimization of objectives whose values can only be determined
approximately through lengthy or expensive computations. We design an any-time
parameter-free algorithm called Dyadic Search, for which we prove near-optimal
optimization error guarantees. As a byproduct of our analysis, we show that the
classical dependence on the global Lipschitz constant in the error bounds is an
artifact of the granularity of the budget. Finally, we illustrate our
theoretical findings with numerical simulations. </font><br> Link: <a href='http://arxiv.org/pdf/2208.06720v2' target="_blank">http://arxiv.org/pdf/2208.06720v2</a><br> <br> <br> <font size='5'> 631 </font> <div style="text-align: right"> 2022-08-11 21:33:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automated Market Making and Loss-Versus-Rebalancing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the market microstructure of constant function market makers
(CFMMs) from the perspective of passive liquidity providers (LPs). In a
Black-Scholes setting, we compare the CFMM's performance to that of a
rebalancing strategy, which replicates the CFMM's trades at market prices. The
CFMM systematically underperforms the rebalancing strategy, because it executes
all trades at worse-than-market prices. The performance gap between the two
strategies, "loss-versus-rebalancing" (LVR, pronounced "lever"), depends on the
volatility of the underlying asset and the marginal liquidity of the CFMM
bonding function. Our model's expressions for CFMM losses match actual losses
from the Uniswap v2 WETH-USDC pair. LVR provides tradeable insight in both the
ex ante and ex post assessment of CFMM LP investment decisions, and can also
inform the design of CFMM protocols. </font><br> Link: <a href='http://arxiv.org/pdf/2208.06046v3' target="_blank">http://arxiv.org/pdf/2208.06046v3</a><br> <br> <br> <font size='5'> 632 </font> <div style="text-align: right"> 2022-08-11 13:50:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessments in Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Assessments such as standardized tests and teacher evaluations of students'
classroom participation are central elements of most educational systems.
Assessments inform the student, parent, teacher, and school about the student
learning progress. Individuals use the information to adjust their study
efforts and to make guide their course choice. Schools and teachers use the
information to evaluate effectiveness and inputs. Assessments are also used to
sort students into tracks, educational programmes, and on the labor market.
Policymakers use assessments to reward or penalise schools and parents use
assessment results to select schools. Consequently, assessments incentivize the
individual, the teacher, and the school to do well.
  Because assessments play an important role in individuals' educational
careers, either through the information or the incentive channel, they are also
important for efficiency, equity, and well-being. The information channel is
important for ensuring the most efficient human capital investments: students
learn about the returns and costs of effort investments and about their
abilities and comparative advantages. However, because students are sorted into
educational programs and on the labor market based on assessment results,
students optimal educational investment might not equal their optimal human
capital investment because of the signaling value. Biases in assessments and
heterogeneity in access to assessments are sources of inequality in education
according to gender, origin, and socioeconomic background. These sources have
long-running implications for equality and opportunity. Finally, because
assessment results also carry important consequences for individuals'
educational opportunities and on the labor market, they are a source of stress
and reduced well-being. </font><br> Link: <a href='http://arxiv.org/pdf/2208.05826v1' target="_blank">http://arxiv.org/pdf/2208.05826v1</a><br> <br> <br> <font size='5'> 633 </font> <div style="text-align: right"> 2022-08-10 20:19:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Two Stage Stochastic Optimization Model for Port Infrastructure Planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates inland port infrastructure investment planning under
uncertain commodity demand conditions. A two-stage stochastic optimization is
developed to model the impact of demand uncertainty on infrastructure planning
and transportation decisions. The two-stage stochastic model minimizes the
total expected costs, including the capacity expansion investment costs
associated with handling equipment and storage, and the expected transportation
costs. To solve the problem, an accelerated Benders decomposition algorithm is
implemented. The Arkansas section of the McCllean-Kerr Arkansas River
Navigation System (MKARNS) is used as a testing ground for the model. Results
show that commodity volume and, as expected, the percent of that volume that
moves via waterways (in ton-miles) increases with increasing investment in port
infrastructure. The model is able to identify a cluster of ports that should
receive investment in port capacity under any investment scenario. The use of a
stochastic approach is justified by calculating the value of the stochastic
solution (VSS). </font><br> Link: <a href='http://arxiv.org/pdf/2208.05550v1' target="_blank">http://arxiv.org/pdf/2208.05550v1</a><br> <br> <br> <font size='5'> 634 </font> <div style="text-align: right"> 2022-08-10 13:42:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Listen to Users, but Only 85% of the Time: How Black Swans Can Save Innovation in a Data-Driven World</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Data-driven design is a proven success factor that more and more digital
businesses embrace. At the same time, academics and practitioners alike warn
that when virtually everything must be tested and proven with numbers, that can
stifle creativity and innovation. This article argues that Taleb's Black Swan
theory can solve this dilemma. It shows that online experimentation, and
therefore digital design, are fat-tailed phenomena and, hence, prone to Black
Swans. It introduces the notion of Black Swan designs -- "crazy" designs that
make sense only in hindsight -- along with four specific criteria. To ensure
incremental improvements and their potential for innovation, businesses should
apply Taleb's barbell strategy: Invest 85-90% of resources into data-driven
approaches and 10-15% into potential Black Swans. </font><br> Link: <a href='http://arxiv.org/pdf/2208.05347v1' target="_blank">http://arxiv.org/pdf/2208.05347v1</a><br> <br> <br> <font size='5'> 635 </font> <div style="text-align: right"> 2022-08-10 09:08:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Weak Supervision in Analysis of News: Application to Economic Policy Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The need for timely data analysis for economic decisions has prompted most
economists and policy makers to search for non-traditional supplementary
sources of data. In that context, text data is being explored to enrich
traditional data sources because it is easy to collect and highly abundant. Our
work focuses on studying the potential of textual data, in particular news
pieces, for measuring economic policy uncertainty (EPU). Economic policy
uncertainty is defined as the public's inability to predict the outcomes of
their decisions under new policies and future economic fundamentals.
Quantifying EPU is of great importance to policy makers, economists, and
investors since it influences their expectations about the future economic
fundamentals with an impact on their policy, investment and saving decisions.
Most of the previous work using news articles for measuring EPU are either
manual or based on a simple keyword search. Our work proposes a machine
learning based solution involving weak supervision to classify news articles
with regards to economic policy uncertainty. Weak supervision is shown to be an
efficient machine learning paradigm for applying machine learning models in low
resource settings with no or scarce training sets, leveraging domain knowledge
and heuristics. We further generated a weak supervision based EPU index that we
used to conduct extensive econometric analysis along with the Irish
macroeconomic indicators to validate whether our generated index foreshadows
weaker macroeconomic performance </font><br> Link: <a href='http://arxiv.org/pdf/2209.05383v2' target="_blank">http://arxiv.org/pdf/2209.05383v2</a><br> <br> <br> <font size='5'> 636 </font> <div style="text-align: right"> 2022-08-10 08:52:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Alfvnic waves in the inhomogeneous solar atmosphere</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The solar atmosphere is known to be replete with magneto-hydrodynamic wave
modes, and there has been significant investment in understanding how these
waves propagate through the Sun's atmopshere and deposit their energy into the
plasma. The waves' journey is made interesting by the vertical variation in
plasma quantities that define the solar atmosphere. In addition to this
large-scale inhomogeneity, a wealth of fine-scale structure through the
chromosphere and corona has been brought to light by high-resolution
observations over the last couple of decades. This fine-scale sturcture
represents inhomogeneity that is thought to be perpendicular to the local
magnetic fields. The implications of this form of inhomogeneity on wave
propagation is still being uncovered, but is known to fundamentally change the
nature of MHD wave modes. It also enables interesting physics to arise
including resonances, turbulence and instabilities. Here we review some of the
key insights into how the inhomogeneity influences Alfv\'enic wave propagation
through the Sun's atmosphere, discussing both inhomogeneities parallel and
perpendicular to the magnetic field. </font><br> Link: <a href='http://arxiv.org/pdf/2208.05222v2' target="_blank">http://arxiv.org/pdf/2208.05222v2</a><br> <br> <br> <font size='5'> 637 </font> <div style="text-align: right"> 2022-08-09 06:52:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Agent-Based Fleet Management Model for First- and Last-Mile Services</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the growth of cars and car-sharing applications, commuters in many
cities, particularly developing countries, are shifting away from public
transport. These shifts have affected two key stakeholders: transit operators
and first- and last-mile (FLM) services. Although most cities continue to
invest heavily in bus and metro projects to make public transit attractive,
ridership in these systems has often failed to reach targeted levels. FLM
service providers also experience lower demand and revenues in the wake of
shifts to other means of transport. Effective FLM options are required to
prevent this phenomenon and make public transport attractive for commuters. One
possible solution is to forge partnerships between public transport and FLM
providers that offer competitive joint mobility options. Such solutions require
prudent allocation of supply and optimised strategies for FLM operations and
ride-sharing. To this end, we build an agent- and event-based simulation model
which captures interactions between passengers and FLM services using
statecharts, vehicle routing models, and other trip matching rules. An
optimisation model for allocating FLM vehicles at different transit stations is
proposed to reduce unserved requests. Using real-world metro transit demand
data from Bengaluru, India, the effectiveness of our approach in improving FLM
connectivity and quantifying the benefits of sharing trips is demonstrated. </font><br> Link: <a href='http://arxiv.org/pdf/2208.04563v2' target="_blank">http://arxiv.org/pdf/2208.04563v2</a><br> <br> <br> <font size='5'> 638 </font> <div style="text-align: right"> 2022-08-08 15:10:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A mean-variance optimized portfolio constructed for investment in a reference security, for an investor with a preference towards an accepted set of securities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider a reference security, understood to be an attractive investment,
with the caveat that an investor is not willing to directly invest in the
security, for presence of constraints, either investor specific or pertaining
to the security itself. The investor, however, is open to a portfolio
constructed with an accepted set of securities, where returns could be
considered similar to the reference security. We demonstrate, under a measure
of similarity, such a portfolio could be selected with a mean-variance
characterization, as defined by Markowitz. Furthermore, we consider the
performance relative to the reference security, with the Sharpe Ratio. The
objective of the paper is to derive an optimal portfolio to address an investor
preference for the accepted set of securities. </font><br> Link: <a href='http://arxiv.org/pdf/2208.04205v2' target="_blank">http://arxiv.org/pdf/2208.04205v2</a><br> <br> <br> <font size='5'> 639 </font> <div style="text-align: right"> 2022-08-07 13:28:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Strategic differences between regional investments into graphene technology and how corporations and universities manage patent portfolios</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Nowadays, patenting activities are essential in converting applied science to
technology in the prevailing innovation model. To gain strategic advantages in
the technological competitions between regions, nations need to leverage the
investments of public and private funds to diversify over all technologies or
specialize in a small number of technologies. In this paper, we investigated
who the leaders are at the regional and assignee levels, how they attained
their leadership positions, and whether they adopted diversification or
specialization strategies, using a dataset of 176,193 patent records on
graphene between 1986 and 2017 downloaded from Derwent Innovation. By applying
a co-clustering method to the IPC subclasses in the patents and using a z-score
method to extract keywords from their titles and abstracts, we identified seven
graphene technology areas emerging in the sequence synthesis - composites -
sensors - devices - catalyst - batteries - water treatment. We then examined
the top regions in their investment preferences and their changes in rankings
over time and found that they invested in all seven technology areas. In
contrast, at the assignee level, some were diversified while others were
specialized. We found that large entities diversified their portfolios across
multiple technology areas, while small entities specialized around their core
competencies. In addition, we found that universities had higher entropy values
than corporations on average, leading us to the hypothesis that corporations
file, buy, or sell patents to enable product development. In contrast,
universities focus only on licensing their patents. We validated this
hypothesis through an aggregate analysis of reassignment and licensing and a
more detailed analysis of three case studies - SAMSUNG, RICE UNIVERSITY, and
DYSON. </font><br> Link: <a href='http://arxiv.org/pdf/2208.03719v1' target="_blank">http://arxiv.org/pdf/2208.03719v1</a><br> <br> <br> <font size='5'> 640 </font> <div style="text-align: right"> 2022-08-04 10:34:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Estimation of growth in fund models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Fund models are statistical descriptions of markets where all asset returns
are spanned by the returns of a lower-dimensional collection of funds, modulo
orthogonal noise. Equivalently, they may be characterised as models where the
global growth-optimal portfolio only involves investment in the aforementioned
funds. The loss of growth due to estimation error in fund models under local
frequentist estimation is determined entirely by the number of funds.
Furthermore, under a general filtering framework for Bayesian estimation, the
loss of growth increases as the investment universe does. A shrinkage method
that targets maximal growth with the least amount of deviation is proposed.
Empirical evidence suggests that shrinkage gives a stable estimate that more
closely follows growth potential than an unrestricted Bayesian estimate. </font><br> Link: <a href='http://arxiv.org/pdf/2208.02573v1' target="_blank">http://arxiv.org/pdf/2208.02573v1</a><br> <br> <br> <font size='5'> 641 </font> <div style="text-align: right"> 2022-08-02 10:34:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Application of Blockchain Smart Contracts in E-Commerce and Government</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With technological advances and the establishment of e-commerce models,
business challenges have shifted to online platforms. The promise of embedding
self-executing and autonomous programs into blockchain technologies has
attracted increased interest and its use in niche solutions. Using qualitative
interviews, this paper sought the opinions of the eleven industry leaders
regarding smart contracts. Findings reveal that the technology is gaining
momentum in e-commerce, particularly in financial transfer, record-keeping,
real estate, and property management, insurance, mortgage, supply chain
management, data storage, authorization of credit, denaturalized intelligence,
aviation sector, shipping of products, invoice financing and other domains. The
significant benefits of widespread adoption and deployment of smart contracts
include their capability to deliver decentralization, efficacy,
cost-effectiveness, transparency, speed, autonomy, transparency, privacy, and
security, encouraging the emergence of novel business models. Albeit these
benefits that revolutionize online transactions, the technology faced
multifaceted challenges. Smart technologies are only a decade old and are not
advanced in security, transparency, cost-effectiveness, and regulatory
framework. Furthermore, organizational, and technical challenges limit their
deployment: incompatibility with legacy systems, scalability, bugs, speed, and
lack of talent and understanding regarding smart contracts. Consequently,
policymakers, developers, researchers, practitioners, and other stakeholders
need to invest effort and time to foster the technologies and address pertinent
issues to enable the global adoption of smart contracts by small and big
businesses. </font><br> Link: <a href='http://arxiv.org/pdf/2208.01350v1' target="_blank">http://arxiv.org/pdf/2208.01350v1</a><br> <br> <br> <font size='5'> 642 </font> <div style="text-align: right"> 2022-08-02 09:24:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-Supervised Traversability Prediction by Learning to Reconstruct Safe Terrain</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Navigating off-road with a fast autonomous vehicle depends on a robust
perception system that differentiates traversable from non-traversable terrain.
Typically, this depends on a semantic understanding which is based on
supervised learning from images annotated by a human expert. This requires a
significant investment in human time, assumes correct expert classification,
and small details can lead to misclassification. To address these challenges,
we propose a method for predicting high- and low-risk terrains from only past
vehicle experience in a self-supervised fashion. First, we develop a tool that
projects the vehicle trajectory into the front camera image. Second, occlusions
in the 3D representation of the terrain are filtered out. Third, an autoencoder
trained on masked vehicle trajectory regions identifies low- and high-risk
terrains based on the reconstruction error. We evaluated our approach with two
models and different bottleneck sizes with two different training and testing
sites with a fourwheeled off-road vehicle. Comparison with two independent test
sets of semantic labels from similar terrain as training sites demonstrates the
ability to separate the ground as low-risk and the vegetation as high-risk with
81.1% and 85.1% accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2208.01329v1' target="_blank">http://arxiv.org/pdf/2208.01329v1</a><br> <br> <br> <font size='5'> 643 </font> <div style="text-align: right"> 2022-08-01 16:04:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Change point detection in dynamic Gaussian graphical models: the impact of COVID-19 pandemic on the US stock market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reliable estimates of volatility and correlation are fundamental in economics
and finance for understanding the impact of macroeconomics events on the market
and guiding future investments and policies. Dependence across financial
returns is likely to be subject to sudden structural changes, especially in
correspondence with major global events, such as the COVID-19 pandemic. In this
work, we are interested in capturing abrupt changes over time in the dependence
across US industry stock portfolios, over a time horizon that covers the
COVID-19 pandemic. The selected stocks give a comprehensive picture of the US
stock market. To this end, we develop a Bayesian multivariate stochastic
volatility model based on a time-varying sequence of graphs capturing the
evolution of the dependence structure. The model builds on the Gaussian
graphical models and the random change points literature. In particular, we
treat the number, the position of change points, and the graphs as object of
posterior inference, allowing for sparsity in graph recovery and change point
detection. The high dimension of the parameter space poses complex
computational challenges. However, the model admits a hidden Markov model
formulation. This leads to the development of an efficient computational
strategy, based on a combination of sequential Monte-Carlo and Markov chain
Monte-Carlo techniques. Model and computational development are widely
applicable, beyond the scope of the application of interest in this work. </font><br> Link: <a href='http://arxiv.org/pdf/2208.00952v3' target="_blank">http://arxiv.org/pdf/2208.00952v3</a><br> <br> <br> <font size='5'> 644 </font> <div style="text-align: right"> 2022-07-31 20:40:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Collaboration in Open Government Data Ecosystems: Open Cross-sector Sharing and Co-development of Data and Software</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Open innovation highlights the potential benefits of external
collaboration and knowledge-sharing, often exemplified through Open Source
Software (OSS). The public sector has thus far mainly focused on the sharing of
Open Government Data (OGD), often with a supply-driven approach with limited
feedback-loops. We hypothesize that public sector organizations can extend the
open innovation benefits by also creating platforms, where OGD, related OSS,
and open standards are collaboratively developed and shared. Objective: The
objective of this study is to explore how public sector organizations in the
role of platform providers facilitate such collaboration in the form of OGD
ecosystems and how the ecosystem's governance may be structured to support the
collaboration. Method: We conduct an exploratory multiple-case study of two
such ecosystems, focused on OGD related to the Swedish labor market and public
transport sector, respectively. Data is gathered through interviews, document
studies, and prolonged engagement at one of the platform providers. Results:
The study presents governance structure and collaboration practices of the two
ecosystems and discusses how these contribute to the platform providers' goals.
The case studies highlight the need for platform providers to take an active
and multi-functional role in enabling the sharing of data and software from and
between the members of the ecosystem. Conclusions: We conclude that OGD
ecosystems offer public sector organizations a possibility to catalyze the
potential innovation output of OGD, but that it requires investment and
adoption of an open and collaborative mindset. </font><br> Link: <a href='http://arxiv.org/pdf/2208.01746v1' target="_blank">http://arxiv.org/pdf/2208.01746v1</a><br> <br> <br> <font size='5'> 645 </font> <div style="text-align: right"> 2022-07-31 20:26:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Public Sector Platforms going Open: Creating and Growing an Ecosystem with Open Collaborative Development</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: By creating ecosystems around platforms of Open Source Software
(OSS) and Open Data (OD), and adopting open collaborative development
practices, platform providers may exploit open innovation benefits. However,
adopting such practices in a traditionally closed organization is a maturity
process that we hypothesize cannot be undergone without friction.
  Objective: This study aims to investigate what challenges may occur for a
newly-turned platform provider in the public sector, aiming to adopt open
collaborative practices to create an ecosystem around the development of the
underpinning platform.
  Method: An exploratory case-study is conducted at a Swedish public sector
platform provider, which is creating an ecosystem around OSS and OD, related to
the labor market. Data is collected through interviews, document studies, and
prolonged engagement.
  Results: Findings highlight a fear among developers of being publicly
questioned for their work, as they represent a government agency undergoing
constant scrutiny. Issue trackers, roadmaps, and development processes are
generally closed, while multiple channels are used for communication, causing
internal and external confusion. Some developers are reluctant to communicate
externally as they believe it interferes with their work. Lack of health
metrics limits possibilities to follow ecosystem growth and for actors to make
investment decisions. Further, an autonomous team structure is reported to
complicate internal communication and enforcement of the common vision, as well
as collaboration. A set of interventions for addressing the challenges are
proposed, based on related work.
  Conclusions: We conclude that several cultural, organizational, and
process-related challenges may reside, and by understanding these early on,
platform providers can be preemptive in their work of building healthy
ecosystems. </font><br> Link: <a href='http://arxiv.org/pdf/2208.00510v1' target="_blank">http://arxiv.org/pdf/2208.00510v1</a><br> <br> <br> <font size='5'> 646 </font> <div style="text-align: right"> 2022-07-31 20:14:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Contribution Management Framework for Firms Engaged in Open Source Software Ecosystems -- A Research Preview</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Context and motivation: Contribution Management helps firms engaged in Open
Source Software (OSS) ecosystems to motivate what they should contribute and
when, but also what they should focus their resources on and to what extent.
Such guidelines are also referred to as contribution strategies. The motivation
for developing tailored contribution strategies is to maximize return on
investment and sustain the influence needed in the ecosystem. Question/Problem:
We aim to develop a framework to help firms understand their current situation
and create a starting point to develop an effective contribution management
process. Principal ideas/results: Through a design science approach, a
prototype framework is created based on literature and validated iteratively
with expert opinions through interviews. Contribution: In this research
preview, we present our initial results after our first design cycle and
consultation with one experienced OSS manager at a large OSS oriented
software-intensive firm. The initial validation highlights importance of
stakeholder identification and analysis, as well as the general need for
contribution management and alignment with internal product planning. This
encourages future work to develop the framework further using expert and case
validation. </font><br> Link: <a href='http://arxiv.org/pdf/2208.02630v1' target="_blank">http://arxiv.org/pdf/2208.02630v1</a><br> <br> <br> <font size='5'> 647 </font> <div style="text-align: right"> 2022-07-30 20:54:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Motivating the Contributions: An Open Innovation Perspective on What to Share as Open Source Software</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Open Source Software (OSS) ecosystems have reshaped the ways how
software-intensive firms develop products and deliver value to customers.
However, firms still need support for strategic product planning in terms of
what to develop internally and what to share as OSS. Existing models accurately
capture commoditization in software business, but lack operational support to
decide what contribution strategy to employ in terms of what and when to
contribute. This study proposes a Contribution Acceptance Process (CAP) model
from which firms can adopt contribution strategies that align with product
strategies and planning. In a design science influenced case study executed at
Sony Mobile, the CAP model was iteratively developed in close collaboration
with the firm's practitioners. The CAP model helps classify artifacts according
to business impact and control complexity so firms may estimate and plan
whether an artifact should be contributed or not. Further, an information
meta-model is proposed that helps operationalize the CAP model at the
organization. The CAP model provides an operational OI perspective on what
firms involved in OSS ecosystems should share, by helping them motivate
contributions through the creation of contribution strategies. The goal is to
help maximize return on investment and sustain needed influence in OSS
ecosystems. </font><br> Link: <a href='http://arxiv.org/pdf/2208.00308v1' target="_blank">http://arxiv.org/pdf/2208.00308v1</a><br> <br> <br> <font size='5'> 648 </font> <div style="text-align: right"> 2022-07-29 20:32:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A method for analyzing stakeholders' influence on an open source software ecosystem's requirements engineering process</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For a firm in an open source software (OSS) ecosystem, the requirements
engineering (RE) process is rather multifaceted. Apart from its typical RE
process, there is a competing process, external to the firm and inherent to the
firm's ecosystem. When trying to impose an agenda in competition with other
firms, and aiming to align internal product planning with the ecosystem's RE
process, firms need to consider who and how influential the other stakeholders
are, and what their agendas are. The aim of the presented research is to help
firms identify and analyze stakeholders in OSS ecosystems, in terms of their
influence and interactions, to create awareness of their agendas, their
collaborators, and how they invest their resources. To arrive at a solution
artifact, we applied a design science research approach where we base artifact
design on the literature and earlier work. A stakeholder influence analysis
(SIA) method is proposed and demonstrated in terms of applicability and utility
through a case study on the Apache Hadoop OSS ecosystem. SIA uses social
network constructs to measure the stakeholders' influence and interactions and
considers the special characteristics of OSS RE to help firms structure their
stakeholder analysis processes in relation to an OSS ecosystem. SIA adds a
strategic aspect to the stakeholder analysis process by addressing the concepts
of influence and interactions, which are important to consider while acting in
collaborative and meritocratic RE cultures of OSS ecosystems. </font><br> Link: <a href='http://arxiv.org/pdf/2208.00062v1' target="_blank">http://arxiv.org/pdf/2208.00062v1</a><br> <br> <br> <font size='5'> 649 </font> <div style="text-align: right"> 2022-07-29 16:49:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Traditional datasets for the radiological diagnosis tend to only provide the
radiology image alongside the radiology report. However, radiology reading as
performed by radiologists is a complex process, and information such as the
radiologist's eye-fixations over the course of the reading has the potential to
be an invaluable data source to learn from. Nonetheless, the collection of such
data is expensive and time-consuming. This leads to the question of whether
such data is worth the investment to collect. This paper utilizes the recently
published Eye-Gaze dataset to perform an exhaustive study on the impact on
performance and explainability of deep learning (DL) classification in the face
of varying levels of input features, namely: radiology images, radiology report
text, and radiologist eye-gaze data. We find that the best classification
performance of X-ray images is achieved with a combination of radiology report
free-text and radiology image, with the eye-gaze data providing no performance
boost. Nonetheless, eye-gaze data serving as secondary ground truth alongside
the class label results in highly explainable models that generate better
attention maps compared to models trained to do classification and attention
map generation without eye-gaze data. </font><br> Link: <a href='http://arxiv.org/pdf/2207.14781v1' target="_blank">http://arxiv.org/pdf/2207.14781v1</a><br> <br> <br> <font size='5'> 650 </font> <div style="text-align: right"> 2022-07-28 16:49:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Electricity Price Forecasting Model based on Gated Recurrent Units</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The participation of consumers and producers in demand response programs has
increased in smart grids, which reduces investment and operation costs of power
systems. Also, with the advent of renewable energy sources, the electricity
market is becoming more complex and unpredictable. To effectively implement
demand response programs, forecasting the future price of electricity is very
crucial for producers in the electricity market. Electricity prices are very
volatile and change under the influence of various factors such as temperature,
wind speed, rainfall, intensity of commercial and daily activities, etc.
Therefore, considering the influencing factors as dependent variables can
increase the accuracy of the forecast. In this paper, a model for electricity
price forecasting is presented based on Gated Recurrent Units. The electrical
load consumption is considered as an input variable in this model. Noise in
electricity price seriously reduces the efficiency and effectiveness of
analysis. Therefore, an adaptive noise reducer is integrated into the model for
noise reduction. The SAEs are then used to extract features from the de-noised
electricity price. Finally, the de-noised features are fed into the GRU to
train predictor. Results on real dataset shows that the proposed methodology
can perform effectively in prediction of electricity price. </font><br> Link: <a href='http://arxiv.org/pdf/2207.14225v1' target="_blank">http://arxiv.org/pdf/2207.14225v1</a><br> <br> <br> <font size='5'> 651 </font> <div style="text-align: right"> 2022-07-26 20:48:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Unprecedented Privacy Risks of the Metaverse</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Thirty study participants playtested an innocent-looking "escape room" game
in virtual reality (VR). Behind the scenes, an adversarial program had
accurately inferred over 25 personal data attributes, from anthropometrics like
height and wingspan to demographics like age and gender, within just a few
minutes of gameplay. As notoriously data-hungry companies become increasingly
involved in VR development, this experimental scenario may soon represent a
typical VR user experience. While virtual telepresence applications (and the
so-called "metaverse") have recently received increased attention and
investment from major tech firms, these environments remain relatively
under-studied from a security and privacy standpoint. In this work, we
illustrate how VR attackers can covertly ascertain dozens of personal data
attributes from seemingly-anonymous users of popular metaverse applications
like VRChat. These attackers can be as simple as other VR users without special
privilege, and the potential scale and scope of this data collection far exceed
what is feasible within traditional mobile and web applications. We aim to shed
light on the unique privacy risks of the metaverse, and provide the first
holistic framework for understanding intrusive data harvesting attacks in these
emerging VR ecosystems. </font><br> Link: <a href='http://arxiv.org/pdf/2207.13176v2' target="_blank">http://arxiv.org/pdf/2207.13176v2</a><br> <br> <br> <font size='5'> 652 </font> <div style="text-align: right"> 2022-07-26 00:12:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trading under the Proof-of-Stake Protocol -- a Continuous-Time Control Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a continuous-time control approach to optimal trading in a
Proof-of-Stake (PoS) blockchain, formulated as a consumption-investment problem
that aims to strike the optimal balance between a participant's (or agent's)
utility from holding/trading stakes and utility from consumption. We present
solutions via dynamic programming and the Hamilton-Jacobi-Bellman (HJB)
equations. When the utility functions are linear or convex, we derive
close-form solutions and show that the bang-bang strategy is optimal (i.e.,
always buy or sell at full capacity). Furthermore, we bring out the explicit
connection between the rate of return in trading/holding stakes and the
participant's risk-adjusted valuation of the stakes. In particular, we show
when a participant is risk-neutral or risk-seeking, corresponding to the
risk-adjusted valuation being a martingale or a sub-martingale, the optimal
strategy must be to either buy all the time, sell all the time, or first buy
then sell, and with both buying and selling executed at full capacity. We also
propose a risk-control version of the consumption-investment problem; and for a
special case, the ''stake-parity'' problem, we show a mean-reverting strategy
is optimal. </font><br> Link: <a href='http://arxiv.org/pdf/2207.12581v2' target="_blank">http://arxiv.org/pdf/2207.12581v2</a><br> <br> <br> <font size='5'> 653 </font> <div style="text-align: right"> 2022-07-23 07:13:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Personalized Promotion Decision Making Based on Direct and Enduring Effect Predictions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Promotions have been trending in the e-commerce marketplace to build up
customer relationships and guide customers towards the desired actions. Since
incentives are effective to engage customers and customers have different
preferences for different types of incentives, the demand for personalized
promotion decision making is increasing over time.
  However, research on promotion decision making has focused specifically on
purchase conversion during the promotion period (the direct effect), while
generally disregarding the enduring effect in the post promotion period. To
achieve a better lift return on investment (lift ROI) on the enduring effect of
the promotion and improve customer retention and loyalty, we propose a
framework of multiple treatment promotion decision making by modeling each
customer's direct and enduring response. First, we propose a customer direct
and enduring effect (CDEE) model which predicts the customer direct and
enduring response. With the help of the predictions of the CDEE, we personalize
incentive allocation to optimize the enduring effect while keeping the cost
under the budget. To estimate the effect of decision making, we apply an
unbiased evaluation approach of business metrics with randomized control trial
(RCT) data. We compare our method with benchmarks using two promotions in
Mercari and achieve significantly better results. </font><br> Link: <a href='http://arxiv.org/pdf/2207.14798v1' target="_blank">http://arxiv.org/pdf/2207.14798v1</a><br> <br> <br> <font size='5'> 654 </font> <div style="text-align: right"> 2022-07-22 19:34:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Next-to-leading order mixed QCD-electroweak corrections to Higgs production at the LHC</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: After ten years from its discovery, the Higgs boson is still under
unprecedented scrutiny. A huge theoretical effort has been invested in
modelling Higgs boson production through gluon fusion, reaching
$\text{N}^3\text{LO}$ predictions in pure QCD. This incredible theoretical
achievement makes the exact computation of sub-leading contributions, such as
mixed QCD-Electroweak corrections, necessary. I will present the analytic
calculation of the gluon- and quark-initiated two-loop four-point contributions
to such class of corrections mediated by light quarks at order $v \alpha^2
\alpha_S^{3/2}$. </font><br> Link: <a href='http://arxiv.org/pdf/2207.11310v1' target="_blank">http://arxiv.org/pdf/2207.11310v1</a><br> <br> <br> <font size='5'> 655 </font> <div style="text-align: right"> 2022-07-19 21:28:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A review on recent advances in scenario aggregation methods for power system analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Worldwide commitments to net zero greenhouse emissions have accelerated
investments in renewable energy resources. The requirements for operating and
planning power systems are becoming stringent because of the need to take into
account the uncertainty associated with renewable generation. Several modeling
frameworks that consider the inherent uncertainty in the operation and planning
of the power system have been extensively studied. Stochastic optimization has
been the most popular approach among these frameworks due to its intuitive
representation, especially when formulated using discrete probabilistic
scenarios to represent the random variables. Although many scenarios
representing all possible uncertain operating conditions would be needed to
accurate evaluate stochastic operation and planning models, the size of the
scenario set impacts computational complexity, posing a significant tradeoff
between uncertainty detail representation and computational tractability.
  During the last decade, a large body of research has focused on developing
new scenario aggregation methods to derive reduced scenario sets that show
properties similar to the original scenario set while decreasing computational
burden. This review provides an up-to-date, comprehensive classification and
analysis of the literature related to scenario aggregation methods for
addressing power system optimization problems. First, we present a general
framework and the aggregation methodologies. Then, the main studies related to
temporal and spatial scenario aggregation are described, followed by a
bibliometric analysis of the main publication sources, authors, and application
problems. Finally, we provide a numerical analysis and discuss 16 aggregation
methods for the transmission expansion planning problem. Finally,
recommendations, opportunities, and conclusions are discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2207.09557v1' target="_blank">http://arxiv.org/pdf/2207.09557v1</a><br> <br> <br> <font size='5'> 656 </font> <div style="text-align: right"> 2022-07-19 18:36:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Thoughts on child safety on commodity platforms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The explosion of global social media and online communication platforms has
changed how we interact with each other and as a society, bringing with it new
security and privacy challenges. Like all technologies, these platforms can be
abused and they are routinely used to attempt to cause harm at scale. One of
the most significant offence types that is enabled by these platforms is child
sexual abuse - both scaling existing abuse and enabling entirely new types of
online-only abuse where the impacts on the victim are equally catastrophic.
Many platforms invest significantly in combating this crime, referring
confirmed evidence of illegality to law enforcement. The introduction of
end-to-end encryption and similar technologies breaks many of the mitigations
in place today and this has led to a debate around the apparent dichotomy of
good child safety and good general user privacy and security. This debate has
concentrated on the problem of detecting offenders sharing known abuse imagery
using a technique known as client side scanning. We will show that the real
problem of online child sexual abuse is much more complex than offender image
sharing, providing a new set of 'harm archetypes' to better group harms into
categories that have similar technical characteristics and, as far as we are
able, bring more clarity to the processes currently used by platforms and law
enforcement in relation to child sexual abuse content and the real world
impacts. We explore, at a high level, a variety of techniques that could be
used as part of any potential solution and examine the benefits and disbenefits
that may accrue in various use cases, and use a hypothetical service as an
example of how various techniques could be brought together to provide both
user privacy and security, while protecting child safety and enabling law
enforcement action. </font><br> Link: <a href='http://arxiv.org/pdf/2207.09506v1' target="_blank">http://arxiv.org/pdf/2207.09506v1</a><br> <br> <br> <font size='5'> 657 </font> <div style="text-align: right"> 2022-07-19 18:09:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economics and Optimal Investment Policies of Attackers and Defenders in Cybersecurity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In our time cybersecurity has grown to be a topic of massive proportion at
the national and enterprise levels. Our thesis is that the economic perspective
and investment decision-making are vital factors in determining the outcome of
the struggle. To build our economic framework, we borrow from the pioneering
work of Gordon and Loeb in which the Defender optimally trades-off investments
for lower likelihood of its system breach. Our two-sided model additionally has
an Attacker, assumed to be rational and also guided by economic considerations
in its decision-making, to which the Defender responds. Our model is a
simplified adaptation of a model proposed during the Cold War for weapons
deployment in the US. Our model may also be viewed as a Stackelberg game and,
from an analytic perspective, as a Max-Min problem, the analysis of which is
known to have to contend with discontinuous behavior. The complexity of our
simple model is rooted in its inherent nonlinearity and, more consequentially,
non-convexity of the objective function in the optimization. The possibilities
of the Attacker's actions add substantially to the risk to the Defender, and
the Defender's rational, risk-neutral optimal investments in general
substantially exceed the optimal investments predicted by the one-sided
Gordon-Loeb model. We obtain a succinct set of three decision types that
categorize all of the Defender's optimal investment decisions. Also, the
Defender's optimal decisions exhibit discontinuous behavior as the initial
vulnerability of its system is varied. The analysis is supplemented by
extensive numerical illustrations. The results from our model open several
major avenues for future work. </font><br> Link: <a href='http://arxiv.org/pdf/2207.09497v1' target="_blank">http://arxiv.org/pdf/2207.09497v1</a><br> <br> <br> <font size='5'> 658 </font> <div style="text-align: right"> 2022-07-19 17:06:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Turn Inequalities for Infinite Product Generating Functions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the $1970$s, Nicolas proved that the partition function $p(n)$ is
log-concave for $ n > 25$. In \cite{HNT21}, a precise conjecture on the
log-concavity for the plane partition function $\func{pp}(n)$ for $n >11$ was
stated. This was recently proven by Ono, Pujahari, and Rolen. In this paper, we
provide a general picture. We associate to double sequences $\{g_d(n)\}_{d,n}$
with $g_d(1)=1$ and $$0 \leq g_{d}\left( n\right) - n^{d}\leq g_{1}\left(
n\right) \left( n-1\right) ^{d-1}$$ polynomials $\{P_n^{g_d}(x)\}_{d,n}$ given
by \begin{equation*} \sum_{n=0}^{\infty} P_n^{g_d}(x) \, q^n :=
\func{exp}\left( x \sum_{n=1}^{\infty} g_d(n) \frac{q^n}{n} \right)
=\prod_{n=1}^{\infty} \left( 1 - q^n \right)^{-x f_d(n)}. \end{equation*} We
recover $ p(n)= P_n^{\sigma_1}(1)$ and $\func{pp}\left( n\right) =
P_n^{\sigma_2}(1)$, where $\sigma_d (n):= \sum_{\ell \mid n} \ell^d$ and
$f_d(n)= n^{d-1}$. Let $n \geq 6$. Then the sequence $\{P_n^{\sigma_d}(1)\}_d$
is log-concave for almost all $d$ if and only if $n$ is divisible by $3$. Let
$\func{id}(n)=n$. Then $P_n^{\func{id}}(x) = \frac{x}{n} L_{n-1}^{(1)}(-x)$,
where $L_{n}^{\left( \alpha \right) }\left( x\right) $ denotes the
$\alpha$-associated Laguerre polynomial. In this paper, we invest in Tur\'an
inequalities \begin{equation*} \Delta_{n}^{g_d}(x) := \left( P_n^{g_d}(x)
\right)^2 - P_{n-1}^{g_d}(x) \, P_{n+1}^{g_d}(x) \geq 0. \end{equation*} Let $n
\geq 6$ and $0 \leq x < 2 - \frac{12}{n+4}$. Then $n$ is divisible by $3$ if
and only if $\Delta_{n}^{g_d}(x) \geq 0$ for almost all $d$. Let $n \geq 6$ and
$n \not\equiv 2 \pmod{3}$. Then the condition on $x$ can be reduced to $x \geq
0$. We determine explicit bounds. As an analogue to Nicolas' result, we have
for $g_1= \func{id}$ that $\Delta_{n}^{\func{id}}(x) \geq 0$ for all $x \geq 0
$ and all $n$. </font><br> Link: <a href='http://arxiv.org/pdf/2207.09409v1' target="_blank">http://arxiv.org/pdf/2207.09409v1</a><br> <br> <br> <font size='5'> 659 </font> <div style="text-align: right"> 2022-07-19 08:18:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Solving the unit-load pre-marshalling problem in block stacking storage systems with multiple access directions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Block stacking storage systems are highly adaptable warehouse systems with
low investment costs. With multiple, deep lanes they can achieve high storage
densities, but accessing some unit loads can be time-consuming. The unit-load
pre-marshalling problem sorts the unit loads in a block stacking storage system
in off-peak time periods to prepare for upcoming orders. The goal is to find a
minimum number of unit-load moves needed to sequence a storage bay in ascending
order based on the retrieval priority group of each unit load. In this paper,
we present two solution approaches for determining the minimum number of
unit-load moves. We show that for storage bays with one access direction, it is
possible to adapt existing, optimal tree search procedures and lower bound
heuristics from the container pre-marshalling problem. For multiple access
directions, we develop a novel, two-step solution approach based on a network
flow model and an A* algorithm with an adapted lower bound that is applicable
in all scenarios. We further analyze the performance of the presented solutions
in computational experiments for randomly generated problem instances and show
that multiple access directions greatly reduce both the total access time of
unit loads and the required sorting effort. </font><br> Link: <a href='http://arxiv.org/pdf/2207.09118v1' target="_blank">http://arxiv.org/pdf/2207.09118v1</a><br> <br> <br> <font size='5'> 660 </font> <div style="text-align: right"> 2022-07-17 21:29:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Estimating Ambient Air Pollution Using Structural Properties of Road Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, the world has become increasingly concerned with air
pollution. Particularly in the global north, countries are implementing systems
to monitor air pollution on a large scale to aid decision-making. Such efforts
are essential but they have at least three shortcomings: (1) they are costly
and are difficult to implement expediently; (2) they focus on urban areas,
which is where most people live, but this choice is prone to inequalities; and
(3) the process of estimating air pollution lacks transparency. In this paper,
we demonstrate that we can estimate air pollution using open-source information
about the structural properties of roads; we focus on England and Wales in the
United Kingdom (UK) in this paper although the methods here described are not
dependent on specific datasets. Our approach makes it possible to implement an
inexpensive method of estimating air pollution concentrations to an accuracy
level that can underpin policymakers' decisions while providing an estimate in
all districts, not just urban areas, and in a process that is transparent and
explainable. Impact Statement. We show that a linear regression model using a
single structural property -- length of the track and unclassified road network
within 0.36% of districts within England and Wales (in the UK) -- can
accurately estimate which districts are the most polluted. The model presents a
transparent and low-cost, yet effective, alternative to more expensive models
such as the one currently used by DEFRA in the UK. The model has apparent
practical uses for policymakers who want to pursue clean-air initiatives but
lack the capital to invest in comprehensive monitoring networks. Its low
implementation cost, accessible model design, and worldwide coverage of the
dataset provide a basis for implementing systems to estimate air pollution
concentrations in low-income countries. </font><br> Link: <a href='http://arxiv.org/pdf/2207.14335v4' target="_blank">http://arxiv.org/pdf/2207.14335v4</a><br> <br> <br> <font size='5'> 661 </font> <div style="text-align: right"> 2022-07-17 21:03:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Spatial Distribution of Solar PV Deployment: An Application of the Region-Based Convolutional Neural Network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a comprehensive analysis of the social and environmental
determinants of solar photovoltaic (PV) deployment rates in Colorado, USA.
Using 652,795 satellite imagery and computer vision frameworks based on a
convolutional neural network, we estimated the proportion of households with
solar PV systems and the roof areas covered by solar panels. At the census
block group level, 7% of Coloradan households have a rooftop PV system, and
2.5% of roof areas in Colorado are covered by solar panels as of 2021. Our
machine learning models predict solar PV deployment based on 43 natural and
social characteristics of neighborhoods. Using four algorithms (Random Forest,
CATBoost, LightGBM, XGBoost), we find that the share of Democratic party votes,
hail risks, strong wind risks, median home value, and solar PV permitting
timelines are the most important predictors of solar PV count per household. In
addition to the size of the houses, PV-to-roof area ratio is highly dependent
on solar PV permitting timelines, proportion of renters and multifamily
housing, and winter weather risks. We also find racial and ethnic disparities
in rooftop solar deployment. The average marginal effects of median household
income on solar deployment are lower in communities with a greater proportion
of African American and Hispanic residents and are higher in communities with a
greater proportion of White and Asian residents. In the ongoing energy
transition, knowing the key predictors of solar deployment can better inform
business and policy decision making for more efficient and equitable grid
infrastructure investment and distributed energy resource management. </font><br> Link: <a href='http://arxiv.org/pdf/2207.08287v1' target="_blank">http://arxiv.org/pdf/2207.08287v1</a><br> <br> <br> <font size='5'> 662 </font> <div style="text-align: right"> 2022-07-16 11:47:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multiscale Causal Structure Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The inference of causal structures from observed data plays a key role in
unveiling the underlying dynamics of the system. This paper exposes a novel
method, named Multiscale-Causal Structure Learning (MS-CASTLE), to estimate the
structure of linear causal relationships occurring at different time scales.
Differently from existing approaches, MS-CASTLE takes explicitly into account
instantaneous and lagged inter-relations between multiple time series,
represented at different scales, hinging on stationary wavelet transform and
non-convex optimization. MS-CASTLE incorporates, as a special case, a
single-scale version named SS-CASTLE, which compares favorably in terms of
computational efficiency, performance and robustness with respect to the state
of the art onto synthetic data. We used MS-CASTLE to study the multiscale
causal structure of the risk of 15 global equity markets, during covid-19
pandemic, illustrating how MS-CASTLE can extract meaningful information thanks
to its multiscale analysis, outperforming SS-CASTLE. We found that the most
persistent and strongest interactions occur at mid-term time resolutions.
Moreover, we identified the stock markets that drive the risk during the
considered period: Brazil, Canada and Italy. The proposed approach can be
exploited by financial investors who, depending to their investment horizon,
can manage the risk within equity portfolios from a causal perspective. </font><br> Link: <a href='http://arxiv.org/pdf/2207.07908v1' target="_blank">http://arxiv.org/pdf/2207.07908v1</a><br> <br> <br> <font size='5'> 663 </font> <div style="text-align: right"> 2022-07-16 01:39:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockchain-enabled tokenization for sustainable and inclusive infrastructure investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Infrastructure is critical for enabling society to function and the economy
to thrive, but there is an increasing mismatch between the need for
infrastructure investments and available capital, which is in consequence of
constraints on public resources and limited capacity to leverage the private
sector co-financing under the current system. With the emergence of distributed
ledger technology, such as blockchain-enabled tokenization, there is a
significant potential to improve investment liquidity, transparency, efficiency
and create new economic models to integrate non-financial values to promote
sustainability and inclusiveness. This research analyzed 21 projects to
investigate how tokenization is implemented in energy infrastructure projects.
Exploratory case study analyses were conducted, which shows the diversity of
tokenization arrangements. The state of the art, potential benefits,
implications, and obstacles associated with the application of tokenization in
infrastructure investment and development are discussed. The purpose of this
research is to understand tokenization within the context of the energy sector
but also to forecast its application in a broad spectrum of infrastructure
projects (e.g., transportation, telecommunication, healthcare, education). </font><br> Link: <a href='http://arxiv.org/pdf/2208.04709v1' target="_blank">http://arxiv.org/pdf/2208.04709v1</a><br> <br> <br> <font size='5'> 664 </font> <div style="text-align: right"> 2022-07-16 01:36:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The future of blockchain-enabled tokenization in infrastructure investment and development: A Delphi-based scenario analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Spurred by the emerging blockchain technology and increased interest in
tokenization, this forecasting research built on extensive literature and
aggregated expertise to explore the potential implementation of
blockchain-enabled tokenization in infrastructure investment and development.
The Delphi-based scenario analysis approach was applied to gather long-term
forecasts and assessments of a research panel consisting of 39 experts in
blockchain tokenization and infrastructure development on how tokenization will
influence the future of infrastructure finance and identify scenarios of
potential applications and impact. International experts were segregated into
two groups salient to this topical area based on both experience ad
self-identification: infrastructure development and blockchain tokenization.
Twenty-three projections for 2035, developed from a literature review, case
study analysis, and expert interviews, concerning perspectives of both the
supply and demand side for the adoption of blockchain tokenization, were
assessed in a two-round Delphi analysis. Regulatory, economic, social, and
technological perspectives of tokenization were taken into consideration.
Assessments were based on both probability and impact of occurrence. Three
groups of scenarios resulted from quantitative and qualitative analysis,
reflecting agreement and differentiation between both expert groups. The
results of this study clearly underlined the potential of tokenization in
infrastructure. Uncertainties and barriers confronting the technologies'
diffusion were discussed. This study contributes to the transfer of general
technical-driven blockchain-enabled tokenization knowledge to
infrastructure-specific tokenization knowledge. Long-term strategic planning is
supported by this study with the scenario data acting as a starting point for
blockchain-related efforts in infrastructure development. </font><br> Link: <a href='http://arxiv.org/pdf/2208.04710v2' target="_blank">http://arxiv.org/pdf/2208.04710v2</a><br> <br> <br> <font size='5'> 665 </font> <div style="text-align: right"> 2022-07-14 19:55:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning Embedded Representation of the Stock Correlation Matrix using Graph Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Understanding non-linear relationships among financial instruments has
various applications in investment processes ranging from risk management,
portfolio construction and trading strategies. Here, we focus on
interconnectedness among stocks based on their correlation matrix which we
represent as a network with the nodes representing individual stocks and the
weighted links between pairs of nodes representing the corresponding pair-wise
correlation coefficients. The traditional network science techniques, which are
extensively utilized in financial literature, require handcrafted features such
as centrality measures to understand such correlation networks. However,
manually enlisting all such handcrafted features may quickly turn out to be a
daunting task. Instead, we propose a new approach for studying nuances and
relationships within the correlation network in an algorithmic way using a
graph machine learning algorithm called Node2Vec. In particular, the algorithm
compresses the network into a lower dimensional continuous space, called an
embedding, where pairs of nodes that are identified as similar by the algorithm
are placed closer to each other. By using log returns of S&P 500 stock data, we
show that our proposed algorithm can learn such an embedding from its
correlation network. We define various domain specific quantitative (and
objective) and qualitative metrics that are inspired by metrics used in the
field of Natural Language Processing (NLP) to evaluate the embeddings in order
to identify the optimal one. Further, we discuss various applications of the
embeddings in investment management. </font><br> Link: <a href='http://arxiv.org/pdf/2207.07183v1' target="_blank">http://arxiv.org/pdf/2207.07183v1</a><br> <br> <br> <font size='5'> 666 </font> <div style="text-align: right"> 2022-07-14 14:44:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Asset Allocation: From Markowitz to Deep Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Asset allocation is an investment strategy that aims to balance risk and
reward by constantly redistributing the portfolio's assets according to certain
goals, risk tolerance, and investment horizon. Unfortunately, there is no
simple formula that can find the right allocation for every individual. As a
result, investors may use different asset allocations' strategy to try to
fulfil their financial objectives. In this work, we conduct an extensive
benchmark study to determine the efficacy and reliability of a number of
optimization techniques. In particular, we focus on traditional approaches
based on Modern Portfolio Theory, and on machine-learning approaches based on
deep reinforcement learning. We assess the model's performance under different
market tendency, i.e., both bullish and bearish markets. For reproducibility,
we provide the code implementation code in this repository. </font><br> Link: <a href='http://arxiv.org/pdf/2208.07158v1' target="_blank">http://arxiv.org/pdf/2208.07158v1</a><br> <br> <br> <font size='5'> 667 </font> <div style="text-align: right"> 2022-07-14 05:05:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploration of an End-to-End Automatic Number-plate Recognition neural network for Indian datasets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Indian vehicle number plates have wide variety in terms of size, font, script
and shape. Development of Automatic Number Plate Recognition (ANPR) solutions
is therefore challenging, necessitating a diverse dataset to serve as a
collection of examples. However, a comprehensive dataset of Indian scenario is
missing, thereby, hampering the progress towards publicly available and
reproducible ANPR solutions. Many countries have invested efforts to develop
comprehensive ANPR datasets like Chinese City Parking Dataset (CCPD) for China
and Application-oriented License Plate (AOLP) dataset for US. In this work, we
release an expanding dataset presently consisting of 1.5k images and a scalable
and reproducible procedure of enhancing this dataset towards development of
ANPR solution for Indian conditions. We have leveraged this dataset to explore
an End-to-End (E2E) ANPR architecture for Indian scenario which was originally
proposed for Chinese Vehicle number-plate recognition based on the CCPD
dataset. As we customized the architecture for our dataset, we came across
insights, which we have discussed in this paper. We report the hindrances in
direct reusability of the model provided by the authors of CCPD because of the
extreme diversity in Indian number plates and differences in distribution with
respect to the CCPD dataset. An improvement of 42.86% was observed in LP
detection after aligning the characteristics of Indian dataset with Chinese
dataset. In this work, we have also compared the performance of the E2E
number-plate detection model with YOLOv5 model, pre-trained on COCO dataset and
fine-tuned on Indian vehicle images. Given that the number Indian vehicle
images used for fine-tuning the detection module and yolov5 were same, we
concluded that it is more sample efficient to develop an ANPR solution for
Indian conditions based on COCO dataset rather than CCPD dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2207.06657v1' target="_blank">http://arxiv.org/pdf/2207.06657v1</a><br> <br> <br> <font size='5'> 668 </font> <div style="text-align: right"> 2022-07-14 01:50:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: StockBot: Using LSTMs to Predict Stock Prices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The evaluation of the financial markets to predict their behaviour have been
attempted using a number of approaches, to make smart and profitable investment
decisions. Owing to the highly non-linear trends and inter-dependencies, it is
often difficult to develop a statistical approach that elucidates the market
behaviour entirely. To this end, we present a long-short term memory (LSTM)
based model that leverages the sequential structure of the time-series data to
provide an accurate market forecast. We then develop a decision making StockBot
that buys/sells stocks at the end of the day with the goal of maximizing
profits. We successfully demonstrate an accurate prediction model, as a result
of which our StockBot can outpace the market and can strategize for gains that
are ~15 times higher than the most aggressive ETFs in the market. </font><br> Link: <a href='http://arxiv.org/pdf/2207.06605v2' target="_blank">http://arxiv.org/pdf/2207.06605v2</a><br> <br> <br> <font size='5'> 669 </font> <div style="text-align: right"> 2022-07-13 17:18:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: N-Grammer: Augmenting Transformers with latent n-grams</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transformer models have recently emerged as one of the foundational models in
natural language processing, and as a byproduct, there is significant recent
interest and investment in scaling these models. However, the training and
inference costs of these large Transformer language models are prohibitive,
thus necessitating more research in identifying more efficient variants. In
this work, we propose a simple yet effective modification to the Transformer
architecture inspired by the literature in statistical language modeling, by
augmenting the model with n-grams that are constructed from a discrete latent
representation of the text sequence. We evaluate our model, the N-Grammer on
language modeling on the C4 data-set as well as text classification on the
SuperGLUE data-set, and find that it outperforms several strong baselines such
as the Transformer and the Primer. We open-source our model for reproducibility
purposes in Jax. </font><br> Link: <a href='http://arxiv.org/pdf/2207.06366v1' target="_blank">http://arxiv.org/pdf/2207.06366v1</a><br> <br> <br> <font size='5'> 670 </font> <div style="text-align: right"> 2022-07-12 23:27:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reward-Sharing Relational Networks in Multi-Agent Reinforcement Learning as a Framework for Emergent Behavior</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we integrate `social' interactions into the MARL setup through
a user-defined relational network and examine the effects of agent-agent
relations on the rise of emergent behaviors. Leveraging insights from sociology
and neuroscience, our proposed framework models agent relationships using the
notion of Reward-Sharing Relational Networks (RSRN), where network edge weights
act as a measure of how much one agent is invested in the success of (or `cares
about') another. We construct relational rewards as a function of the RSRN
interaction weights to collectively train the multi-agent system via a
multi-agent reinforcement learning algorithm. The performance of the system is
tested for a 3-agent scenario with different relational network structures
(e.g., self-interested, communitarian, and authoritarian networks). Our results
indicate that reward-sharing relational networks can significantly influence
learned behaviors. We posit that RSRN can act as a framework where different
relational networks produce distinct emergent behaviors, often analogous to the
intuited sociological understanding of such networks. </font><br> Link: <a href='http://arxiv.org/pdf/2207.05886v2' target="_blank">http://arxiv.org/pdf/2207.05886v2</a><br> <br> <br> <font size='5'> 671 </font> <div style="text-align: right"> 2022-07-12 09:15:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The finance industry is producing an increasing amount of datasets that
investment professionals can consider to be influential on the price of
financial assets. These datasets were initially mainly limited to exchange
data, namely price, capitalization and volume. Their coverage has now
considerably expanded to include, for example, macroeconomic data, supply and
demand of commodities, balance sheet data and more recently extra-financial
data such as ESG scores. This broadening of the factors retained as influential
constitutes a serious challenge for statistical modeling. Indeed, the
instability of the correlations between these factors makes it practically
impossible to identify the joint laws needed to construct scenarios.
Fortunately, spectacular advances in Deep Learning field in recent years have
given rise to GANs. GANs are a type of generative machine learning models that
produce new data samples with the same characteristics as a training data
distribution in an unsupervised way, avoiding data assumptions and human
induced biases. In this work, we are exploring the use of GANs for synthetic
financial scenarios generation. This pilot study is the result of a
collaboration between Fujitsu and Advestis and it will be followed by a
thorough exploration of the use cases that can benefit from the proposed
solution. We propose a GANs-based algorithm that allows the replication of
multivariate data representing several properties (including, but not limited
to, price, market capitalization, ESG score, controversy score,. . .) of a set
of stocks. This approach differs from examples in the financial literature,
which are mainly focused on the reproduction of temporal asset price scenarios.
We also propose several metrics to evaluate the quality of the data generated
by the GANs. This approach is well fit for the generation of scenarios, the
time direction simply arising as a subsequent (eventually conditioned)
generation of data points drawn from the learned distribution. Our method will
allow to simulate high dimensional scenarios (compared to $\lesssim 10$
features currently employed in most recent use cases) where network complexity
is reduced thanks to a wisely performed feature engineering and selection.
Complete results will be presented in a forthcoming study. </font><br> Link: <a href='http://arxiv.org/pdf/2209.03935v1' target="_blank">http://arxiv.org/pdf/2209.03935v1</a><br> <br> <br> <font size='5'> 672 </font> <div style="text-align: right"> 2022-07-11 23:45:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Susceptibility of Continual Learning Against Adversarial Attacks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider all three scenarios (i.e, task-incremental leaning, domain-incremental
learning and class-incremental learning) of continual learning and explore
three regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations, we identify potential
limitations in continual learning approaches against adversarial attacks. Our
empirical study recommends that the research community consider the robustness
of the proposed continual learning approaches and invest extensive efforts in
mitigating catastrophic forgetting. </font><br> Link: <a href='http://arxiv.org/pdf/2207.05225v3' target="_blank">http://arxiv.org/pdf/2207.05225v3</a><br> <br> <br> <font size='5'> 673 </font> <div style="text-align: right"> 2022-07-11 15:40:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning Mutual Fund Categorization using Natural Language Processing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Categorization of mutual funds or Exchange-Traded-funds (ETFs) have long
served the financial analysts to perform peer analysis for various purposes
starting from competitor analysis, to quantifying portfolio diversification.
The categorization methodology usually relies on fund composition data in the
structured format extracted from the Form N-1A. Here, we initiate a study to
learn the categorization system directly from the unstructured data as depicted
in the forms using natural language processing (NLP). Positing as a multi-class
classification problem with the input data being only the investment strategy
description as reported in the form and the target variable being the Lipper
Global categories, and using various NLP models, we show that the
categorization system can indeed be learned with high accuracy. We discuss
implications and applications of our findings as well as limitations of
existing pre-trained architectures in applying them to learn fund
categorization. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04959v1' target="_blank">http://arxiv.org/pdf/2207.04959v1</a><br> <br> <br> <font size='5'> 674 </font> <div style="text-align: right"> 2022-07-11 13:33:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Research Joint Ventures: The Role of Financial Constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper provides a novel theory of research joint ventures for financially
constrained firms. When firms choose R&D portfolios, an RJV can help to
coordinate research efforts, reducing investments in duplicate projects. This
can free up resources, increase the variety of pursued projects and thereby
increase the probability of discovering the innovation. RJVs improve innovation
outcomes when market competition is weak and external financing conditions are
bad. An RJV may increase the innovation probability and nevertheless lower
total R&D costs. RJVs that increase innovation also increase consumer surplus
and tend to be profitable, but innovation-reducing RJVs also exist. Finally, we
compare RJVs to innovation-enhancing mergers. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04856v3' target="_blank">http://arxiv.org/pdf/2207.04856v3</a><br> <br> <br> <font size='5'> 675 </font> <div style="text-align: right"> 2022-07-11 05:28:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Storage and Solar Capacity of a Residential Household under Net Metering and Time-of-Use Pricing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Incentive programs and ongoing reduction in costs are driving joint
installation of solar PV panels and storage systems in residential households.
There is a need for optimal investment decisions to reduce the electricity
consumption costs of the households further. In this paper, we first develop
analytical expression of storage investment decision and then of solar
investment decision for a household which is under net metering billing
mechanism with time of use pricing condition. Using real data of a residential
household in Austin, TX, USA, we study how the investment decisions would
provide benefit for a period of one year. Results show significant profit when
using storage devices and solar panels optimally for the system. It is
important to note that though our approach can help significantly to take
investment decisions, the solution will still be sub-optimal for somebody who
needs optimal investment jointly on both storage and solar systems. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04635v4' target="_blank">http://arxiv.org/pdf/2207.04635v4</a><br> <br> <br> <font size='5'> 676 </font> <div style="text-align: right"> 2022-07-10 21:12:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-task Envisioning Transformer-based Autoencoder for Corporate Credit Rating Migration Early Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Corporate credit ratings issued by third-party rating agencies are quantified
assessments of a company's creditworthiness. Credit Ratings highly correlate to
the likelihood of a company defaulting on its debt obligations. These ratings
play critical roles in investment decision-making as one of the key risk
factors. They are also central to the regulatory framework such as BASEL II in
calculating necessary capital for financial institutions. Being able to predict
rating changes will greatly benefit both investors and regulators alike. In
this paper, we consider the corporate credit rating migration early prediction
problem, which predicts the credit rating of an issuer will be upgraded,
unchanged, or downgraded after 12 months based on its latest financial
reporting information at the time. We investigate the effectiveness of
different standard machine learning algorithms and conclude these models
deliver inferior performance. As part of our contribution, we propose a new
Multi-task Envisioning Transformer-based Autoencoder (META) model to tackle
this challenging problem. META consists of Positional Encoding,
Transformer-based Autoencoder, and Multi-task Prediction to learn effective
representations for both migration prediction and rating prediction. This
enables META to better explore the historical data in the training stage for
one-year later prediction. Experimental results show that META outperforms all
baseline models. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04539v1' target="_blank">http://arxiv.org/pdf/2207.04539v1</a><br> <br> <br> <font size='5'> 677 </font> <div style="text-align: right"> 2022-07-08 21:17:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pore-Scale Visualization of Hydrogen Storage in a Sandstone at Subsurface Pressure and Temperature Conditions: Trapping, Dissolution and Wettability</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The global commitment to achieve net-zero has led to increasing investment
towards the production and usage of green hydrogen (H2).However, the massive
quantity needed to match future demand will require new storage facilities.
Underground storage of H2 is a potentially viable solution, but poses unique
challenges due to the distinctive physical and chemical properties of H2, that
have yet to be studied quantitatively in the subsurface environment. We have
performed in situ X-ray flow experiments to investigate the fundamentals of
pore-scale fluid displacement processes during H2 injection into an initially
brine saturated Bentheimer sandstone sample. Two different injection schemes
were followed, the displacement of H2 with H2-equilibrated brine and
non-H2-equilibrated brine both at temperature and pressure conditions
representative of deep underground reservoirs. H2 was found to be non-wetting
to brine after both displacement cycles, with average contact angles between
53.72 and 52.72, respectively. We also found a higher recovery of H2 (43.1%)
for non-H2-equilibrated brine compared to that of H2-equilibrated brine
(31.6%), indicating potential dissolution of H2 in unequilibrated brine at
reservoir conditions. Our results suggest that H2 storage may indeed be a
suitable strategy for energy storage, but considerable further research is
needed to fully comprehend the pore-scale interactions at reservoir conditions. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04128v1' target="_blank">http://arxiv.org/pdf/2207.04128v1</a><br> <br> <br> <font size='5'> 678 </font> <div style="text-align: right"> 2022-07-08 09:20:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust optimal investment and risk control for an insurer with general insider information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we study the robust optimal investment and risk control
problem for an insurer who owns the insider information about the financial
market and the insurance market under model uncertainty. Both financial risky
asset process and insurance risk process are assumed to be very general jump
diffusion processes. The insider information is of the most general form rather
than the initial enlargement type. We use the theory of forward integrals to
give the first half characterization of the robust optimal strategy and
transform the anticipating stochastic differential game problem into the
nonanticipative stochastic differential game problem. Then we adopt the
stochastic maximum principle to obtain the total characterization of the robust
strategy. We discuss the two typical situations when the insurer is `small' and
`large' by Malliavin calculus. For the `small' insurer, we obtain the
closed-form solution in the continuous case and the half closed-form solution
in the case with jumps. For the `large' insurer, we reduce the problem to the
quadratic backward stochastic differential equation (BSDE) and obtain the
closed-form solution in the continuous case without model uncertainty. We
discuss some impacts of the model uncertainty, insider information and the
`large' insurer on the optimal strategy. </font><br> Link: <a href='http://arxiv.org/pdf/2207.04052v2' target="_blank">http://arxiv.org/pdf/2207.04052v2</a><br> <br> <br> <font size='5'> 679 </font> <div style="text-align: right"> 2022-07-07 20:48:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Optimal Integrated Planning of Electricity and Hydrogen Infrastructure for Large-Scale Renewable Energy Transport</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The imminent advent of large-scale green hydrogen (H2) production raises the
central question of which of the two options, transporting "green" molecules,
or transporting "green" electrons, is the most cost-effective one. This paper
proposes a first-of-its-kind mathematical framework for the optimal integrated
planning of electricity and H2 infrastructure for transporting large-scale
variable renewable energy (VRE). In contrast to most existing works, this work
incorporates essential nonlinearities such as voltage drops due to losses in
high-voltage alternating current (HVAC) and high-voltage direct current (HVDC)
transmission lines, losses in HVDC converter stations, reactive power flow,
pressure drops in pipelines, and linepack, all of which play an important role
in determining the optimal infrastructure investment decision. Capturing these
nonlinearities requires casting the problem as a nonconvex mixed-integer
nonlinear program (MINLP), whose complexity is further exacerbated by its large
size due to the relatively high temporal resolution of RES forecasts. This work
then leverages recent advancements in convex relaxations to instead solve a
tractable alternative in the form of a mixed-integer quadratically constrained
programming (MIQCP) problem. The impact of other fundamental factors such as
transmission distance and RES capacity is also thoroughly analysed on a
canonical two-node system. The integrated planning model is then demonstrated
on a real-world case study involving renewable energy zones in Australia. </font><br> Link: <a href='http://arxiv.org/pdf/2207.03567v2' target="_blank">http://arxiv.org/pdf/2207.03567v2</a><br> <br> <br> <font size='5'> 680 </font> <div style="text-align: right"> 2022-07-06 19:57:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal investment strategy to maximize the expected utility of an insurance company under Cramer Lundberg dynamic</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we examine the combined problem of optimal portfolio selection
rules for an insurer in a continuous time model where the surplus of an
insurance company is modelled as a compound Poisson process. The company can
invest its surplus in a risk free asset and in a risky asset, governed by the
Black-Scholes equation. According to utility theory, in a financial market
where investors are facing uncertainty, an investor is not concerned with
wealth maximization per se but with utility maximization. It is therefore
possible to introduce an increasing and concave utility function $\phi(x,t)$
representing the expected utility of a risk averse investor (insurance
company). Therefore, the goal of this work is not anymore to maximize the
expected portfolio value or minimize the ruin probability or maximizing the
expectation of the present value of all dividends paid to the shareholders up
to the ruin, but to maximize the expected utility stemming from the wealth
during the life contract [0,T]. In this direction, using the Dynamic
Programming Principle of the problem, we obtain the Hamilton-Jacobi-Bellman
equation by our optimization problem (HJB). Finally, we present numerical
solutions in some cases, obtaining as optimal strategy the well known Merton's
strategy. </font><br> Link: <a href='http://arxiv.org/pdf/2207.02947v1' target="_blank">http://arxiv.org/pdf/2207.02947v1</a><br> <br> <br> <font size='5'> 681 </font> <div style="text-align: right"> 2022-07-06 18:00:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Milestones of research activity in quantum computing: EPS grand challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We argue that quantum computing underwent an inflection point circa 2017.
Long promised funding materialised which prompted public and private
investments around the world. Techniques from machine learning suddenly
influenced central aspects of the field. On one hand, machine learning was used
to emulate quantum systems. On the other hand, quantum algorithms became viewed
as a new type of machine learning model (creating the new model of {\it
variational} quantum computation). Here we sketch some milestones which have
lead to this inflection point. We argue that the next inflection point would
occur around when practical problems will be first solved by quantum computers.
We anticipate that by 2050 this would have become commonplace, were the world
would still be adjusting to the possibilities brought by quantum computers. </font><br> Link: <a href='http://arxiv.org/pdf/2207.02857v1' target="_blank">http://arxiv.org/pdf/2207.02857v1</a><br> <br> <br> <font size='5'> 682 </font> <div style="text-align: right"> 2022-07-06 14:25:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Incentivizing Proof-of-Stake Blockchain for Secured Data Collection in UAV-Assisted IoT: A Multi-Agent Reinforcement Learning Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Internet of Things (IoT) can be conveniently deployed while empowering
various applications, where the IoT nodes can form clusters to finish certain
missions collectively. In this paper, we propose to employ unmanned aerial
vehicles (UAVs) to assist the clustered IoT data collection with
blockchain-based security provisioning. In particular, the UAVs generate
candidate blocks based on the collected data, which are then audited through a
lightweight proof-of-stake consensus mechanism within the UAV-based blockchain
network. To motivate efficient blockchain while reducing the operational cost,
a stake pool is constructed at the active UAV while encouraging stake
investment from other UAVs with profit sharing. The problem is formulated to
maximize the overall profit through the blockchain system in unit time by
jointly investigating the IoT transmission, incentives through investment and
profit sharing, and UAV deployment strategies. Then, the problem is solved in a
distributed manner while being decoupled into two layers. The inner layer
incorporates IoT transmission and incentive design, which are tackled with
large-system approximation and one-leader-multi-follower Stackelberg game
analysis, respectively. The outer layer for UAV deployment is undertaken with a
multi-agent deep deterministic policy gradient approach. Results show the
convergence of the proposed learning process and the UAV deployment, and also
demonstrated is the performance superiority of our proposal as compared with
the baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2207.02705v1' target="_blank">http://arxiv.org/pdf/2207.02705v1</a><br> <br> <br> <font size='5'> 683 </font> <div style="text-align: right"> 2022-07-05 23:42:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Regularized Predictive Models for Beef Eating Quality of Individual Meals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Faced with changing markets and evolving consumer demands, beef industries
are investing in grading systems to maximise value extraction throughout their
entire supply chain. The Meat Standards Australia (MSA) system is a
customer-oriented total quality management system that stands out
internationally by predicting quality grades of specific muscles processed by a
designated cooking method. The model currently underpinning the MSA system
requires laborious effort to estimate and its prediction performance may be
less accurate in the presence of unbalanced data sets where many "muscle x
cook" combinations have few observations and/or few predictors of palatability
are available. This paper proposes a novel predictive method for beef eating
quality that bridges a spectrum of muscle x cook-specific models. At one
extreme, each muscle x cook combination is modelled independently; at the other
extreme a pooled predictive model is obtained across all muscle x cook
combinations. Via a data-driven regularization method, we cover all muscle x
cook-specific models along this spectrum. We demonstrate that the proposed
predictive method attains considerable accuracy improvements relative to
independent or pooled approaches on unique MSA data sets. </font><br> Link: <a href='http://arxiv.org/pdf/2207.02362v1' target="_blank">http://arxiv.org/pdf/2207.02362v1</a><br> <br> <br> <font size='5'> 684 </font> <div style="text-align: right"> 2022-07-05 14:15:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Climate-Contingent Finance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Climate adaptation could yield significant benefits. However, the uncertainty
of which future climate scenarios will occur decreases the feasibility of
proactively adapting. Climate adaptation projects could be underwritten by
benefits paid for in the climate scenarios that each adaptation project is
designed to address because other entities would like to hedge the financial
risk of those scenarios. Because the return on investment is a function of the
level of climate change, it is optimal for the adapting entity to finance
adaptation with repayment as a function of the climate. It is also optimal for
entities with more financial downside under a more extreme climate to serve as
an investing counterparty because they can obtain higher than market rates of
return when they need it most.
  In this way, parties proactively adapting would reduce the risk they
over-prepare, while their investors would reduce the risk they under-prepare.
This is superior to typical insurance because, by investing in
climate-contingent mechanisms, investors are not merely financially hedging but
also outright preventing physical damage, and therefore creating economic
value. This coordinates capital through time and place according to parties'
risk reduction capabilities and financial profiles, while also providing a
diversifying investment return.
  Climate-contingent finance can be generalized to any situation where entities
share exposure to a risk where they lack direct control over whether it occurs
(e.g., climate change, or a natural pandemic), and one type of entity can take
proactive actions to benefit from addressing the effects of the risk if it
occurs (e.g., through innovating on crops that would do well under extreme
climate change or vaccination technology that could address particular viruses)
with funding from another type of entity that seeks a targeted return to
ameliorate the downside. </font><br> Link: <a href='http://arxiv.org/pdf/2207.02064v1' target="_blank">http://arxiv.org/pdf/2207.02064v1</a><br> <br> <br> <font size='5'> 685 </font> <div style="text-align: right"> 2022-07-04 20:35:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Economics of Privacy and Utility: Investment Strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The inevitable leakage of privacy as a result of unrestrained disclosure of
personal information has motivated extensive research on robust
privacy-preserving mechanisms. However, existing research is mostly limited to
solving the problem in a static setting with disregard for the privacy leakage
over time. Unfortunately, this treatment of privacy is insufficient in
practical settings where users continuously disclose their personal information
over time resulting in an accumulated leakage of the users' sensitive
information. In this paper, we consider privacy leakage over a finite time
horizon and investigate optimal strategies to maximize the utility of the
disclosed data while limiting the finite-horizon privacy leakage. We consider a
simple privacy mechanism that involves compressing the user's data before each
disclosure to meet the desired constraint on future privacy. We further
motivate several algorithms to optimize the dynamic privacy-utility tradeoff
and evaluate their performance via extensive synthetic performance tests. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10253v1' target="_blank">http://arxiv.org/pdf/2208.10253v1</a><br> <br> <br> <font size='5'> 686 </font> <div style="text-align: right"> 2022-07-04 19:51:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: High temperature nanocomposites with photonic group velocity suppression of thermal emission</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quenching of thermal emission above 0 K is an unusual material property,
essential for future energy, transportation, and space technologies. Despite
the great effort invested, nearly complete quenching of thermal radiation
rather than some reduction of its flux has only been achieved at low
temperatures (below 373 K) and in narrow spectral windows using complex
techniques suitable only for small scale objects. In this work, we present a
light and flexible composite material that can suppress propagating photonic
modes and, in this way, quench thermal radiation while preserving heat transfer
(by thermal conduction) at a room and higher temperature below 600 K. This has
been achieved by altering the local photonic density of states and
consequentially the thermal properties of carbon nanotubes forming a
percolating nanofiber network with a thermostable polymeric matrix. </font><br> Link: <a href='http://arxiv.org/pdf/2207.01695v1' target="_blank">http://arxiv.org/pdf/2207.01695v1</a><br> <br> <br> <font size='5'> 687 </font> <div style="text-align: right"> 2022-07-04 15:54:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Time-consistent pension policy with minimum guarantee and sustainability constraint</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper proposes and investigates an optimal pair investment/pension
policy for a pay-as-you-go (PAYG) pension scheme. The social planner can invest
in a buffer fund in order to guarantee a minimal pension amount. The model aims
at taking into account complex dynamic phenomena such as the demographic risk
and its evolution over time, the time and age dependence of agents preferences,
and financial risks. The preference criterion of the social planner is modeled
by a consistent dynamic utility defined on a stochastic domain, which
incorporates the heterogeneity of overlapping generations and its evolution
over time. The preference criterion and the optimization problem also
incorporate sustainability, adequacy and fairness constraints. The paper
designs and solves the social planner's dynamic decision criterion, and
computes the optimal investment/pension policy in a general framework. A
detailed analysis for the case of dynamic power utilities is provided. </font><br> Link: <a href='http://arxiv.org/pdf/2207.01536v1' target="_blank">http://arxiv.org/pdf/2207.01536v1</a><br> <br> <br> <font size='5'> 688 </font> <div style="text-align: right"> 2022-07-04 08:06:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Permutation-Based Heuristic for Buy Low, Sell High</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Buy low, sell high is one of the basic rules of thumb used in investment,
although it is not considered to be a beneficial strategy. In this paper, we
show how the appropriate permutation-based representation (i.e., the epistemic
form) of a minute-by-minute trading time-series, alongside the use of a simple
decision heuristic (i.e., the epistemic game), may surprisingly result in
significant benefits. Using our heuristic for selecting seven stocks, we ran
two experiments on the data. The results provide empirical support for the
possible benefit of using simple decision models in investment, even in the
context of minute-by-minute trading. </font><br> Link: <a href='http://arxiv.org/pdf/2207.01245v1' target="_blank">http://arxiv.org/pdf/2207.01245v1</a><br> <br> <br> <font size='5'> 689 </font> <div style="text-align: right"> 2022-07-03 18:49:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Probing Supermassive Black Hole Binaries with Orbital Resonances of Laser-Ranged Satellite</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Coalescing supermassive black hole binaries (SMBHBs) are the primary source
candidates for low frequency gravitational wave (GW) detections, which could
bring us deep insights into galaxy evolutions over cosmic time and violent
processes of spacetime dynamics. Promising candidates had been found based on
optical and X-ray observations, which claims for new and ready-to-use GW
detection approaches before the operations of space-borne antennas. We show
that, satellite laser ranging (SLR) missions could serve as probes of
coalescing SMBHBs through the GW-induced resonant effects. Lasting and
characteristic imprints caused by such resonances in the residual distances or
accelerations from SLR measurements are studied, and the detection SNR is
analyzed with both the current and future improved ranging precisions. Within
redshift $z \sim 1$, the threshold SNR=5 requires 1-2 years of accumulated data
for the current precision and months of data for improved precision, which are
workable for the data processing of SLR missions. Meanwhile, joint detections
with multiple SLR missions could further improve the total SNR and the
confidence level. Such a detection scheme could fulfill the requirement of a
tentative SMBHB probe during the preparing stage of LISA and Taiji, and it
requires no further investment to any new and advanced facilities. It is also
worthwhile to look back and re-process the archived data from the past decades,
in where resonant signals from SMBHBs might be hidden. </font><br> Link: <a href='http://arxiv.org/pdf/2207.01100v3' target="_blank">http://arxiv.org/pdf/2207.01100v3</a><br> <br> <br> <font size='5'> 690 </font> <div style="text-align: right"> 2022-07-03 04:06:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stochastic arbitrage with market index options</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Opportunities for stochastic arbitrage in an options market arise when it is
possible to construct a portfolio of options which provides a positive option
premium and which, when combined with a direct investment in the underlying
asset, generates a payoff which stochastically dominates the payoff from the
direct investment in the underlying asset. We provide linear and mixed
integer-linear programs for computing the stochastic arbitrage opportunity
providing the maximum option premium to an investor. We apply our programs to
18 years of data on monthly put and call options on the Standard & Poors 500
index, confining attention to options with moderate moneyness, and using two
specifications of the underlying asset return distribution, one symmetric and
one skewed. The pricing of market index options with moderate moneyness appears
to be broadly consistent with our skewed specification of market returns. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00949v2' target="_blank">http://arxiv.org/pdf/2207.00949v2</a><br> <br> <br> <font size='5'> 691 </font> <div style="text-align: right"> 2022-07-02 11:20:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Arigat: Effects of Adaptive Guidance on Engagement and Performance in Augmented Reality Learning Environments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Experiential learning (ExL) is the process of learning through experience or
more specifically "learning through reflection on doing". In this paper, we
propose a simulation of these experiences, in Augmented Reality (AR),
addressing the problem of language learning. Such systems provide an excellent
setting to support "adaptive guidance", in a digital form, within a real
environment. Adaptive guidance allows the instructions and learning content to
be customised for the individual learner, thus creating a unique learning
experience. We developed an adaptive guidance AR system for language learning,
we call Arigat\=o (Augmented Reality Instructional Guidance & Tailored
Omniverse), which offers immediate assistance, resources specific to the
learner's needs, manipulation of these resources, and relevant feedback.
Considering guidance, we employ this prototype to investigate the effect of the
amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed
vs. adaptive-associations) on the engagement and consequently the learning
outcomes of language learning in an AR environment. The results for the amount
of guidance show that compared to the adaptive-amount, the fixed-amount of
guidance group scored better in the immediate and delayed (after 7 days) recall
tests. However, this group also invested a significantly higher mental effort
to complete the task. The results for the type of guidance show that the
adaptive-associations group outperforms the fixed-associations group in the
immediate, delayed (after 7 days) recall tests, and learning efficiency. The
adaptive-associations group also showed significantly lower mental effort and
spent less time to complete the task. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00798v1' target="_blank">http://arxiv.org/pdf/2207.00798v1</a><br> <br> <br> <font size='5'> 692 </font> <div style="text-align: right"> 2022-07-02 02:50:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Study on Impact of Capital Structure on Profitability of Companies Listed in Indian Stock Exchange with respect to Automobile Industry</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Current research helps in understanding both positive and negative impacts of
capital structure on profits of Indian automobile companies by using variables
like Return on Capital Employed, Return on Long Term Funds, Return on Net
Worth, Gross Profit Margin, and Operating Profit, and Return on Asset. The
study hypothesized that RoCE, RoLT, and RoNW have a positive effect and GP, OP
and ROA have a negative impact on debt-equity and interest coverage ratios i.e
capital structure of the companies. Also, the study proves that the
relationship between profitability and capital structure variables is strongly
significant. The hypothesis was tested by using fixed effect and random effect
models by considering 10 years of data (from 2010-2019) from 17 automobile
companies. The result of the study recommends that the firms can improve their
performance by using an optimal capital structure. Also, a fair mix of debt and
equity should be established to ensure that the firm maintains capital
adequacy. Firms can thus be able to meet their financial compulsions and
investments that can promise attractive returns. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00720v1' target="_blank">http://arxiv.org/pdf/2207.00720v1</a><br> <br> <br> <font size='5'> 693 </font> <div style="text-align: right"> 2022-07-02 02:26:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A study on Determinants of Dividend Policy and its Impact on Financial Performances: A Panel Data Analysis for Indian Listed Firms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Determination of the correct mix of dividend and retained earnings and its
effect on profitability has been a subject of controversy in financial
management literature. This paper seeks to contribute to the ongoing debate by
examining the relationship between dividend payout policy and the financial
performance of 60 firms listed on the National Stock Exchange between
2009-2018. The Return on Assets (ROA) served as a surrogate for the dependent
variable, profitability, while the Dividend Pay-out ratio proxied for dividend
policy and was the only explanatory variable. Control variables include firm
size, asset tangibility, and leverage. Regression result reveals a positive and
significant relationship between dividend payout policy (DPO) and firm
performance (ROA). It is recommended that companies should endeavor to put in
place a robust dividend payout policy that would encourage investment in
projects that give positive Net Present Value. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00715v1' target="_blank">http://arxiv.org/pdf/2207.00715v1</a><br> <br> <br> <font size='5'> 694 </font> <div style="text-align: right"> 2022-07-01 21:35:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economic Consequences of the COVID-19 Pandemic on Sub-Saharan Africa: A historical perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper examined the economic consequences of the COVID-19 pandemic on
sub-Saharan Africa (SSA) using the historical approach and analysing the policy
responses of the region to past crises and their economic consequences. The
study employed the manufacturing-value-added share of GDP as a performance
indicator. The analysis shows that the wrong policy interventions to past
crises led the sub-Saharan African sub-region into its deplorable economic
situation. The study observed that the region leapfrogged prematurely to import
substitution, export promotion, and global value chains. Based on these
experiences, the region should adopt a gradual approach in responding to the
COVID-19 economic consequences. The sub-region should first address relevant
areas of sustainability, including proactive investment in research and
development to develop homegrown technology, upgrade essential infrastructural
facilities, develop security infrastructure, and strengthen the financial
sector. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00666v1' target="_blank">http://arxiv.org/pdf/2207.00666v1</a><br> <br> <br> <font size='5'> 695 </font> <div style="text-align: right"> 2022-07-01 13:59:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Shai-am: A Machine Learning Platform for Investment Strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The finance industry has adopted machine learning (ML) as a form of
quantitative research to support better investment decisions, yet there are
several challenges often overlooked in practice. (1) ML code tends to be
unstructured and ad hoc, which hinders cooperation with others. (2) Resource
requirements and dependencies vary depending on which algorithm is used, so a
flexible and scalable system is needed. (3) It is difficult for domain experts
in traditional finance to apply their experience and knowledge in ML-based
strategies unless they acquire expertise in recent technologies. This paper
presents Shai-am, an ML platform integrated with our own Python framework. The
platform leverages existing modern open-source technologies, managing
containerized pipelines for ML-based strategies with unified interfaces to
solve the aforementioned issues. Each strategy implements the interface defined
in the core framework. The framework is designed to enhance reusability and
readability, facilitating collaborative work in quantitative research. Shai-am
aims to be a pure AI asset manager for solving various tasks in financial
markets. </font><br> Link: <a href='http://arxiv.org/pdf/2207.00436v1' target="_blank">http://arxiv.org/pdf/2207.00436v1</a><br> <br> <br> <font size='5'> 696 </font> <div style="text-align: right"> 2022-06-30 16:29:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Solving Bilevel AC OPF Problems by Smoothing the Complementary Conditions -- Part II: Solution Techniques and Case Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This is a second part of the research on AC optimal power flow being used in
the lower level of the bilevel strategic bidding or investment models. As an
example of a suitable upper-level problem, we observe a strategic bidding of
energy storage and propose a novel formulation based on the smoothing
technique. After presenting the idea and scope of our work, as well as the
model itself and the solution algorithm in the companion paper (Part I), this
paper presents a number of existing solution techniques and the proposed one
based on smoothing the complementary conditions. The superiority of the
proposed algorithm and smoothing techniques is demonstrated in terms of
accuracy and computational tractability over multiple transmission networks of
different sizes and different OPF models. The results indicate that the
proposed approach outperforms all other options in both metrics by a
significant margin. This is especially noticeable in the metric of accuracy
where out of total 422 optimizations over 9 meshed networks the greatest AC OPF
error is 0.023% that is further reduced to 3.3e-4% in the second iteration of
our algorithm. </font><br> Link: <a href='http://arxiv.org/pdf/2207.01509v1' target="_blank">http://arxiv.org/pdf/2207.01509v1</a><br> <br> <br> <font size='5'> 697 </font> <div style="text-align: right"> 2022-06-30 10:08:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Young Physicists Forum and the Importance for Education and Capacity Development for Africa</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Higher education and advanced scientific research lead to social, economic,
and political development of any country. All developed societies like the
current 2022 G7 countries: Canada, France, Germany, Italy, Japan, the UK, and
the US have all not only heavily invested in higher education but also in
advanced scientific research in their respective countries. Similarly, for
African countries to develop socially, economically, and politically, they must
follow suit by massively investing in higher education and local scientific
research. </font><br> Link: <a href='http://arxiv.org/pdf/2206.15171v1' target="_blank">http://arxiv.org/pdf/2206.15171v1</a><br> <br> <br> <font size='5'> 698 </font> <div style="text-align: right"> 2022-06-29 16:45:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A hybrid level-based learning swarm algorithm with mutation operator for solving large-scale cardinality-constrained portfolio optimization problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we propose a hybrid variant of the level-based learning swarm
optimizer (LLSO) for solving large-scale portfolio optimization problems. Our
goal is to maximize a modified formulation of the Sharpe ratio subject to
cardinality, box and budget constraints. The algorithm involves a projection
operator to deal with these three constraints simultaneously and we implicitly
control transaction costs thanks to a rebalancing constraint. We also introduce
a suitable exact penalty function to manage the turnover constraint. In
addition, we develop an ad hoc mutation operator to modify candidate exemplars
in the highest level of the swarm. The experimental results, using three
large-scale data sets, show that the inclusion of this procedure improves the
accuracy of the solutions. Then, a comparison with other variants of the LLSO
algorithm and two state-of-the-art swarm optimizers points out the outstanding
performance of the proposed solver in terms of exploration capabilities and
solution quality. Finally, we assess the profitability of the portfolio
allocation strategy in the last five years using an investible pool of 1119
constituents from the MSCI World Index. </font><br> Link: <a href='http://arxiv.org/pdf/2206.14760v1' target="_blank">http://arxiv.org/pdf/2206.14760v1</a><br> <br> <br> <font size='5'> 699 </font> <div style="text-align: right"> 2022-06-29 11:50:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Environmental-Social-Governance Preferences and Investments in Crypto-Assets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Individuals invest in Environmental-Social-Governance (ESG)-assets not only
because of (higher) expected returns but also driven by ethical and social
considerations. Less is known about ESG-conscious investor subjective beliefs
about crypto-assets and how do these compare to traditional assets.
Controversies surrounding the ESG footprint of certain crypto-asset classes -
mainly on grounds of their energy-intensive crypto mining - offer a potentially
informative object of inquiry. Leveraging a unique representative household
finance survey for the Austrian population, we examine whether investors' ESG
preferences can explain cross-sectional differences in individual portfolio
exposure to crypto-assets. We find a strong association between investors' ESG
preferences and the crypto-investment exposure. The ESG-conscious investor
attention is higher for crypto-assets compared to traditional asset classes
such as bonds and shares. </font><br> Link: <a href='http://arxiv.org/pdf/2206.14548v1' target="_blank">http://arxiv.org/pdf/2206.14548v1</a><br> <br> <br> <font size='5'> 700 </font> <div style="text-align: right"> 2022-06-28 17:04:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Model stealing attacks present a dilemma for public machine learning APIs. To
protect financial investments, companies may be forced to withhold important
information about their models that could facilitate theft, including
uncertainty estimates and prediction explanations. This compromise is harmful
not only to users but also to external transparency. Model stealing defenses
seek to resolve this dilemma by making models harder to steal while preserving
utility for benign users. However, existing defenses have poor performance in
practice, either requiring enormous computational overheads or severe utility
trade-offs. To meet these challenges, we present a new approach to model
stealing defenses called gradient redirection. At the core of our approach is a
provably optimal, efficient algorithm for steering an adversary's training
updates in a targeted manner. Combined with improvements to surrogate networks
and a novel coordinated defense strategy, our gradient redirection defense,
called GRAD${}^2$, achieves small utility trade-offs and low computational
overhead, outperforming the best prior defenses. Moreover, we demonstrate how
gradient redirection enables reprogramming the adversary with arbitrary
behavior, which we hope will foster work on new avenues of defense. </font><br> Link: <a href='http://arxiv.org/pdf/2206.14157v1' target="_blank">http://arxiv.org/pdf/2206.14157v1</a><br> <br> <br> <font size='5'> 701 </font> <div style="text-align: right"> 2022-06-28 16:42:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Business Cycle Synchronization in the EU: A Regional-Sectoral Look through Soft-Clustering and Wavelet Decomposition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper elaborates on the sectoral-regional view of the business cycle
synchronization in the EU -- a necessary condition for the optimal currency
area. We argue that complete and tidy clustering of the data improves the
decision maker's understanding of the business cycle and, by extension, the
quality of economic decisions. We define the business cycles by applying a
wavelet approach to drift-adjusted gross value added data spanning over 2000Q1
to 2021Q2. For the application of the synchronization analysis, we propose the
novel soft-clustering approach, which adjusts hierarchical clustering in
several aspects. First, the method relies on synchronicity dissimilarity
measures, noting that, for time series data, the feature space is the set of
all points in time. Then, the ``soft'' part of the approach strengthens the
synchronization signal by using silhouette measures. Finally, we add a
probabilistic sparsity algorithm to drop out the most asynchronous ``noisy''
data improving the silhouette scores of the most and less synchronous groups.
The method, hence, splits the sectoral-regional data into three groups: the
synchronous group that shapes the EU business cycle; the less synchronous group
that may hint at cycle forecasting relevant information; the asynchronous group
that may help investors to diversify through-the-cycle risks of the investment
portfolios. The results support the core-periphery hypothesis. </font><br> Link: <a href='http://arxiv.org/pdf/2206.14128v1' target="_blank">http://arxiv.org/pdf/2206.14128v1</a><br> <br> <br> <font size='5'> 702 </font> <div style="text-align: right"> 2022-06-28 11:49:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Grid Tariffs for Peak Demand Reduction: Is there a Price Signal Conflict with Electricity Spot Prices?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The electricity grid is expected to require vast investments due to the
decarbonization-by-electrification trend, calling for a change in grid tariff
design which provides proper incentives for reducing peak loads. However, price
signals from grid tariffs could be distorted from electricity spot prices which
also represents a significant of the total consumer electricity bill. This
paper attempts to identify whether there is a price signal conflict between
grid tariffs and spot prices. Four different grid tariff designs are compared,
using a generic demand response model as part of a cost-minimizing linear
program to simulate the reduction in peak load. The method is applied to
metered electricity demand from 3608 consumers in Oslo, Norway. Results show
that new grid tariff designs reduce peak loads by 1-4%, and that reduction in
peak load is smaller when consumers are subject to electricity spot prices. </font><br> Link: <a href='http://arxiv.org/pdf/2206.13916v1' target="_blank">http://arxiv.org/pdf/2206.13916v1</a><br> <br> <br> <font size='5'> 703 </font> <div style="text-align: right"> 2022-06-26 20:49:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cost-Asymmetric Memory Hard Password Hashing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the past decade, billions of user passwords have been exposed to the
dangerous threat of offline password cracking attacks. An offline attacker who
has stolen the cryptographic hash of a user's password can check as many
password guesses as s/he likes limited only by the resources that s/he is
willing to invest to crack the password. Pepper and key-stretching are two
techniques that have been proposed to deter an offline attacker by increasing
guessing costs. Pepper ensures that the cost of rejecting an incorrect password
guess is higher than the (expected) cost of verifying a correct password guess.
This is useful because most of the offline attacker's guesses will be
incorrect. Unfortunately, as we observe the traditional peppering defense seems
to be incompatible with modern memory hard key-stretching algorithms such as
Argon2 or Scrypt. We introduce an alternative to pepper which we call
Cost-Asymmetric Memory Hard Password Authentication which benefits from the
same cost-asymmetry as the classical peppering defense i.e., the cost of
rejecting an incorrect password guess is larger than the expected cost to
authenticate a correct password guess. When configured properly we prove that
our mechanism can only reduce the percentage of user passwords that are cracked
by a rational offline attacker whose goal is to maximize (expected) profit
i.e., the total value of cracked passwords minus the total guessing costs. We
evaluate the effectiveness of our mechanism on empirical password datasets
against a rational offline attacker. Our empirical analysis shows that our
mechanism can reduce significantly the percentage of user passwords that are
cracked by a rational attacker by up to 10%. </font><br> Link: <a href='http://arxiv.org/pdf/2206.12970v1' target="_blank">http://arxiv.org/pdf/2206.12970v1</a><br> <br> <br> <font size='5'> 704 </font> <div style="text-align: right"> 2022-06-26 11:44:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Spectrum Sharing Among Multiple-Seller and Multiple-Buyer Operators of A Mobile Network: A Stochastic Geometry Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sharing the spectrum among mobile network operators (MNOs) is a promising
approach to improve the spectrum utilization and to increase the monetary
income of MNOs. In this paper, we model a nonorthogonal spectrum sharing system
for a large-scale cellular network where multiple seller MNOs lease their
licensed sub-bands to several buyer MNOs. We first analyze the per-user
expected rate and the per-MNO expected profit using stochastic geometry. Then,
we formulate the joint problem of power control and licensed sub-band sharing
to maximize the expected profit of all MNOs as a multiobjective optimization
problem (MOOP) under the users' quality of service requirement and the
nonnegative return on investment constraints. To transform the MOOP into a
single objective form, we use a combination of the $\epsilon$-constraint and
weighted sum methods. However, the transformed problem is nonconvex because of
the presence of binary variables and nonconvex rate functions in the objective
function and constraints. We address this problem by using a penalty function
and approximating the nonconvex rate functions by a constrained stochastic
successive convex approximation method. Finally, the numerical results show the
correctness and performance of the proposed algorithm under various conditions. </font><br> Link: <a href='http://arxiv.org/pdf/2206.12852v1' target="_blank">http://arxiv.org/pdf/2206.12852v1</a><br> <br> <br> <font size='5'> 705 </font> <div style="text-align: right"> 2022-06-25 02:27:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A gated group sequential design for seamless Phase II/III trial with subpopulation selection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Due to the high cost and high failure rate of Phase III trials, seamless
Phase II/III designs are more and more popular to trial efficiency. A potential
attraction of Phase II/III design is to allow a randomized proof-of-concept
stage prior to committing to the full cost of the Phase III trial. Population
selection during the trial allows a trial to adapt and focus investment where
it is most likely to provide patient benefit. Motivated by a clinical trial to
find the population that potential benefits with dual-primary endpoints
progression free survival (PFS) and overall survival (OS), we propose a gated
group sequential design for a seamless Phase II/III trial design with
population selection. The investigated design controls the familywise error
rate and allows multiple interim analyses to enable early stopping for efficacy
or futility. Simulations and an illustrative example suggest that the proposed
gated group sequential design can have more power than the commonly used
classical group sequential design, and reduces the patient's exposure to less
effective treatment if the complementary sub-group has less significant
treatment effect. The proposed design has the potential to save drug
development cost and more quickly fulfill unmet medical needs. </font><br> Link: <a href='http://arxiv.org/pdf/2206.12536v1' target="_blank">http://arxiv.org/pdf/2206.12536v1</a><br> <br> <br> <font size='5'> 706 </font> <div style="text-align: right"> 2022-06-24 23:07:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cost-efficiency in Incomplete Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the topic of cost-efficiency in incomplete markets. A
portfolio payoff is called cost-efficient if it achieves a given probability
distribution at some given investment horizon with a minimum initial budget.
Extensive literature exists for the case of a complete financial market. We
show how the problem can be extended to incomplete markets and that the main
results from the theory of complete markets still hold in adapted form. In
particular, we find that in incomplete markets, the optimal portfolio choice
for law-invariant non-decreasing preferences must be "perfectly"
cost-efficient. This notion of perfect cost-efficiency is shown to be
equivalent to the fact that the payoff can be rationalized, i.e., it is the
solution to an expected utility problem. </font><br> Link: <a href='http://arxiv.org/pdf/2206.12511v1' target="_blank">http://arxiv.org/pdf/2206.12511v1</a><br> <br> <br> <font size='5'> 707 </font> <div style="text-align: right"> 2022-06-24 12:28:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Intersecting near-optimal spaces: European power systems with more resilience to weather variability</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We suggest a new methodology for designing robust energy systems. For this,
we investigate so-called near-optimal solutions to energy system optimisation
models; solutions whose objective values deviate only marginally from the
optimum. Using a refined method for obtaining explicit geometric descriptions
of these near-optimal feasible spaces, we find designs that are as robust as
possible to perturbations. This contributes to the ongoing debate on how to
define and work with robustness in energy systems modelling. We apply our
methods in an investigation using multiple decades of weather data. For the
first time, we run a capacity expansion model of the European power system (one
node per country) with a 3-hourly temporal resolution with 41 years of weather
data. While an optimisation with 41 weather years is at the limits of
computational feasibility, we use the near-optimal feasible spaces of single
years to gain an understanding of the design space over the full time period.
Specifically, we intersect all near-optimal feasible spaces for the individual
years in order to get designs that are likely to be feasible over the entire
time period. We find significant potential for investment flexibility, and
verify the feasibility of these designs by simulating the resulting dispatch
problem with four decades of weather data. They are characterised by a shift
towards more onshore wind and solar power, while emitting up to 50% less $CO_2$
than a cost-optimal solution over that period. Our work builds on recent
developments in the field, including techniques such as Modelling to Generate
Alternatives and Modelling All Alternatives, and provides new insights into the
geometry of near-optimal feasible spaces and the importance of multi-decade
weather variability for energy systems design. We also provide an effective way
of working with a multi-decade time frame in a highly parallelised manner. </font><br> Link: <a href='http://arxiv.org/pdf/2206.12242v2' target="_blank">http://arxiv.org/pdf/2206.12242v2</a><br> <br> <br> <font size='5'> 708 </font> <div style="text-align: right"> 2022-06-23 07:52:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A core-selecting auction for portfolio's packages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce the "local-global" approach for a divisible portfolio and
perform an equilibrium analysis for two variants of core-selecting auctions.
Our main novelty is extending the Nearest-VCG pricing rule in a dynamic
two-round setup, mitigating bidders' free-riding incentives and further
reducing the sellers' costs. The two-round setup admits an
information-revelation mechanism that may offset the "winner's curse", and it
is in accord with the existing iterative procedure of combinatorial auctions.
With portfolio trading becoming an increasingly important part of investment
strategies, our mechanism contributes to increasing interest in portfolio
auction protocols. </font><br> Link: <a href='http://arxiv.org/pdf/2206.11516v2' target="_blank">http://arxiv.org/pdf/2206.11516v2</a><br> <br> <br> <font size='5'> 709 </font> <div style="text-align: right"> 2022-06-22 14:02:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Recursive Overbetting of a Satellite Investment Account</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper builds a core-satellite model of semi-static Kelly betting and
log-optimal investment. We study the problem of a saver whose core portfolio
consists in unlevered (1x) retirement plans with no access to margin debt.
However, the agent has a satellite investment account with recourse to
significant, but not unlimited, leverage; accordingly, we study optimal
controllers for the satellite gearing ratio. On a very short time horizon, the
best policy is to overbet the satellite, whereby the overriding objective is to
raise the aggregate beta toward a growth-optimal level. On an infinite horizon,
by contrast, the correct behavior is to blithely ignore the core and optimize
the exponential growth rate of the satellite, which will anyways come to
dominate the entire bankroll in the limit. For time horizons strictly between
zero and infinity, the optimal strategy is not so simple: there is a key
trade-off between the instantaneous growth rate of the composite bankroll, and
that of the satellite itself, which suffers ongoing volatility drag from the
overbetting. Thus, a very perspicacious policy is called for, since any losses
in the satellite will constrain the agent's access to leverage in the
continuation problem. We characterize the optimal feedback controller, and
compute it in earnest by solving the corresponding HJB equation recursively and
backward in time. This solution is then compared to the best open-loop
controller, which, in spite of its relative simplicity, is expected to perform
similarly in practical situations. </font><br> Link: <a href='http://arxiv.org/pdf/2206.11105v2' target="_blank">http://arxiv.org/pdf/2206.11105v2</a><br> <br> <br> <font size='5'> 710 </font> <div style="text-align: right"> 2022-06-22 13:59:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: 40 Years of Designing Code Comprehension Experiments: A Systematic Mapping Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The relevance of code comprehension in a developer's daily work was
recognized more than 40 years ago. Over the years, several studies have
gathered evidence that developers do indeed invest a considerable amount of
their daily work in code comprehension. Consequently, many studies were
conducted to find out how developers could be supported during code
comprehension and which code characteristics contribute to better
comprehension. Today, such experiments are more common than ever. While this is
great for advancing the field, the number of publications makes it difficult to
keep an overview. Additionally, designing rigorous experiments with human
participants is a challenging task, and the multitude of design decisions and
options can make it difficult for researchers to select a suitable design.
  We therefore conducted a systematic mapping study of 95 source code
comprehension experiments published between 1979 and 2019. By systematically
structuring the design characteristics of code comprehension studies, we
provide a basis for subsequent discussion of the huge diversity of design
options in the face of a lack of basic research on their consequences and
comparability. We describe what topics have been studied, as well as how these
studies have been designed, conducted, and reported. Frequently chosen design
options and deficiencies are pointed out. We conclude with five concrete action
items that we as a research community should address moving forward to improve
publications of code comprehension experiments. </font><br> Link: <a href='http://arxiv.org/pdf/2206.11102v1' target="_blank">http://arxiv.org/pdf/2206.11102v1</a><br> <br> <br> <font size='5'> 711 </font> <div style="text-align: right"> 2022-06-22 13:37:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AlphaMLDigger: A Novel Machine Learning Solution to Explore Excess Return on Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: How to quickly and automatically mine effective information and serve
investment decisions has attracted more and more attention from academia and
industry. And new challenges have arisen with the global pandemic. This paper
proposes a two-phase AlphaMLDigger that effectively finds excessive returns in
a highly fluctuated market. In phase 1, a deep sequential natural language
processing (NLP) model is proposed to transfer Sina Microblog blogs to market
sentiment. In phase 2, the predicted market sentiment is combined with social
network indicator features and stock market history features to predict the
stock movements with different Machine Learning models and optimizers. The
results show that the ensemble models achieve an accuracy of 0.984 and
significantly outperform the baseline model. In addition, we find that COVID-19
brings data shift to China's stock market. </font><br> Link: <a href='http://arxiv.org/pdf/2206.11072v2' target="_blank">http://arxiv.org/pdf/2206.11072v2</a><br> <br> <br> <font size='5'> 712 </font> <div style="text-align: right"> 2022-06-21 19:11:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis of Hydrogen Production Costs across the United States and over the next 30 years</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Hydrogen can play an important role for decarbonization. While hydrogen is
usually produced through SMR, it can also be produced through water
electrolysis which is cleaner. The relative cost and carbon intensity of
hydrogen production through SMR and electrolysis vary throughout the United
States because of differences in the grid. While many hydrogen cost models
exist, no regional hydrogen study has been conducted across the US. We studied
how the Levelized Cost of Hydrogen (LCOH) and carbon intensity for producing
hydrogen vary across the US. We looked at electrolysis technologies (Alkaline,
PEM, and SOEC) and compared them to SMR. In 2020, SMR with 90 percent CCUS has
a lower average LCOH and carbon intensity for hydrogen production than
electrolysis by SOEC. For states with cleaner grids, hydrogen produced through
SOEC has a lower carbon intensity than hydrogen produced using SMR with 90
percent CCUS. Washington has one of the lowest carbon footprints and the lowest
LCOH to produce hydrogen through electrolysis (alkaline). We predict that the
LCOH for hydrogen production will be 3.2 USD per kg for Alkaline, 3.1 USD per
kg for PEM, and 2.6 USD per kg for SOEC by 2050 with constant electricity
prices. These projected LCOHs are still higher than the LCOH for hydrogen
produced through SMR with 90 percent CCUS. If electricity costs decrease to 2c
per kWh, we expect to reach cost-parity with SMR with 90 percent CCUS. The
results suggest that significant investment in decarbonizing the grid and
lowering the cost of electricity needs to be made to make electrolysis more
competitive compared to SMR. </font><br> Link: <a href='http://arxiv.org/pdf/2206.10689v2' target="_blank">http://arxiv.org/pdf/2206.10689v2</a><br> <br> <br> <font size='5'> 713 </font> <div style="text-align: right"> 2022-06-21 15:59:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment and Equilibrium Pricing under Ambiguity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider portfolio selection under nonparametric $\alpha$-maxmin ambiguity
in the neighbourhood of a reference distribution. We show strict concavity of
the portfolio problem under ambiguity aversion. Implied demand functions are
nondifferentiable, resemble observed bid-ask spreads, and are consistent with
existing parametric limiting participation results under ambiguity. Ambiguity
seekers exhibit a discontinuous demand function, implying an empty set of
reservation prices. If agents have identical, or sufficiently similar prior
beliefs, the first-best equilibrium is no trade. Simple conditions yield the
existence of a Pareto-efficient second-best equilibrium, implying that
heterogeneity in ambiguity preferences is sufficient for mutually beneficial
transactions among all else homogeneous traders. These equilibria reconcile
many observed phenomena in liquid high-information financial markets, such as
liquidity dry-ups, portfolio inertia, and negative risk premia. </font><br> Link: <a href='http://arxiv.org/pdf/2206.10489v1' target="_blank">http://arxiv.org/pdf/2206.10489v1</a><br> <br> <br> <font size='5'> 714 </font> <div style="text-align: right"> 2022-06-20 16:27:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study examines issues of algorithmic fairness in the context of systems
that inform tax audit selection by the United States Internal Revenue Service
(IRS). While the field of algorithmic fairness has developed primarily around
notions of treating like individuals alike, we instead explore the concept of
vertical equity -- appropriately accounting for relevant differences across
individuals -- which is a central component of fairness in many public policy
settings. Applied to the design of the U.S. individual income tax system,
vertical equity relates to the fair allocation of tax and enforcement burdens
across taxpayers of different income levels. Through a unique collaboration
with the Treasury Department and IRS, we use access to anonymized individual
taxpayer microdata, risk-selected audits, and random audits from 2010-14 to
study vertical equity in tax administration. In particular, we assess how the
use of modern machine learning methods for selecting audits may affect vertical
equity. First, we show how the use of more flexible machine learning
(classification) methods -- as opposed to simpler models -- shifts audit
burdens from high to middle-income taxpayers. Second, we show that while
existing algorithmic fairness techniques can mitigate some disparities across
income, they can incur a steep cost to performance. Third, we show that the
choice of whether to treat risk of underreporting as a classification or
regression problem is highly consequential. Moving from classification to
regression models to predict underreporting shifts audit burden substantially
toward high income individuals, while increasing revenue. Last, we explore the
role of differential audit cost in shaping the audit distribution. We show that
a narrow focus on return-on-investment can undermine vertical equity. Our
results have implications for the design of algorithmic tools across the public
sector. </font><br> Link: <a href='http://arxiv.org/pdf/2206.09875v1' target="_blank">http://arxiv.org/pdf/2206.09875v1</a><br> <br> <br> <font size='5'> 715 </font> <div style="text-align: right"> 2022-06-19 02:09:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Gender Pay Gap in China: Insights from a Discrimination Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Equal pay is an essential component of gender equality, one of the
Sustainable Development Goals of the United Nations. Using resume data of over
ten million Chinese online job seekers in 2015, we study the current gender pay
gap in China. The results show that on average women only earned 71.57\% of
what men earned in China. The gender pay gap exists across all age groups and
educational levels. Contrary to the commonly held view that developments in
education, economy, and a more open culture would reduce the gender pay gap,
the fusion analysis of resume data and socio-economic data presents that they
have not helped reach the gender pay equality in China. China seems to be stuck
in a place where traditional methods cannot make further progress. Our analysis
further shows that 81.47\% of the variance in the gender pay gap can be
potentially attributed to discrimination. In particular, compared with the
unmarried, both the gender pay gap itself and proportion potentially attributed
to discrimination of the married are larger, indicating that married women
suffer greater inequality and more discrimination than unmarried ones. Taken
together, we suggest that more research attention should be paid to the effect
of discrimination in understanding gender pay gap based on the family
constraint theory. We also suggest the Chinese government to increase
investment in family-supportive policies and grants in addition to female
education. </font><br> Link: <a href='http://arxiv.org/pdf/2206.09306v1' target="_blank">http://arxiv.org/pdf/2206.09306v1</a><br> <br> <br> <font size='5'> 716 </font> <div style="text-align: right"> 2022-06-18 16:21:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Risk Filtering and Risk-Averse Control of Markovian Systems Subject to Model Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider a Markov decision process subject to model uncertainty in a
Bayesian framework, where we assume that the state process is observed but its
law is unknown to the observer. In addition, while the state process and the
controls are observed at time $t$, the actual cost that may depend on the
unknown parameter is not known at time $t$. The controller optimizes total cost
by using a family of special risk measures, that we call risk filters and that
are appropriately defined to take into account the model uncertainty of the
controlled system. These key features lead to non-standard and non-trivial
risk-averse control problems, for which we derive the Bellman principle of
optimality. We illustrate the general theory on two practical examples: optimal
investment and clinical trials. </font><br> Link: <a href='http://arxiv.org/pdf/2206.09235v1' target="_blank">http://arxiv.org/pdf/2206.09235v1</a><br> <br> <br> <font size='5'> 717 </font> <div style="text-align: right"> 2022-06-17 15:28:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Long-term future particle accelerators</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Particle accelerators have enabled forefront research in high energy physics
and other research areas for more than half a century. Accelerators have
directly contributed to 26 Nobel Prizes in Physics since 1939 as well as
another 20 Nobel Prizes in Chemistry, Medicine and Physics with X-rays.
Although high energy physics has been the main driving force for the
development of the particle accelerators, accelerator facilities have
continually been expanding applications in many areas of research and
technology. For instance, active areas of accelerator applications include
radiotherapy to treat cancer, production of short-lived medical isotopes,
synchrotron light sources, free-electron lasers, beam lithography for
microcircuits, thin-film technology and radiation processing of food.
Currently, the largest and most powerful accelerator is the Large Hadron
Collider (LHC) at CERN, which accelerates protons to multi-TeV energies in a 27
km high-vacuum ring. To go beyond the maximum capabilities of the LHC, the next
generation of circular and linear particle colliders under consideration, based
on radiofrequency acceleration, will require multi-billion investment,
kilometric infrastructure and a massive power consumption. These factors pose
serious challenges in an increasingly resource-limited world. Therefore, it is
important to look for alternative and sustainable acceleration techniques. This
review article pays special attention to novel accelerator techniques to
overcome present acceleration limitations towards more compact and
cost-effective long term future accelerators. </font><br> Link: <a href='http://arxiv.org/pdf/2206.08834v1' target="_blank">http://arxiv.org/pdf/2206.08834v1</a><br> <br> <br> <font size='5'> 718 </font> <div style="text-align: right"> 2022-06-17 11:55:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Neural network based human reliability analysis method in production systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Purpose: In addition to playing an important role in creating economic
security and investment development, insurance companies also invest. The
country's insurance industry as one of the country's financial institutions has
a special place in the investment process and special attention to appropriate
investment policies in the field of insurance industry is essential. So that
the efficiency of this industry in allocating the existing budget stimulates
other economic sectors. This study seeks to model investment in the performance
of dynamic networks of insurance companies.
  Methodology: In this paper, a new investment model is designed to examine the
dynamic network performance of insurance companies in Iran. The designed model
is implemented using GAMS software and the outputs of the model are analyzed
based on regression method. The required information has been collected based
on the statistics of insurance companies in Iran between 1393 and 1398.
  Findings: After evaluating these units, out of 15 companies evaluated, 6
companies had unit performance and were introduced as efficient companies. The
average efficiency of insurance companies is 0.78 and the standard deviation is
0.2. The results show that the increase in the value of investments is due to
the large reduction in costs and in terms of capital and net profit of
companies is a large number that has a clear and strong potential for insurance
companies.
  Originality/Value: In this paper, investment modeling is performed to examine
the performance of dynamic networks of insurance companies in Iran. </font><br> Link: <a href='http://arxiv.org/pdf/2206.11850v1' target="_blank">http://arxiv.org/pdf/2206.11850v1</a><br> <br> <br> <font size='5'> 719 </font> <div style="text-align: right"> 2022-06-15 19:46:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On efforts to decouple early universe cosmology and quantum gravity phenomenology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Big Bang singularity in standard model cosmology suggests a program of
study in 'early universe' quantum gravity phenomenology. Inflation is usually
thought to undermine this program's prospects by means of a dynamical diluting
argument, but such a view has recently been disputed within inflationary
cosmology, in the form of a 'trans-Planckian censorship' conjecture. Meanwhile,
trans-Planckian censorship has been used outside of inflationary cosmology to
motivate alternative early universe scenarios that are tightly linked to
ongoing theorizing in quantum gravity. Against the resulting trend toward early
universe quantum gravity phenomenology within and without inflation, Ijjas and
Steindhardt suggest a further alternative: a 'generalized cosmic censorship'
principle. I contrast the generalized cosmic censorship principle with the
logic of its namesake, the cosmic censorship conjectures. I also remark on
foundational concerns in the effective field theory approach to cosmology
beyond the standard model, which would be based on that principle. </font><br> Link: <a href='http://arxiv.org/pdf/2206.07783v3' target="_blank">http://arxiv.org/pdf/2206.07783v3</a><br> <br> <br> <font size='5'> 720 </font> <div style="text-align: right"> 2022-06-15 03:01:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deployment of AGRI-BOT in Greenhouse Administration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Modern agriculture is constantly evolving to increase production despite
unfavorable environmental conditions. A promising approach is 'greenhouse
cultivation' providing a microclimate to the cultivated plants to overcome
unfavorable climate. However, massive-sized greenhouses develop non-uniform
micro-climate throughout the complex requiring high degree of human
supervision. We propose deploying an Agri-Bot to create and maintain positive
ecological conditions in the greenhouse, reducing labor costs and increasing
production. The prototype will contain two primary systems, the navigation
system and the data analytics system. The navigation system will be controlled
by an Arduino, and data analytics will be handled using an ESP8266 microchip.
Numerous sensors for measuring the greenhouse parameters will be mounted on the
robot. It will follow a predefined path, while taking readings at checkpoints.
The microchip will collect and process data from sensors, transmit to the
cloud, and give commands to the actuators. The soil and climate parameters like
temperature, humidity, light intensity, soil moisture, pH will be measured
periodically. When the parameters are not within a specified range, the
Agri-Bot will take corrective actions like switching on blowers/heaters,
starting irrigation etc. If external intervention is required, eg., fertilizer,
it will indicate accordingly. Deploying such an Agri-Bot for monitoring and
controlling microclimate in large-scale greenhouses can mitigate labor costs
while increasing productivity. In spite of an initial cost, it can provide a
high return on investment by providing flexibility, low power consumption and
easy management to help greenhouse be water efficient, provide evenly dispersed
and controlled sunlight intensity, temperature and humidity. </font><br> Link: <a href='http://arxiv.org/pdf/2206.07266v1' target="_blank">http://arxiv.org/pdf/2206.07266v1</a><br> <br> <br> <font size='5'> 721 </font> <div style="text-align: right"> 2022-06-15 02:20:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockchain based Secure Energy Marketplace Scheme to Motivate P2P Microgrids</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the past years trend of microgrids is increasing very fast to reduce
peak-hour costs. However, in these systems, third parties are still involved in
selling surplus energy. This results in increased cost of energy and there are
many operational and security barriers in such systems. These issues can be
solved by the decentralized distributed system of microgrids where a consumer
can locally sell their surplus energy to another consumer. To deploy such a
system, one must consider security barriers for the transaction of energy. This
paper proposes a solution to these problems by devising a scheme as a
marketplace where users interact with each other to buy and sell energy at
better rates and get energy-generating resources on lease so that users do not
have to worry about capital investment. Agreement between owner of resources
and consumer is recorded on blockchain based smart contracts. In this paper, a
survey is performed for existing well known, decentralized energy solutions.
This paper also proposes an extra layer of security to leverage a shielded
execution environment so that information of energy generated, utilized, and
shared cannot be changed by consumers and third parties even if the system is
compromised. </font><br> Link: <a href='http://arxiv.org/pdf/2206.07248v2' target="_blank">http://arxiv.org/pdf/2206.07248v2</a><br> <br> <br> <font size='5'> 722 </font> <div style="text-align: right"> 2022-06-14 13:12:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Repenser le financement des entreprises vertueuses et les politiques prudentielles en int{}grant la solvabilit{} socio-environnementale</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite the amount of savings available and the money supply managed by
financial institutions, significant market failures and the failure of carbon
pricing strategies prevent sufficient financing of the transition, notably
through bank credit. Aware of the links between natural, monetary and
productive aggregates, we propose the development of ''eco-systemic''
prudential policies by exposing the interdependence between macro, micro and
environmental prudential measures. These would be based on a reorientation of
corporate accounting standards towards the concept of socio-environmental
solvency, notably the CARE-TDL model (integration of human and natural capital
alongside financial capital on the liabilities side of the balance sheet). In
an ecosystemic framework, this solvency of virtuous companies would compensate
in accounting terms for the lack of financial solvency. The State would then be
the guarantor in order to facilitate their access to financing, also reduced by
Basel III and Solvency II. This policy develops a system of reallocation of
financing capacities from non-virtuous companies to the most virtuous ones with
public guarantees, aiming to reduce the debt ratio while increasing green
investments, with monetary policies of rates but also of volumes and ratios
differentiated according to the types of assets and the greening of bank
balance sheets, and finally forms of public-private partnership. Facilitating
the financing of green companies would green capital but increase it, partly
neutralising the positive environmental impact. It is therefore necessary to
limit the credit expansion of ''brown'' companies. This would reduce risky
operations and favour less leveraged investments more connected to the real
economy, reducing systemic financial risk. -- The Agenda 2030 Policy Briefs
series (PoCFiN Kedge Business School - SDSN France - Institut Rousseau)
mobilises economists and practitioners to identify an agenda of economic and
financial reforms to achieve the 2030 Agenda, at territorial, national and
supranational levels. These are published after peer review. </font><br> Link: <a href='http://arxiv.org/pdf/2206.06820v1' target="_blank">http://arxiv.org/pdf/2206.06820v1</a><br> <br> <br> <font size='5'> 723 </font> <div style="text-align: right"> 2022-06-14 07:35:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Involution game with spatio-temporal heterogeneity of social resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: When group members claim a portion of limited resources, it is tempting to
invest more effort to get a larger share. However, if everyone acts similarly,
they all get the same piece they would obtain without extra effort. This is the
involution game dilemma that can be detected in several real-life situations.
It is also a realistic assumption that resources are not uniform in space and
time, which may influence the system's resulting involution level. We here
introduce spatio-temporal heterogeneity of social resources and explore their
consequences on involution. When spatial heterogeneity is applied, network
reciprocity can mitigate the involution for rich resources, which would be
critical otherwise in a homogeneous population. Interestingly, when the
resource level is modest, spatial heterogeneity causes more intensive
involution in a system where most cooperator agents, who want to keep
investment at a low level, are present. This picture is partly the opposite in
the extreme case when more investment is less effective. Spatial heterogeneity
can also produce a counterintuitive effect when the presence of alternative
resource levels cannot explain the emergence of involution. If we apply
temporal heterogeneity additionally, then the impact of spatial heterogeneity
practically vanishes, and we turn back to the behavior observed in a
homogeneous population earlier. Our observations are also supported by solving
the corresponding replicator equations numerically. </font><br> Link: <a href='http://arxiv.org/pdf/2206.06652v1' target="_blank">http://arxiv.org/pdf/2206.06652v1</a><br> <br> <br> <font size='5'> 724 </font> <div style="text-align: right"> 2022-06-13 09:02:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Systemic-risk and evolutionary stable strategies in a financial network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider a financial network represented at any time instance by a random
liability graph which evolves over time. The agents connect through credit
instruments borrowed from each other or through direct lending, and these
create the liability edges. These random edges are modified (locally) by the
agents over time, as they learn from their experiences and (possibly imperfect)
observations. The settlement of the liabilities of various agents at the end of
the contract period (at any time instance) can be expressed as solutions of
random fixed point equations. Our first step is to derive the solutions of
these equations (asymptotically and one for each time instance), using a recent
result on random fixed point equations. The agents, at any time instance, adapt
one of the two available strategies, risky or less risky investments, with an
aim to maximize their returns. We aim to study the emerging strategies of such
replicator dynamics that drives the financial network. We theoretically reduce
the analysis of the complex system to that of an appropriate ordinary
differential equation (ODE). Using the attractors of the resulting ODE we show
that the replicator dynamics converges to one of the two pure evolutionary
stable strategies (all risky or all less risky agents); one can have mixed
limit only when the observations are imperfect. We verify our theoretical
findings using exhaustive Monte Carlo simulations. The dynamics avoid the
emergence of the systemic-risk regime (where majority default). However, if all
the agents blindly adapt risky strategy it can lead to systemic risk regime. </font><br> Link: <a href='http://arxiv.org/pdf/2207.07574v2' target="_blank">http://arxiv.org/pdf/2207.07574v2</a><br> <br> <br> <font size='5'> 725 </font> <div style="text-align: right"> 2022-06-12 20:27:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Reinforcement Learning for Optimal Investment and Saving Strategy Selection in Heterogeneous Profiles: Intelligent Agents working towards retirement</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The transition from defined benefit to defined contribution pension plans
shifts the responsibility for saving toward retirement from governments and
institutions to the individuals. Determining optimal saving and investment
strategy for individuals is paramount for stable financial stance and for
avoiding poverty during work-life and retirement, and it is a particularly
challenging task in a world where form of employment and income trajectory
experienced by different occupation groups are highly diversified. We introduce
a model in which agents learn optimal portfolio allocation and saving
strategies that are suitable for their heterogeneous profiles. We use deep
reinforcement learning to train agents. The environment is calibrated with
occupation and age dependent income evolution dynamics. The research focuses on
heterogeneous income trajectories dependent on agent profiles and incorporates
the behavioural parameterisation of agents. The model provides a flexible
methodology to estimate lifetime consumption and investment choices for
heterogeneous profiles under varying scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2206.05835v1' target="_blank">http://arxiv.org/pdf/2206.05835v1</a><br> <br> <br> <font size='5'> 726 </font> <div style="text-align: right"> 2022-06-10 17:30:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ROI-Constrained Bidding via Curriculum-Guided Bayesian Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Real-Time Bidding (RTB) is an important mechanism in modern online
advertising systems. Advertisers employ bidding strategies in RTB to optimize
their advertising effects subject to various financial requirements, especially
the return-on-investment (ROI) constraint. ROIs change non-monotonically during
the sequential bidding process, and often induce a see-saw effect between
constraint satisfaction and objective optimization. While some existing
approaches show promising results in static or mildly changing ad markets, they
fail to generalize to highly dynamic ad markets with ROI constraints, due to
their inability to adaptively balance constraints and objectives amidst
non-stationarity and partial observability. In this work, we specialize in
ROI-Constrained Bidding in non-stationary markets. Based on a Partially
Observable Constrained Markov Decision Process, our method exploits an
indicator-augmented reward function free of extra trade-off parameters and
develops a Curriculum-Guided Bayesian Reinforcement Learning (CBRL) framework
to adaptively control the constraint-objective trade-off in non-stationary ad
markets. Extensive experiments on a large-scale industrial dataset with two
problem settings reveal that CBRL generalizes well in both in-distribution and
out-of-distribution data regimes, and enjoys superior learning efficiency and
stability. </font><br> Link: <a href='http://arxiv.org/pdf/2206.05240v5' target="_blank">http://arxiv.org/pdf/2206.05240v5</a><br> <br> <br> <font size='5'> 727 </font> <div style="text-align: right"> 2022-06-09 14:43:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Information Geometry of Risks and Returns</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We reveal a geometric structure underlying both hedging and investment
products. The structure follows from a simple formula expressing investment
risks in terms of returns. This informs optimal product designs. Optimal pure
hedging (including cost-optimal products) and hybrid hedging (where a partial
hedge is built into an optimal investment product) are considered. Duality
between hedging and investment is demonstrated with applications to optimal
risk recycling. A geometric interpretation of rationality is presented. </font><br> Link: <a href='http://arxiv.org/pdf/2206.08753v3' target="_blank">http://arxiv.org/pdf/2206.08753v3</a><br> <br> <br> <font size='5'> 728 </font> <div style="text-align: right"> 2022-06-08 20:08:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Role of Blockchain in Revolutionizing Online Transactional Security</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper highlights the necessity to use modern blockchain technology in
traditional banking sector to reduce frauds and enable high-security
transactions on a permanent blockchain ledger. Reviewing different channels
through which the traditional banking servers could integrate blockchain use,
it is signified how a huge anti-fraud stand can be taken against bank servers
allowing fraudulent transactions daily. Usage of a blockchain-based ledger is
highly impactful in terms of security of a banking organization.
Blockchain-based currency tokens, also referred to as Cryptocurrencies are not
regulated by the government, highly volatile, and anonymous to use.
Furthermore, there is no security for any funds invested in a cryptocurrency
market. However, the integration of a blockchain ledger in a traditional
banking organization would strengthen the security to provide more stability
and confidence to its customers and at the same time, make blockchain a more
reliable method to consider due to being trusted by large financial
organizations. </font><br> Link: <a href='http://arxiv.org/pdf/2206.04141v1' target="_blank">http://arxiv.org/pdf/2206.04141v1</a><br> <br> <br> <font size='5'> 729 </font> <div style="text-align: right"> 2022-06-07 22:22:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predictable Forward Performance Processes in Complete Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We establish existence of Predictable Forward Performance Processes (PFPPs)
in complete markets, which has been previously shown only in the binomial
setting. Our market model can be a discrete-time or a continuous-time model,
and the investment horizon can be finite or infinite. We show that the main
step in construction of PFPPs is solving a one-period problem involving an
integral equation, which is the counterpart of the functional equation found in
the binomial case. Although this integral equation has been partially studied
in the existing literature, we provide a new solution method using the Fourier
transform for tempered distributions. We also provide closed-form solutions for
PFPPs with inverse marginal functions that are completely monotonic and
establish uniqueness of PFPPs within this class. We apply our results to two
special cases. The first one is the binomial market and is included to relate
our work to the existing literature. The second example considers a generalized
Black-Scholes model which, to the best of our knowledge, is a new result. </font><br> Link: <a href='http://arxiv.org/pdf/2206.03608v2' target="_blank">http://arxiv.org/pdf/2206.03608v2</a><br> <br> <br> <font size='5'> 730 </font> <div style="text-align: right"> 2022-06-07 08:58:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantitative Stock Investment by Routing Uncertainty-Aware Trading Experts: A Multi-Task Learning Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantitative investment is a fundamental financial task that highly relies on
accurate stock prediction and profitable investment decision making. Despite
recent advances in deep learning (DL) have shown stellar performance on
capturing trading opportunities in the stochastic stock market, we observe that
the performance of existing DL methods is sensitive to random seeds and network
initialization. To design more profitable DL methods, we analyze this
phenomenon and find two major limitations of existing works. First, there is a
noticeable gap between accurate financial predictions and profitable investment
strategies. Second, investment decisions are made based on only one individual
predictor without consideration of model uncertainty, which is inconsistent
with the workflow in real-world trading firms. To tackle these two limitations,
we first reformulate quantitative investment as a multi-task learning problem.
Later on, we propose AlphaMix, a novel two-stage mixture-of-experts (MoE)
framework for quantitative investment to mimic the efficient bottom-up trading
strategy design workflow of successful trading firms. In Stage one, multiple
independent trading experts are jointly optimized with an individual
uncertainty-aware loss function. In Stage two, we train neural routers
(corresponding to the role of a portfolio manager) to dynamically deploy these
experts on an as-needed basis. AlphaMix is also a universal framework that is
applicable to various backbone network architectures with consistent
performance gains. Through extensive experiments on long-term real-world data
spanning over five years on two of the most influential financial markets (US
and China), we demonstrate that AlphaMix significantly outperforms many
state-of-the-art baselines in terms of four financial criteria. </font><br> Link: <a href='http://arxiv.org/pdf/2207.07578v1' target="_blank">http://arxiv.org/pdf/2207.07578v1</a><br> <br> <br> <font size='5'> 731 </font> <div style="text-align: right"> 2022-06-07 08:09:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Route Network Planning Method for Urban Air Delivery</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: High-tech giants and start-ups are investing in drone technologies to provide
urban air delivery service, which is expected to solve the last-mile problem
and mitigate road traffic congestion. However, air delivery service will not
scale up without proper traffic management for drones in dense urban
environment. Currently, a range of Concepts of Operations (ConOps) for unmanned
aircraft system traffic management (UTM) are being proposed and evaluated by
researchers, operators, and regulators. Among these, the tube-based (or
corridor-based) ConOps has emerged in operations in some regions of the world
for drone deliveries and is expected to continue serving certain scenarios that
with dense and complex airspace and requires centralized control in the future.
Towards the tube-based ConOps, we develop a route network planning method to
design routes (tubes) in a complex urban environment in this paper. In this
method, we propose a priority structure to decouple the network planning
problem, which is NP-hard, into single-path planning problems. We also
introduce a novel space cost function to enable the design of dense and aligned
routes in a network. The proposed method is tested on various scenarios and
compared with other state-of-the-art methods. Results show that our method can
generate near-optimal route networks with significant computational
time-savings. </font><br> Link: <a href='http://arxiv.org/pdf/2206.03085v2' target="_blank">http://arxiv.org/pdf/2206.03085v2</a><br> <br> <br> <font size='5'> 732 </font> <div style="text-align: right"> 2022-06-06 19:02:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ESG-Valued Portfolio Optimization and Dynamic Asset Pricing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ESG ratings provide a quantitative measure for socially responsible
investment. We present a unified framework for incorporating numeric ESG
ratings into dynamic pricing theory. Specifically, we introduce an ESG-valued
return that is a linearly constrained transformation of financial return and
ESG score. This leads to a more complex portfolio optimization problem in a
space governed by reward, risk and ESG score. The framework preserves the
traditional risk aversion parameter and introduces an ESG affinity parameter.
We apply this framework to develop ESG-valued: portfolio optimization; capital
market line; risk measures; option pricing; and the computation of shadow
riskless rates. </font><br> Link: <a href='http://arxiv.org/pdf/2206.02854v1' target="_blank">http://arxiv.org/pdf/2206.02854v1</a><br> <br> <br> <font size='5'> 733 </font> <div style="text-align: right"> 2022-06-06 16:35:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Impact of micro-credit on the livelihoods of clients -- A study on Sunamganj District</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The objective of this paper is to assess the impact of micro credit on the
livelihoods of the clients in the haor area of Sunamganj district, Sylhet,
Bangladesh. The major findings of the study are that 66.2 percent respondents
of borrowers and 98.7 non-borrowers are head of the family and an average 76.6
percent and among the borrowers 32 percent is husband/wife while 1.3 percent of
non-borrowers and on average 22.2. In terms of sex 64.7 percent of borrowers
and 92.5 percent of non-borrowers are male while 35.3 percent of borrowers and
7.5 percent of non-borrowers are female. The impact of micro-credit in terms of
formal and informal credit receiving households based on DID method showed that
total income, total expenditure and investment have been increased 13.57
percent, 10.39 percent and 26.17 percent. All the elements of total income have
been increased except debt which has been decreased by 2.39 percent. But the
decrease in debt is the good sign of positive impact of debt. Consumption of
food has been increased but non-food has been decreased. All the elements of
investment have been increased except some factors. The savings has been
decreased due excess increase in investment. The study suggested that for
breaking vicious cycle of poverty by micro-credit the duration of loans should
be at least five year and the volume of loans must be minimum 500,000 and
repayment should at not be less than monthly. The rate of interest should not
be more than 5 percent. </font><br> Link: <a href='http://arxiv.org/pdf/2206.02798v1' target="_blank">http://arxiv.org/pdf/2206.02798v1</a><br> <br> <br> <font size='5'> 734 </font> <div style="text-align: right"> 2022-06-06 10:38:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-model assessment of heat decarbonisation options in the UK using electricity and hydrogen</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Delivering low-carbon heat will require the substitution of natural gas with
low-carbon alternatives such as electricity and hydrogen. The objective of this
paper is to develop a method to soft-link two advanced, investment-optimising
energy system models, RTN (Resource-Technology Network) and WeSIM
(Whole-electricity System Investment Model), in order to assess cost-efficient
heat decarbonisation pathways for the UK while utilising the respective
strengths of the two models. The linking procedure included passing on hourly
electricity prices from WeSIM as input to RTN, and returning capacities and
locations of hydrogen generation and shares of electricity and hydrogen in heat
supply from RTN to WeSIM. The outputs demonstrate that soft-linking can improve
the quality of the solution, while providing useful insights into the
cost-efficient pathways for zero-carbon heating. Quantitative results point to
the cost-effectiveness of using a mix of electricity and hydrogen technologies
for delivering zero-carbon heat, also demonstrating a high level of interaction
between electricity and hydrogen infrastructure in a zero-carbon system.
Hydrogen from gas reforming with carbon capture and storage can play a
significant role in the medium term, while remaining a cost-efficient option
for supplying peak heat demand in the longer term, with the bulk of heat demand
being supplied by electric heat pumps. </font><br> Link: <a href='http://arxiv.org/pdf/2206.02483v1' target="_blank">http://arxiv.org/pdf/2206.02483v1</a><br> <br> <br> <font size='5'> 735 </font> <div style="text-align: right"> 2022-06-04 10:48:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Forecasting the production of Distillate Fuel Oil Refinery and Propane Blender net production by using Time Series Algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Oil production forecasting is an important step in controlling the
cost-effect and monitoring the functioning of petroleum reservoirs. As a
result, oil production forecasting makes it easier for reservoir engineers to
develop feasible projects, which helps to avoid risky investments and achieve
long-term growth. As a result, reliable petroleum reservoir forecasting is
critical for controlling and managing the effective cost of oil reservoirs. Oil
production is influenced by reservoir qualities such as porosity, permeability,
compressibility, fluid saturation, and other well operational parameters.
Three-time series algorithms i.e., Seasonal Naive method, Exponential
Smoothening and ARIMA to forecast the Distillate Fuel Oil Refinery and Propane
Blender net production for the next two years. </font><br> Link: <a href='http://arxiv.org/pdf/2208.05964v1' target="_blank">http://arxiv.org/pdf/2208.05964v1</a><br> <br> <br> <font size='5'> 736 </font> <div style="text-align: right"> 2022-06-03 16:39:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-Stage Decision Rules for Power Generation & Storage Investments with Performance Guarantees</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop multi-stage linear decision rules (LDRs) for dynamic power system
generation and energy storage investment planning under uncertainty and propose
their chance-constrained optimization with performance guarantees. First, the
optimized LDRs guarantee operational and carbon policy feasibility of the
resulting dynamic investment plan even when the planning uncertainty
distribution is ambiguous. Second, the optimized LDRs internalize the tolerance
of the system planner towards the stochasticity (variance) of uncertain
investment outcomes. They can eventually produce a quasi-deterministic
investment plan, which is insensitive to uncertainty (as in deterministic
planning) but robust to its realizations (as in stochastic planning). Last, we
certify the performance of the optimized LDRs with the bound on their
sub-optimality due to their linear functional form. Using this bound, we
guarantee that the preference of LDRs over less restrictive -- yet poorly
scalable -- scenario-based optimization does not lead to financial losses
exceeding this bound. We use a testbed of the U.S. Southeast power system to
reveal the trade-offs between the cost, stochasticity, and feasibility of
LDR-based investments. We also conclude that the LDR sub-optimality depends on
the amount of uncertainty and the tightness of chance constraints on
operational, investment and policy variables. </font><br> Link: <a href='http://arxiv.org/pdf/2206.01675v2' target="_blank">http://arxiv.org/pdf/2206.01675v2</a><br> <br> <br> <font size='5'> 737 </font> <div style="text-align: right"> 2022-06-03 09:33:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-Asset Bubbles Equilibrium Price Dynamics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The price-bubble and crash process formation is theoretically investigated in
a two-asset equilibrium model. Sufficient and necessary conditions are derived
for the existence of average equilibrium price dynamics of different
agent-based models, where agents are distinguished in terms of factor and
investment trading strategies. In line with experimental results, we show that
assets with a positive average dividend, i.e., with a strictly declining
fundamental value, display at the equilibrium price the typical hump-shaped
bubble observed in experimental asset markets. Moreover, a misvaluation effect
is observed in the asset with a constant fundamental value, triggered by the
other asset that displays the price bubble shape when a sharp price decline is
exhibited at the end of the market. </font><br> Link: <a href='http://arxiv.org/pdf/2206.01468v6' target="_blank">http://arxiv.org/pdf/2206.01468v6</a><br> <br> <br> <font size='5'> 738 </font> <div style="text-align: right"> 2022-06-02 15:08:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: 2D-MRI of the Central Nervous System: The effect of a deep learning-based reconstruction pipeline on the overall image quality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Purpose of this study was to evaluate the effect of a robust magnetic
resonance reconstruction pipeline equipped with a deep convolutional neural
network on the overall image quality, in terms of Gibbs artifact reduction, and
SNR improvement. Sixteen (16) healthy volunteers enrolled in this study and
were imaged at 3T. Representative images of each image series that were
reconstructed through the pipeline that leverages a deep learning (DL)
algorithm were retrospectively benchmarked against corresponding images
reconstructed through a conventional pipeline. DL-reconstructed images showed
significant SNR improvements compared to the corresponding conventionally
reconstructed images. In addition to that, Gibbs artifacts were effectively
eliminated, when the raw data were reconstructed through the DL pipeline. Gibbs
artifact reduction was qualitatively assessed by two experienced medical
physicists and two experienced radiologists. DL-based reconstruction can lead
to an SNR surplus which can be further invested into either higher spatial
resolution and thinner slices, or into shorter scan times. </font><br> Link: <a href='http://arxiv.org/pdf/2206.01082v1' target="_blank">http://arxiv.org/pdf/2206.01082v1</a><br> <br> <br> <font size='5'> 739 </font> <div style="text-align: right"> 2022-06-02 14:29:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adaptive Robust Online Portfolio Selection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The online portfolio selection (OLPS) problem differs from classical
portfolio model problems, as it involves making sequential investment
decisions. Many OLPS strategies described in the literature capture market
movement based on various beliefs and are shown to be profitable. In this
paper, we propose a robust optimization (RO)-based strategy that takes
transaction costs into account. Moreover, unlike existing studies that
calibrate model parameters from benchmark data sets, we develop a novel
adaptive scheme that decides the parameters sequentially. With a wide range of
parameters as input, our scheme captures market uptrend and protects against
market downtrend while controlling trading frequency to avoid excessive
transaction costs. We numerically demonstrate the advantages of our adaptive
scheme against several benchmarks under various settings. Our adaptive scheme
may also be useful in general sequential decision-making problems. Finally, we
compare the performance of our strategy with that of existing OLPS strategies
using both benchmark and newly collected data sets. Our strategy outperforms
these existing OLPS strategies in terms of cumulative returns and competitive
Sharpe ratios across diversified data sets, demonstrating its
adaptability-driven superiority. </font><br> Link: <a href='http://arxiv.org/pdf/2206.01064v1' target="_blank">http://arxiv.org/pdf/2206.01064v1</a><br> <br> <br> <font size='5'> 740 </font> <div style="text-align: right"> 2022-06-01 09:27:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparative statics with adjustment costs and the le Chatelier principle</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop a theory of monotone comparative statics for models with
adjustment costs. We show that comparative-statics conclusions may be drawn
under the usual ordinal complementarity assumptions on the objective function,
assuming very little about costs: only a mild monotonicity condition is
required. We use this insight to prove a general le Chatelier principle: under
the ordinal complementarity assumptions, if short-run adjustment is subject to
a monotone cost, then the long-run response to a shock is greater than the
short-run response. We extend these results to a fully dynamic model of
adjustment over time: the le Chatelier principle remains valid, and under
slightly stronger assumptions, optimal adjustment follows a monotone path. We
apply our results to models of capital investment and of sticky prices. </font><br> Link: <a href='http://arxiv.org/pdf/2206.00347v2' target="_blank">http://arxiv.org/pdf/2206.00347v2</a><br> <br> <br> <font size='5'> 741 </font> <div style="text-align: right"> 2022-05-31 20:58:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Disclosure of Investment Advisor and Broker-Dealer Relationships: Impact on Comprehension and Decision Making</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently enacted regulations aimed to enhance retail investors' understanding
about different types of investment accounts. Toward this goal, the Securities
and Exchange Commission (SEC) mandated that SEC-registered investment advisors
and broker-dealers provide a brief relationship summary (Form CRS) to retail
investors. The present study examines the impact of this regulation on
investors and considers its market implications. The effects of Form CRS were
evaluated based on three outcome variables: perceived helpfulness,
comprehension, and decision making. The study also examined whether personal
characteristics, such as investment experience, influenced the disclosure's
impact on decision making. Results indicated that participants perceived the
disclosure as helpful and it significantly enhanced comprehension about the two
types of investment accounts. Critically, participants also showed increased
preference and choice for broker-dealers after the disclosure. Increased
preference for broker-dealers was associated with greater investment
experience, greater comprehension gains, and access to more information from a
longer disclosure. These findings suggest that Form CRS may promote informed
decision making among retail investors while simultaneously increasing the
selection of broker-dealer accounts. </font><br> Link: <a href='http://arxiv.org/pdf/2206.00117v2' target="_blank">http://arxiv.org/pdf/2206.00117v2</a><br> <br> <br> <font size='5'> 742 </font> <div style="text-align: right"> 2022-05-31 20:47:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: To Collaborate or Not in Distributed Statistical Estimation with Resource Constraints?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study how the amount of correlation between observations collected by
distinct sensors/learners affects data collection and collaboration strategies
by analyzing Fisher information and the Cramer-Rao bound. In particular, we
consider a simple setting wherein two sensors sample from a bivariate Gaussian
distribution, which already motivates the adoption of various strategies,
depending on the correlation between the two variables and resource
constraints. We identify two particular scenarios: (1) where the knowledge of
the correlation between samples cannot be leveraged for collaborative
estimation purposes and (2) where the optimal data collection strategy involves
investing scarce resources to collaboratively sample and transfer information
that is not of immediate interest and whose statistics are already known, with
the sole goal of increasing the confidence on an estimate of the parameter of
interest. We discuss two applications, IoT DDoS attack detection and
distributed estimation in wireless sensor networks, that may benefit from our
results. </font><br> Link: <a href='http://arxiv.org/pdf/2206.00111v1' target="_blank">http://arxiv.org/pdf/2206.00111v1</a><br> <br> <br> <font size='5'> 743 </font> <div style="text-align: right"> 2022-05-31 07:31:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mandatory Disclosure of Standardized Sustainability Metrics: The Case of the EU Taxonomy Regulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sustainability reporting enables investors to make informed decisions and is
hoped to facilitate the transition to a green economy. The European Union's
taxonomy regulation enacts rules to discern sustainable activities and
determine the resulting green revenue, whose disclosure is mandatory for many
companies. In an experiment, we explore how this standardized metric is
received by investors relative to a sustainability rating. We find that green
revenue affects the investment probability more than the rating if the two
metrics disagree. If they agree, a strong rating has an incremental effect on
the investment probability. The effects are robust to variation in investors'
attitudes. Our findings imply that a mandatory standardized sustainability
metric is an effective means of channeling investment, which complements rather
than substitutes sustainability ratings. </font><br> Link: <a href='http://arxiv.org/pdf/2205.15576v1' target="_blank">http://arxiv.org/pdf/2205.15576v1</a><br> <br> <br> <font size='5'> 744 </font> <div style="text-align: right"> 2022-05-30 21:51:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A hybrid-model approach for reducing the performance gap in building energy forecasting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The performance gap between predicted and actual energy consumption in the
building domain remains an unsolved problem in practice. The gap exists
differently in both current mainstream methods: the first-principles model and
the machine learning (ML) model. Inspired by the concept of time-series
decomposition to identify different uncertainties, we proposed a hybrid-model
approach by combining both methods to minimize this gap: 1. Use the
first-principles method as an encoding tool to convert the building static
features and predictable patterns in time-series simulation results; 2. The ML
method combines the results as extra inputs with historical records
simultaneously, trains the model to capture the implicit performance
difference, and aligns to calibrate the output. To extend this approach in
practice, a new concept in the modeling process: Level-of-Information (LOI), is
introduced to leverage the balance between the investment of simulation
modeling detail and the accuracy boost. The approach is tested over a
three-year period, with hourly measured energy load from an operating
commercial building in Shanghai. The result presents a dominant accuracy
enhancement: The hybrid-model shows higher accuracy in prediction with better
interpretability; More important, it releases the practitioners from modeling
workload and computational resources in refining simulation. In summary, the
approach provides a nexus for integrating domain knowledge via building
simulation with data-driven methods. This mindset applies to solving general
engineering problems and leads to improved prediction accuracy. The result and
source data are available at
https://github.com/ResearchGroup-G/PerformanceGap-Hybrid-Approach. </font><br> Link: <a href='http://arxiv.org/pdf/2206.00460v1' target="_blank">http://arxiv.org/pdf/2206.00460v1</a><br> <br> <br> <font size='5'> 745 </font> <div style="text-align: right"> 2022-05-30 19:32:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can I invest in Metaverse? The effect of obtained information and perceived risk on purchase intention by the perspective of the information adoption model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Metaverse is a virtual universe that combines the physical world and the
digital world. People can socialize, play games and even shop with their
avatars created in this virtual environment. Metaverse, which is growing very
fast in terms of investment, is both a profitable and risky area for consumers.
In order to enter the Metaverse for investment purposes, it is necessary to do
a certain research and gather information. In this direction, the aim of the
study is to determine the effect of the quality of the information obtained by
the consumers about the metaverse world, the reliability of the information and
the perceived risk, on the purchase intention from the point of view of the
information adoption model. For the research, data were collected online from
495 consumers who were interested in metaverse investment. AMOS and SPSS
package programs were used in the analysis. First, descriptive statistical
analyzes were made for the basic structure of the variables. Then the
reliability and validity of the model were tested. Finally, the structural
equation model was used to test the proposed model. According to the findings,
the reliability and quality of the information affect the purchase intention
positively and significantly, while the perceived risk affects the purchase
intention negatively and significantly. </font><br> Link: <a href='http://arxiv.org/pdf/2205.15398v2' target="_blank">http://arxiv.org/pdf/2205.15398v2</a><br> <br> <br> <font size='5'> 746 </font> <div style="text-align: right"> 2022-05-30 15:19:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Asymptotic dependence modelling of the BRICS stock markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the use of empirical data, this paper focuses on solving financial and
investment issues involving extremal dependence of ten pairwise combinations of
the five BRICS (Brazil, Russia, India, China, and South Africa) stock markets.
Daily closing equity indices from 5 January 2010 to 6 August 2018 are used in
the study. Unlike previous literature, we use bivariate point process and
conditional multivariate extreme value models to investigate the extremal
dependence of the stock market returns. However, it is observed that the point
process was able to model many more extreme observations or exceedances that
contribute to the likelihood estimation. It gives more information than the
threshold excess method of the CMEV model. This study shows varying levels of
low extremal dependence structure whose outcomes are highly beneficial to
investors, portfolio managers and other market participants interested in
maximising investment returns and financial gains. </font><br> Link: <a href='http://arxiv.org/pdf/2205.15169v1' target="_blank">http://arxiv.org/pdf/2205.15169v1</a><br> <br> <br> <font size='5'> 747 </font> <div style="text-align: right"> 2022-05-30 14:04:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Payday loans -- blessing or growth suppressor? Machine Learning Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The upsurge of real estate involves a variety of factors that have got
influenced by many domains. Indeed, the unrecognized sector that would affect
the economy for which regulatory proposals are being drafted to keep this in
control is the payday loans. This research paper revolves around the impact of
payday loans in the real estate market. The research paper draws a first-hand
experience of obtaining the index for the concentration of real estate in an
area of reference by virtue of payday loans in Toronto, Ontario in particular,
which sets out an ideology to create, evaluate and demonstrate the scenario
through research analysis. The purpose of this indexing via payday loans is the
basic - debt: income ratio which states that when the income of the person
bound to pay the interest of payday loans increases, his debt goes down
marginally which hence infers that the person invests in fixed assets like real
estate which hikes up its growth. </font><br> Link: <a href='http://arxiv.org/pdf/2205.15320v1' target="_blank">http://arxiv.org/pdf/2205.15320v1</a><br> <br> <br> <font size='5'> 748 </font> <div style="text-align: right"> 2022-05-29 16:00:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Managing Risk in DeFi Portfolios</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Decentralized Finance (DeFi) is a new financial industry built on blockchain
technologies. Decentralized financial services have consequently increased the
ability to lend, borrow, and invest in decentralized investment vehicles,
allowing investors to bypass third party intermediaries. DeFi's promise is to
reduce the cost of transaction and management fees whilst increasing trust
between agents of the Financial Industry 3.0. This paper provides an overview
of the different components of DeFi, as well as the risks involved in investing
through these new vehicles. We will also propose an allocation methodology
which will integrate and quantify these risks. </font><br> Link: <a href='http://arxiv.org/pdf/2205.14699v3' target="_blank">http://arxiv.org/pdf/2205.14699v3</a><br> <br> <br> <font size='5'> 749 </font> <div style="text-align: right"> 2022-05-28 15:35:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Visual Perception of Building and Household Vulnerability from Streets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In developing countries, building codes often are outdated or not enforced.
As a result, a large portion of the housing stock is substandard and vulnerable
to natural hazards and climate related events. Assessing housing quality is key
to inform public policies and private investments. Standard assessment methods
are typically carried out only on a sample / pilot basis due to its high costs
or, when complete, tend to be obsolete due to the lack of compliance with
recommended updating standards or not accessible to most users with the level
of detail needed to take key policy or business decisions. Thus, we propose an
evaluation framework that is cost-efficient for first capture and future
updates, and is reliable at the block level. The framework complements existing
work of using street view imagery combined with deep learning to automatically
extract building information to assist the identification of housing
characteristics. We then check its potential for scalability and higher level
reliability. For that purpose, we create an index, which synthesises the
highest possible level of granularity of data at the housing unit and at the
household level at the block level, and assess whether the predictions made by
our model could be used to approximate vulnerability conditions with a lower
budget and in selected areas. Our results indicated that the predictions from
the images are clearly correlated with the index. </font><br> Link: <a href='http://arxiv.org/pdf/2205.14460v1' target="_blank">http://arxiv.org/pdf/2205.14460v1</a><br> <br> <br> <font size='5'> 750 </font> <div style="text-align: right"> 2022-05-26 06:49:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Innovation Begets Innovation and Concentration: The Case of Upstream Oil & Gas in the North Sea</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the effect of technology adoption on competition by leveraging
a unique dataset on production, costs, and asset characteristics for North Sea
upstream oil & gas companies. Relying on heterogeneity in the geological
suitability of fields and a landmark decision of the Norwegian Supreme Court
that increased the returns of capital investment in Norway relative to the UK,
we show that technology adoption increases market concentration. Firms with
prior technology-specific know-how specialize more in fields suitable for the
same technology but also invest more in high-risk-high-return fields (e.g.,
ultra-deep recovery), diversifying their technology portfolio and ultimately
gaining larger shares of the North Sea market. Our analyses illustrate how
technology adoption can lead to market concentration both directly through
specialization and indirectly via experimentation. </font><br> Link: <a href='http://arxiv.org/pdf/2205.13186v2' target="_blank">http://arxiv.org/pdf/2205.13186v2</a><br> <br> <br> <font size='5'> 751 </font> <div style="text-align: right"> 2022-05-25 20:03:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Concurrent Neural Tree and Data Preprocessing AutoML for Image Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep Neural Networks (DNN's) are a widely-used solution for a variety of
machine learning problems. However, it is often necessary to invest a
significant amount of a data scientist's time to pre-process input data, test
different neural network architectures, and tune hyper-parameters for optimal
performance. Automated machine learning (autoML) methods automatically search
the architecture and hyper-parameter space for optimal neural networks.
However, current state-of-the-art (SOTA) methods do not include traditional
methods for manipulating input data as part of the algorithmic search space. We
adapt the Evolutionary Multi-objective Algorithm Design Engine (EMADE), a
multi-objective evolutionary search framework for traditional machine learning
methods, to perform neural architecture search. We also integrate EMADE's
signal processing and image processing primitives. These primitives allow EMADE
to manipulate input data before ingestion into the simultaneously evolved DNN.
We show that including these methods as part of the search space shows
potential to provide benefits to performance on the CIFAR-10 image
classification benchmark dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2205.13033v1' target="_blank">http://arxiv.org/pdf/2205.13033v1</a><br> <br> <br> <font size='5'> 752 </font> <div style="text-align: right"> 2022-05-25 16:02:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cloud Computing -- Everything As A Service</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Compute infrastructure hosted by a cloud provider allows an application to
scale without limit. The application developer no longer needs to worry about
the up-front investment in a server farm provisioned for a worst-case load
scenario. However, managing cloud deployments requires a sophisticated
framework that can autoscale the infrastructure and guarantee the up-time of
running container images. This paper surveys existing research addressing the
management and orchestration of cloud deployments as well as the modelling
framework to abstract away the low-level details of the host infrastructure. We
investigate blockchain distributed ledgers, quantum computing and Internet of
Things application stacks to show how they can utilize cloud deployments. </font><br> Link: <a href='http://arxiv.org/pdf/2206.07094v1' target="_blank">http://arxiv.org/pdf/2206.07094v1</a><br> <br> <br> <font size='5'> 753 </font> <div style="text-align: right"> 2022-05-25 13:27:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: No Time for Downtime: Understanding Post-Attack Behaviors by Customers of Managed DNS Providers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We leverage large-scale DNS measurement data on authoritative name servers to
study the reactions of domain owners affected by the 2016 DDoS attack on Dyn.
We use industry sources of information about domain names to study the
influence of factors such as industry sector and website popularity on the
willingness of domain managers to invest in high availability of online
services. Specifically, we correlate business characteristics of domain owners
with their resilience strategies in the wake of DoS attacks affecting their
domains. Our analysis revealed correlations between two properties of domains
-- industry sector and popularity -- and post-attack strategies. Specifically,
owners of more popular domains were more likely to re-act to increase the
diversity of their authoritative DNS service for their domains. Similarly,
domains in certain industry sectors were more likely to seek out such diversity
in their DNS service. For example, domains categorized as General News were
nearly 6 times more likely to re-act than domains categorized as Internet
Services. Our results can inform managed DNS and other network service
providers regarding the potential impact of downtime on their customer
portfolio. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12765v1' target="_blank">http://arxiv.org/pdf/2205.12765v1</a><br> <br> <br> <font size='5'> 754 </font> <div style="text-align: right"> 2022-05-25 12:54:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine learning method for return direction forecasting of Exchange Traded Funds using classification and regression models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article aims to propose and apply a machine learning method to analyze
the direction of returns from Exchange Traded Funds (ETFs) using the historical
return data of its components, helping to make investment strategy decisions
through a trading algorithm. In methodological terms, regression and
classification models were applied, using standard datasets from Brazilian and
American markets, in addition to algorithmic error metrics. In terms of
research results, they were analyzed and compared to those of the Na\"ive
forecast and the returns obtained by the buy & hold technique in the same
period of time. In terms of risk and return, the models mostly performed better
than the control metrics, with emphasis on the linear regression model and the
classification models by logistic regression, support vector machine (using the
LinearSVC model), Gaussian Naive Bayes and K-Nearest Neighbors, where in
certain datasets the returns exceeded by two times and the Sharpe ratio by up
to four times those of the buy & hold control model. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12746v2' target="_blank">http://arxiv.org/pdf/2205.12746v2</a><br> <br> <br> <font size='5'> 755 </font> <div style="text-align: right"> 2022-05-24 21:13:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Economic Viability of an In-Home Monitoring System in the context of an Aged Care Setting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The aged care sector in Australia faces significant challenges. While many of
these issues have been clearly identified, their urgency has been further
highlighted during the COVID-19 pandemic. Technology such as in-home monitoring
is one way to address some of these challenges. However, the efficacy of
technology must be considered together with its implementation and running
costs to ensure that there is a return on investment, and it is economically
viable as a solution. A pilot program was run in-home monitoring system to test
the efficacy of this system. This paper focuses on an economic analysis to
better understand the financial viability of such systems. Using a secondary
analysis approach, the findings identified that revenue could be generated by
providing carers with additional services such as real-time monitoring of the
client, which can foster deeper relationships with the customer, along with
savings of healthcare costs to carers, service providers and Government.
Savings are related to the earlier intervention of critical events that are
identified by the system, as delays in treatment of some critical events can
create much more severe and costly health outcomes. Further health costs
savings can be made via trend analysis which can show more nuanced health
deterioration that is often missed. The implementation of preventative measures
via this identification can reduce the chances of critical events occurring
which have much higher costs. Overall, monitoring systems lead to a transition
from a reactive to a preventative services offering, delivering more targeted
and personalised care. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12265v1' target="_blank">http://arxiv.org/pdf/2205.12265v1</a><br> <br> <br> <font size='5'> 756 </font> <div style="text-align: right"> 2022-05-24 13:40:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Classification of Phonological Parameters in Sign Languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Signers compose sign language phonemes that enable communication by combining
phonological parameters such as handshape, orientation, location, movement, and
non-manual features. Linguistic research often breaks down signs into their
constituent parts to study sign languages and often a lot of effort is invested
into the annotation of the videos. In this work we show how a single model can
be used to recognise the individual phonological parameters within sign
languages with the aim of either to assist linguistic annotations or to
describe the signs for the sign recognition models. We use Danish Sign Language
data set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using
pose estimation model, which are then used for training the multi-label Fast
R-CNN model to support multi-label modelling. Moreover, we show that there is a
significant co-dependence between the orientation and location phonological
parameters in the generated data and we incorporate this co-dependence in the
model to achieve better performance. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12072v1' target="_blank">http://arxiv.org/pdf/2205.12072v1</a><br> <br> <br> <font size='5'> 757 </font> <div style="text-align: right"> 2022-05-24 12:09:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Economic Topology Optimization of District Heating Networks using a Pipe Penalization Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the presented study, a pipe penalization approach for the economic
topology optimization of District Heating Networks is proposed, drawing
inspiration from density-based topology optimization. For District Heating
Networks, the upfront investment is a crucial factor for the rollout of this
technology. Today, the pipe routing is usually designed relying on a
linearization of the underlying heat transport problem. This study proposes to
solve the optimal pipe routing problem as a non-linear topology optimization
problem, drawing inspiration from density-based topology optimization. The
optimization problem is formulated around a non-linear heat transport model and
minimizes a detailed net present value representation of the heating network
cost. By relaxing the combinatorial problem of pipe placement, this approach
remains scalable for large-scale applications. A discrete network topology and
near-discrete pipe design is achieved by using an intermediate pipe
penalization strategy. For a realistic test case, the proposed algorithm
achieves a discrete network topology and near-discrete pipe design that
outperforms simple post-processing steps. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12019v2' target="_blank">http://arxiv.org/pdf/2205.12019v2</a><br> <br> <br> <font size='5'> 758 </font> <div style="text-align: right"> 2022-05-24 01:19:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hardware/Software Co-Assurance using the Rust Programming Language and ACL2</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Rust programming language has garnered significant interest and use as a
modern, type-safe, memory-safe, and potentially formally analyzable programming
language. Our interest in Rust stems from its potential as a hardware/software
co-assurance language, with application to critical systems such as autonomous
vehicles. We report on the first known use of Rust as a High-Level Synthesis
(HLS) language. Most incumbent HLS languages are a subset of C. A Rust-based
HLS brings a single modern, type-safe, and memory-safe expression language for
both hardware and software realizations with high assurance. As a a study of
the suitability of Rust as an HLS, we have crafted a Rust subset, inspired by
Russinoff's Restricted Algorithmic C (RAC), which we have imaginatively named
Restricted Algorithmic Rust, or RAR. In our first implementation of a RAR
toolchain, we simply transpile the RAR source into RAC. By so doing, we
leverage a number of existing hardware/software co-assurance tools with a
minimum investment of time and effort. In this paper, we describe the RAR Rust
subset, detail our prototype RAR toolchain, and describe the implementation and
verification of several representative algorithms and data structures written
in RAR, with proofs of correctness conducted using the ACL2 theorem prover. </font><br> Link: <a href='http://arxiv.org/pdf/2205.11709v1' target="_blank">http://arxiv.org/pdf/2205.11709v1</a><br> <br> <br> <font size='5'> 759 </font> <div style="text-align: right"> 2022-05-23 12:11:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Artificial Bee Colony optimization-based approach for sizing and composition of Arctic offshore drilling support fleets considering cost-efficiency</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article presents an optimization-based approach for sizing and
composition of an Arctic offshore drilling support fleet considering
cost-efficiency. The approach studies the main types of duties related to
Arctic offshore drillings: supply, towing, anchor handling, standby, oil spill
response, firefighting, and ice management. The approach considers the combined
effect of the expected costs of accidental events, the versatility of
individual support vessels, and ice management. The approach applies an
Artificial Bee Colony algorithm-based optimization procedure. As demonstrated
through case studies, the approach may help to find a range of cost-efficient
fleet compositions. Some of the obtained solutions are similar to corresponding
real-life fleets, indicating that the approach works in principle. Sensitivity
analyses indicate that the consideration of the expected costs from accidental
events significantly impacts the obtained solution, and that investments to
reduce these costs may improve the overall cost-efficiency of an Arctic
offshore drilling support fleet. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12263v1' target="_blank">http://arxiv.org/pdf/2205.12263v1</a><br> <br> <br> <font size='5'> 760 </font> <div style="text-align: right"> 2022-05-23 07:57:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Gradient Hedging for Intensively Exploring Salient Interpretation beyond Neuron Activation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Hedging is a strategy for reducing the potential risks in various types of
investments by adopting an opposite position in a related asset. Motivated by
the equity technique, we introduce a method for decomposing output predictions
into intensive salient attributions by hedging the evidence for a decision. We
analyze the conventional approach applied to the evidence for a decision and
discuss the paradox of the conservation rule. Subsequently, we define the
viewpoint of evidence as a gap of positive and negative influence among the
gradient-derived initial contribution maps and propagate the antagonistic
elements to the evidence as suppressors, following the criterion of the degree
of positive attribution defined by user preference. In addition, we reflect the
severance or sparseness contribution of inactivated neurons, which are mostly
irrelevant to a decision, resulting in increased robustness to
interpretability. We conduct the following assessments in a verified
experimental environment: pointing game, most relevant first region insertion,
outside-inside relevance ratio, and mean average precision on the PASCAL VOC
2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our
method outperforms existing attribution methods in distinctive, intensive, and
intuitive visualization with robustness and applicability in general models. </font><br> Link: <a href='http://arxiv.org/pdf/2205.11109v1' target="_blank">http://arxiv.org/pdf/2205.11109v1</a><br> <br> <br> <font size='5'> 761 </font> <div style="text-align: right"> 2022-05-22 18:08:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Contextual Information-Directed Sampling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Information-directed sampling (IDS) has recently demonstrated its potential
as a data-efficient reinforcement learning algorithm. However, it is still
unclear what is the right form of information ratio to optimize when contextual
information is available. We investigate the IDS design through two contextual
bandit problems: contextual bandits with graph feedback and sparse linear
contextual bandits. We provably demonstrate the advantage of contextual IDS
over conditional IDS and emphasize the importance of considering the context
distribution. The main message is that an intelligent agent should invest more
on the actions that are beneficial for the future unseen contexts while the
conditional IDS can be myopic. We further propose a computationally-efficient
version of contextual IDS based on Actor-Critic and evaluate it empirically on
a neural network contextual bandit. </font><br> Link: <a href='http://arxiv.org/pdf/2205.10895v2' target="_blank">http://arxiv.org/pdf/2205.10895v2</a><br> <br> <br> <font size='5'> 762 </font> <div style="text-align: right"> 2022-05-21 22:31:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Server breaches are an unfortunate reality on today's Internet. In the
context of deep neural network (DNN) models, they are particularly harmful,
because a leaked model gives an attacker "white-box" access to generate
adversarial examples, a threat model that has no practical robust defenses. For
practitioners who have invested years and millions into proprietary DNNs, e.g.
medical imaging, this seems like an inevitable disaster looming on the horizon.
  In this paper, we consider the problem of post-breach recovery for DNN
models. We propose Neo, a new system that creates new versions of leaked
models, alongside an inference time filter that detects and removes adversarial
examples generated on previously leaked models. The classification surfaces of
different model versions are slightly offset (by introducing hidden
distributions), and Neo detects the overfitting of attacks to the leaked model
used in its generation. We show that across a variety of tasks and attack
methods, Neo is able to filter out attacks from leaked models with very high
accuracy, and provides strong protection (7--10 recoveries) against attackers
who repeatedly breach the server. Neo performs well against a variety of strong
adaptive attacks, dropping slightly in # of breaches recoverable, and
demonstrates potential as a complement to DNN defenses in the wild. </font><br> Link: <a href='http://arxiv.org/pdf/2205.10686v2' target="_blank">http://arxiv.org/pdf/2205.10686v2</a><br> <br> <br> <font size='5'> 763 </font> <div style="text-align: right"> 2022-05-21 14:02:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pushing compute and AI onto detector silicon</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In order to take full advantage of the U.S. Department of Energy's
billion-dollar investments into the next-generation research infrastructure
(e.g., exascale, light sources, colliders), advances are required not only in
detector technology but also in computing and specifically AI. Let us consider
an example from X-ray science. Nanoscale X-ray imaging is a crucial tool to
enable a wide range of scientific explorations from materials science and
biology to mechanical and civil engineering. The next-generation light sources
will increase the X-ray beam brightness and coherent flux by 100 to 1,000
times. In order to image larger samples, the continuous frame rate of pixel
array detectors must be increased, approaching 1 MHz, which requires several
Tbps (aggregated) to transfer pixel data out to a data acquisition system.
Using 65-nm CMOS technology, an optimistic raw data rate off such a chip is
about 100-200 Gbps. However, a continuous 1 MHz detector with only $256 \times
256$ pixels at 16-bit resolution, for example, will require 1,000 Gbps (i.e., 1
Tbps) bandwidth off the chip! It is impractical to have multiple high-speed
transceivers running in parallel to provide such bandwidth and represents the
first data bottleneck. New approaches are necessary to reduce the data size by
performing data compression or AI-based feature extraction directly inside a
detector silicon chip in a streaming manner before sending it off-chip. </font><br> Link: <a href='http://arxiv.org/pdf/2205.10602v1' target="_blank">http://arxiv.org/pdf/2205.10602v1</a><br> <br> <br> <font size='5'> 764 </font> <div style="text-align: right"> 2022-05-20 08:39:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trend analysis and forecasting air pollution in Rwanda</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Air pollution is a major public health problem worldwide although the lack of
data is a global issue for most low and middle income countries. Ambient air
pollution in the form of fine particulate matter (PM2.5) exceeds the World
Health Organization guidelines in Rwanda with a daily average of around 42.6
microgram per meter cube. Monitoring and mitigation strategies require an
expensive investment in equipment to collect pollution data. Low-cost sensor
technology and machine learning methods have appeared as an alternative
solution to get reliable information for decision making. This paper analyzes
the trend of air pollution in Rwanda and proposes forecasting models suitable
to data collected by a network of low-cost sensors deployed in Rwanda. </font><br> Link: <a href='http://arxiv.org/pdf/2205.10024v1' target="_blank">http://arxiv.org/pdf/2205.10024v1</a><br> <br> <br> <font size='5'> 765 </font> <div style="text-align: right"> 2022-05-19 15:15:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Women, artificial intelligence, and key positions in collaboration networks: Towards a more equal scientific ecosystem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Scientific collaboration in almost every discipline is mainly driven by the
need of sharing knowledge, expertise, and pooled resources. Science is becoming
more complex which has encouraged scientists to involve more in collaborative
research projects in order to better address the challenges. As a highly
interdisciplinary field with a rapidly evolving scientific landscape,
artificial intelligence calls for researchers with special profiles covering a
diverse set of skills and expertise. Understanding gender aspects of scientific
collaboration is of paramount importance, especially in a field such as
artificial intelligence that has been attracting large investments. Using
social network analysis, natural language processing, and machine learning and
focusing on artificial intelligence publications for the period from 2000 to
2019, in this work, we comprehensively investigated the effects of several
driving factors on acquiring key positions in scientific collaboration networks
through a gender lens. It was found that, regardless of gender, scientific
performance in terms of quantity and impact plays a crucial in possessing the
"social researcher" in the network. However, subtle differences were observed
between female and male researchers in acquiring the "local influencer" role. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12339v1' target="_blank">http://arxiv.org/pdf/2205.12339v1</a><br> <br> <br> <font size='5'> 766 </font> <div style="text-align: right"> 2022-05-19 12:40:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Defending Against Adversarial Attacks by Energy Storage Facility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Adversarial attacks on data-driven algorithms applied in the power system
will be a new type of threat to grid security. Literature has demonstrated that
the adversarial attack on the deep-neural network can significantly mislead the
load fore-cast of a power system. However, it is unclear how the new type of
attack impacts the operation of the grid system. In this research, we manifest
that the adversarial algorithm attack induces a significant cost-increase risk
which will be exacerbated by the growing penetration of intermittent renewable
energy. In Texas, a 5% adversarial attack can increase the total generation
cost by 17% in a quarter, which accounts for around $20 million. When
wind-energy penetration increases to over 40%, the 5% adversarial attack will
inflate the genera-tion cost by 23%. Our research discovers a novel approach to
defending against the adversarial attack: investing in the energy-storage
system. All current literature focuses on developing algorithms to defend
against adversarial attacks. We are the first research revealing the capability
of using the facility in a physical system to defend against the adversarial
algorithm attack in a system of the Internet of Things, such as a smart grid
system. </font><br> Link: <a href='http://arxiv.org/pdf/2205.09522v2' target="_blank">http://arxiv.org/pdf/2205.09522v2</a><br> <br> <br> <font size='5'> 767 </font> <div style="text-align: right"> 2022-05-19 09:49:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Time-dependent effects hinder cooperation on the public goods game</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The public goods game is a model of a society investing some assets and
regaining a profit, although can also model biological populations. In the
classic public goods game only two strategies compete: either cooperate or
defect; a third strategy is often implemented to asses punishment, which is a
mechanism to promote cooperation. The conditions of the game can be of a
dynamical nature, therefore we study time-dependent effects such an as
oscillation in the enhancement factor, which accounts for productivity changes
over time. Furthermore, we continue to study time dependencies on the game with
a delay on the punishment time. We conclude that both the oscillations on the
productivity and the punishment delay concur in the detriment of cooperation. </font><br> Link: <a href='http://arxiv.org/pdf/2205.09434v1' target="_blank">http://arxiv.org/pdf/2205.09434v1</a><br> <br> <br> <font size='5'> 768 </font> <div style="text-align: right"> 2022-05-18 21:22:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Case Study of Building Shared Understanding of Non-Functional Requirements in a Remote Software Organization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building a shared understanding of non-functional requirements (NFRs) is a
known but understudied challenge in requirements engineering, especially in
organizations that adopt continuous software engineering (CSE) practices.
During the peak of the COVID-19 pandemic, many CSE organizations complied with
working remotely due to the imposed health restrictions; some continued to work
remotely while implementing business processes to facilitate team communication
and productivity. In remote CSE organizations, managing NFRs becomes more
challenging due to the limitations to team communication coupled with the
incentive to deliver products quickly. While previous research has identified
the factors that lead to a lack of shared understanding of NFRs in CSE, we
still have a significant gap in understanding how CSE organizations,
particularly in remote work, build a shared understanding of NFRs in their
software development. We conduct a three-month ethnography-informed case study
of a remote CSE organization. Through thematic analysis of our qualitative data
from interviews and observations, we identify a number of practices in
developing a shared understanding of NFRs. The collaborative workspace the
organization uses for remote interaction is Gather, which simulates physical
workspaces, and which our findings suggest allows for informal communications
instrumental for building shared understanding. As actionable insights, we
discuss our findings in light of proactive practices that represent
opportunities for software organizations to invest in building a shared
understanding of NFRs in their development. </font><br> Link: <a href='http://arxiv.org/pdf/2205.09220v1' target="_blank">http://arxiv.org/pdf/2205.09220v1</a><br> <br> <br> <font size='5'> 769 </font> <div style="text-align: right"> 2022-05-18 16:49:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Centralized and decentral approaches to succeed the 100% energiewende in Germany in the European context: A model-based analysis of generation, network, and storage investments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we explore centralized and more decentral approaches to
succeed the energiewende in Germany, in the European context. We use the AnyMOD
framework to model a future renewable-based European energy system, based on a
techno-economic optimization, i.e. cost minimization with given demand,
including both investment and the subsequent dispatch of capacity. The model
includes 29 regions for European countries, and 38 NUTS-2 regions in Germany.
First the entire energy system on the European level is optimized. Based on
these results, the electricity system for the German regions is optimized to
achieve great regional detail to analyse spatial effects. The model allows a
comparison between a stylized central scenario with high amounts of wind
offshore deployed, and a decentral scenario using mainly the existing grid, and
thus relying more on local capacities. The results reveal that the cost for the
second optimization of these two scenarios are about the same: The central
scenario is characterized by network expansion in order to transport the
electricity from the wind offshore sites, whereas the decentral scenario leads
to more photovoltaic and battery deployment closer to the areas with a high
demand for energy. A scenarios with higher energy efficiency and lower demand
projections lead to a significant reduction of investment requirements, and to
different localizations thereof. </font><br> Link: <a href='http://arxiv.org/pdf/2205.09066v1' target="_blank">http://arxiv.org/pdf/2205.09066v1</a><br> <br> <br> <font size='5'> 770 </font> <div style="text-align: right"> 2022-05-18 14:18:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SoK: The Impact of Unlabelled Data in Cyberthreat Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine learning (ML) has become an important paradigm for cyberthreat
detection (CTD) in the recent years. A substantial research effort has been
invested in the development of specialized algorithms for CTD tasks. From the
operational perspective, however, the progress of ML-based CTD is hindered by
the difficulty in obtaining the large sets of labelled data to train ML
detectors. A potential solution to this problem are semisupervised learning
(SsL) methods, which combine small labelled datasets with large amounts of
unlabelled data.
  This paper is aimed at systematization of existing work on SsL for CTD and,
in particular, on understanding the utility of unlabelled data in such systems.
To this end, we analyze the cost of labelling in various CTD tasks and develop
a formal cost model for SsL in this context. Building on this foundation, we
formalize a set of requirements for evaluation of SsL methods, which elucidates
the contribution of unlabelled data. We review the state-of-the-art and observe
that no previous work meets such requirements. To address this problem, we
propose a framework for assessing the benefits of unlabelled data in SsL. We
showcase an application of this framework by performing the first benchmark
evaluation that highlights the tradeoffs of 9 existing SsL methods on 9 public
datasets. Our findings verify that, in some cases, unlabelled data provides a
small, but statistically significant, performance gain. This paper highlights
that SsL in CTD has a lot of room for improvement, which should stimulate
future research in this field. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08944v1' target="_blank">http://arxiv.org/pdf/2205.08944v1</a><br> <br> <br> <font size='5'> 771 </font> <div style="text-align: right"> 2022-05-18 06:36:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mean-variance portfolio selection with dynamic attention behavior in a hidden Markov model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we study closed-loop equilibrium strategies for mean-variance
portfolio selection problem in a hidden Markov model with dynamic attention
behavior. In addition to the investment strategy, the investor's attention to
news is introduced as a control of the accuracy of the news signal process. The
objective is to find equilibrium strategies by numerically solving an extended
HJB equation by using Markov chain approximation method. An iterative algorithm
is constructed and its convergence is established. Numerical examples are also
provided to illustrate the results. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08743v1' target="_blank">http://arxiv.org/pdf/2205.08743v1</a><br> <br> <br> <font size='5'> 772 </font> <div style="text-align: right"> 2022-05-17 20:14:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Well Posedness of Utility Maximization Problems Under Partial Information in a Market with Gaussian Drift</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates well posedness of utility maximization problems for
financial markets where stock returns depend on a hidden Gaussian mean
reverting drift process. Since that process is potentially unbounded well
posedness cannot be guaranteed for utility functions which are not bounded from
above. For power utility with relative risk aversion smaller than those of
log-utility this leads to restrictions on the choice of model parameters such
as the investment horizon and parameters controlling the variance of the asset
price and drift processes. We derive sufficient conditions to the model
parameters leading to bounded maximum expected utility of terminal wealth for
models with full and partial information. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08614v1' target="_blank">http://arxiv.org/pdf/2205.08614v1</a><br> <br> <br> <font size='5'> 773 </font> <div style="text-align: right"> 2022-05-17 15:25:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cyber Risk Assessment for Capital Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cyber risk is an omnipresent risk in the increasingly digitized world that is
known to be difficult to quantify and assess. Despite the fact that cyber risk
shows distinct characteristics from conventional risks, most existing models
for cyber risk in the insurance literature have been purely based on
frequency-severity analysis, which was developed for classical property and
casualty risks. In contrast, the cybersecurity engineering literature employs
different approaches, under which cyber incidents are viewed as threats or
hacker attacks acting on a particular set of vulnerabilities. There appears a
gap in cyber risk modeling between engineering and insurance literature. This
paper presents a novel model to capture these unique dynamics of cyber risk
known from engineering and to model loss distributions based on industry loss
data and a particular company's cybersecurity profile. The analysis leads to a
new tool for allocating resources of the company between cybersecurity
investments and loss-absorbing reserves. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08435v2' target="_blank">http://arxiv.org/pdf/2205.08435v2</a><br> <br> <br> <font size='5'> 774 </font> <div style="text-align: right"> 2022-05-17 12:04:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A two-steps approach to improve the performance of Android malware detectors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The popularity of Android OS has made it an appealing target to malware
developers. To evade detection, including by ML-based techniques, attackers
invest in creating malware that closely resemble legitimate apps. In this
paper, we propose GUIDED RETRAINING, a supervised representation learning-based
method that boosts the performance of a malware detector. First, the dataset is
split into "easy" and "difficult" samples, where difficulty is associated to
the prediction probabilities yielded by a malware detector: for difficult
samples, the probabilities are such that the classifier is not confident on the
predictions, which have high error rates. Then, we apply our GUIDED RETRAINING
method on the difficult samples to improve their classification. For the subset
of "easy" samples, the base malware detector is used to make the final
predictions since the error rate on that subset is low by construction. For the
subset of "difficult" samples, we rely on GUIDED RETRAINING, which leverages
the correct predictions and the errors made by the base malware detector to
guide the retraining process. GUIDED RETRAINING focuses on the difficult
samples: it learns new embeddings of these samples using Supervised Contrastive
Learning and trains an auxiliary classifier for the final predictions. We
validate our method on four state-of-the-art Android malware detection
approaches using over 265k malware and benign apps, and we demonstrate that
GUIDED RETRAINING can reduce up to 40.41% prediction errors made by the malware
detectors. Our method is generic and designed to enhance the classification
performance on a binary classification task. Consequently, it can be applied to
other classification problems beyond Android malware detection. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08265v1' target="_blank">http://arxiv.org/pdf/2205.08265v1</a><br> <br> <br> <font size='5'> 775 </font> <div style="text-align: right"> 2022-05-17 02:28:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Impact of Economic Constraints on the Projected Timeframe for Human-Crewed Deep Space Exploration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep space exploration offers the most profound opportunity for the expansion
of humanity and our understanding of the Universe, but remains extremely
challenging. Progress will continue to be paced by uncrewed missions followed
up by crewed missions to ever further destinations. Major space powers continue
to invest in crewed deep space exploration as an important national strategy.
An improved model based on previous work is developed, which projects the
earliest possible launch dates for human-crewed missions from cis-lunar space
to selected destinations in the Solar System and beyond based on NASA's
historic budget trend and overall development trends of deep space exploration
research. The purpose of the analysis is to provide a projected timeframe for
crewed missions beyond Mars. Our findings suggest the first human missions from
a spacefaring nation or international collaboration to the Asteroid Belt and
Jovian System could be scheduled as soon as ~2071 to ~2087 and ~2101 to ~2121,
respectively, while a launch to the Saturn System may occur by the year ~2132,
with an uncertainty window of ~2129 to ~2153. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08061v1' target="_blank">http://arxiv.org/pdf/2205.08061v1</a><br> <br> <br> <font size='5'> 776 </font> <div style="text-align: right"> 2022-05-17 00:55:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A review on non-Hermitian skin effect</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The past decades have witnessed the flourishing of non-Hermitian physics in
non-conservative systems, leading to unprecedented phenomena of unidirectional
invisibility, enhanced sensitivity and more recently the novel topological
features such as bulk Fermi arcs. Among them, growing efforts have been
invested to an intriguing phenomenon, known as the non-Hermitian skin effect
(NHSE). Here, we review the recent progress in this emerging field. By starting
from the one-dimensional (1D) case, the fundamental concepts of NHSE, its
minimal model, the physical meanings and consequences are elaborated in
details. In particular, we discuss the NHSE enriched by lattice symmetries,
which gives rise to unique non-Hermitian topological properties with revised
bulk-boundary correspondence (BBC) and new definitions of topological
invariants. Then we extend the discussions to two and higher dimensions, where
dimensional surprises enable even more versatile NHSE phenomena. Extensions of
NHSE assisted with extra degrees of freedom such as long-range coupling,
pseudospins, magnetism, non-linearity and crystal defects are also reviewed.
This is followed by the contemporary experimental progress for NHSE. Finally,
we provide the outlooks to possible future directions and developments. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08037v1' target="_blank">http://arxiv.org/pdf/2205.08037v1</a><br> <br> <br> <font size='5'> 777 </font> <div style="text-align: right"> 2022-05-16 08:52:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fairness in Participatory Budgeting via Equality of Resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce a family of normative principles to assess fairness in the
context of participatory budgeting. These principles are based on the
fundamental idea that budget allocations should be fair in terms of the
resources invested into meeting the wishes of individual voters. This is in
contrast to earlier proposals that are based on specific assumptions regarding
the satisfaction of voters with a given budget allocation. We analyse these new
principles in axiomatic, algorithmic, and experimental terms. </font><br> Link: <a href='http://arxiv.org/pdf/2205.07517v2' target="_blank">http://arxiv.org/pdf/2205.07517v2</a><br> <br> <br> <font size='5'> 778 </font> <div style="text-align: right"> 2022-05-15 17:09:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mack-Net model: Blending Mack's model with Recurrent Neural Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In general insurance companies, a correct estimation of liabilities plays a
key role due to its impact on management and investing decisions. Since the
Financial Crisis of 2007-2008 and the strengthening of regulation, the focus is
not only on the total reserve but also on its variability, which is an
indicator of the risk assumed by the company. Thus, measures that relate
profitability with risk are crucial in order to understand the financial
position of insurance firms. Taking advantage of the increasing computational
power, this paper introduces a stochastic reserving model whose aim is to
improve the performance of the traditional Mack's reserving model by applying
an ensemble of Recurrent Neural Networks. The results demonstrate that blending
traditional reserving models with deep and machine learning techniques leads to
a more accurate assessment of general insurance liabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2205.07334v1' target="_blank">http://arxiv.org/pdf/2205.07334v1</a><br> <br> <br> <font size='5'> 779 </font> <div style="text-align: right"> 2022-05-13 18:44:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Comparator Adaptive Online Learning with Switching Cost</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Practical online learning tasks are often naturally defined on unconstrained
domains, where optimal algorithms for general convex losses are characterized
by the notion of comparator adaptivity. In this paper, we design such
algorithms in the presence of switching cost - the latter penalizes the typical
optimism in adaptive algorithms, leading to a delicate design trade-off. Based
on a novel dual space scaling strategy discovered by a continuous-time
analysis, we propose a simple algorithm that improves the existing comparator
adaptive regret bound [ZCP22a] to the optimal rate. The obtained benefits are
further extended to the expert setting, and the practicality of the proposed
algorithm is demonstrated through a sequential investment task. </font><br> Link: <a href='http://arxiv.org/pdf/2205.06846v3' target="_blank">http://arxiv.org/pdf/2205.06846v3</a><br> <br> <br> <font size='5'> 780 </font> <div style="text-align: right"> 2022-05-13 16:26:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Two strategies for boreal forestry with goodwill in capitalization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Two strategies for boreal forestry with goodwill in estate capitalization are
introduced. A strategy focusing on Real Estate (RE) is financially superior to
Timber Sales (TS). The feasibility of the RE requires the presence of forest
land end users in the real estate market, like insurance companies or
investment trusts, and the periodic boundary condition does not apply.
Commercial thinnings do not enter the RE strategy in a stand-level discussion.
However, they may appear in estates with a variable age structure and enable an
extension of stand rotation times. </font><br> Link: <a href='http://arxiv.org/pdf/2205.06744v1' target="_blank">http://arxiv.org/pdf/2205.06744v1</a><br> <br> <br> <font size='5'> 781 </font> <div style="text-align: right"> 2022-05-13 16:06:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The energy return on investment of whole energy systems: application to Belgium</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Planning the defossilization of energy systems by facilitating high
penetration of renewables and maintaining access to abundant and affordable
primary energy resources is a nontrivial multi-objective problem. However, so
far, most long-term policies to decrease the carbon footprint of our societies
consider the cost of the system as the leading indicator in the energy system
models. This paper is the first to develop a novel approach by adding the
energy return on investment (EROI) to a whole energy system optimization model.
We built the database with all EROI technologies and resources considered. In
addition, moving away from fossil-based to carbon-neutral energy systems raises
the issue of the uncertainty of low-carbon technologies and resource data.
Thus, we conducted a global sensitivity analysis to identify the main
parameters driving the variations in the EROI of the system. This novel
approach can be applied to any energy system, and we use a real-world case
study to illustrate the model: the 2035 Belgian energy system for several
greenhouse gas emissions targets.
  The main results are threefold: (i) the EROI of the system decreases from 8.9
to 3.9 when greenhouse gas emissions are reduced by 5; (ii) the renewable fuels
- mainly imported renewable gas - represent the largest share of the system
primary energy mix; (iii) in the sensitivity analysis, the renewable fuels
drive 67% of the variation of the EROI of the system for low greenhouse gas
emissions scenarios. The decrease in the EROI raises questions about meeting
the climate targets without adverse socio-economic impact. Most countries rely
massively on fossil fuels, and they could encounter an EROI decline when
shifting to carbon neutrality. Thus, this study demonstrates the importance of
considering other criteria, such as EROI, in energy system models. </font><br> Link: <a href='http://arxiv.org/pdf/2205.06727v4' target="_blank">http://arxiv.org/pdf/2205.06727v4</a><br> <br> <br> <font size='5'> 782 </font> <div style="text-align: right"> 2022-05-11 18:16:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A time-varying study of Chinese investor sentiment, stock market liquidity and volatility: Based on deep learning BERT model and TVP-VAR model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Based on the commentary data of the Shenzhen Stock Index bar on the EastMoney
website from January 1, 2018 to December 31, 2019. This paper extracts the
embedded investor sentiment by using a deep learning BERT model and
investigates the time-varying linkage between investment sentiment, stock
market liquidity and volatility using a TVP-VAR model. The results show that
the impact of investor sentiment on stock market liquidity and volatility is
stronger. Although the inverse effect is relatively small, it is more
pronounced with the state of the stock market. In all cases, the response is
more pronounced in the short term than in the medium to long term, and the
impact is asymmetric, with shocks stronger when the market is in a downward
spiral. </font><br> Link: <a href='http://arxiv.org/pdf/2205.05719v2' target="_blank">http://arxiv.org/pdf/2205.05719v2</a><br> <br> <br> <font size='5'> 783 </font> <div style="text-align: right"> 2022-05-11 08:10:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial Task & Hyperbolic Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid spread of information over social media influences quantitative
trading and investments. The growing popularity of speculative trading of
highly volatile assets such as cryptocurrencies and meme stocks presents a
fresh challenge in the financial realm. Investigating such "bubbles" - periods
of sudden anomalous behavior of markets are critical in better understanding
investor behavior and market dynamics. However, high volatility coupled with
massive volumes of chaotic social media texts, especially for underexplored
assets like cryptocoins pose a challenge to existing methods. Taking the first
step towards NLP for cryptocoins, we present and publicly release
CryptoBubbles, a novel multi-span identification task for bubble detection, and
a dataset of more than 400 cryptocoins from 9 exchanges over five years
spanning over two million tweets. Further, we develop a set of
sequence-to-sequence hyperbolic models suited to this multi-span identification
task based on the power-law dynamics of cryptocurrencies and user behavior on
social media. We further test the effectiveness of our models under zero-shot
settings on a test set of Reddit posts pertaining to 29 "meme stocks", which
see an increase in trade volume due to social media hype. Through quantitative,
qualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins
and meme-stocks, we show the practical applicability of CryptoBubbles and
hyperbolic models. </font><br> Link: <a href='http://arxiv.org/pdf/2206.06320v1' target="_blank">http://arxiv.org/pdf/2206.06320v1</a><br> <br> <br> <font size='5'> 784 </font> <div style="text-align: right"> 2022-05-09 18:01:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Performance of ESPRESSO's high resolution 4x2 binning for characterizing intervening absorbers towards faint quasars</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As of October 2021 (Period 108), the European Southern Observatory (ESO)
offers a new mode of the ESPRESSO spectrograph designed to use the High
Resolution grating with 4x2 binning (spatial by spectral; HR42 mode) with the
specific objective of observing faint targets with a single Unit Telescope at
Paranal. We validated the new HR42 mode using four hours of on-target
observations of the quasar J0003-2603, known to host an intervening metal-poor
absorber along the line of sight. The capabilities of the ESPRESSO HR42 mode
(resolving power R~137 000) were evaluated by comparing to a UVES spectrum of
the same target with a similar integration time but lower resolving power (R~48
000). For both data sets we tested the ability to decompose the velocity
profile of the intervening absorber using Voigt profile fitting and extracted
the total column densities of CIV, NI, SiII, AlII, FeII, and NiII. With ~3x the
resolving power and ~2x lower S/N for a nearly equivalent exposure time, the
ESPRESSO data is able to just as accurately characterize the individual
components of the absorption lines as the comparison UVES data, but has the
added bonus of identifying narrower components not detected by UVES. For UVES
to provide similar spectral resolution (R>100 000; 0.3'' slit) and the broad
wavelength coverage of ESPRESSO, the Exposure Time Calculator (ETC) supplied by
ESO estimates 8 hrs of exposure time spread over two settings; requiring double
the time investment than that of ESPRESSO's HR42 mode whilst not properly
sampling the UVES spectral resolution element. Thus ESPRESSO's HR42 mode offers
nearly triple the resolving power of UVES (0.8'' slit to match typical ambient
conditions at Paranal) and provides more accurate characterization of quasar
absorption features for an equivalent exposure time. </font><br> Link: <a href='http://arxiv.org/pdf/2205.04488v1' target="_blank">http://arxiv.org/pdf/2205.04488v1</a><br> <br> <br> <font size='5'> 785 </font> <div style="text-align: right"> 2022-05-09 16:21:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-Serviced IoT: Practical and Private IoT Computation Offloading with Full User Control</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid increase in the adoption of Internet-of-Things (IoT) devices raises
critical privacy concerns as these devices can access a variety of sensitive
data. The current status quo of relying on manufacturers' cloud services to
process this data is especially problematic since users cede control once their
data leaves their home. Multiple recent incidents further call into question if
vendors can indeed be trusted with users' data. At the same time, users desire
compelling features supported by IoT devices and ML-based cloud inferences
which compels them to subscribe to manufacturer-managed cloud services. An
alternative to use a local in-home hub requires substantial hardware
investment, management, and scalability limitations. This paper proposes
Self-Serviced IoT (SSIoT), a clean-slate approach of using a hybrid hub-cloud
setup to enable privacy-aware computation offload for IoT applications.
Uniquely, SSIoT enables opportunistic computation offload to public cloud
providers while still ensuring that the end-user retains complete end-to-end
control of their private data reducing the trust required from public cloud
providers. We show that SSIoT can leverage emerging function-as-a-service
computation (e.g. AWS Lambda) to make these offloads cost-efficient, scalable
and high performance as long as key limitations of being stateless, limited
resources, and security isolation can be addressed. We build an end-to-end
prototype of SSIoT and evaluate it using several micro-benchmarks and example
applications representing real-world IoT use cases. Our results show that SSIoT
is highly scalable, as compared to local-only approaches which struggle with as
little as 2-4 apps in parallel. We also show that SSIoT is cost-efficient
(operating a smart doorbell for $10 a year) at the cost of minimal additional
latency as compared to a local-only hub, even with a hardware ML accelerator. </font><br> Link: <a href='http://arxiv.org/pdf/2205.04405v1' target="_blank">http://arxiv.org/pdf/2205.04405v1</a><br> <br> <br> <font size='5'> 786 </font> <div style="text-align: right"> 2022-05-08 07:01:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Univariate and Multivariate LSTM Model for Short-Term Stock Market Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Designing robust and accurate prediction models has been a viable research
area since a long time. While proponents of a well-functioning market
predictors believe that it is difficult to accurately predict market prices but
many scholars disagree. Robust and accurate prediction systems will not only be
helpful to the businesses but also to the individuals in making their financial
investments. This paper presents an LSTM model with two different input
approaches for predicting the short-term stock prices of two Indian companies,
Reliance Industries and Infosys Ltd. Ten years of historic data (2012-2021) is
taken from the yahoo finance website to carry out analysis of proposed
approaches. In the first approach, closing prices of two selected companies are
directly applied on univariate LSTM model. For the approach second, technical
indicators values are calculated from the closing prices and then collectively
applied on Multivariate LSTM model. Short term market behaviour for upcoming
days is evaluated. Experimental outcomes revel that approach one is useful to
determine the future trend but multivariate LSTM model with technical
indicators found to be useful in accurately predicting the future price
behaviours. </font><br> Link: <a href='http://arxiv.org/pdf/2205.06673v1' target="_blank">http://arxiv.org/pdf/2205.06673v1</a><br> <br> <br> <font size='5'> 787 </font> <div style="text-align: right"> 2022-05-06 09:09:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Financial Markets and the Real Economy: A Statistical Field Perspective on Capital Allocation and Accumulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper provides a general method to directly translate a classical
economic framework with a large number of agents into a field-formalism model.
This type of formalism allows the analytical treatment of economic models with
an arbitrary number of agents, while preserving the system's interactions and
microeconomic features of the individual level.We apply this methodology to
model the interactions between financial markets and the real economy,
described in a classical framework of a large number of heterogeneous agents,
investors and firms. Firms are spread among sectors but may shift between
sectors to improve their returns. They compete by producing differentiated
goods and reward their investors by paying dividends and through their stocks'
valuation. Investors invest in firms and move along sectors based on firms'
expected long-run returns.The field-formalism model derived from this framework
allows for collective states to emerge. We show that the number of firms in
each sector depends on the aggregate financial capital invested in the sector
and its firms' expected long-term returns. Capital accumulation in each sector
depends both on short-term returns and expected long-term returns relative to
neighbouring sectors.For each sector, three patterns of accumulation emerge. In
the first pattern, the dividend component of short-term returns is determinant
for sectors with small number of firms and low capital. In the second pattern,
both short and long-term returns in the sector drive intermediate-to-high
capital. In the third pattern, higher expectations of long-term returns drive
massive inputs of capital.Instability in capital accumulation may arise among
and within sectors. We therefore widen our approach and study the dynamics of
the collective configurations, in particular interactions between average
capital and expected long-term returns, and show that overall stability
crucially depends on the expectations' formation process.Expectations that are
highly reactive to capital variations stabilize high capital configurations,
and drive low-to-moderate capital sectors towards zero or a higher level of
capital, depending on their initial capital. Inversely, low-to moderate capital
configurations are stabilized by expectations moderately reactive to capital
variations, and drive high capital sectors towards more moderate level of
capital equilibria.Eventually, the combination of expectations both highly
sensitive to exogenous conditions and highly reactive to variations in capital
imply that large fluctuations of capital in the system, at the possible expense
of the real economy. </font><br> Link: <a href='http://arxiv.org/pdf/2205.03087v1' target="_blank">http://arxiv.org/pdf/2205.03087v1</a><br> <br> <br> <font size='5'> 788 </font> <div style="text-align: right"> 2022-05-06 07:21:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dynamic programming principle for delayed stochastic recursive optimal control problem and HJB equation with non-Lipschitz generator</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we study the delayed stochastic recursive optimal control
problem with a non-Lipschitz generator, in which both the dynamics of the
control system and the recursive cost functional depend on the past path
segment of the state process in a general form. First, the dynamic programming
principle for this control problem is obtained. Then, by the generalized
comparison theorem of backward stochastic differential equations and the
stability of viscosity solutions, we establish the connection between the value
function and the viscosity solution of the associated Hamilton-Jacobi-Bellman
equation. Finally, an application to the consumption-investment problem under
the delayed continuous-time Epstein-Zin utility with a non-Lipschitz generator
is presented. </font><br> Link: <a href='http://arxiv.org/pdf/2205.03052v2' target="_blank">http://arxiv.org/pdf/2205.03052v2</a><br> <br> <br> <font size='5'> 789 </font> <div style="text-align: right"> 2022-05-05 20:31:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can transit investments in low-income neighbourhoods increase transit use? Exploring the nexus of income, car-ownership, and transit accessibility in Toronto</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transportation equity advocates recommend improving public transit in
low-income neighbourhoods to alleviate socio-spatial inequalities and increase
quality of life. However, transportation planners often overlook transit
investments in neighbourhoods with "transit-captive" populations because they
are assumed to result in less mode-shifting, congestion relief, and
environmental benefits, compared to investments that aim to attract choice
riders in wealthier communities. In North American cities, while many
low-income households are already transit users, some also own and use private
vehicles. It suggests that transit improvements in low-income communities could
indeed result in more transit use and less car use. Accordingly, the main
objective of this article is to explore the statistical relationship between
transit use and transit accessibility as well as how this varies by household
income and vehicle ownership in the Greater Toronto and Hamilton Area (GTHA).
Using stratified regression models, we find that low-income households with one
or more cars per adult have the most elastic relationship between transit
accessibility and transit use; they are more likely to be transit riders if
transit improves. However, we confirm that in auto-centric areas with poor
transit, the transit use of low-income households drops off sharply as car
ownership increases. On the other hand, a sensitivity analysis suggests more
opportunities for increasing transit ridership among car-deficit households
when transit is improved. These findings indicate that improving transit in
low-income inner suburbs, where most low-income car-owning households are
living, would align social with environmental planning goals. </font><br> Link: <a href='http://arxiv.org/pdf/2205.04556v1' target="_blank">http://arxiv.org/pdf/2205.04556v1</a><br> <br> <br> <font size='5'> 790 </font> <div style="text-align: right"> 2022-05-04 12:59:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Who Will Support My Project? Interactive Search of Potential Crowdfunding Investors Through InSearch</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Crowdfunding provides project founders with a convenient way to reach online
investors. However, it is challenging for founders to find the most potential
investors and successfully raise money for their projects on crowdfunding
platforms. A few machine learning based methods have been proposed to recommend
investors' interest in a specific crowdfunding project, but they fail to
provide project founders with explanations in detail for these recommendations,
thereby leading to an erosion of trust in predicted investors. To help
crowdfunding founders find truly interested investors, we conducted
semi-structured interviews with four crowdfunding experts and presents
inSearch, a visual analytic system. inSearch allows founders to search for
investors interactively on crowdfunding platforms. It supports an effective
overview of potential investors by leveraging a Graph Neural Network to model
investor preferences. Besides, it enables interactive exploration and
comparison of the temporal evolution of different investors' investment
details. </font><br> Link: <a href='http://arxiv.org/pdf/2205.02041v2' target="_blank">http://arxiv.org/pdf/2205.02041v2</a><br> <br> <br> <font size='5'> 791 </font> <div style="text-align: right"> 2022-05-02 13:15:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fast Continuous and Integer L-shaped Heuristics Through Supervised Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a methodology at the nexus of operations research and machine
learning (ML) leveraging generic approximators available from ML to accelerate
the solution of mixed-integer linear two-stage stochastic programs. We aim at
solving problems where the second stage is highly demanding. Our core idea is
to gain large reductions in online solution time while incurring small
reductions in first-stage solution accuracy by substituting the exact
second-stage solutions with fast, yet accurate supervised ML predictions. This
upfront investment in ML would be justified when similar problems are solved
repeatedly over time, for example, in transport planning related to fleet
management, routing and container yard management.
  Our numerical results focus on the problem class seminally addressed with the
integer and continuous L-shaped cuts. Our extensive empirical analysis is
grounded in standardized families of problems derived from stochastic server
location (SSLP) and stochastic multi knapsack (SMKP) problems available in the
literature. The proposed method can solve the hardest instances of SSLP in less
than 9% of the time it takes the state-of-the-art exact method, and in the case
of SMKP the same figure is 20%. Average optimality gaps are in most cases less
than 0.1%. </font><br> Link: <a href='http://arxiv.org/pdf/2205.00897v2' target="_blank">http://arxiv.org/pdf/2205.00897v2</a><br> <br> <br> <font size='5'> 792 </font> <div style="text-align: right"> 2022-05-02 07:26:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Multi-stage deep architecture for summary generation of soccer videos</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Video content is present in an ever-increasing number of fields, both
scientific and commercial. Sports, particularly soccer, is one of the
industries that has invested the most in the field of video analytics, due to
the massive popularity of the game and the emergence of new markets. Previous
state-of-the-art methods on soccer matches video summarization rely on
handcrafted heuristics to generate summaries which are poorly generalizable,
but these works have yet proven that multiple modalities help detect the best
actions of the game. On the other hand, machine learning models with higher
generalization potential have entered the field of summarization of
general-purpose videos, offering several deep learning approaches. However,
most of them exploit content specificities that are not appropriate for sport
whole-match videos. Although video content has been for many years the main
source for automatizing knowledge extraction in soccer, the data that records
all the events happening on the field has become lately very important in
sports analytics, since this event data provides richer context information and
requires less processing. We propose a method to generate the summary of a
soccer match exploiting both the audio and the event metadata. The results show
that our method can detect the actions of the match, identify which of these
actions should belong to the summary and then propose multiple candidate
summaries which are similar enough but with relevant variability to provide
different options to the final editor. Furthermore, we show the generalization
capability of our work since it can transfer knowledge between datasets from
different broadcasting companies, different competitions, acquired in different
conditions, and corresponding to summaries of different lengths </font><br> Link: <a href='http://arxiv.org/pdf/2205.00694v1' target="_blank">http://arxiv.org/pdf/2205.00694v1</a><br> <br> <br> <font size='5'> 793 </font> <div style="text-align: right"> 2022-05-01 10:51:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Binary Networked Public Goods Game with Altruism</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the classical Binary Networked Public Goods (BNPG) game, a player can
either invest in a public project or decide not to invest. Based on the
decisions of all the players, each player receives a reward as per his/her
utility function. However, classical models of BNPG game do not consider
altruism, which players often exhibit and can significantly affect equilibrium
behavior. Yu et al. [24] extended the classical BNPG game to capture the
altruistic aspect of the players. We, in this paper, first study the problem of
deciding the existence of a Pure Strategy Nash Equilibrium (PSNE) in a BNPG
game with altruism. This problem is already known to be NP-complete. We
complement this hardness result by showing that the problem admits efficient
algorithms when the input network is either a tree or a complete graph. We
further study the Altruistic Network Modification problem, where the task is to
compute if a target strategy profile can be made a PSNE by adding or deleting a
few edges. This problem is also known to be NP-complete. We strengthen this
hardness result by exhibiting intractability results even for trees. A perhaps
surprising finding of our work is that the above problem remains NP-hard even
for bounded degree graphs when the altruism network is undirected, but becomes
polynomial-time solvable when the altruism network is directed. We also show
some results on computing an MSNE and some parameterized complexity results. In
summary, our results show that it is much easier to predict how the players in
a BNPG game will behave compared to how the players in a BNPG game can be made
to behave in a desirable way. </font><br> Link: <a href='http://arxiv.org/pdf/2205.00442v1' target="_blank">http://arxiv.org/pdf/2205.00442v1</a><br> <br> <br> <font size='5'> 794 </font> <div style="text-align: right"> 2022-04-29 07:38:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Encrypted, Anonymized System for Protected Health Information Verification Built via Proof of Stake</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Digital Health Passes (DHP), systems of digitally validating quarantine and
vaccination status such as the New York IBM Excelsior Pass, demonstrate a
lawful means to approach some benefits offered by "true elimination" treatment
strategies-which focus on the complete elimination of cases instead of
investing more in controlling the progression of the disease-of COVID-19.
Current implementations of DHPs require region-based control and central
storage of Protected Health Information (PHI)-creating a challenge to
widespread use across different jurisdictions with incompatible data management
systems and a lack of standardized patient privacy controls. In this work, a
mechanism for decentralized PHI storage and validation is proposed through a
novel two-stage handshaking mechanism update to blockchain proof-of-stake
consensus. The proposed mechanism, when used to support a DHP, allows
individuals to validate their quarantine and testing universally with any
jurisdiction while allowing their right of independent movement and the
protection of their PHI. Implementational details on the protocol are given,
and the protocol is shown to withstand a 1% disturbance attack at only 923
participants via a Monte-Carlo simulation: further validating its stability. </font><br> Link: <a href='http://arxiv.org/pdf/2205.02753v1' target="_blank">http://arxiv.org/pdf/2205.02753v1</a><br> <br> <br> <font size='5'> 795 </font> <div style="text-align: right"> 2022-04-28 10:01:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fuzzy Expert System for Stock Portfolio Selection: An Application to Bombay Stock Exchange</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Selection of proper stocks, before allocating investment ratios, is always a
crucial task for the investors. Presence of many influencing factors in stock
performance have motivated researchers to adopt various Artificial Intelligence
(AI) techniques to make this challenging task easier. In this paper a novel
fuzzy expert system model is proposed to evaluate and rank the stocks under
Bombay Stock Exchange (BSE). Dempster-Shafer (DS) evidence theory is used for
the first time to automatically generate the consequents of the fuzzy rule base
to reduce the effort in knowledge base development of the expert system. Later
a portfolio optimization model is constructed where the objective function is
considered as the ratio of the difference of fuzzy portfolio return and the
risk free return to the weighted mean semi-variance of the assets that has been
used. The model is solved by applying Ant Colony Optimization (ACO) algorithm
by giving preference to the top ranked stocks. The performance of the model
proved to be satisfactory for short-term investment period when compared with
the recent performance of the stocks. </font><br> Link: <a href='http://arxiv.org/pdf/2204.13385v2' target="_blank">http://arxiv.org/pdf/2204.13385v2</a><br> <br> <br> <font size='5'> 796 </font> <div style="text-align: right"> 2022-04-27 08:11:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Advantages of maintaining a multi-task project-specific bot: an experience report</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Bots are becoming a popular method for automating basic everyday tasks in
many software projects. This is true in particular because of the availability
of many off-the-shelf task-specific bots that teams can quickly adopt (which
are sometimes completed with additional task-specific custom bots). Based on
our experience in the Coq project, where we have developed and maintained a
multi-task project-specific bot, we argue that this alternative approach to
project automation should receive more attention because it strikes a good
balance between productivity and adaptibility. In this article, we describe the
kind of automation that our bot implements, what advantages we have gained by
maintaining a project-specific bot, and the technology and architecture choices
that have made it possible. We draw conclusions that should generalize to other
medium-sized software teams willing to invest in project automation without
disrupting their workflows. </font><br> Link: <a href='http://arxiv.org/pdf/2204.12758v1' target="_blank">http://arxiv.org/pdf/2204.12758v1</a><br> <br> <br> <font size='5'> 797 </font> <div style="text-align: right"> 2022-04-27 00:22:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Framework for disruptive AI/ML Innovation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This framework enables C suite executive leaders to define a business plan
and manage technological dependencies for building AI/ML Solutions. The
business plan of this framework provides components and background information
to define strategy and analyze cost. Furthermore, the business plan represents
the fundamentals of AI/ML Innovation and AI/ML Solutions. Therefore, the
framework provides a menu for managing and investing in AI/ML. Finally, this
framework is constructed with an interdisciplinary and holistic view of AI/ML
Innovation and builds on advances in business strategy in harmony with
technological progress for AI/ML. This framework incorporates value chain,
supply chain, and ecosystem strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2204.12641v1' target="_blank">http://arxiv.org/pdf/2204.12641v1</a><br> <br> <br> <font size='5'> 798 </font> <div style="text-align: right"> 2022-04-25 17:55:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semi-Parametric Neural Image Synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Much of this success is due to
the scalability of these architectures and hence caused by a dramatic increase
in model complexity and in the computational resources invested in training
these models. Our work questions the underlying paradigm of compressing large
training data into ever growing parametric representations. We rather present
an orthogonal, semi-parametric approach. We complement comparably small
diffusion or autoregressive models with a separate image database and a
retrieval strategy. During training we retrieve a set of nearest neighbors from
this external database for each training instance and condition the generative
model on these informative samples. While the retrieval approach is providing
the (local) content, the model is focusing on learning the composition of
scenes based on this content. As demonstrated by our experiments, simply
swapping the database for one with different contents transfers a trained model
post-hoc to a novel domain. The evaluation shows competitive performance on
tasks which the generative model has not been trained on, such as
class-conditional synthesis, zero-shot stylization or text-to-image synthesis
without requiring paired text-image data. With negligible memory and
computational overhead for the external database and retrieval we can
significantly reduce the parameter count of the generative model and still
outperform the state-of-the-art. </font><br> Link: <a href='http://arxiv.org/pdf/2204.11824v3' target="_blank">http://arxiv.org/pdf/2204.11824v3</a><br> <br> <br> <font size='5'> 799 </font> <div style="text-align: right"> 2022-04-24 19:20:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Taming Hybrid-Cloud Fast and Scalable Graph Analytics at Twitter</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We have witnessed a boosted demand for graph analytics at Twitter in recent
years, and graph analytics has become one of the key parts of Twitter's
large-scale data analytics and machine learning for driving engagement, serving
the most relevant content, and promoting healthier conversations. However,
infrastructure for graph analytics has historically not been an area of
investment at Twitter, resulting in a long timeline and huge engineering effort
for each project to deal with graphs at the Twitter scale. How do we build a
unified graph analytics user experience to fulfill modern data analytics on
various graph scales spanning from thousands to hundreds of billions of
vertices and edges?
  To bring fast and scalable graph analytics capability into production, we
investigate the challenges we are facing in large-scale graph analytics at
Twitter and propose a unified graph analytics platform for efficient, scalable,
and reliable graph analytics across on-premises and cloud, to fulfill the
requirements of diverse graph use cases and challenging scales. We also conduct
quantitative benchmarking on Twitter's production-level graph use cases between
popular graph analytics frameworks to certify our solution. </font><br> Link: <a href='http://arxiv.org/pdf/2204.11338v2' target="_blank">http://arxiv.org/pdf/2204.11338v2</a><br> <br> <br> <font size='5'> 800 </font> <div style="text-align: right"> 2022-04-23 16:32:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Socio-Political Feedback on the Path to Net Zero</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Anthropogenic emissions of CO2 must soon approach net-zero to stabilize the
global mean temperature. Although several international agreements have
advocated for coordinated climate actions, their implementation has remained
below expectations. One of the main challenges of international cooperation is
the different degrees of socio-political acceptance of decarbonization. Here we
interrogate a minimalistic model of the coupled natural-human system
representing the impact of such socio-political acceptance on clean energy
investments and the path to net-zero emissions. We show that incentives and
carbon pricing are essential tools to achieve net-zero before critical CO2
concentrations are reached, and deep international coordination is necessary
for a rapid and effective transition. Although a perfect coordination scenario
remains unlikely, as investments in clean energy are ultimately limited by
myopic economic strategies and a policy system that promotes free-riding, more
realistic decentralized cooperation with partial efforts from each actor could
still lead to significant emissions cuts. </font><br> Link: <a href='http://arxiv.org/pdf/2204.11101v1' target="_blank">http://arxiv.org/pdf/2204.11101v1</a><br> <br> <br> <font size='5'> 801 </font> <div style="text-align: right"> 2022-04-22 14:57:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analyzing the Adoption Challenges of the Internet of Things (IoT) and Artificial Intelligence (AI) for Smart Cities in China</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Smart cities play a vital role in the growth of a nation. In recent years,
several countries have made huge investments in developing smart cities to
offer sustainable living. However, there are some challenges to overcome in
smart city development, such as traffic and transportation man-agement, energy
and water distribution and management, air quality and waste management
monitoring, etc. The capabilities of the Internet of Things (IoT) and
artificial intelligence (AI) can help to achieve some goals of smart cities,
and there are proven examples from some cities like Singapore, Copenhagen, etc.
However, the adoption of AI and the IoT in developing countries has some
challenges. The analysis of challenges hindering the adoption of AI and the IoT
are very limited. This study aims to fill this research gap by analyzing the
causal relationships among the challenges in smart city development, and
contains several parts that conclude the previous scholars work, as well as
independent research and investigation, such as data collection and analysis
based on DEMATEL. In this paper, we have reviewed the literature to extract key
chal-lenges for the adoption of AI and the IoT. These helped us to proceed with
the investigation and analyze the adoption status. Therefore, using the PRISMA
method, 10 challenges were identified from the literature review. Subsequently,
determination of the causal inter-relationships among the key challenges based
on expert opinions using DEMATEL is performed. This study explored the driving
and dependent power of the challenges, and causal relationships between the
barriers were established. </font><br> Link: <a href='http://arxiv.org/pdf/2205.01067v1' target="_blank">http://arxiv.org/pdf/2205.01067v1</a><br> <br> <br> <font size='5'> 802 </font> <div style="text-align: right"> 2022-04-21 16:20:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Heterogeneous rarity patterns drive price dynamics in NFT collections</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We quantify Non Fungible Token (NFT) rarity and investigate how it impacts
market behaviour by analysing a dataset of 3.7M transactions collected between
January 2018 and June 2022, involving 1.4M NFTs distributed across 410
collections. First, we consider the rarity of an NFT based on the set of
human-readable attributes it possesses and show that most collections present
heterogeneous rarity patterns, with few rare NFTs and a large number of more
common ones. Then, we analyze market performance and show that, on average,
rarer NFTs: (i) sell for higher prices, (ii) are traded less frequently, (iii)
guarantee higher returns on investment (ROIs), and (iv) are less risky, i.e.,
less prone to yield negative returns. We anticipate that these findings will be
of interest to researchers as well as NFT creators, collectors, and traders. </font><br> Link: <a href='http://arxiv.org/pdf/2204.10243v4' target="_blank">http://arxiv.org/pdf/2204.10243v4</a><br> <br> <br> <font size='5'> 803 </font> <div style="text-align: right"> 2022-04-21 13:55:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Investigating the concentration of High Yield Investment Programs in the United Kingdom</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Ponzi schemes that offer absurdly high rates of return by relying on more and
more people paying into the scheme have been documented since at least the
mid-1800s. Ponzi schemes have shifted online in the Internet age, and some are
re-branded as HYIPs or High Yield Investment Programs. This paper focuses on
understanding HYIPs' continuous presence and presents various possible reasons
behind their existence in today's world. A look into the countries where these
schemes purport to exist, we find that 62.89% of all collected HYIPs claim to
be in the United Kingdom (UK), and a further 55.56% are officially registered
in the UK as a 'limited company' with a registration number provided by the UK
Companies House, a UK agency that registers companies. We investigate other
factors influencing these schemes, including the HYIPs' social media platforms
and payment processors. The lifetime of the HYIPs helps to understand the
success/failure of the investment schemes and helps indicate the schemes that
could attract more investors. Using Cox proportional regression analysis, we
find that having a valid UK address significantly affects the lifetime of an
HYIP. </font><br> Link: <a href='http://arxiv.org/pdf/2205.08569v1' target="_blank">http://arxiv.org/pdf/2205.08569v1</a><br> <br> <br> <font size='5'> 804 </font> <div style="text-align: right"> 2022-04-20 17:00:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Estimating city-wide hourly bicycle flow using a hybrid LSTM MDN</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cycling can reduce greenhouse gas emissions and air pollution and increase
public health. With this in mind, policy-makers in cities worldwide seek to
improve the bicycle mode-share. However, they often struggle against the fear
and the perceived riskiness of cycling. Efforts to increase the bicycle's
mode-share involve many measures, one of them being the improvement of cycling
safety. This requires the analysis of the factors surrounding accidents and the
outcome. However, meaningful analysis of cycling safety requires accurate
bicycle flow data that is generally sparse or not even available at a segment
level. Therefore, safety engineers often rely on aggregated variables or
calibration factors that fail to account for variations in the cycling traffic
caused by external factors. This paper fills this gap by presenting a Deep
Learning based approach, the Long Short-Term Memory Mixture Density Network
(LSTMMDN), to estimate hourly bicycle flow in Copenhagen, conditional on
weather, temporal and road conditions at the segment level. This method
addresses the shortcomings in the calibration factor method and results in
66-77\% more accurate bicycle traffic estimates. To quantify the impact of more
accurate bicycle traffic estimates in cycling safety analysis, we estimate
bicycle crash risk models to evaluate bicycle crashes in Copenhagen. The models
are identical except for the exposure variables being used. One model is
estimated using the LSTMMDN estimates, one using the calibration-based
estimates, and one using yearly mean traffic estimates. The results show that
investing in more advanced methods for obtaining bicycle volume estimates can
benefit the quality, mitigating efforts by improving safety analyses and other
performance measures. </font><br> Link: <a href='http://arxiv.org/pdf/2204.09620v1' target="_blank">http://arxiv.org/pdf/2204.09620v1</a><br> <br> <br> <font size='5'> 805 </font> <div style="text-align: right"> 2022-04-20 06:57:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies
on a pre-trained general NMT model which is adapted to the new domain on a
sample of in-domain parallel data. Without parallel data, there is no way to
estimate the potential benefit of DA, nor the amount of parallel samples it
would require. It is however a desirable functionality that could help MT
practitioners to make an informed decision before investing resources in
dataset creation. We propose a Domain adaptation Learning Curve prediction
(DaLC) model that predicts prospective DA performance based on in-domain
monolingual samples in the source language. Our model relies on the NMT encoder
representations combined with various instance and corpus-level features. We
demonstrate that instance-level is better able to distinguish between different
domains compared to corpus-level frameworks proposed in previous studies.
Finally, we perform in-depth analyses of the results highlighting the
limitations of our approach, and provide directions for future research. </font><br> Link: <a href='http://arxiv.org/pdf/2204.09259v1' target="_blank">http://arxiv.org/pdf/2204.09259v1</a><br> <br> <br> <font size='5'> 806 </font> <div style="text-align: right"> 2022-04-19 20:40:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mobility Analysis Workflow (MAW): An accessible, interoperable, and reproducible container system for processing raw mobile data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Mobility analysis, or understanding and modeling of people's mobility
patterns in terms of when, where, and how people move from one place to
another, is fundamentally important as such information is the basis for
large-scale investment decisions on the nation's multi-modal transportation
infrastructure. Recent rise of using passively generated mobile data from
mobile devices have raised questions on using such data for capturing the
mobility patterns of a population because: 1) there is a great variety of
different kinds of mobile data and their respective properties are unknown; and
2) data pre-processing and analysis methods are often not explicitly reported.
The high stakes involved with mobility analysis and issues associated with the
passively generated mobile data call for mobility analysis (including data,
methods and results) to be accessible to all, interoperable across different
computing systems, reproducible and reusable by others. In this study, a
container system named Mobility Analysis Workflow (MAW) that integrates data,
methods and results, is developed. Built upon the containerization technology,
MAW allows its users to easily create, configure, modify, execute and share
their methods and results in the form of Docker containers. Tools for
operationalizing MAW are also developed and made publicly available on GitHub.
One use case of MAW is the comparative analysis for the impacts of different
pre-processing and mobility analysis methods on inferred mobility patterns.
This study finds that different pre-processing and analysis methods do have
impacts on the resulting mobility patterns. The creation of MAW and a better
understanding of the relationship between data, methods and resulting mobility
patterns as facilitated by MAW represent an important first step toward
promoting reproducibility and reusability in mobility analysis with
passively-generated data. </font><br> Link: <a href='http://arxiv.org/pdf/2204.09125v1' target="_blank">http://arxiv.org/pdf/2204.09125v1</a><br> <br> <br> <font size='5'> 807 </font> <div style="text-align: right"> 2022-04-18 14:36:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Research on Domain Information Mining and Theme Evolution of Scientific Papers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, with the increase of social investment in scientific
research, the number of research results in various fields has increased
significantly. Cross-disciplinary research results have gradually become an
emerging frontier research direction. There is a certain dependence between a
large number of research results. It is difficult to effectively analyze
today's scientific research results when looking at a single research field in
isolation. How to effectively use the huge number of scientific papers to help
researchers becomes a challenge. This paper introduces the research status at
home and abroad in terms of domain information mining and topic evolution law
of scientific and technological papers from three aspects: the semantic feature
representation learning of scientific and technological papers, the field
information mining of scientific and technological papers, and the mining and
prediction of research topic evolution rules of scientific and technological
papers. </font><br> Link: <a href='http://arxiv.org/pdf/2204.08476v1' target="_blank">http://arxiv.org/pdf/2204.08476v1</a><br> <br> <br> <font size='5'> 808 </font> <div style="text-align: right"> 2022-04-18 06:50:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Securing Signal-free Intersections against Strategic Jamming Attacks: A Macroscopic Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the security-by-design of a signal-free intersection for
connected and autonomous vehicles in the face of strategic jamming attacks. We
use a fluid model to characterize macroscopic traffic flow through the
intersection, where the saturation rate is derived from a vehicle coordination
algorithm. We model jamming attacks as sudden increase in communication latency
induced on vehicle-to-infrastructure connectivity; such latency triggers the
safety mode for vehicle coordination and thus reduces the intersection
saturation rate. A strategic attacker selects the attacking rate, while a
system operator selects key design parameters, either the saturation rate or
the recovery rate. Both players' actions induce technological costs and jointly
determine the mean travel delay. By analyzing the equilibrium of the security
game, we study the preferable level of investment in the intersection's nominal
discharging capability or recovery capability. </font><br> Link: <a href='http://arxiv.org/pdf/2204.08187v2' target="_blank">http://arxiv.org/pdf/2204.08187v2</a><br> <br> <br> <font size='5'> 809 </font> <div style="text-align: right"> 2022-04-14 13:48:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: JUE Insight: The (Non-)Effect of Opportunity Zones on Housing Prices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Will the Opportunity Zones (OZ) program, America's largest new place-based
policy in decades, generate neighborhood change? We compare single-family
housing price growth in OZs with price growth in areas that were eligible but
not included in the program. We also compare OZs to their nearest geographic
neighbors. Our most credible estimates rule out price impacts greater than 0.5
percentage points with 95% confidence, suggesting that, so far, home buyers
don't believe that this subsidy will generate major neighborhood change. OZ
status reduces prices in areas with little employment, perhaps because buyers
think that subsidizing new investment will increase housing supply. Mixed
evidence suggests that OZs may have increased residential permitting. </font><br> Link: <a href='http://arxiv.org/pdf/2204.06967v1' target="_blank">http://arxiv.org/pdf/2204.06967v1</a><br> <br> <br> <font size='5'> 810 </font> <div style="text-align: right"> 2022-04-14 12:18:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many important decisions in daily life are made with the help of advisors,
e.g., decisions about medical treatments or financial investments. Whereas in
the past, advice has often been received from human experts, friends, or
family, advisors based on artificial intelligence (AI) have become more and
more present nowadays. Typically, the advice generated by AI is judged by a
human and either deemed reliable or rejected. However, recent work has shown
that AI advice is not always beneficial, as humans have shown to be unable to
ignore incorrect AI advice, essentially representing an over-reliance on AI.
Therefore, the aspired goal should be to enable humans not to rely on AI advice
blindly but rather to distinguish its quality and act upon it to make better
decisions. Specifically, that means that humans should rely on the AI in the
presence of correct advice and self-rely when confronted with incorrect advice,
i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis.
Current research lacks a metric for AR. This prevents a rigorous evaluation of
factors impacting AR and hinders further development of human-AI
decision-making. Therefore, based on the literature, we derive a measurement
concept of AR. We propose to view AR as a two-dimensional construct that
measures the ability to discriminate advice quality and behave accordingly. In
this article, we derive the measurement concept, illustrate its application and
outline potential future research. </font><br> Link: <a href='http://arxiv.org/pdf/2204.06916v1' target="_blank">http://arxiv.org/pdf/2204.06916v1</a><br> <br> <br> <font size='5'> 811 </font> <div style="text-align: right"> 2022-04-13 15:57:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Where Do You Want To Invest? Predicting Startup Funding From Freely, Publicly Available Web Information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider in this paper the problem of predicting the ability of a startup
to attract investments using freely, publicly available data. Information about
startups on the web usually comes either as unstructured data from news, social
networks, and websites or as structured data from commercial databases, such as
Crunchbase. The possibility of predicting the success of a startup from
structured databases has been studied in the literature and it has been shown
that initial public offerings (IPOs), mergers and acquisitions (M\&A) as well
as funding events can be predicted with various machine learning techniques. In
such studies, heterogeneous information from the web and social networks is
usually used as a complement to the information coming from databases. However,
building and maintaining such databases demands tremendous human effort. We
thus study here whether one can solely rely on readily available sources of
information, such as the website of a startup, its social media activity as
well as its presence on the web, to predict its funding events. As illustrated
in our experiments, the method we propose yields results comparable to the ones
making also use of structured data available in private databases. </font><br> Link: <a href='http://arxiv.org/pdf/2204.06479v1' target="_blank">http://arxiv.org/pdf/2204.06479v1</a><br> <br> <br> <font size='5'> 812 </font> <div style="text-align: right"> 2022-04-12 18:43:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Cybersecurity Investments Using SIS Model: Weakly Connected Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the problem of minimizing the (time) average security costs in large
systems comprising many interdependent subsystems, where the state evolution is
captured by a susceptible-infected-susceptible (SIS) model. The security costs
reflect security investments, economic losses and recovery costs from
infections and failures following successful attacks. However, unlike in
existing studies, we assume that the underlying dependence graph is only weakly
connected, but not strongly connected. When the dependence graph is not
strongly connected, existing approaches to computing optimal security
investments cannot be applied. Instead, we show that it is still possible to
find a good solution by perturbing the problem and establishing necessary
continuity results that then allow us to leverage the existing algorithms. </font><br> Link: <a href='http://arxiv.org/pdf/2204.06035v1' target="_blank">http://arxiv.org/pdf/2204.06035v1</a><br> <br> <br> <font size='5'> 813 </font> <div style="text-align: right"> 2022-04-12 13:31:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A deep learning method for solving stochastic optimal control problems driven by fully-coupled FBSDEs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we mainly focus on the numerical solution of high-dimensional
stochastic optimal control problem driven by fully-coupled forward-backward
stochastic differential equations (FBSDEs in short) through deep learning. We
first transform the problem into a stochastic Stackelberg differential
game(leader-follower problem), then a cross-optimization method (CO method) is
developed where the leader's cost functional and the follower's cost functional
are optimized alternatively via deep neural networks. As for the numerical
results, we compute two examples of the investment-consumption problem solved
through stochastic recursive utility models, and the results of both examples
demonstrate the effectiveness of our proposed algorithm. </font><br> Link: <a href='http://arxiv.org/pdf/2204.05796v1' target="_blank">http://arxiv.org/pdf/2204.05796v1</a><br> <br> <br> <font size='5'> 814 </font> <div style="text-align: right"> 2022-04-12 10:59:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal droop control placement in distribution network via an exact OPF relaxation method</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the last decade, the integration of Renewable Energy Sources (RES) in
distribution networks has been constantly increasing due to their many
technical, economical, and environmental benefits. However, the large-scale
penetration of RESs is limited by the grid security constraints, e.g., voltage
and current limits. The control of inverter-interfaced RESs can guarantee to
comply with those constraints while preserving the RESs performance. However,
the installation of additional controllable converter units introduces
supplementary investment costs and has therefore to be limited. In this paper,
a Mixed-Integer Second-Order Cone (MISOCP) optimization problem is developed to
optimally place the Q-V and P-V droop controllers for the RES converters. The
objectives of the optimization problem are to minimize investment costs,
maintenance costs, and the cost of energy purchase from the grid subject to the
system constraints. To provide an accurate and convex formulation of these
constraints, we adapt an augmented relaxation method, recently proposed to
address the optimal power flow problem in radial distribution networks. Our
method is tested on a standard IEEE 34-bus network and the results are compared
to those provided by alternative approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2204.05704v1' target="_blank">http://arxiv.org/pdf/2204.05704v1</a><br> <br> <br> <font size='5'> 815 </font> <div style="text-align: right"> 2022-04-11 15:40:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The short-term effect of COVID-19 pandemic on China's crude oil futures market: A study based on multifractal analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ongoing COVID-19 shocked financial markets globally, including China's
crude oil future market, which is the third most traded crude oil futures after
WTI and Brent. As China's first crude oil futures accessible to foreign
investors, the Shanghai crude oil futures (SC) have attracted significant
interest since launch at the Shanghai International Energy Exchange. The impact
of COVID-19 on the new crude oil futures is an important issue for investors
and policy makers. Therefore this paper studies the short-term influence of
COVID-19 pandemic on SC via multifractal analysis. We compare market efficiency
of SC before and during the pandemic with the multifractal detrended
fluctuation analysis and other commonly-used random walk tests. Then we
generate shuffled and surrogate data to investigate the components of
multifractal nature in SC. And we examine cross-correlations between SC returns
and other financial assets returns as well as SC trading volume changes by the
multifractal detrended cross-correlation analysis. The results show that market
efficiency of SC and its cross-correlations with other assets increase
significantly after the outbreak of COVID-19. Besides that, the sources of its
multifractal nature have changed since the pandemic. The findings provide
evidence for the short-term impacts of COVID-19 on SC. The results may have
important implications for assets allocation, investment strategies and risk
monitoring. </font><br> Link: <a href='http://arxiv.org/pdf/2204.05199v1' target="_blank">http://arxiv.org/pdf/2204.05199v1</a><br> <br> <br> <font size='5'> 816 </font> <div style="text-align: right"> 2022-04-11 14:45:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Gaining Insights into Unrecognized User Utterances in Task-Oriented Dialog Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapidly growing market demand for automatic dialogue agents capable of
goal-oriented behavior has caused many tech-industry leaders to invest
considerable efforts into task-oriented dialog systems. The success of these
systems is highly dependent on the accuracy of their intent identification --
the process of deducing the goal or meaning of the user's request and mapping
it to one of the known intents for further processing. Gaining insights into
unrecognized utterances -- user requests the systems fail to attribute to a
known intent -- is therefore a key process in continuous improvement of
goal-oriented dialog systems.
  We present an end-to-end pipeline for processing unrecognized user
utterances, deployed in a real-world, commercial task-oriented dialog system,
including a specifically-tailored clustering algorithm, a novel approach to
cluster representative extraction, and cluster naming. We evaluated the
proposed components, demonstrating their benefits in the analysis of
unrecognized user requests. </font><br> Link: <a href='http://arxiv.org/pdf/2204.05158v2' target="_blank">http://arxiv.org/pdf/2204.05158v2</a><br> <br> <br> <font size='5'> 817 </font> <div style="text-align: right"> 2022-04-10 22:04:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Measuring the False Sense of Security</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, several papers have demonstrated how widespread gradient masking is
amongst proposed adversarial defenses. Defenses that rely on this phenomenon
are considered failed, and can easily be broken. Despite this, there has been
little investigation into ways of measuring the phenomenon of gradient masking
and enabling comparisons of its extent amongst different networks. In this
work, we investigate gradient masking under the lens of its mensurability,
departing from the idea that it is a binary phenomenon. We propose and motivate
several metrics for it, performing extensive empirical tests on defenses
suspected of exhibiting different degrees of gradient masking. These are
computationally cheaper than strong attacks, enable comparisons between models,
and do not require the large time investment of tailor-made attacks for
specific models. Our results reveal metrics that are successful in measuring
the extent of gradient masking across different networks </font><br> Link: <a href='http://arxiv.org/pdf/2204.04778v1' target="_blank">http://arxiv.org/pdf/2204.04778v1</a><br> <br> <br> <font size='5'> 818 </font> <div style="text-align: right"> 2022-04-10 14:44:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Educational Inequality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This chapter provides new evidence on educational inequality and reviews the
literature on the causes and consequences of unequal education. We document
large achievement gaps between children from different socio-economic
backgrounds, show how patterns of educational inequality vary across countries,
time, and generations, and establish a link between educational inequality and
social mobility. We interpret this evidence from the perspective of economic
models of skill acquisition and investment in human capital. The models account
for different channels underlying unequal education and highlight how
endogenous responses in parents' and children's educational investments
generate a close link between economic inequality and educational inequality.
Given concerns over the extended school closures during the Covid-19 pandemic,
we also summarize early evidence on the impact of the pandemic on children's
education and on possible long-run repercussions for educational inequality. </font><br> Link: <a href='http://arxiv.org/pdf/2204.04701v1' target="_blank">http://arxiv.org/pdf/2204.04701v1</a><br> <br> <br> <font size='5'> 819 </font> <div style="text-align: right"> 2022-04-09 12:02:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Refining time-space traffic diagrams: A multiple linear regression model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A time-space traffic (TS) diagram, which presents traffic states in
time-space cells with color, is an important traffic analysis and visualization
tool. Despite its importance for transportation research and engineering, most
TS diagrams that have already existed or are being produced are too coarse to
exhibit detailed traffic dynamics due to the limitations of existing
information technology and traffic infrastructure investment. To increase the
resolution of a TS diagram and enable it to present ample traffic details, this
paper introduces the TS diagram refinement problem and proposes a multiple
linear regression-based model to solve the problem. Two tests, which attempt to
increase the resolution of a TS diagram 4 and 16 times, are carried out to
evaluate the performance of the proposed model. Data collected at different
times, in different locations and even in different countries are employed to
thoroughly evaluate the accuracy and transferability of the proposed model.
Strict tests with diverse data show that the proposed model, despite its
simplicity, is able to refine a TS diagram with promising accuracy and reliable
transferability. The proposed refinement model will "save" widely existing TS
diagrams from their blurry "faces" and enable TS diagrams to show more traffic
details. </font><br> Link: <a href='http://arxiv.org/pdf/2204.04457v3' target="_blank">http://arxiv.org/pdf/2204.04457v3</a><br> <br> <br> <font size='5'> 820 </font> <div style="text-align: right"> 2022-04-08 09:15:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Psychology of Mineral Wealth: Empirical Evidence from Kazakhstan</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite rapidly-expanding academic and policy interest in the links between
natural resource wealth and development failures (commonly referred to as the
resource curse) little attention has been devoted to the psychology behind the
phenomenon. Rent-seeking and excessive reliance on mineral revenues can be
attributed largely to social psychology. Mineral booms (whether due to the
discovery of mineral reserves or to the drastic rise in commodity prices) start
as positive income shocks that can subsequently evolve into influential and
expectation-changing public and media narratives; these lead consecutively to
unrealistic demands that favor immediate consumption of accrued mineral
revenues and to the postponement of productive investment. To our knowledge,
this paper is the first empirical analysis that tests hypotheses regarding the
psychological underpinnings of resource mismanagement in mineral-rich states.
Our study relies on an extensive personal survey (of 1977 respondents) carried
out in Almaty, Kazakhstan, between May and August 2018. We find empirical
support for a positive link between exposure to news and inflated expectations
regarding mineral availability, as well as evidence that the latter can
generate preferences for excessive consumption, and hence, rent-seeking. </font><br> Link: <a href='http://arxiv.org/pdf/2204.03948v1' target="_blank">http://arxiv.org/pdf/2204.03948v1</a><br> <br> <br> <font size='5'> 821 </font> <div style="text-align: right"> 2022-04-07 15:31:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present a large-scale study of imitating human demonstrations on tasks
that require a virtual robot to search for objects in new environments -- (1)
ObjectGoal Navigation (e.g. 'find & go to a chair') and (2) Pick&Place (e.g.
'find mug, pick mug, find counter, place mug on counter'). First, we develop a
virtual teleoperation data-collection infrastructure -- connecting Habitat
simulator running in a web browser to Amazon Mechanical Turk, allowing remote
users to teleoperate virtual robots, safely and at scale. We collect 80k
demonstrations for ObjectNav and 12k demonstrations for Pick&Place, which is an
order of magnitude larger than existing human demonstration datasets in
simulation or on real robots.
  Second, we attempt to answer the question -- how does large-scale imitation
learning (IL) (which hasn't been hitherto possible) compare to reinforcement
learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no
bells or whistles) using 70k human demonstrations outperforms RL using 240k
agent-gathered trajectories. The IL-trained agent demonstrates efficient
object-search behavior -- it peeks into rooms, checks corners for small
objects, turns in place to get a panoramic view -- none of these are exhibited
as prominently by the RL agent, and to induce these behaviors via RL would
require tedious reward engineering. Finally, accuracy vs. training data size
plots show promising scaling behavior, suggesting that simply collecting more
demonstrations is likely to advance the state of art further. On Pick&Place,
the comparison is starker -- IL agents achieve ${\sim}$18% success on episodes
with new object-receptacle locations when trained with 9.5k human
demonstrations, while RL agents fail to get beyond 0%. Overall, our work
provides compelling evidence for investing in large-scale imitation learning.
  Project page: https://ram81.github.io/projects/habitat-web. </font><br> Link: <a href='http://arxiv.org/pdf/2204.03514v2' target="_blank">http://arxiv.org/pdf/2204.03514v2</a><br> <br> <br> <font size='5'> 822 </font> <div style="text-align: right"> 2022-04-07 11:59:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantum Simulation for High Energy Physics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It is for the first time that Quantum Simulation for High Energy Physics
(HEP) is studied in the U.S. decadal particle-physics community planning, and
in fact until recently, this was not considered a mainstream topic in the
community. This fact speaks of a remarkable rate of growth of this subfield
over the past few years, stimulated by the impressive advancements in Quantum
Information Sciences (QIS) and associated technologies over the past decade,
and the significant investment in this area by the government and private
sectors in the U.S. and other countries. High-energy physicists have quickly
identified problems of importance to our understanding of nature at the most
fundamental level, from tiniest distances to cosmological extents, that are
intractable with classical computers but may benefit from quantum advantage.
They have initiated, and continue to carry out, a vigorous program in theory,
algorithm, and hardware co-design for simulations of relevance to the HEP
mission. This community whitepaper is an attempt to bring this exciting and yet
challenging area of research to the spotlight, and to elaborate on what the
promises, requirements, challenges, and potential solutions are over the next
decade and beyond. </font><br> Link: <a href='http://arxiv.org/pdf/2204.03381v1' target="_blank">http://arxiv.org/pdf/2204.03381v1</a><br> <br> <br> <font size='5'> 823 </font> <div style="text-align: right"> 2022-04-07 03:09:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bayesian vector autoregressive analysis of macroeconomic and transport influences on urban traffic accidents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The macro influencing factors analysis of urban traffic safety is important
to guide the direction of urban development to reduce the frequency of traffic
accidents. In this study, a Bayesian vector autoregressive(BVAR) model was
developed to exploring the impact of six macro-level economic and transport
factors, including population, GDP, private vehicle ownership, bus ownership,
subway rail mileage and road average speed on traffic accidents with the small
sample size transport annual report data in Beijing. The results show that the
BVAR model was suitable for time series analysis of traffic accidents in small
sample situations. In macroeconomic factors, GDP growth was considered to
reduce the number of traffic accidents in the long term, while population
growth had a positive effect on traffic accidents in the short term. With the
respect to macro-transport factors, road average speed and private vehicle
ownership was perceived to increase traffic accidents in long duration, whereas
bus ownership and subway rail mileage had long-term negative effects, with the
greatest positive effect for road average speed and the greatest negative
effect for subway rail mileage. This study suggests that government departments
can reduce the number of traffic accidents by increasing investment in public
transportation infrastructures, limiting private vehicles and road speed. </font><br> Link: <a href='http://arxiv.org/pdf/2204.03177v1' target="_blank">http://arxiv.org/pdf/2204.03177v1</a><br> <br> <br> <font size='5'> 824 </font> <div style="text-align: right"> 2022-04-06 19:39:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: To Participate Or Not To Participate: An Investigation Of Strategic Participation In Standards</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Essential functionality in the ICT (Information and Communication Technology)
space draws from standards such as HTTP (IETF RFC 2616, Bluetooth (IEEE 802.15)
and various telecommunication standards (4G, 5G). They have fuelled rapid
growth of ICT sector in the last decades by ensuring interoperability and
consistency in computing environment. Research shows that firms that backed ICT
standards and participated in standards development, have emerged as industry
innovators. Standards development thus clearly has benefits for participating
companies as well as technology development and innovation in general. However,
significant costs are also associated with development of standards and need to
be better understood to support investment in standardization necessary for
todays ICT environment. We present a conceptual model that considers the
potential for market innovation across a standards lifecycle and efficiency
from standardization work, to build a forward-looking decision model that can
guide an organizations standards development activities. We investigate and
formalize motivations that drive firms to participate in standardization,
specifically, changes in market innovation. Our model can serve as a strategic
decision framework to drive assessments of a firms participation in standards
development. We test our model with a use case on an established access control
approach that was standardized more than two decades ago, Role Based Access
Control (RBAC) using historical data. The investigation of the case study shows
that change in market innovation is a significant indicator of success in
standards development and are viable criteria to model a firms decision to
participate (or not to participate) in a specific area of standardization. </font><br> Link: <a href='http://arxiv.org/pdf/2204.03055v1' target="_blank">http://arxiv.org/pdf/2204.03055v1</a><br> <br> <br> <font size='5'> 825 </font> <div style="text-align: right"> 2022-04-06 14:51:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Community Action on FAIR Data will Fuel a Revolution in Materials Research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Data - arguably the most important product of worldwide materials research
investment - are rarely shared. The small and biased proportion of results
published are buried in plots and text licensed by journals. This situation
wastes resources, hinders innovation, and, in the current era of data-driven
discovery, is no longer tenable. In this comment, we identify opportunities for
synergistic, collaborative, and global actions to assemble large quantities of
FAIR (Findable, Accessible, Interoperable, Reusable) (1) materials data. </font><br> Link: <a href='http://arxiv.org/pdf/2204.02881v2' target="_blank">http://arxiv.org/pdf/2204.02881v2</a><br> <br> <br> <font size='5'> 826 </font> <div style="text-align: right"> 2022-04-06 13:03:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Global Readiness of Language Technology for Healthcare: What would it Take to Combat the Next Pandemic?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The COVID-19 pandemic has brought out both the best and worst of language
technology (LT). On one hand, conversational agents for information
dissemination and basic diagnosis have seen widespread use, and arguably, had
an important role in combating the pandemic. On the other hand, it has also
become clear that such technologies are readily available for a handful of
languages, and the vast majority of the global south is completely bereft of
these benefits. What is the state of LT, especially conversational agents, for
healthcare across the world's languages? And, what would it take to ensure
global readiness of LT before the next pandemic? In this paper, we try to
answer these questions through survey of existing literature and resources, as
well as through a rapid chatbot building exercise for 15 Asian and African
languages with varying amount of resource-availability. The study confirms the
pitiful state of LT even for languages with large speaker bases, such as
Sinhala and Hausa, and identifies the gaps that could help us prioritize
research and investment strategies in LT for healthcare. </font><br> Link: <a href='http://arxiv.org/pdf/2204.02790v1' target="_blank">http://arxiv.org/pdf/2204.02790v1</a><br> <br> <br> <font size='5'> 827 </font> <div style="text-align: right"> 2022-04-06 01:23:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Next-Generation Superconducting RF Technology based on Advanced Thin Film Technologies and Innovative Materials for Accelerator Enhanced Performance and Energy Reach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Superconducting RF is a key technology for future particle accelerators, now
relying on advanced surfaces beyond bulk Nb for a leap in performance and
efficiency. The SRF thin film strategy aims at transforming the current SRF
technology by using highly functional materials, addressing all the necessary
functions. The community is deploying efforts in three research thrusts to
develop next-generation thin-film based cavities. Nb on Cu cavities are
developed to perform as good as or better than bulk Nb at reduced cost and with
better thermal stability. Recent results showing improved accelerating field
and dramatically reduced Q slope show their potential for many applications.
The second research thrust is to develop cavities coated with materials that
can operate at higher temperatures or sustain higher fields. Proof of principle
has been established for the merit of Nb3Sn for SRF application. Research is
now needed to further exploit the material and reach its full potential with
novel deposition techniques. The third line of research is to push SRF
performance beyond the capabilities of the superconductors alone with
multilayered coatings. In parallel, developments are needed to provide quality
substrates, cooling schemes and cryomodule design tailored to thin film
cavities. Recent results in these three research thrusts suggest that SRF thin
film technologies are at the eve of a technological revolution. For them to
mature, active community support and sustained funding are needed to address
fundamental developments supporting material deposition techniques, surface and
RF research, technical challenges associated with scaling and
industrialization. With dedicated and sustained investment, next-generation
thin-film based cavities will become a reality with high performance and
efficiency, facilitating energy sustainable science while enabling higher
luminosity, and higher energy. </font><br> Link: <a href='http://arxiv.org/pdf/2204.02536v1' target="_blank">http://arxiv.org/pdf/2204.02536v1</a><br> <br> <br> <font size='5'> 828 </font> <div style="text-align: right"> 2022-04-05 08:56:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modeling COVID-19 optimal testing strategies in long-term care facilities: An optimization-based approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Long-term care facilities have been widely affected by the COVID-19 pandemic.
Retirement homes are particularly vulnerable due to the higher mortality risk
of infected elderly individuals. Once an outbreak occurs, suppressing the
spread of the virus in retirement homes is challenging because the residents
are in contact with each other, and isolation measures cannot be widely
enforced. Regular testing strategies, on the other hand, have been shown to
effectively prevent outbreaks in retirement homes. However, high frequency
testing may consume substantial staff working time, which results in a
trade-off between the time invested in testing, and the time spent providing
essential care to residents. Thus, developing an optimal testing strategy is
crucial to proactively detect infections while guaranteeing efficient use of
limited staff time in these facilities. Although numerous efforts have been
made to prevent the virus from spreading in long-term care facilities, this is
the first study to develop testing strategies based on formal optimization
methods. This paper proposes two novel optimization models for testing
schedules. The models aim to minimize the risk of infection in retirement
homes, considering the trade-off between the probability of infection and staff
workload. We employ a probabilistic approach in conjunction with the
optimization models, to compute the risk of infection, including contact rates,
incidence status, and the probability of infection of the residents. To solve
the models, we propose an enhanced local search algorithm by leveraging the
symmetry property of the optimal solution. We perform several experiments with
realistically sized instances and show that the proposed approach can derive
optimal testing strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2204.02062v1' target="_blank">http://arxiv.org/pdf/2204.02062v1</a><br> <br> <br> <font size='5'> 829 </font> <div style="text-align: right"> 2022-04-04 21:28:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Data-Driven Framework for Identifying Investment Opportunities in Private Equity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The core activity of a Private Equity (PE) firm is to invest into companies
in order to provide the investors with profit, usually within 4-7 years. To
invest into a company or not is typically done manually by looking at various
performance indicators of the company and then making a decision often based on
instinct. This process is rather unmanageable given the large number of
companies to potentially invest. Moreover, as more data about company
performance indicators becomes available and the number of different indicators
one may want to consider increases, manual crawling and assessment of
investment opportunities becomes inefficient and ultimately impossible. To
address these issues, this paper proposes a framework for automated data-driven
screening of investment opportunities and thus the recommendation of businesses
to invest in. The framework draws on data from several sources to assess the
financial and managerial position of a company, and then uses an explainable
artificial intelligence (XAI) engine to suggest investment recommendations. The
robustness of the model is validated using different AI algorithms, class
imbalance-handling methods, and features extracted from the available data
sources. </font><br> Link: <a href='http://arxiv.org/pdf/2204.01852v1' target="_blank">http://arxiv.org/pdf/2204.01852v1</a><br> <br> <br> <font size='5'> 830 </font> <div style="text-align: right"> 2022-04-01 23:48:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identifying Exoplanets with Machine Learning Methods: A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The discovery of habitable exoplanets has long been a heated topic in
astronomy. Traditional methods for exoplanet identification include the wobble
method, direct imaging, gravitational microlensing, etc., which not only
require a considerable investment of manpower, time, and money, but also are
limited by the performance of astronomical telescopes. In this study, we
proposed the idea of using machine learning methods to identify exoplanets. We
used the Kepler dataset collected by NASA from the Kepler Space Observatory to
conduct supervised learning, which predicts the existence of exoplanet
candidates as a three-categorical classification task, using decision tree,
random forest, na\"ive Bayes, and neural network; we used another NASA dataset
consisted of the confirmed exoplanets data to conduct unsupervised learning,
which divides the confirmed exoplanets into different clusters, using k-means
clustering. As a result, our models achieved accuracies of 99.06%, 92.11%,
88.50%, and 99.79%, respectively, in the supervised learning task and
successfully obtained reasonable clusters in the unsupervised learning task. </font><br> Link: <a href='http://arxiv.org/pdf/2204.00721v1' target="_blank">http://arxiv.org/pdf/2204.00721v1</a><br> <br> <br> <font size='5'> 831 </font> <div style="text-align: right"> 2022-04-01 15:45:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Consumption-investment decisions with endogenous reference point and drawdown constraint</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a consumption-investment decision model where past consumption
peak $h$ plays a crucial role. There are two important consumption levels: the
lowest constrained level and a reference level, at which the risk aversion in
terms of consumption rate is changed. We solve this stochastic control problem
and derive the value function, optimal consumption plan, and optimal investment
strategy in semi-explicit forms. We find five important thresholds of wealth,
all as functions of $h$, and most of them are nonlinear functions. As can be
seen from numerical results and theoretical analysis, this intuitive and simple
model has significant economic implications, and there are at least three
important predictions: the marginal propensity to consume out of wealth is
generally decreasing but can be increasing for intermediate wealth levels, and
it jumps inversely proportional to the risk aversion at the reference point;
the implied relative risk aversion is roughly a smile in wealth; the welfare of
the poor is more vulnerable to wealth shocks than the wealthy. Moreover,
locally changing the risk aversion influences the optimal strategies globally,
revealing some risk allocation behaviors. </font><br> Link: <a href='http://arxiv.org/pdf/2204.00530v2' target="_blank">http://arxiv.org/pdf/2204.00530v2</a><br> <br> <br> <font size='5'> 832 </font> <div style="text-align: right"> 2022-04-01 08:53:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Preventing Distillation-based Attacks on Neural Network IP</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Neural networks (NNs) are already deployed in hardware today, becoming
valuable intellectual property (IP) as many hours are invested in their
training and optimization. Therefore, attackers may be interested in copying,
reverse engineering, or even modifying this IP. The current practices in
hardware obfuscation, including the widely studied logic locking technique, are
insufficient to protect the actual IP of a well-trained NN: its weights. Simply
hiding the weights behind a key-based scheme is inefficient (resource-hungry)
and inadequate (attackers can exploit knowledge distillation). This paper
proposes an intuitive method to poison the predictions that prevent
distillation-based attacks; this is the first work to consider such a poisoning
approach in hardware-implemented NNs. The proposed technique obfuscates a NN so
an attacker cannot train the NN entirely or accurately. We elaborate a threat
model which highlights the difference between random logic obfuscation and the
obfuscation of NN IP. Based on this threat model, our security analysis shows
that the poisoning successfully and significantly reduces the accuracy of the
stolen NN model on various representative datasets. Moreover, the accuracy and
prediction distributions are maintained, no functionality is disturbed, nor are
high overheads incurred. Finally, we highlight that our proposed approach is
flexible and does not require manipulation of the NN toolchain. </font><br> Link: <a href='http://arxiv.org/pdf/2204.00292v1' target="_blank">http://arxiv.org/pdf/2204.00292v1</a><br> <br> <br> <font size='5'> 833 </font> <div style="text-align: right"> 2022-04-01 00:44:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Semi- and weakly-supervised learning have recently attracted considerable
attention in the object detection literature since they can alleviate the cost
of annotation needed to successfully train deep learning models. State-of-art
approaches for semi-supervised learning rely on student-teacher models trained
using a multi-stage process, and considerable data augmentation. Custom
networks have been developed for the weakly-supervised setting, making it
difficult to adapt to different detectors. In this paper, a weakly
semi-supervised training method is introduced that reduces these training
challenges, yet achieves state-of-the-art performance by leveraging only a
small fraction of fully-labeled images with information in weakly-labeled
images. In particular, our generic sampling-based learning strategy produces
pseudo-ground-truth (GT) bounding box annotations in an online fashion,
eliminating the need for multi-stage training, and student-teacher network
configurations. These pseudo GT boxes are sampled from weakly-labeled images
based on the categorical score of object proposals accumulated via a score
propagation process. Empirical results on the Pascal VOC dataset, indicate that
the proposed approach improves performance by 5.0% when using VOC 2007 as
fully-labeled, and VOC 2012 as weak-labeled data. Also, with 5-10% fully
annotated images, we observed an improvement of more than 10% in mAP, showing
that a modest investment in image-level annotation, can substantially improve
detection performance. </font><br> Link: <a href='http://arxiv.org/pdf/2204.00147v2' target="_blank">http://arxiv.org/pdf/2204.00147v2</a><br> <br> <br> <font size='5'> 834 </font> <div style="text-align: right"> 2022-03-31 15:22:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Searching and identifying leptoquarks through low-energy polarized scattering processes $e^-p\to e^-_c$</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the potential for searching and identifying the leptoquark
(LQ) effects in the charm sector through the low-energy polarized scattering
processes $\vec{e}^{\,-}p\to e^-\Lambda_c$, $e^-\vec{p}\to e^-\Lambda_c$, and
$\vec{e}^{\,-}\vec{p}\to e^-\Lambda_c$. Considering only the longitudinally
polarized processes, we show that the different LQ models can be disentangled
from each other by measuring the four spin asymmetries, $A_{L}^e$, $A_{L}^p$,
$A_{L3}^{ep}$, and $A_{L6}^{ep}$, constructed in terms of the polarized cross
sections. Although it is challenging to accomplish the same goal with
transversely polarized processes, we find that investing them in future
experiments is especially beneficial, since they can directly probe into the
imaginary part of the Wilson coefficients in the general low-energy effective
Lagrangian. With our properly designed experimental setups, it is also
demonstrated that promising event rates can be expected for all these processes
and, even in the worst-case scenario -- no LQ signals are observed at all, they
can still provide a competitive potential for constraining the new physics,
compared with those from the conventional charmed-hadron weak decays and the
high-$p_T$ dilepton invariant mass tails at high-energy colliders. </font><br> Link: <a href='http://arxiv.org/pdf/2203.17104v2' target="_blank">http://arxiv.org/pdf/2203.17104v2</a><br> <br> <br> <font size='5'> 835 </font> <div style="text-align: right"> 2022-03-31 09:44:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Desenvolvimento de ferramenta de simulao para auxlio no ensino da disciplina de robtica industrial</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Currently, robotics is one of the fastest growing areas not only in the
industrial sector but also in the consumer and service sectors. Several areas
benefit from the technological advancement of robotics, especially the
industrial area those benefits from gains in productivity and quality. However,
to supply this growing demand it is necessary for the newly graduated
professionals to have a deeper understanding of how to design and control a
robotic manipulator. It is logical that in order to obtain this more in-depth
knowledge of robotics, it is necessary to have an experience with a real
robotic manipulator, since the practice is a much more efficient way of
learning than theory. However, it is known that a robotic arm is not a cheap
investment, and its maintenance is not cheap either. Therefore, many
educational institutions are not able to provide this type of experience to
their students. With this in mind, and through the use of Unity 3D, which is a
game development software, a robotic arm simulator has been developed to
correlate classroom theory with what actually happens in practice. The robotic
manipulators implemented on this simulator can be controlled by both inverse
kinematics (which is the industry standard) and direct kinematics. </font><br> Link: <a href='http://arxiv.org/pdf/2203.16920v1' target="_blank">http://arxiv.org/pdf/2203.16920v1</a><br> <br> <br> <font size='5'> 836 </font> <div style="text-align: right"> 2022-03-31 04:22:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Environmental, Social, and Governance (ESG) are non-financial factors that
are garnering attention from investors as they increasingly look to apply these
as part of their analysis to identify material risks and growth opportunities.
Some of this attention is also driven by clients who, now more aware than ever,
are demanding for their money to be managed and invested responsibly. As the
interest in ESG grows, so does the need for investors to have access to
consumable ESG information. Since most of it is in text form in reports,
disclosures, press releases, and 10-Q filings, we see a need for sophisticated
NLP techniques for classification tasks for ESG text. We hypothesize that an
ESG domain-specific pre-trained model will help with such and study building of
the same in this paper. We explored doing this by fine-tuning BERTs pre-trained
weights using ESG specific text and then further fine-tuning the model for a
classification task. We were able to achieve accuracy better than the original
BERT and baseline models in environment-specific classification tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2203.16788v1' target="_blank">http://arxiv.org/pdf/2203.16788v1</a><br> <br> <br> <font size='5'> 837 </font> <div style="text-align: right"> 2022-03-30 20:39:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hawkes Process Modeling of Block Arrivals in Bitcoin Blockchain</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper constructs a multi-variate Hawkes process model of Bitcoin block
arrivals and price jumps. Hawkes processes are selfexciting point processes
that can capture the self- and cross-excitation effects of block mining and
Bitcoin price volatility. We use publicly available blockchain datasets to
estimate the model parameters via maximum likelihood estimation. The results
show that Bitcoin price volatility boost block mining rate and Bitcoin
investment return demonstrates mean reversion. Quantile-Quantile plots show
that the proposed Hawkes process model is a better fit to the blockchain
datasets than a Poisson process model. </font><br> Link: <a href='http://arxiv.org/pdf/2203.16666v1' target="_blank">http://arxiv.org/pdf/2203.16666v1</a><br> <br> <br> <font size='5'> 838 </font> <div style="text-align: right"> 2022-03-30 12:38:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Research topic trend prediction of scientific papers based on spatial enhancement and dynamic graph convolution network</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, with the increase of social investment in scientific
research, the number of research results in various fields has increased
significantly. Accurately and effectively predicting the trends of future
research topics can help researchers discover future research hotspots.
However, due to the increasingly close correlation between various research
themes, there is a certain dependency relationship between a large number of
research themes. Viewing a single research theme in isolation and using
traditional sequence problem processing methods cannot effectively explore the
spatial dependencies between these research themes. To simultaneously capture
the spatial dependencies and temporal changes between research topics, we
propose a deep neural network-based research topic hotness prediction
algorithm, a spatiotemporal convolutional network model. Our model combines a
graph convolutional neural network (GCN) and Temporal Convolutional Network
(TCN), specifically, GCNs are used to learn the spatial dependencies of
research topics a and use space dependence to strengthen spatial
characteristics. TCN is used to learn the dynamics of research topics' trends.
Optimization is based on the calculation of weighted losses based on time
distance. Compared with the current mainstream sequence prediction models and
similar spatiotemporal models on the paper datasets, experiments show that, in
research topic prediction tasks, our model can effectively capture
spatiotemporal relationships and the predictions outperform state-of-art
baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2203.16256v1' target="_blank">http://arxiv.org/pdf/2203.16256v1</a><br> <br> <br> <font size='5'> 839 </font> <div style="text-align: right"> 2022-03-30 07:33:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal reinsurance design under solvency constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the optimal risk transfer from an insurance company to a
reinsurer. The problem formulation considered in this paper is closely
connected to the optimal portfolio problem in finance, with some crucial
distinctions. In particular, the insurance company's surplus is here (as is
routinely the case) approximated by a Brownian motion, as opposed to the
geometric Brownian motion used to model assets in finance. Furthermore, risk
exposure is dialled "down" via reinsurance, rather than "up" via risky
investments. This leads to interesting qualitative differences in the optimal
designs.
  In this paper, using the martingale method, we derive the optimal design as a
function of proportional, non-cheap reinsurance design that maximises the
quadratic utility of the terminal value of the insurance surplus. We also
consider several realistic constraints on the terminal value: a strict lower
boundary, the probability (Value at Risk) constraint, and the expected
shortfall (conditional Value at Risk) constraints under the $\mathbb{P}$ and
$\mathbb{Q}$ measures, respectively. In all cases, the optimal reinsurance
designs boil down to a combination of proportional protection and option-like
protection (stop-loss) of the residual proportion with various deductibles.
Proportions and deductibles are set such that the initial capital is fully
allocated. Comparison of the optimal designs with the optimal portfolios in
finance is particularly interesting. Results are illustrated. </font><br> Link: <a href='http://arxiv.org/pdf/2203.16108v5' target="_blank">http://arxiv.org/pdf/2203.16108v5</a><br> <br> <br> <font size='5'> 840 </font> <div style="text-align: right"> 2022-03-29 17:52:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Not Another School Resource Map: Meeting Underserved Families' Information Needs Requires Trusting Relationships and Personalized Care</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Public school districts across the United States have implemented school
choice systems that have the potential to improve underserved students' access
to educational opportunities. However, research has shown that learning about
and applying for schools can be extremely time-consuming and expensive, making
it difficult for these systems to create more equitable access to resources in
practice. A common factor surfaced in prior work is unequal access to
information about the schools and enrollment process. In response, governments
and non-profits have invested in providing more information about schools to
parents, for instance, through detailed online dashboards. However, we know
little about what information is actually useful for historically marginalized
and underserved families. We conducted interviews with 10 low-income families
and families of color to learn about the challenges they faced navigating an
online school choice and enrollment system. We complement this data with four
interviews with people who have supported families through the enrollment
process in a wide range of roles, from school principal to non-profit staff
("parent advocates"). Our findings highlight the value of personalized support
and trusting relationships to delivering relevant and helpful information. We
contrast this against online information resources and dashboards, which tend
to be impersonal, target a broad audience, and make strong assumptions about
what parents should look for in a school without sensitivity to families'
varying circumstances. We advocate for an assets-based design approach to
information support in public school enrollment, which would ask how we can
support the local, one-on-one support that community members already provide. </font><br> Link: <a href='http://arxiv.org/pdf/2203.15795v2' target="_blank">http://arxiv.org/pdf/2203.15795v2</a><br> <br> <br> <font size='5'> 841 </font> <div style="text-align: right"> 2022-03-28 19:47:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Discovering material information using hierarchical Reformer model on financial regulatory filings</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Most applications of machine learning for finance are related to forecasting
tasks for investment decisions. Instead, we aim to promote a better
understanding of financial markets with machine learning techniques. Leveraging
the tremendous progress in deep learning models for natural language
processing, we construct a hierarchical Reformer ([15]) model capable of
processing a large document level dataset, SEDAR, from canadian financial
regulatory filings. Using this model, we show that it is possible to predict
trade volume changes using regulatory filings. We adapt the pretraining task of
HiBERT ([36]) to obtain good sentence level representations using a large
unlabelled document dataset. Finetuning the model to successfully predict trade
volume changes indicates that the model captures a view from financial markets
and processing regulatory filings is beneficial. Analyzing the attention
patterns of our model reveals that it is able to detect some indications of
material information without explicit training, which is highly relevant for
investors and also for the market surveillance mandate of financial regulators. </font><br> Link: <a href='http://arxiv.org/pdf/2204.05979v1' target="_blank">http://arxiv.org/pdf/2204.05979v1</a><br> <br> <br> <font size='5'> 842 </font> <div style="text-align: right"> 2022-03-27 17:37:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving The Diagnosis of Thyroid Cancer by Machine Learning and Clinical Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Thyroid cancer is a common endocrine carcinoma that occurs in the thyroid
gland. Much effort has been invested in improving its diagnosis, and
thyroidectomy remains the primary treatment method. A successful operation
without unnecessary side injuries relies on an accurate preoperative diagnosis.
Current human assessment of thyroid nodule malignancy is prone to errors and
may not guarantee an accurate preoperative diagnosis. This study proposed a
machine framework to predict thyroid nodule malignancy based on a novel
clinical dataset we collected. The 10-fold cross-validation, bootstrap
analysis, and permutation predictor importance were applied to estimate and
interpret the model performance under uncertainty. The comparison between model
prediction and expert assessment shows the advantage of our framework over
human judgment in predicting thyroid nodule malignancy. Our method is accurate,
interpretable, and thus useable as additional evidence in the preoperative
diagnosis for thyroid cancer. </font><br> Link: <a href='http://arxiv.org/pdf/2203.15804v1' target="_blank">http://arxiv.org/pdf/2203.15804v1</a><br> <br> <br> <font size='5'> 843 </font> <div style="text-align: right"> 2022-03-27 13:54:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Budget-Constrained Reinforcement of Ranked Objects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Commercial entries, such as hotels, are ranked according to score by a search
engine or recommendation system, and the score of each can be improved upon by
making a targeted investment, e.g., advertising. We study the problem of how a
principal, who owns or supports a set of entries, can optimally allocate a
budget to maximize their ranking. Representing the set of ranked scores as a
probability distribution over scores, we treat this question as a game between
distributions.
  We show that, in the general case, the best ranking is achieved by equalizing
the scores of several disjoint score ranges. We show that there is a unique
optimal reinforcement strategy, and provide an efficient algorithm implementing
it. </font><br> Link: <a href='http://arxiv.org/pdf/2203.14305v1' target="_blank">http://arxiv.org/pdf/2203.14305v1</a><br> <br> <br> <font size='5'> 844 </font> <div style="text-align: right"> 2022-03-26 21:31:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Novel Neuromorphic Processors Realization of Spiking Deep Reinforcement Learning for Portfolio Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The process of continuously reallocating funds into financial assets, aiming
to increase the expected return of investment and minimizing the risk, is known
as portfolio management. Processing speed and energy consumption of portfolio
management have become crucial as the complexity of their real-world
applications increasingly involves high-dimensional observation and action
spaces and environment uncertainty, which their limited onboard resources
cannot offset. Emerging neuromorphic chips inspired by the human brain increase
processing speed by up to 1000 times and reduce power consumption by several
orders of magnitude. This paper proposes a spiking deep reinforcement learning
(SDRL) algorithm that can predict financial markets based on unpredictable
environments and achieve the defined portfolio management goal of profitability
and risk reduction. This algorithm is optimized forIntel's Loihi neuromorphic
processor and provides 186x and 516x energy consumption reduction is observed
compared to the competitors, respectively. In addition, a 1.3x and 2.0x
speed-up over the high-end processors and GPUs, respectively. The evaluations
are performed on cryptocurrency market between 2016 and 2021 the benchmark. </font><br> Link: <a href='http://arxiv.org/pdf/2203.14159v1' target="_blank">http://arxiv.org/pdf/2203.14159v1</a><br> <br> <br> <font size='5'> 845 </font> <div style="text-align: right"> 2022-03-25 20:54:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Risk-Driven Probabilistic Approach to Quantify Resilience in Power Distribution Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: It is of growing concern to ensure resilience in power distribution systems
to extreme weather events. However, there are no clear methodologies or metrics
available for resilience assessment that allows system planners to assess the
impact of appropriate planning measures and new operational procedures for
resilience enhancement. In this paper, we propose a resilience metric using
parameters that define system attributes and performance. To represent extreme
events (tail probability), the conditional value-at-risk of each of the
parameters are combined using Choquet Integral to evaluate the overall
resilience. The effectiveness of the proposed resilience metric is studied
within the simulation-based framework under extreme weather scenarios with the
help of a modified IEEE 123-bus system. With the proposed framework, system
operators will have additional flexibility to prioritize one investment over
the others to enhance the resilience of the grid. </font><br> Link: <a href='http://arxiv.org/pdf/2203.13907v1' target="_blank">http://arxiv.org/pdf/2203.13907v1</a><br> <br> <br> <font size='5'> 846 </font> <div style="text-align: right"> 2022-03-25 16:47:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Straightening skewed markets with an index tracking optimizationless portfolio</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Among professionals and academics alike, it is well known that active
portfolio management is unable to provide additional risk-adjusted returns
relative to their benchmarks. For this reason, passive wealth management has
emerged in recent decades to offer returns close to benchmarks at a lower cost.
In this article, we first refine the existing results on the theoretical
properties of oblique Brownian motion. Then, assuming that the returns follow
skew geometric Brownian motions and that they are correlated, we describe some
statistical properties for the \emph{ex-post}, the \emph{ex-ante} tracking
errors, and the forecasted tracking portfolio. To this end, we develop an
innovative statistical methodology, based on a benchmark-asset principal
component factorization, to determine a tracking portfolio that replicates the
performance of a benchmark by investing in a subset of the investable universe.
This strategy, named hybrid Principal Component Analysis (hPCA), is applied
both on normal and skew distributions. In the case of skew-normal returns, we
propose a framework for calibrating the model parameters, based on the maximum
likelihood estimation method. For testing and validation, we compare four
alternative models for index tracking. The first two are based on the hPCA when
returns are assumed to be normal or skew-normal. The third model adopts a
standard optimization-based approach and the last one is used in the financial
sector by some practitioners. For validation and testing, we present a thorough
comparison of these strategies on real-world data, both in terms of performance
and computational efficiency. A noticeable result is that, not only, the
suggested lean PCA-based portfolio selection approach compares well versus
cumbersome algorithms for optimization-based portfolios, but, also, it could
provide a better service to the asset management industry. </font><br> Link: <a href='http://arxiv.org/pdf/2203.13766v1' target="_blank">http://arxiv.org/pdf/2203.13766v1</a><br> <br> <br> <font size='5'> 847 </font> <div style="text-align: right"> 2022-03-24 15:39:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Intelligent Systematic Investment Agent: an ensemble of deep learning and evolutionary strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine learning driven trading strategies have garnered a lot of interest
over the past few years. There is, however, limited consensus on the ideal
approach for the development of such trading strategies. Further, most
literature has focused on trading strategies for short-term trading, with
little or no focus on strategies that attempt to build long-term wealth. Our
paper proposes a new approach for developing long-term investment strategies
using an ensemble of evolutionary algorithms and a deep learning model by
taking a series of short-term purchase decisions. Our methodology focuses on
building long-term wealth by improving systematic investment planning (SIP)
decisions on Exchange Traded Funds (ETF) over a period of time. We provide
empirical evidence of superior performance (around 1% higher returns) using our
ensemble approach as compared to the traditional daily systematic investment
practice on a given ETF. Our results are based on live trading decisions made
by our algorithm and executed on the Robinhood trading platform. </font><br> Link: <a href='http://arxiv.org/pdf/2203.13125v1' target="_blank">http://arxiv.org/pdf/2203.13125v1</a><br> <br> <br> <font size='5'> 848 </font> <div style="text-align: right"> 2022-03-23 17:58:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Submission to the Commission on Taxation and Welfare on introducing a site value tax</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This submission to the Irish Commission on Taxation and Welfare advocates the
introduction of a site value tax in Ireland. Ireland has high and volatile
property prices, constraining social and economic development. Site values are
the main driver of these phenomena. Taxing site values would reduce both the
level and volatility of property prices, and thus help to alleviate these
problems. Site value tax has many other beneficial features. For example, it
captures price gains due to the community and government rather than owners'
efforts and thus diminishes the incentive to buy land for speculative reasons.
Site value tax can be used to finance infrastructural investments, help
facilitate site assembly for development and as a support for the maintenance
of protected structures. Site value tax is also a tax on wealth. </font><br> Link: <a href='http://arxiv.org/pdf/2203.12611v1' target="_blank">http://arxiv.org/pdf/2203.12611v1</a><br> <br> <br> <font size='5'> 849 </font> <div style="text-align: right"> 2022-03-23 14:28:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Plasma Processing for In-Situ Field Emission Mitigation of Superconducting Radiofrequency (SRF) Cryomodules</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Field emission (FE) is one of the main limiting factors of superconducting
radio-frequency (SRF) cavities operating in accelerators and it occurs whenever
contaminants, like dust, metal flakes or even absorbates, are present on the
surface of the cavity high electric field region. Field emission reduces the
maximum achievable accelerating field and generates free electrons that may
interact with the beam, damage or activate the beamline. One practical method
that can be used to mitigate this problem is in-situ plasma cleaning, or plasma
processing. The development of a processing that can be applied in-situ is
extremely advantageous, since it enables the recovery of the cryomodule
performance without the need of disassembling the whole cryomodule, which is an
extremely expensive and time-consuming process. On the other hand, plasma
processing only requires the cryomodule warm-up to room-temperature and the
subsequent processing of the contaminated cavities. The entire process is
reasonably quick and involves a limited number of personnel. For these reasons
we would like to advocate for continuing to invest in the R\&D of plasma
processing to optimize its applicability in cryomodules and for extending the
technique to other frequency ranges and cavities geometries. </font><br> Link: <a href='http://arxiv.org/pdf/2203.12442v1' target="_blank">http://arxiv.org/pdf/2203.12442v1</a><br> <br> <br> <font size='5'> 850 </font> <div style="text-align: right"> 2022-03-23 13:34:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exabel's Factor Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Factor models have become a common and valued tool for understanding the
risks associated with an investing strategy. In this report we describe
Exabel's factor model, we quantify the fraction of the variability of the
returns explained by the different factors, and we show some examples of annual
returns of portfolios with different factor exposure. </font><br> Link: <a href='http://arxiv.org/pdf/2203.12408v1' target="_blank">http://arxiv.org/pdf/2203.12408v1</a><br> <br> <br> <font size='5'> 851 </font> <div style="text-align: right"> 2022-03-22 14:43:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Modeling and Simulation of Portfolio Allocation Schemes: an Approach based on Network Community Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present a study on portfolio investments in financial applications. We
describe a general modeling and simulation framework and study the impact on
the use of different metrics to measure the correlation among assets. In
particular, besides the traditional Pearson's correlation, we employ the
Detrended Cross-Correlation Analysis (DCCA) and Detrended Partial
Cross-Correlation Analysis (DPCCA). Moreover, a novel portfolio allocation
scheme is introduced that treats assets as a complex network and uses
modularity to detect communities of correlated assets. Weights of the
allocation are then distributed among different communities for the sake of
diversification. Simulations compare this novel scheme against Critical Line
Algorithm (CLA), Inverse Variance Portfolio (IVP), the Hierarchical Risk Parity
(HRP). Synthetic times series are generated using the Gaussian model, Geometric
Brownian motion, GARCH, ARFIMA and modified ARFIMA models. Results show that
the proposed scheme outperforms state of the art approaches in many scenarios.
We also validate simulation results via backtesting, whose results confirm the
viability of the proposal. </font><br> Link: <a href='http://arxiv.org/pdf/2203.11780v1' target="_blank">http://arxiv.org/pdf/2203.11780v1</a><br> <br> <br> <font size='5'> 852 </font> <div style="text-align: right"> 2022-03-22 14:32:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sustainable Development Goal Target Interactions in the Philippines: A Two-Method Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In 2015, the United Nations adopted 17 Sustainable Development Goals (SDGs)
with 169 targets for transformation toward a more sustainable future by 2030.
This study seeks to evaluate and analyze SDG target interactions in the
Philippines to resolve conflicting targets, and prioritize targets that
reinforce others and have no conflicts. To evaluate all 14196 target
interactions, two methods are employed. First, experts with over five years of
SDG-related experience evaluated interactions using a 7-point scale. Second, a
non-parametric Spearman rank correlation is used on official indicator data
with resulting coefficients serving as interaction scores. Interaction scores
are then coded as synergies (interact positively), trade-offs (negatively) or
non-classified (neutrally). Targets are also modelled as nodes and interactions
as edges in graphs presented in sdginteractions.herokuapp.com. Results from the
two methods were synthesized to formulate recommendations for concerned
parties. This includes resolving negative intra-goal target interactions
involving targets 3.1 'Reduce maternal mortality', 3.6 'Reduce road injuries
and deaths', and 3.7 'Universal access to sexual and reproductive care, family
planning, and education'. Ugly targets (at least one negative interaction)
including target 3.6, 3.7, and 8.2 'Diversify, innovate, and upgrade for
economic productivity' need to be resolved. Targets that reinforce their
corresponding SDGs should be prioritized, including 1.1 'Eradicate extreme
poverty', 4.2 'Equal access to quality pre-primary education', 6.2 'End open
defecation and provide access to sanitation and hygiene', 8.1 'Sustainable
economic growth', and 9.4 'Upgrade all industries and infrastructures for
sustainability'. Beautiful targets (no negative interactions) should also be
prioritized, including target 8.5 and 17.5 'Invest in least developed
countries'. </font><br> Link: <a href='http://arxiv.org/pdf/2203.11768v2' target="_blank">http://arxiv.org/pdf/2203.11768v2</a><br> <br> <br> <font size='5'> 853 </font> <div style="text-align: right"> 2022-03-22 09:00:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Explainability in reinforcement learning: perspective and position</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence (AI) has been embedded into many aspects of people's
daily lives and it has become normal for people to have AI make decisions for
them. Reinforcement learning (RL) models increase the space of solvable
problems with respect to other machine learning paradigms. Some of the most
interesting applications are in situations with non-differentiable expected
reward function, operating in unknown or underdefined environment, as well as
for algorithmic discovery that surpasses performance of any teacher, whereby
agent learns from experimental experience through simple feedback. The range of
applications and their social impact is vast, just to name a few: genomics,
game-playing (chess, Go, etc.), general optimization, financial investment,
governmental policies, self-driving cars, recommendation systems, etc. It is
therefore essential to improve the trust and transparency of RL-based systems
through explanations. Most articles dealing with explainability in artificial
intelligence provide methods that concern supervised learning and there are
very few articles dealing with this in the area of RL. The reasons for this are
the credit assignment problem, delayed rewards, and the inability to assume
that data is independently and identically distributed (i.i.d.). This position
paper attempts to give a systematic overview of existing methods in the
explainable RL area and propose a novel unified taxonomy, building and
expanding on the existing ones. The position section describes pragmatic
aspects of how explainability can be observed. The gap between the parties
receiving and generating the explanation is especially emphasized. To reduce
the gap and achieve honesty and truthfulness of explanations, we set up three
pillars: proactivity, risk attitudes, and epistemological constraints. To this
end, we illustrate our proposal on simple variants of the shortest path
problem. </font><br> Link: <a href='http://arxiv.org/pdf/2203.11547v1' target="_blank">http://arxiv.org/pdf/2203.11547v1</a><br> <br> <br> <font size='5'> 854 </font> <div style="text-align: right"> 2022-03-21 08:32:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Stochastic Planning Method for Low-carbon Building-level Integrated Energy System Considering Electric-Heat-V2G Coupling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The concept of low-carbon building is proposed to ameliorate the climate
change caused by environmental problems and realize carbon neutrality at the
building level in urban areas. In addition, renewable energy curtailment in the
power distribution system, as well as low efficiency due to independent
operation of traditional energy systems, has been addressed by the application
of integrated energy system (IES) to some extent. In this paper, we propose a
planning method for low-carbon building-level IES, in which electric vehicles
(EV) and the mode of Vehicle to Grid (V2G) are considered and further increase
the flexibility of low-carbon buildings. The proposed planning model optimize
the investment, operation costs and CO2 emission for building-level IES, so as
to achieve the maximum benefit of the construction of the low-carbon building
and help the realization of carbon neutrality. Moreover, we consider the
uncertainty of distributed renewable energy, multi-energy load fluctuation and
the random behavior of EV users, then formulating a two-stage stochastic
programming model with chance constraints, in which heuristic moment matching
scenario generation (HMMSG) and sample average approximation (SAA) method are
applied. In case study, a real IES commercial building in Shanghai, where
photovoltaic (PV), energy storage system (ESS), fuel cell (FC), EV, etc. are
included as planning options, is used as numerical example to verify the
effectiveness of the proposed planning method, with functions of ESS and EV in
IES are analyzed in detail in different operation scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2203.10799v1' target="_blank">http://arxiv.org/pdf/2203.10799v1</a><br> <br> <br> <font size='5'> 855 </font> <div style="text-align: right"> 2022-03-20 20:44:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Equitable Continuous Organizations with Self-Assessed Valuations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Organizations are often unable to align the interests of all stakeholders
with the financial success of the organization (e.g. due to regulation).
However, continuous organizations (COs) introduce a paradigm shift. COs offer
immediate liquidity, are permission-less and can align incentives. CO shares
are issued continuously in the form of tokens via a smart contract on a
blockchain. Token prices are designed to increase as more tokens are minted.
When the share supply is low, near-zero prices make it advantageous to buy and
hold tokens until interest in the CO increases, enabling a profitable sale.
This attribute of COs, known as investment efficiency, is desirable. Yet, it
can yield allocative inefficiency via the "holdout problem," i.e. latecomers
may find a CO more valuable than early tokenholders, but be unable to attain
the same token holdings due to inflated prices. With the aim of increasing
overall equity, we introduce a voting mechanism into COs. We show this balances
allocative and investment efficiency, and may dissuade speculative trading
behaviors, thereby decreasing investment risk. </font><br> Link: <a href='http://arxiv.org/pdf/2203.10644v1' target="_blank">http://arxiv.org/pdf/2203.10644v1</a><br> <br> <br> <font size='5'> 856 </font> <div style="text-align: right"> 2022-03-18 22:02:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimizing Transmission Infrastructure Investments to Support Line De-energization for Mitigating Wildfire Ignition Risk</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Wildfires pose a growing risk to public safety in regions like the western
United States, and, historically, electric power systems have ignited some of
the most destructive wildfires. To reduce wildfire ignition risks, power system
operators preemptively de-energize high-risk power lines during extreme
wildfire conditions as part of "Public Safety Power Shutoff" (PSPS) events.
While capable of substantially reducing acute wildfire risks, PSPS events can
also result in significant amounts of load shedding as the partially
de-energized system may not be able to supply all customer demands. In this
work, we investigate the extent to which infrastructure investments can support
system operations during PSPS events by enabling reduced load shedding and
wildfire ignition risk. We consider the installation of grid-scale batteries,
solar PV, and line hardening or maintenance measures (e.g., undergrounding or
increased vegetation management). Optimally selecting the locations, types, and
sizes of these infrastructure investments requires considering the line
de-energizations associated with PSPS events. Accordingly, this paper proposes
a multi-period optimization formulation that locates and sizes infrastructure
investments while simultaneously choosing line de-energizations to minimize
wildfire ignition risk and load shedding. This formulation is evaluated using
two geolocated test cases along with realistic infrastructure investment
parameters and actual wildfire risk data from the US Geological Survey. We
evaluate the performance of investment choices by simulating de-energization
decisions for the entire 2021 wildfire season with optimized infrastructure
placements. With investment decisions varying significantly for different test
cases, budgets, and operator priorities, the numerical results demonstrate the
proposed formulation's value in tailoring investment choices to different
settings. </font><br> Link: <a href='http://arxiv.org/pdf/2203.10176v2' target="_blank">http://arxiv.org/pdf/2203.10176v2</a><br> <br> <br> <font size='5'> 857 </font> <div style="text-align: right"> 2022-03-18 17:08:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why should the U.S. care about high energy physics in Africa and Latin America?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Research, education and training in high energy physics (HEP) often draw
international collaborations even when priorities and long term visions are
defined regionally or nationally. Yet in many developing regions, HEP
activities are limited in both human capacity and expertise, as well as in
resource mobilisation. In this paper, the benefits -- to the U.S. HEP program
-- of engagements with developing countries are identified and studied through
specific examples of Africa and Latin America; conversely, the impact of HEP
education and research for developing countries are also pointed out. In the
context of the U.S. strategic planning for high energy physics, the authors
list recommendations on investments that will benefit both developed and
developing nations. </font><br> Link: <a href='http://arxiv.org/pdf/2203.10060v1' target="_blank">http://arxiv.org/pdf/2203.10060v1</a><br> <br> <br> <font size='5'> 858 </font> <div style="text-align: right"> 2022-03-15 15:47:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis Facilities for HL-LHC</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The HL-LHC presents significant challenges for the HEP analysis community.
The number of events in each analysis is expected to increase by an order of
magnitude and new techniques are expected to be required; both challenges
necessitate new services and approaches for analysis facilities. These services
are expected to provide new capabilities, a larger scale, and different access
modalities (complementing -- but distinct from -- traditional batch-oriented
approaches). To facilitate this transition, the US-LHC community is actively
investing in analysis facilities to provide a testbed for those developing new
analysis systems and to demonstrate new techniques for service delivery. This
whitepaper outlines the existing activities within the US LHC community in this
R&D area, the short- to medium-term goals, and the outline of common goals and
milestones. </font><br> Link: <a href='http://arxiv.org/pdf/2203.08010v2' target="_blank">http://arxiv.org/pdf/2203.08010v2</a><br> <br> <br> <font size='5'> 859 </font> <div style="text-align: right"> 2022-03-15 14:29:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021 Theory Frontier White Paper: Data-Driven Cosmology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past few decades, astronomical and cosmological data sets firmly
established the existence of physics beyond the Standard Model of particle
physics by providing strong evidence for the existence of dark matter, dark
energy, and non-zero neutrino mass. In addition, the generation of primordial
perturbations most likely also relies on physics beyond the Standard Model of
particle physics. Theory work, ranging from models of the early universe in
string theory that that led to novel phenomenological predictions to the
development of effective field theories to large-scale cosmological simulations
that include the physics of galaxy evolution, has played a key role in
analyzing and interpreting these data sets and in suggesting novel paths
forward to isolate the origins of this new physics. Over the next decade, even
more sensitive surveys are beginning to take data and are being planned. In
this white paper, we describe key areas of the theory program that will be
needed to optimize the physics return on investment from these new
observational opportunities. </font><br> Link: <a href='http://arxiv.org/pdf/2203.07946v1' target="_blank">http://arxiv.org/pdf/2203.07946v1</a><br> <br> <br> <font size='5'> 860 </font> <div style="text-align: right"> 2022-03-15 13:13:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021 Cosmic Frontier White Paper: 21cm Radiation as a Probe of Physics Across Cosmic Ages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The 21cm line refers to a forbidden transition in neutral hydrogen associated
with alignment of spins of the proton and electron. It is a very low energy
transition that is emitted whenever there is neutral hydrogen in the Universe.
Since baryons are mostly (~75%) hydrogen, one can in principle detect this
emission throughout much of the history of the Universe. The dominant emission
mechanism is different across cosmic ages. Before the photons decouple from
matter, hydrogen is in an ionized state and does not emit in 21cm. After
recombination and during the Dark Ages, at z ~ 30-1000, the 21cm emission is
associated with density fluctuations in the neutral hydrogen medium. After the
first stars turn on and galaxies begin to form, the 21cm emission traces
bubbles of ionized hydrogen in the sea of the neutral medium. This epoch,
spanning z ~ 6-30, is often referred to as cosmic dawn and the Epoch of
Reionization (EoR). At redshifts below z<6, the intergalactic medium is largely
ionized, but pockets of self-shielded neutral gas form in dense galactic
environments and 21cm emission traces the distribution of galaxies. The vastly
different emission mechanisms allow us to probe very different physics at
different redshifts, corresponding to different observational frequencies. The
instrumental challenges, namely building very sensitive and exquisitely
calibrated radio telescopes, however, share many commonalities across frequency
bands. The potential of the 21cm probe has been recognized by the Decadal
Survey of Astronomy & Astrophysics, whose Panel on Cosmology identified the
Dark Ages as its sole discovery area. We argue that HEP should recognize the
potential of 21cm as a probe of fundamental physics across many axes and invest
in the technology development that will enable full exploitation of this rich
technique. </font><br> Link: <a href='http://arxiv.org/pdf/2203.07864v1' target="_blank">http://arxiv.org/pdf/2203.07864v1</a><br> <br> <br> <font size='5'> 861 </font> <div style="text-align: right"> 2022-03-14 16:40:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass 2021 Cosmic Frontier White Paper: Cosmology with Millimeter-Wave Line Intensity Mapping</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Next-generation tests of fundamental physics and cosmology using large scale
structure require measurements over large volumes of the Universe, including
high redshifts inaccessible to present-day surveys. Line intensity mapping, an
emerging technique that detects the integrated emission of atomic and molecular
lines without resolving sources, can efficiently map cosmic structure over a
wide range of redshifts. Observations at millimeter wavelengths detect far-IR
emission lines such as CO/[CII], and take advantage of observational and
analysis techniques developed by CMB experiments. These measurements can
provide constraints with unprecedented precision on the physics of inflation,
neutrino masses, light relativistic species, dark energy and modified gravity,
and dark matter, among many other science goals. In this white paper we
forecast the sensitivity requirements for future ground-based mm-wave intensity
mapping experiments to enable transformational cosmological constraints. We
outline a staged experimental program to steadily improve sensitivity, and
describe the necessary investments in developing detector technology and
analysis techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2203.07258v1' target="_blank">http://arxiv.org/pdf/2203.07258v1</a><br> <br> <br> <font size='5'> 862 </font> <div style="text-align: right"> 2022-03-14 16:33:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021: Vera C. Rubin Observatory as a Flagship Dark Matter Experiment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Establishing that Vera C. Rubin Observatory is a flagship dark matter
experiment is an essential pathway toward understanding the physical nature of
dark matter. In the past two decades, wide-field astronomical surveys and
terrestrial laboratories have jointly created a phase transition in the
ecosystem of dark matter models and probes. Going forward, any robust
understanding of dark matter requires astronomical observations, which still
provide the only empirical evidence for dark matter to date. We have a unique
opportunity right now to create a dark matter experiment with Rubin Observatory
Legacy Survey of Space and Time (LSST). This experiment will be a coordinated
effort to perform dark matter research, and provide a large collaborative team
of scientists with the necessary organizational and funding supports. This
approach leverages existing investments in Rubin. Studies of dark matter with
Rubin LSST will also guide the design of, and confirm the results from, other
dark matter experiments. Supporting a collaborative team to carry out a dark
matter experiment with Rubin LSST is the key to achieving the dark matter
science goals that have already been identified as high priority by the
high-energy physics and astronomy communities. </font><br> Link: <a href='http://arxiv.org/pdf/2203.07252v1' target="_blank">http://arxiv.org/pdf/2203.07252v1</a><br> <br> <br> <font size='5'> 863 </font> <div style="text-align: right"> 2022-03-14 00:38:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowmass2021: Opportunities from Cross-survey Analyses of Static Probes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cosmological data in the next decade will be characterized by high-precision,
multi-wavelength measurements of thousands of square degrees of the same
patches of sky. By performing multi-survey analyses that harness the correlated
nature of these datasets, we will gain access to new science, and increase the
precision and robustness of science being pursued by each individual survey.
However, effective application of such analyses requires a qualitatively new
level of investment in cross-survey infrastructure, including simulations,
associated modeling, coordination of data sharing, and survey strategy. The
scientific gains from this new level of investment are multiplicative, as the
benefits can be reaped by even present-day instruments, and can be applied to
new instruments as they come online. </font><br> Link: <a href='http://arxiv.org/pdf/2203.06795v2' target="_blank">http://arxiv.org/pdf/2203.06795v2</a><br> <br> <br> <font size='5'> 864 </font> <div style="text-align: right"> 2022-03-13 00:33:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Regression Monte Carlo for Impulse Control</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: I develop a numerical algorithm for stochastic impulse control in the spirit
of Regression Monte Carlo for optimal stopping. The approach consists in
generating statistical surrogates (aka functional approximators) for the
continuation function. The surrogates are recursively trained by empirical
regression over simulated state trajectories. In parallel, the same surrogates
are used to learn the intervention function characterizing the optimal impulse
amounts. I discuss appropriate surrogate types for this task, as well as the
choice of training sets. Case studies from forest rotation and irreversible
investment illustrate the numerical scheme and highlight its flexibility and
extensibility. Implementation in \texttt{R} is provided as a publicly available
package posted on GitHub. </font><br> Link: <a href='http://arxiv.org/pdf/2203.06539v1' target="_blank">http://arxiv.org/pdf/2203.06539v1</a><br> <br> <br> <font size='5'> 865 </font> <div style="text-align: right"> 2022-03-11 21:13:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enabling U.S. participation in Future Higgs Factories</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Exciting proposals for a new Higgs factory collider, aimed at the search for
new physics and precision studies of particles and forces, especially
measurement of the Higgs boson couplings at the loop level, will be evaluated
as part of the Snowmass process. Potential facilities include (among others)
ILC, FCC-ee, C3, CEPC, CLIC, muon collider and advanced accelerator concepts
being investigated by Snowmass topical group AF6, potentially located in Asia,
Europe, or the United States. The European Strategy has endorsed an
electron-positron Higgs factory as its highest priority after HL-LHC. Much of
the detector, software, and physics preparative studies needed for these
machines is in common, and is currently being implemented by physicists
world-wide. In this white paper for the 2021 Snowmass process we look at
current global activity on future Higgs factories and give examples of
investments that could be made in these common areas over the next five years
to establish a leadership role for the U.S. in a future Higgs factory, wherever
it is built. The U.S. high energy physics program confronts a number of
challenges that a strong role in the study of the Higgs boson can address.
These include, in addition to the scientific results, maintaining leading roles
in international partnerships, nurturing and advancing world-leading
capabilities and expert resources, and maintaining and attracting talent. The
international effort would benefit from increased U.S. participation, and the
U.S., in turn, would maintain stature through the partnership. </font><br> Link: <a href='http://arxiv.org/pdf/2203.06255v2' target="_blank">http://arxiv.org/pdf/2203.06255v2</a><br> <br> <br> <font size='5'> 866 </font> <div style="text-align: right"> 2022-03-11 18:27:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Symmetry Group Equivariant Architectures for Physics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Physical theories grounded in mathematical symmetries are an essential
component of our understanding of a wide range of properties of the universe.
Similarly, in the domain of machine learning, an awareness of symmetries such
as rotation or permutation invariance has driven impressive performance
breakthroughs in computer vision, natural language processing, and other
important applications. In this report, we argue that both the physics
community and the broader machine learning community have much to understand
and potentially to gain from a deeper investment in research concerning
symmetry group equivariant machine learning architectures. For some
applications, the introduction of symmetries into the fundamental structural
design can yield models that are more economical (i.e. contain fewer, but more
expressive, learned parameters), interpretable (i.e. more explainable or
directly mappable to physical quantities), and/or trainable (i.e. more
efficient in both data and computational requirements). We discuss various
figures of merit for evaluating these models as well as some potential benefits
and limitations of these methods for a variety of physics applications.
Research and investment into these approaches will lay the foundation for
future architectures that are potentially more robust under new computational
paradigms and will provide a richer description of the physical systems to
which they are applied. </font><br> Link: <a href='http://arxiv.org/pdf/2203.06153v1' target="_blank">http://arxiv.org/pdf/2203.06153v1</a><br> <br> <br> <font size='5'> 867 </font> <div style="text-align: right"> 2022-03-11 18:17:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cryogenic User Facilities for R&D on Noble Liquid Detectors and Low Temperature Devices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cryogenic test facilities are critical infrastructure for physics experiments
in a variety of fields, perhaps most notably for particle detection with noble
liquid detectors, low-temperature device development, and quantum information
research. However, considerable investment and technical knowledge are required
to construct and operate such facilities. This white paper discusses proposals
for user facilities aimed at broadening the availability of testing
capabilities for the scientific community. </font><br> Link: <a href='http://arxiv.org/pdf/2203.06146v2' target="_blank">http://arxiv.org/pdf/2203.06146v2</a><br> <br> <br> <font size='5'> 868 </font> <div style="text-align: right"> 2022-03-11 06:29:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DeepTrust: A Reliable Financial Knowledge Retrieval Framework For Explaining Extreme Pricing Anomalies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Extreme pricing anomalies may occur unexpectedly without a trivial cause, and
equity traders typically experience a meticulous process to source disparate
information and analyze its reliability before integrating it into the trusted
knowledge base. We introduce DeepTrust, a reliable financial knowledge
retrieval framework on Twitter to explain extreme price moves at speed, while
ensuring data veracity using state-of-the-art NLP techniques. Our proposed
framework consists of three modules, specialized for anomaly detection,
information retrieval and reliability assessment. The workflow starts with
identifying anomalous asset price changes using machine learning models trained
with historical pricing data, and retrieving correlated unstructured data from
Twitter using enhanced queries with dynamic search conditions. DeepTrust
extrapolates information reliability from tweet features, traces of generative
language model, argumentation structure, subjectivity and sentiment signals,
and refine a concise collection of credible tweets for market insights. The
framework is evaluated on two self-annotated financial anomalies, i.e., Twitter
and Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms
the baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and
18.88% on precision, respectively, proving its capability in screening
unreliable information precisely. At the same time, information retrieval and
reliability assessment modules are analyzed individually on their effectiveness
and causes of limitations, with identified subjective and objective factors
that influence the performance. As a collaborative project with Refinitiv, this
framework paves a promising path towards building a scalable commercial
solution that assists traders to reach investment decisions on pricing
anomalies with authenticated knowledge from social media platforms in
real-time. </font><br> Link: <a href='http://arxiv.org/pdf/2203.08144v1' target="_blank">http://arxiv.org/pdf/2203.08144v1</a><br> <br> <br> <font size='5'> 869 </font> <div style="text-align: right"> 2022-03-10 19:37:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A persistent-homology-based turbulence index & some applications of TDA on financial markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Topological Data Analysis (TDA) is a modern approach to Data Analysis
focusing on the topological features of data; it has been widely studied in
recent years and used extensively in Biology, Physics, and many other areas.
However, financial markets have been studied slightly through TDA. Here we
present a quick review of some recent applications of TDA on financial markets,
including applications in the early detection of turbulence periods in
financial markets and how TDA can help to get new insights while investing.
Also, we propose a new turbulence index based on persistent homology -- the
fundamental tool for TDA -- that seems to capture critical transitions in
financial data; we tested our index with different financial time series
(S&P500, Russel 2000, S&P/BMV IPC and Nikkei 225) and crash events (Black
Monday crash, dot-com crash, 2007-08 crash and COVID-19 crash). Furthermore, we
include an introduction to persistent homology so the reader can understand
this paper without knowing TDA. </font><br> Link: <a href='http://arxiv.org/pdf/2203.05603v3' target="_blank">http://arxiv.org/pdf/2203.05603v3</a><br> <br> <br> <font size='5'> 870 </font> <div style="text-align: right"> 2022-03-10 17:03:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HiSA-SMFM: Historical and Sentiment Analysis based Stock Market Forecasting Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One of the pillars to build a country's economy is the stock market. Over the
years, people are investing in stock markets to earn as much profit as possible
from the amount of money that they possess. Hence, it is vital to have a
prediction model which can accurately predict future stock prices. With the
help of machine learning, it is not an impossible task as the various machine
learning techniques if modeled properly may be able to provide the best
prediction values. This would enable the investors to decide whether to buy,
sell or hold the share. The aim of this paper is to predict the future of the
financial stocks of a company with improved accuracy. In this paper, we have
proposed the use of historical as well as sentiment data to efficiently predict
stock prices by applying LSTM. It has been found by analyzing the existing
research in the area of sentiment analysis that there is a strong correlation
between the movement of stock prices and the publication of news articles.
Therefore, in this paper, we have integrated these factors to predict the stock
prices more accurately. </font><br> Link: <a href='http://arxiv.org/pdf/2203.08143v1' target="_blank">http://arxiv.org/pdf/2203.08143v1</a><br> <br> <br> <font size='5'> 871 </font> <div style="text-align: right"> 2022-03-10 14:07:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Scalable Security Investment Methods for Voltage Stability of Power Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop investment approaches to secure electric power systems against
load attacks that may cause voltage instability. The attacker attempts to alter
the reactive power setpoints of the loads covertly and intelligently to reduce
the voltage stability margin of the grid. The defender, or the system operator,
aims to compensate for this reduction by retuning the reactive power dispatch
of control devices such as shunt capacitor banks. The question is: how much
financial investment should the attacker and the defender plan for to succeed
in their respective objectives? To address this question, we formulate a
cost-based Stackelberg game, where the defender is aware of the attacker's
budget, and a robust-defense sequential algorithm for the realistic case when
the defender is not fully informed about the attacker's resources. We
demonstrate that these methods operate reliably under time-varying load
uncertainties. To provide scalability to large-scale power system models, we
develop a genetic algorithm where both players evolve their candidate solutions
in opposite directions simultaneously. Finally, the proposed methods are
validated using IEEE prototype models, demonstrating that reliable and robust
defense is feasible unless the defender's resources are severely limited
relative to the attacker's resources. </font><br> Link: <a href='http://arxiv.org/pdf/2203.05377v1' target="_blank">http://arxiv.org/pdf/2203.05377v1</a><br> <br> <br> <font size='5'> 872 </font> <div style="text-align: right"> 2022-03-09 17:46:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparing Classical-Quantum Portfolio Optimization with Enhanced Constraints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One of the problems frequently mentioned as a candidate for quantum advantage
is that of selecting a portfolio of financial assets to maximize returns while
minimizing risk. In this paper we formulate several real-world constraints for
use in a Quantum Annealer (QA), extending the scenarios in which the algorithm
can be implemented. Specifically, we show how to add fundamental analysis to
the portfolio optimization problem, adding in asset-specific and global
constraints based on chosen balance sheet metrics. We also expand on previous
work in improving the constraint to enforce investment bands in sectors and
limiting the number of assets to invest in, creating a robust and flexible
solution amenable to QA.
  Importantly, we analyze the current state-of-the-art algorithms for solving
such a problem using D-Wave's Quantum Processor and compare the quality of the
solutions obtained to commercially-available optimization software. We explore
a variety of traditional and new constraints that make the problem
computationally harder to solve and show that even with these additional
constraints, classical algorithms outperform current hybrid solutions in the
static portfolio optimization model. </font><br> Link: <a href='http://arxiv.org/pdf/2203.04912v1' target="_blank">http://arxiv.org/pdf/2203.04912v1</a><br> <br> <br> <font size='5'> 873 </font> <div style="text-align: right"> 2022-03-08 19:03:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Digital Twin in Practice: Emergent Insights from an ethnographic-action research study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Based on an ethnographic action research study for a Digital Twin (DT)
deployment on an automated highway maintenance project, this paper reports some
of the stumbling blocks that practitioners face while deploying a DT in
practice. At the outset, the scope of the case study was broadly defined in
terms of digitalization, and software development and deployment, which later
pivoted towards the concept of Digital Twin during the collective reflection
sessions between the project participants. Through an iterative learning cycle
via discussions among the various project stakeholders, the case study led to
uncovering the roadblocks in practice faced by the Architecture, Engineering,
and Construction (AEC) practitioners. This research finds that the
practitioners are facing difficulty in: (1) Creating a shared understanding due
to the lack of consensus on the Digital Twin concept, (2) Adapting and
investing in Digital Twin due to inability to exhaustively evaluate and select
the appropriate capabilities in a Digital Twin, and (3) Allocation of resources
for Digital Twin development due to the inability to assess the impact of DT on
the organizational conditions and processes. </font><br> Link: <a href='http://arxiv.org/pdf/2203.07030v1' target="_blank">http://arxiv.org/pdf/2203.07030v1</a><br> <br> <br> <font size='5'> 874 </font> <div style="text-align: right"> 2022-03-08 12:48:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Risk sharing in equity-linked insurance products: Stackelberg equilibrium between an insurer and a reinsurer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Equity-linked insurance products often have capital guarantees. Common
investment strategies ensuring these guarantees are challenged nowadays by low
interest rates. Thus, we study an alternative strategy when an insurance
company shares financial risk with a reinsurance company. We model this
situation as a Stackelberg game. The reinsurer is the leader in the game and
maximizes its expected utility by selecting its optimal investment strategy and
a safety loading in the reinsurance contract it offers to the insurer. The
reinsurer can assess how the insurer will rationally react on each action of
the reinsurer. The insurance company is the follower and maximizes its expected
utility by choosing its investment strategy and the amount of reinsurance the
company purchases at the price offered by the reinsurer. In this game, we
derive the Stackelberg equilibrium for general utility functions. For power
utility functions, we calculate the equilibrium explicitly and find that the
reinsurer selects the largest reinsurance premium such that the insurer may
still buy the maximal amount of reinsurance. Since in the equilibrium the
insurer is indifferent in the amount of reinsurance, in practice, the reinsurer
should consider charging a smaller reinsurance premium than the equilibrium
one. Therefore, we propose several criteria for choosing such a discount rate
and investigate its wealth-equivalent impact on the utilities of both parties. </font><br> Link: <a href='http://arxiv.org/pdf/2203.04053v1' target="_blank">http://arxiv.org/pdf/2203.04053v1</a><br> <br> <br> <font size='5'> 875 </font> <div style="text-align: right"> 2022-03-08 02:31:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Quantum Volume in Practice: What Users Can Expect from NISQ Devices</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Quantum volume (QV) has become the de-facto standard benchmark to quantify
the capability of Noisy Intermediate-Scale Quantum (NISQ) devices. While QV
values are often reported by NISQ providers for their systems, we perform our
own series of QV calculations on 24 NISQ devices currently offered by IBM~Q,
IonQ, Rigetti, Oxford Quantum Circuits, and Quantinuum (formerly Honeywell).
Our approach characterizes the performances that an advanced user of these NISQ
devices can expect to achieve with a reasonable amount of optimization, but
without white-box access to the device. In particular, we compile QV circuits
to standard gate sets of the vendor using compiler optimization routines where
available, and we perform experiments across different qubit subsets. We find
that running QV tests requires very significant compilation cycles, QV values
achieved in our tests typically lag behind officially reported results and also
depend significantly on the classical compilation effort invested. </font><br> Link: <a href='http://arxiv.org/pdf/2203.03816v4' target="_blank">http://arxiv.org/pdf/2203.03816v4</a><br> <br> <br> <font size='5'> 876 </font> <div style="text-align: right"> 2022-03-07 11:21:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Learning based Anomaly Detection for Smart Shirt: A Systematic Review</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, the popularity and use of Artificial Intelligence (AI) and
large investments on theInternet of Medical Things (IoMT) will be common to use
products such as smart socks, smartpants, and smart shirts. These products are
known as Smart Textile or E-textile, which has theability to monitor and
collect signals that our body emits. These signals make it possible to
extractanomalous components using Machine Learning (ML) techniques that play an
essential role in thisarea. This study presents a Systematic Review of the
Literature (SLR) on Anomaly Detection usingML techniques in Smart Shirt. The
objectives of the SLR are: (i) to identify what type of anomalythe smart shirt;
(ii) what ML techniques are being used; (iii) which datasets are being used;
(iv)identify smart shirt or signal acquisition devices; (v) list the
performance metrics used to evaluatethe ML model; (vi) the results of the
techniques in general; (vii) types of ML algorithms are beingapplied.The SLR
selected 11 primary studies published between 2017-2021. The results showed
that6 types of anomalies were identified, with the Fall anomaly being the most
cited. The Support VectorMachines (SVM) algorithm is most used. Most of the
primary studies used public or private datasets.The Hexoskin smart shirt was
most cited. The most used metric performance was Accuracy. Onaverage, almost
all primary studies presented a result above 90%, and all primary studies used
theSupervisioned type of ML. </font><br> Link: <a href='http://arxiv.org/pdf/2203.03300v1' target="_blank">http://arxiv.org/pdf/2203.03300v1</a><br> <br> <br> <font size='5'> 877 </font> <div style="text-align: right"> 2022-03-06 02:52:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimally Scheduling Public Safety Power Shutoffs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In an effort to reduce power system-caused wildfires, utilities carry out
public safety power shutoffs (PSPS) in which portions of the grid are
de-energized to mitigate the risk of ignition. The decision to call a PSPS must
balance reducing ignition risks and the negative impact of service
interruptions. In this work, we consider three PSPS scheduling scenarios, which
we model as dynamic programs. In the first two scenarios, we assume that N
PSPSs are budgeted as part of the investment strategy. In the first scenario, a
penalty is incurred for each PSPS declared past the Nth event. In the second,
we assume that some costs can be recovered if the number of PSPSs is below $N$
while still being subject to a penalty if above N. In the third, the system
operator wants to minimize the number of PSPS such that the total expected cost
is below a threshold. We provide optimal or asymptotically optimal policies for
each case, the first two of which have closed-form expressions. Lastly, we
establish the applicability of the first PSPS model's policy to critical-peak
pricing, and obtain an optimal scheduling policy to reduce the peak demand
based on weather observations. </font><br> Link: <a href='http://arxiv.org/pdf/2203.02861v3' target="_blank">http://arxiv.org/pdf/2203.02861v3</a><br> <br> <br> <font size='5'> 878 </font> <div style="text-align: right"> 2022-03-04 19:45:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reinventing High Performance Computing: Challenges and Opportunities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The world of computing is in rapid transition, now dominated by a world of
smartphones and cloud services, with profound implications for the future of
advanced scientific computing. Simply put, high-performance computing (HPC) is
at an important inflection point. For the last 60 years, the world's fastest
supercomputers were almost exclusively produced in the United States on behalf
of scientific research in the national laboratories. Change is now in the wind.
While costs now stretch the limits of U.S. government funding for advanced
computing, Japan and China are now leaders in the bespoke HPC systems funded by
government mandates. Meanwhile, the global semiconductor shortage and political
battles surrounding fabrication facilities affect everyone. However, another,
perhaps even deeper, fundamental change has occurred. The major cloud vendors
have invested in global networks of massive scale systems that dwarf today's
HPC systems. Driven by the computing demands of AI, these cloud systems are
increasingly built using custom semiconductors, reducing the financial leverage
of traditional computing vendors. These cloud systems are now breaking barriers
in game playing and computer vision, reshaping how we think about the nature of
scientific computation. Building the next generation of leading edge HPC
systems will require rethinking many fundamentals and historical approaches by
embracing end-to-end co-design; custom hardware configurations and packaging;
large-scale prototyping, as was common thirty years ago; and collaborative
partnerships with the dominant computing ecosystem companies, smartphone, and
cloud computing vendors. </font><br> Link: <a href='http://arxiv.org/pdf/2203.02544v1' target="_blank">http://arxiv.org/pdf/2203.02544v1</a><br> <br> <br> <font size='5'> 879 </font> <div style="text-align: right"> 2022-03-04 15:19:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Standing Forest Coin (SFC)</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article describes a proposal to create a digital currency that allows
the decentralized collection of resources directed to initiatives and
activities that aim to protect the Brazilian Amazon ecosystem by using
blockchain and digital contracts. In addition to the digital currency, the goal
is to design a smart contract based in oracles to ensure credibility and
security for investors and donors of financial resources invested in projects
within the Standing Forest Coin (SFC - standingforest.org). </font><br> Link: <a href='http://arxiv.org/pdf/2203.12600v1' target="_blank">http://arxiv.org/pdf/2203.12600v1</a><br> <br> <br> <font size='5'> 880 </font> <div style="text-align: right"> 2022-03-03 17:41:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Query Processing on Tensor Computation Runtimes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The huge demand for computation in artificial intelligence (AI) is driving
unparalleled investments in hardware and software systems for AI. This leads to
an explosion in the number of specialized hardware devices, which are now
offered by major cloud vendors. By hiding the low-level complexity through a
tensor-based interface, tensor computation runtimes (TCRs) such as PyTorch
allow data scientists to efficiently exploit the exciting capabilities offered
by the new hardware. In this paper, we explore how database management systems
can ride the wave of innovation happening in the AI space.
  We design, build, and evaluate Tensor Query Processor (TQP): TQP transforms
SQL queries into tensor programs and executes them on TCRs. TQP is able to run
the full TPC-H benchmark by implementing novel algorithms for relational
operators on the tensor routines. At the same time, TQP can support various
hardware while only requiring a fraction of the usual development effort.
Experiments show that TQP can improve query execution time by up to 10$\times$
over specialized CPU- and GPU-only systems. Finally, TQP can accelerate queries
mixing ML predictions and SQL end-to-end, and deliver up to 9$\times$ speedup
over CPU baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2203.01877v4' target="_blank">http://arxiv.org/pdf/2203.01877v4</a><br> <br> <br> <font size='5'> 881 </font> <div style="text-align: right"> 2022-03-02 08:06:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Smart Tracking Tray System for A Smart and Sustainable Wet Lab Community</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The laboratories and research institutes are the major places for
cutting-edge scientific exploration. Hundreds of millions of research papers
were formed from front-line labs. Behind this glorious achievement were
unsustainable facts. More and more human investment is required in innovative
experimental design and analysis of results. However, the laboratory operating
environment has not been subversively transformed for centuries. This abstract
proposed a smart tracking system, consisting of IoT and Data Visualization
technologies, to track the chemicals in an automatic and timely approach.
Positive feedback has been collected from pilot tests in several labs. The
system benefits various lab users in their daily work and improves their
working efficiency. In the long run, it will play an essential role in
promoting the efficient use of lab resources and achieving the goal of
sustainable labs. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00918v1' target="_blank">http://arxiv.org/pdf/2203.00918v1</a><br> <br> <br> <font size='5'> 882 </font> <div style="text-align: right"> 2022-03-01 15:16:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The SKA as a prebiotic molecule detector</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One of the theories for the origin of life proposes that a significant
fraction of prebiotic material could have arrived to Earth from outer space
between 4.1 and 3.8 billion years ago. This suggests that those prebiotic
compounds could have originated in interstellar space, to be later on
incorporated to small Solar-system bodies and planetesimals. The recent
discovery of prebiotic molecules such as hydroxylamine and ethanolamine in the
interstellar medium, strongly supports this hypothesis. However, some species
such as sugars, key for the synthesis of ribonucleotides and for metabolic
processes, remain to be discovered in space. The unmatched sensitivity of the
Square Kilometer Array (SKA) at centimeter wavelengths will be able to detect
even more complex and heavier prebiotic molecules than existing
instrumentation. In this contribution, we illustrate the potential of the SKA
to detect simple sugars with three and four carbon atoms, using a moderate
investment of observing time. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00534v1' target="_blank">http://arxiv.org/pdf/2203.00534v1</a><br> <br> <br> <font size='5'> 883 </font> <div style="text-align: right"> 2022-03-01 13:26:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Making use of supercomputers in financial machine learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article is the result of a collaboration between Fujitsu and Advestis.
This collaboration aims at refactoring and running an algorithm based on
systematic exploration producing investment recommendations on a
high-performance computer of the Fugaku, to see whether a very high number of
cores could allow for a deeper exploration of the data compared to a cloud
machine, hopefully resulting in better predictions. We found that an increase
in the number of explored rules results in a net increase in the predictive
performance of the final ruleset. Also, in the particular case of this study,
we found that using more than around 40 cores does not bring a significant
computation time gain. However, the origin of this limitation is explained by a
threshold-based search heuristic used to prune the search space. We have
evidence that for similar data sets with less restrictive thresholds, the
number of cores actually used could very well be much higher, allowing
parallelization to have a much greater effect. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00427v1' target="_blank">http://arxiv.org/pdf/2203.00427v1</a><br> <br> <br> <font size='5'> 884 </font> <div style="text-align: right"> 2022-03-01 08:04:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Visual perception tasks often require vast amounts of labelled data,
including 3D poses and image space segmentation masks. The process of creating
such training data sets can prove difficult or time-intensive to scale up to
efficacy for general use. Consider the task of pose estimation for rigid
objects. Deep neural network based approaches have shown good performance when
trained on large, public datasets. However, adapting these networks for other
novel objects, or fine-tuning existing models for different environments,
requires significant time investment to generate newly labelled instances.
Towards this end, we propose ProgressLabeller as a method for more efficiently
generating large amounts of 6D pose training data from color images sequences
for custom scenes in a scalable manner. ProgressLabeller is intended to also
support transparent or translucent objects, for which the previous methods
based on depth dense reconstruction will fail. We demonstrate the effectiveness
of ProgressLabeller by rapidly create a dataset of over 1M samples with which
we fine-tune a state-of-the-art pose estimation network in order to markedly
improve the downstream robotic grasp success rates. ProgressLabeller is
open-source at https://github.com/huijieZH/ProgressLabeller. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00283v2' target="_blank">http://arxiv.org/pdf/2203.00283v2</a><br> <br> <br> <font size='5'> 885 </font> <div style="text-align: right"> 2022-03-01 03:08:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting the Future Performance of the Planned Seismic Network in Mainland China: II. Earthquake Early Warning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The China Earthquake Administration (CEA) has launched an ambitious
nationwide earthquake early warning (EEW) system project that is currently
under development, which will consist of approximately 15,000 seismic stations
and be the largest EEW system in the world. The new EEW system is planned to go
online by the end of 2022. In 23% and 3% of Mainland China, the inter-station
distance will soon be smaller than 50 km and 25 km, respectively. The
effectiveness of the EEW system expected inside Mainland China can be
quantified via the metric given by the radius of the blind zone (no-warning
zone). Using theoretical network-based method, we generate the spatial
distribution of the blind zone radii predicted for the new seismic network
based on its configuration. The densified new seismic network is expected to
have excellent EEW performance from the perspective of blind zone. The area
covered by blind zones that are smaller than 30 km will soon rise from 1.6% to
24.3% inside Mainland China, which means that the area will increase by 2.6
million km2 (almost the size of Kazakhstan). We claim that every 1,000,000 RMB
(158,000 USD) invested to densifying the planned network will increase the area
where the blind zone radius is smaller than 30 km by 3,000 km2. Continuing to
increase the density of stations in some key regions with the blind zone radii
ranging from 20 to 40 km is still necessary to control the unexpected expansion
of blind zones due to the possible (and common) stations failure. Our
investigation provides a useful reference for the real functioning and further
optimization of the EEW system in Mainland China. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00198v1' target="_blank">http://arxiv.org/pdf/2203.00198v1</a><br> <br> <br> <font size='5'> 886 </font> <div style="text-align: right"> 2022-02-28 14:09:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Active learning with binary models for real time data labelling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data.
Most of the ML and DL applications involve supervised learning which requires
labelled data. In the initial phases of ML realm lack of data used to be a
problem, now we are in a new era of big data. The supervised ML algorithms
require data to be labelled and of good quality. Labelling task requires a
large amount of money and time investment. Data labelling require a skilled
person who will charge high for this task, consider the case of the medical
field or the data is in bulk that requires a lot of people assigned to label
it. The amount of data that is well enough for training needs to be known,
money and time can not be wasted to label the whole data. This paper mainly
aims to propose a strategy that helps in labelling the data along with oracle
in real-time. With balancing on model contribution for labelling is 89 and 81.1
for furniture type and intel scene image data sets respectively. Further with
balancing being kept off model contribution is found to be 83.47 and 78.71 for
furniture type and flower data sets respectively. </font><br> Link: <a href='http://arxiv.org/pdf/2203.00439v2' target="_blank">http://arxiv.org/pdf/2203.00439v2</a><br> <br> <br> <font size='5'> 887 </font> <div style="text-align: right"> 2022-02-27 20:40:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ZipZap: A Blockchain Solution for Local Energy Trading</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the last few years, electric utility companies have increasingly invested
into transactive energy systems. This trend was primarily caused by the
integration of distributed energy resources (DERs) and internet-of-things (IoT)
devices into their existing distribution networks. Influenced by the general
interest in blockchain technologies, many industry specialists are considering
new, more efficient peer-to-peer market structures for DERs. Since
blockchain-based energy exchanges can automate transactions between their
members and provide increased levels of security thanks to smart contracts,
these new initiatives may eventually revolutionize how customers interact with
utility companies. In this paper, we explore the trade-off between cost and
traceability in the form of on-chain and off-chain solutions. We also propose
ZipZap, a first step towards a blockchain-based local smart grid system. ZipZap
is an ERC-1155 compliant solution with four different prototypes: Heavyweight,
Featherweight, Lightweight and Weightless. The first three prototypes were
developed in Solidity and deployed using Ethereum. Heavyweight is fully
on-chain, whereas Featherweight and Lightweight showcase various levels of
hybridization. Weightless, in turn, was deployed using Quorum, a gas-free
alternative to Ethereum. Our evaluation uses realistic parameters and measures
the impact of different types of metadata storage scopes, with some Ethereum
prototypes showcasing gas cost reductions of more than 97% in comparison to our
fully on-chain baseline. </font><br> Link: <a href='http://arxiv.org/pdf/2202.13450v1' target="_blank">http://arxiv.org/pdf/2202.13450v1</a><br> <br> <br> <font size='5'> 888 </font> <div style="text-align: right"> 2022-02-27 01:16:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Perceptions of the State of D&I and D&I Initiative in the ASF</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Open Source Software (OSS) Foundations and projects are investing in creating
Diversity and Inclusion (D&I) initiatives. However, little is known about
contributors' perceptions about the usefulness and success of such initiatives.
We aim to close this gap by investigating how contributors perceive the state
of D&I in their community. In collaboration with the Apache Software Foundation
(ASF), we surveyed 600+ OSS contributors and conducted 11 follow-up interviews.
We used mixed methods to analyze our data-quantitative analysis of Likert-scale
questions and qualitative analysis of open-ended survey question and the
interviews to understand contributors' perceptions and critiques of the D&I
initiative and how to improve it. Our results indicate that the ASF
contributors felt that the state of D&I was still lacking, especially regarding
gender, seniority, and English proficiency. Regarding the D&I initiative, some
participants felt that the effort was unnecessary, while others agreed with the
effort but critiqued its implementation. These findings show that D&I
initiatives in OSS communities are a good start, but there is room for
improvements. Our results can inspire the creation of new and the refinement of
current initiatives. </font><br> Link: <a href='http://arxiv.org/pdf/2202.13260v1' target="_blank">http://arxiv.org/pdf/2202.13260v1</a><br> <br> <br> <font size='5'> 889 </font> <div style="text-align: right"> 2022-02-26 01:27:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A New BAT and PageRank algorithm for Propagation Probability in Social Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Social networks have increasingly become important and popular in modern
times. Moreover, the influence of social networks plays a vital role in various
organizations including government organizations, academic research or
corporate organizations. Therefore, how to strategize the optimal propagation
strategy in social networks has also become more important. By increasing the
precision of evaluating the propagation probability of social network, it can
indirectly influence the investment of cost, manpower and time for information
propagation to achieve the best return. This study proposes a new algorithm,
which includes a scale-free network, Barabasi-Albert model,
Binary-Addition-Tree (BAT) algorithm, PageRank algorithm, personalized PageRank
algorithm and a new BAT algorithm, to calculate the propagation probability in
social networks. The results obtained after implementing the simulation
experiment of social network models show the studied model and the proposed
algorithm provide an effective method to increase the efficiency of information
propagation in social networks. In this way, the maximum propagation efficiency
is achieved with the minimum investment. </font><br> Link: <a href='http://arxiv.org/pdf/2202.13033v1' target="_blank">http://arxiv.org/pdf/2202.13033v1</a><br> <br> <br> <font size='5'> 890 </font> <div style="text-align: right"> 2022-02-25 18:11:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Venture Capital investments through the lens of Network and Functional Data Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper we characterize the performance of venture capital-backed firms
based on their ability to attract investment. The aim of the study is to
identify relevant predictors of success built from the network structure of
firms' and investors' relations. Focusing on deal-level data for the health
sector, we first create a bipartite network among firms and investors, and then
apply functional data analysis (FDA) to derive progressively more refined
indicators of success captured by a binary, a scalar and a functional outcome.
More specifically, we use different network centrality measures to capture the
role of early investments for the success of the firm. Our results, which are
robust to different specifications, suggest that success has a strong positive
association with centrality measures of the firm and of its large investors,
and a weaker but still detectable association with centrality measures of small
investors and features describing firms as knowledge bridges. Finally, based on
our analyses, success is not associated with firms' and investors' spreading
power (harmonic centrality), nor with the tightness of investors' community
(clustering coefficient) and spreading ability (VoteRank). </font><br> Link: <a href='http://arxiv.org/pdf/2202.12859v2' target="_blank">http://arxiv.org/pdf/2202.12859v2</a><br> <br> <br> <font size='5'> 891 </font> <div style="text-align: right"> 2022-02-25 03:01:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bidding Agent Design in the LinkedIn Ad Marketplace</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We establish a general optimization framework for the design of automated
bidding agent in dynamic online marketplaces. It optimizes solely for the
buyer's interest and is agnostic to the auction mechanism imposed by the
seller. As a result, the framework allows, for instance, the joint optimization
of a group of ads across multiple platforms each running its own auction
format. Bidding strategy derived from this framework automatically guarantees
the optimality of budget allocation across ad units and platforms. Common
constraints such as budget delivery schedule, return on investments and
guaranteed results, directly translates to additional parameters in the bidding
formula. We share practical learnings of the deployed bidding system in the
LinkedIn ad marketplace based on this framework. </font><br> Link: <a href='http://arxiv.org/pdf/2202.12472v1' target="_blank">http://arxiv.org/pdf/2202.12472v1</a><br> <br> <br> <font size='5'> 892 </font> <div style="text-align: right"> 2022-02-23 05:19:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Optimal Investment in a Large Population of Competitive and Heterogeneous Agents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies a stochastic utility maximization game under relative
performance concerns in finite agent and infinite agent settings, where a
continuum of agents interact through a graphon (see definition below). We
consider an incomplete market model in which agents have CARA utilities, and we
obtain characterizations of Nash equilibria in both the finite agent and
graphon paradigms. Under modest assumptions on the denseness of the interaction
graph among the agents, we establish convergence results for the Nash
equilibria and optimal utilities of the finite player problem to the infinite
player problem. This result is achieved as an application of a general backward
propagation of chaos type result for systems of interacting forward-backward
stochastic differential equations, where the interaction is heterogeneous and
through the control processes, and the generator is of quadratic growth. In
addition, characterizing the graphon game gives rise to a novel form of
infinite dimensional forward-backward stochastic differential equation of
Mckean-Vlasov type, for which we provide well-posedness results. An interesting
consequence of our result is the computation of the competition indifference
capital, i.e., the capital making an investor indifferent between whether or
not to compete. </font><br> Link: <a href='http://arxiv.org/pdf/2202.11314v3' target="_blank">http://arxiv.org/pdf/2202.11314v3</a><br> <br> <br> <font size='5'> 893 </font> <div style="text-align: right"> 2022-02-23 04:16:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Implicit Mentoring: The Unacknowledged Developer Efforts in Open Source</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Mentoring is traditionally viewed as a dyadic, top-down apprenticeship. This
perspective, however, overlooks other forms of informal mentoring taking place
in everyday activities in which developers invest time and effort, but remain
unacknowledged. Here, we investigate the different flavors of mentoring in Open
Source Software (OSS) to define and identify implicit mentoring. We first
define implicit mentoring--situations where contributors guide others through
instructions and suggestions embedded in everyday (OSS) activities--through
formative interviews with OSS contributors, a literature review, and
member-checking. Next, through an empirical investigation of Pull Requests
(PRs) in 37 Apache Projects, we build a classifier to extract implicit
mentoring and characterize it through the dual lenses of experience and gender.
Our analysis of 107,895 PRs shows that implicit mentoring occurs (27.41% of all
PRs include implicit mentoring) and it does not follow the traditional dyadic,
top-down apprenticeship model. When considering the gender of mentor-mentee
pairs, we found pervasive homophily--a preference to mentor those who are of
the same gender--in 93.81% cases. In the cross-gender mentoring instances,
women were more likely to mentor men. </font><br> Link: <a href='http://arxiv.org/pdf/2202.11300v2' target="_blank">http://arxiv.org/pdf/2202.11300v2</a><br> <br> <br> <font size='5'> 894 </font> <div style="text-align: right"> 2022-02-22 17:33:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stacked Periodograms as a Probe of Exoplanetary Populations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Ongoing, extreme-precision Doppler radial velocity surveys seek planets with
masses less than several M$_\oplus$; population-level studies to determine the
distribution of planetary masses, however, remain difficult due to the required
observational time investment, as well as challenges associated with robustly
detecting the lowest mass planets. We outline a novel approach that leverages
extensive, existing RV datasets to constrain masses of exoplanet populations:
stacking periodograms of RV timeseries across many targets. We show that an
exoplanet population may be statistically identifiable in the stacked
periodogram, even when individual planets do not pass the threshold of
detection. We discuss analytical, statistical properties of the stacked
periodogram, perform simulations to demonstrate the efficacy of the method, and
investigate the influence of semi-structured window functions and stellar
activity. Analysis of the Lick-Carnegie Exoplanet Survey data set reveals a
marginally significant ($1.6\sigma$) signal consistent with a population of
exoplanets occupying $3-7$ day periods with typical $K$ between $1.6-5.1$ m
s$^{-1}$. More detailed investigation of signals associated with stellar
activity and yearly systematics may be necessary to confirm this result or
detect other underlying Keplerian contributions. </font><br> Link: <a href='http://arxiv.org/pdf/2202.11050v1' target="_blank">http://arxiv.org/pdf/2202.11050v1</a><br> <br> <br> <font size='5'> 895 </font> <div style="text-align: right"> 2022-02-22 13:32:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Information Design in Games: Certification Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Several players participate in a game with a continuum of actions. A designer
chooses an information structure -- a joint distribution of a state and private
signals -- and evaluates it according to the expected designer's payoff in the
induced Bayes Nash equilibrium. We show an information structure is
designer-optimal whenever the equilibrium play it induces can also be induced
in an auxiliary contracting problem.
  This finding gives rise to a tractable solution method, which we use to study
two novel applications. In an investment game, an optimal structure fully
informs a single investor while providing no information to others. This
structure is robustly optimal, for any state distribution and number of
investors. In a price competition game, an optimal structure is Gaussian and
recommends prices linearly in the state. This structure is uniquely optimal. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10883v3' target="_blank">http://arxiv.org/pdf/2202.10883v3</a><br> <br> <br> <font size='5'> 896 </font> <div style="text-align: right"> 2022-02-22 11:48:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Macroeconomic Effect of Uncertainty and Financial Shocks: a non-Gaussian VAR approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Great Recession highlighted the role of financial and uncertainty shocks
as drivers of business cycle fluctuations. However, the fact that uncertainty
shocks may affect economic activity by tightening financial conditions makes
empirically distinguishing these shocks difficult. This paper examines the
macroeconomic effects of the financial and uncertainty shocks in the United
States in an SVAR model that exploits the non-normalities of the time series to
identify the uncertainty and the financial shock. The results show that
macroeconomic uncertainty and financial shocks seem to affect business cycles
independently as well as through dynamic interaction. Uncertainty shocks appear
to tighten financial conditions, whereas there appears to be no causal
relationship between financial conditions and uncertainty. Moreover, the
results suggest that uncertainty shocks may have persistent effects on output
and investment that last beyond the business cycle. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10834v1' target="_blank">http://arxiv.org/pdf/2202.10834v1</a><br> <br> <br> <font size='5'> 897 </font> <div style="text-align: right"> 2022-02-22 11:47:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Some applications of linear algebra and geometry in real life</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, some real-world motivated examples are provided illustrating
the power of linear algebra tools as the product of matrices, determinants,
eigenvalues and eigenvectors. In this sense, some practical applications
related to computer graphics, geometry, areas, volumes are presented, along
with some problems connected to sports and investments. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10833v1' target="_blank">http://arxiv.org/pdf/2202.10833v1</a><br> <br> <br> <font size='5'> 898 </font> <div style="text-align: right"> 2022-02-22 11:17:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Canonical Portfolios: Optimal Asset and Signal Combination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present a novel framework for analyzing the optimal asset and signal
combination problem, which builds upon the dynamic portfolio selection problem
of Brandt and Santa-Clara (2006) in two phases. First, we reformulate their
original investment problem into a tractable vehicle that admits a closed-form
solution, scaling to large dimensions by imposing a joint Gaussian structure on
the asset returns and signals. Second, we recast the optimal portfolio of
correlated assets and signals into a set of uncorrelated managed portfolios
through the lens of Canonical Correlation Analysis of Hotelling (1936). The new
investment environment of uncorrelated managed portfolios offers unique
economic insights into the joint correlation structure of our optimal portfolio
policy. We also operationalize our theoretical framework to bridge the gap
between theory and practice, showcasing the improved performance of our
proposed method over natural competing benchmarks. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10817v3' target="_blank">http://arxiv.org/pdf/2202.10817v3</a><br> <br> <br> <font size='5'> 899 </font> <div style="text-align: right"> 2022-02-22 08:25:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Risk Parity Portfolios with Skewness Risk: An Application to Factor Investing and Alternative Risk Premia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article develops a model that takes into account skewness risk in risk
parity portfolios. In this framework, asset returns are viewed as stochastic
processes with jumps or random variables generated by a Gaussian mixture
distribution. This dual representation allows us to show that skewness and jump
risks are equivalent. As the mixture representation is simple, we obtain
analytical formulas for computing asset risk contributions of a given
portfolio. Therefore, we define risk budgeting portfolios and derive existence
and uniqueness conditions. We then apply our model to the
equity/bond/volatility asset mix policy. When assets exhibit jump risks like
the short volatility strategy, we show that skewness-based risk parity
portfolios produce better allocation than volatility-based risk parity
portfolios. Finally, we illustrate how this model is suitable to manage the
skewness risk of long-only equity factor portfolios and to allocate between
alternative risk premia. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10721v1' target="_blank">http://arxiv.org/pdf/2202.10721v1</a><br> <br> <br> <font size='5'> 900 </font> <div style="text-align: right"> 2022-02-22 02:18:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On financial market correlation structures and diversification benefits across and within equity sectors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study how to assess the potential benefit of diversifying an equity
portfolio by investing within and across equity sectors. We analyse 20 years of
US stock price data, which includes the global financial crisis (GFC) and the
COVID-19 market crash, as well as periods of financial stability, to determine
the `all weather' nature of equity portfolios. We establish that one may use
the leading eigenvalue of the cross-correlation matrix of log returns as well
as graph-theoretic diagnostics such as modularity to quantify the collective
behaviour of the market or a subset of it. We confirm that financial crises are
characterised by a high degree of collective behaviour of equities, whereas
periods of financial stability exhibit less collective behaviour. We argue that
during times of increased collective behaviour, risk reduction via sector-based
portfolio diversification is ineffective. Using the degree of collectivity as a
proxy for the benefit of diversification, we perform an extensive sampling of
equity portfolios to confirm the old financial adage that 30-40 stocks provide
sufficient diversification. Using hierarchical clustering, we discover a `best
value' equity portfolio for diversification consisting of 36 equities sampled
uniformly from 9 sectors. We further show that it is typically more beneficial
to diversify across sectors rather than within. Our findings have implications
for cost-conscious retail investors seeking broad diversification across equity
markets. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10623v2' target="_blank">http://arxiv.org/pdf/2202.10623v2</a><br> <br> <br> <font size='5'> 901 </font> <div style="text-align: right"> 2022-02-21 14:31:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Yields: The Galapagos Syndrome Of Cryptofinance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this chapter structures that generate yield in cryptofinance will be
analyzed and related to leverage. While the majority of crypto-assets do not
have intrinsic yields in and of themselves, similar to cash holdings of fiat
currency, revolutionary innovation based on smart contracts, which enable
decentralised finance, does generate return. Examples include lending or
providing liquidity to an automated market maker on a decentralised exchange,
as well as performing block formation in a proof of stake blockchain. On
centralised exchanges, perpetual and finite duration futures can trade at a
premium or discount to the spot market for extended periods with one side of
the transaction earning a yield. Disparities in yield exist between products
and venues as a result of market segmentation and risk profile differences.
Cryptofinance was initially shunned by legacy finance and developed
independently. This led to curious and imaginative adaptions, reminiscent of
Darwin's finches, including stable coins for dollar transfers, perpetuals for
leverage, and a new class of exchanges for trading and investment. </font><br> Link: <a href='http://arxiv.org/pdf/2202.10265v1' target="_blank">http://arxiv.org/pdf/2202.10265v1</a><br> <br> <br> <font size='5'> 902 </font> <div style="text-align: right"> 2022-02-20 18:51:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Benchmarking the Linear Algebra Awareness of TensorFlow and PyTorch</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Linear algebra operations, which are ubiquitous in machine learning, form
major performance bottlenecks. The High-Performance Computing community invests
significant effort in the development of architecture-specific optimized
kernels, such as those provided by the BLAS and LAPACK libraries, to speed up
linear algebra operations. However, end users are progressively less likely to
go through the error prone and time-consuming process of directly using said
kernels; instead, frameworks such as TensorFlow (TF) and PyTorch (PyT), which
facilitate the development of machine learning applications, are becoming more
and more popular. Although such frameworks link to BLAS and LAPACK, it is not
clear whether or not they make use of linear algebra knowledge to speed up
computations. For this reason, in this paper we develop benchmarks to
investigate the linear algebra optimization capabilities of TF and PyT. Our
analyses reveal that a number of linear algebra optimizations are still
missing; for instance, reducing the number of scalar operations by applying the
distributive law, and automatically identifying the optimal parenthesization of
a matrix chain. In this work, we focus on linear algebra computations in TF and
PyT; we both expose opportunities for performance enhancement to the benefit of
the developers of the frameworks and provide end users with guidelines on how
to achieve performance gains. </font><br> Link: <a href='http://arxiv.org/pdf/2202.09888v1' target="_blank">http://arxiv.org/pdf/2202.09888v1</a><br> <br> <br> <font size='5'> 903 </font> <div style="text-align: right"> 2022-02-18 07:59:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Interpretable Reinforcement Learning Manage Prosperity Your Way?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Personalisation of products and services is fast becoming the driver of
success in banking and commerce. Machine learning holds the promise of gaining
a deeper understanding of and tailoring to customers' needs and preferences.
Whereas traditional solutions to financial decision problems frequently rely on
model assumptions, reinforcement learning is able to exploit large amounts of
data to improve customer modelling and decision-making in complex financial
environments with fewer assumptions. Model explainability and interpretability
present challenges from a regulatory perspective which demands transparency for
acceptance; they also offer the opportunity for improved insight into and
understanding of customers. Post-hoc approaches are typically used for
explaining pretrained reinforcement learning models. Based on our previous
modeling of customer spending behaviour, we adapt our recent reinforcement
learning algorithm that intrinsically characterizes desirable behaviours and we
transition to the problem of asset management. We train inherently
interpretable reinforcement learning agents to give investment advice that is
aligned with prototype financial personality traits which are combined to make
a final recommendation. We observe that the trained agents' advice adheres to
their intended characteristics, they learn the value of compound growth, and,
without any explicit reference, the notion of risk as well as improved policy
convergence. </font><br> Link: <a href='http://arxiv.org/pdf/2202.09064v2' target="_blank">http://arxiv.org/pdf/2202.09064v2</a><br> <br> <br> <font size='5'> 904 </font> <div style="text-align: right"> 2022-02-18 04:09:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why, How and Where of Delays in Software Security Patch Management: An Empirical Investigation in the Healthcare Sector</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Numerous security attacks that resulted in devastating consequences can be
traced back to a delay in applying a security patch. Despite the criticality of
timely patch application, not much is known about why and how delays occur when
applying security patches in practice, and how the delays can be mitigated.
Based on longitudinal data collected from 132 delayed patching tasks over a
period of four years and observations of patch meetings involving eight teams
from two organisations in the healthcare domain, and using quantitative and
qualitative data analysis approaches, we identify a set of reasons relating to
technology, people and organisation as key explanations that cause delays in
patching. Our findings also reveal that the most prominent cause of delays is
attributable to coordination delays in the patch management process and a
majority of delays occur during the patch deployment phase. Towards mitigating
the delays, we describe a set of strategies employed by the studied
practitioners. This research serves as the first step towards understanding the
practical reasons for delays and possible mitigation strategies in
vulnerability patch management. Our findings provide useful insights for
practitioners to understand what and where improvement is needed in the patch
management process and guide them towards taking timely actions against
potential attacks. Also, our findings help researchers to invest effort into
designing and developing computer-supported tools to better support a timely
security patch management process. </font><br> Link: <a href='http://arxiv.org/pdf/2202.09016v2' target="_blank">http://arxiv.org/pdf/2202.09016v2</a><br> <br> <br> <font size='5'> 905 </font> <div style="text-align: right"> 2022-02-15 21:39:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Attracting and Retaining OSS Contributors with a Maintainer Dashboard</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Tools and artifacts produced by open source software (OSS) have been woven
into the foundation of the technology industry. To keep this foundation intact,
the open source community needs to actively invest in sustainable approaches to
bring in new contributors and nurture existing ones. We take a first step at
this by collaboratively designing a maintainer dashboard that provides
recommendations on how to attract and retain open source contributors. For
example, by highlighting project goals (e.g., a social good cause) to attract
diverse contributors and mechanisms to acknowledge (e.g., a "rising
contributor" badge) existing contributors. Next, we conduct a project-specific
evaluation with maintainers to better understand use cases in which this tool
will be most helpful at supporting their plans for growth. From analyzing
feedback, we find recommendations to be useful at signaling projects as
welcoming and providing gentle nudges for maintainers to proactively recognize
emerging contributors. However, there are complexities to consider when
designing recommendations such as the project current development state (e.g.,
deadlines, milestones, refactoring) and governance model. Finally, we distill
our findings to share what the future of recommendations in open source looks
like and how to make these recommendations most meaningful over time. </font><br> Link: <a href='http://arxiv.org/pdf/2202.07740v1' target="_blank">http://arxiv.org/pdf/2202.07740v1</a><br> <br> <br> <font size='5'> 906 </font> <div style="text-align: right"> 2022-02-14 23:31:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Reliability-aware Distributed Framework to Schedule Residential Charging of Electric Vehicles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Residential consumers have become active participants in the power
distribution network after being equipped with residential EV charging
provisions. This creates a challenge for the network operator tasked with
dispatching electric power to the residential consumers through the existing
distribution network infrastructure in a reliable manner. In this paper, we
address the problem of scheduling residential EV charging for multiple
consumers while maintaining network reliability. An additional challenge is the
restricted exchange of information: where the consumers do not have access to
network information and the network operator does not have access to consumer
load parameters. We propose a distributed framework which generates an optimal
EV charging schedule for individual residential consumers based on their
preferences and iteratively updates it until the network reliability
constraints set by the operator are satisfied. We validate the proposed
approach for different EV adoption levels in a synthetically created digital
twin of an actual power distribution network. The results demonstrate that the
new approach can achieve a higher level of network reliability compared to the
case where residential consumers charge EVs based solely on their individual
preferences, thus providing a solution for the existing grid to keep up with
increased adoption rates without significant investments in increasing grid
capacity. </font><br> Link: <a href='http://arxiv.org/pdf/2202.07092v1' target="_blank">http://arxiv.org/pdf/2202.07092v1</a><br> <br> <br> <font size='5'> 907 </font> <div style="text-align: right"> 2022-02-14 20:48:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Multibillion Dollar Software Supply Chain of Ethereum</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of blockchain technologies has triggered tremendous research
interest, coding efforts, and monetary investments in the last decade. Ethereum
is the single largest programmable blockchain platform today. It features
cryptocurrency trading, digital art, and decentralized finance through smart
contracts. So-called Ethereum nodes operate the blockchain, relying on a vast
supply chain of third-party software dependencies maintained by diverse
organizations. These software suppliers have a direct impact on the reliability
and the security of Ethereum. In this article, we perform an analysis of the
software supply chain of Java Ethereum nodes and distill the challenges of
maintaining and securing this blockchain technology. </font><br> Link: <a href='http://arxiv.org/pdf/2202.07029v4' target="_blank">http://arxiv.org/pdf/2202.07029v4</a><br> <br> <br> <font size='5'> 908 </font> <div style="text-align: right"> 2022-02-13 21:58:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Research Landscape of Pakistan: A Data-driven Analysis of Scopus Indexed Scientific Literature</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Scientific contribution and research performance of a university, research
group, or institute needs to be evaluated all the more with the increasing
volume and fast-developing disciplines of research. The need of the time is to
develop tools for strategic planning and management that will help research
bodies to rank and benchmark themselves against international standards. This
will enable them to invest appropriately in research areas of promising
strength and gain maximally from them, thus fulfilling the ultimate purpose of
positive impact of research on society. Our tool is capable of rating and
benchmarking universities as well as research institutes in not only the major
disciplines and sub-disciplines, but at the finest level of niche areas of
science and technology too with the help of its innovative bibliometric
indicators based on publications and citation analysis. The tool accepts inputs
like discipline/subject area, university, and country and time window while
using data retrieved from bibliography database, Scopus, to benchmark and rate
the research body under consideration. We have evaluated that there are many
niche subject areas in which small or medium size universities are performing
good in comparison to the large universities. Most of these subject areas are
of more significance in the present day and the future. Government and funds
allocating bodies should take this factor in account that investing the right
money at right place will give far better results than we they are having right
now </font><br> Link: <a href='http://arxiv.org/pdf/2202.06421v1' target="_blank">http://arxiv.org/pdf/2202.06421v1</a><br> <br> <br> <font size='5'> 909 </font> <div style="text-align: right"> 2022-02-13 05:15:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Perception of Filipinos on the Advent of Cryptocurrency and Non-Fungible Token (NFT) Games</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aims to shed light on the rise of play-to-earn games in the
Philippines alongside cryptocurrency. The lack of research and public
understanding of its benefits and drawbacks prompted the researchers to
investigate its market. As such, the study tried to look into the risks and
benefits of crypto gaming if it would be regulated by the government, and how
market volatility influences the churn rate of crypto games. The research used
a descriptive study to determine the perception of people who are engaged in
playing a crypto-game named as Axie Infinity. The results showed that most
players spend their time playing Axie Infinity for about 1 to 4 hours a day.
Predominantly, the return of investments for playing the game will take about 1
to 3 months. It also showed that these players agreed that there is a possible
financial instability in a volatile market. With this, they have a high trust
issue in terms of price manipulation, privacy and security, and its design and
usability. Understanding the cryptocurrency market requires comprehending the
perspective of the people who are engaged in a play-to-earn game, and their
concerns are critical for any government actions aimed at regulating
self-employed income earners playing (Non-fungible Tokens) NFT games in the
Philippines. </font><br> Link: <a href='http://arxiv.org/pdf/2202.07467v1' target="_blank">http://arxiv.org/pdf/2202.07467v1</a><br> <br> <br> <font size='5'> 910 </font> <div style="text-align: right"> 2022-02-12 21:51:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Open access to orbit and runaway space debris growth</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As Earth's orbits fill with satellites and debris, debris-producing
collisions between orbiting bodies become more likely. Runaway space debris
growth, known as Kessler Syndrome, may render Earth's orbits unusable for
centuries. We present a dynamic physico-economic model of Earth orbit use under
rational expectations with endogenous collision risk and Kessler Syndrome. When
satellites can be destroyed in collisions with debris and other satellites, the
open-access equilibrium manifold allows for multiple steady states. When debris
can collide to produce more debris, at least one steady state may be a tipping
point and Kessler Syndrome can occur along equilibrium paths. We show open
access is increasingly and inefficiently likely to cause Kessler Syndrome as
satellites become more profitable. Calibrated simulations reveal Kessler
Syndrome is expected to occur in low-Earth orbit around 2048 under recent
historical sectoral growth trends, and may occur as early as 2035 if the space
economy grows consistent with projections by major investment banks. These
results highlight the urgent need for modeling and policy approaches which
incorporate open access and positive feedbacks in debris growth. </font><br> Link: <a href='http://arxiv.org/pdf/2202.07442v1' target="_blank">http://arxiv.org/pdf/2202.07442v1</a><br> <br> <br> <font size='5'> 911 </font> <div style="text-align: right"> 2022-02-12 21:39:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ensemble and Multimodal Approach for Forecasting Cryptocurrency Price</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since the birth of Bitcoin in 2009, cryptocurrencies have emerged to become a
global phenomenon and an important decentralized financial asset. Due to this
decentralization, the value of these digital currencies against fiat currencies
is highly volatile over time. Therefore, forecasting the crypto-fiat currency
exchange rate is an extremely challenging task. For reliable forecasting, this
paper proposes a multimodal AdaBoost-LSTM ensemble approach that employs all
modalities which derive price fluctuation such as social media sentiments,
search volumes, blockchain information, and trading data. To better support
investment decision making, the approach forecasts also the fluctuation
distribution. The conducted extensive experiments demonstrated the
effectiveness of relying on multimodalities instead of only trading data.
Further experiments demonstrate the outperformance of the proposed approach
compared to existing tools and methods with a 19.29% improvement. </font><br> Link: <a href='http://arxiv.org/pdf/2202.08967v1' target="_blank">http://arxiv.org/pdf/2202.08967v1</a><br> <br> <br> <font size='5'> 912 </font> <div style="text-align: right"> 2022-02-12 07:48:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Applying multi product lines to equity market software ecosystem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Context: In recent decades, many financial markets and their participants
have changed their working method from a completely manual and traditional one
to an automatic one, benefiting from complex software systems. There are
different approaches to the development of such software systems. Objective: In
this paper, we study the application of the Multi Product Line (MPL) approach
in the software ecosystem (SECO) of the equity market. By profiting from the
concepts and practices of the MPL approach, we want to design a SECO that makes
the real-time and automated flow of financial transaction data between market
participants' software pieces possible. Method: We first provide some
background information about the equity market, its participants and their
relations, and two primary order life-cycles in which these players cooperate.
After that, we analyze the variability in each market participant's software.
Next, we describe the employed architecture and the implementation approach.
Finally, we discuss three scenarios by which the whole proposed SECO is tested
and validated. Results: To implement the mentioned working method, named
Straight-through Processing (STP), different technical and non-technical
elements' contribution is essential. Attaining success in developing the equity
market's SECO addresses the technical aspect and prepares the technical
infrastructure for the rest of the work. Conclusion: The successful validation
of the equity market's SECO indicates that the adoption of the MPL approach is
a viable strategy for the development of equity market SECOs. It also suggests
that this approach is worthy of more attention and investment. </font><br> Link: <a href='http://arxiv.org/pdf/2202.06008v1' target="_blank">http://arxiv.org/pdf/2202.06008v1</a><br> <br> <br> <font size='5'> 913 </font> <div style="text-align: right"> 2022-02-11 19:56:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Equilibrium Defaultable Corporate Debt and Investment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In dynamic capital structure models with an investor break-even condition,
the firm's Bellman equation may not generate a contraction mapping, so the
standard existence and uniqueness conditions do not apply. First, we provide an
example showing the problem in a classical trade-off model. The firm can issue
one-period defaultable debt, invest in capital and pay a dividend. If the firm
cannot meet the required debt payment, it is liquidated. Second, we show how to
use a dual to the original problem and a change of measure, such that existence
and uniqueness can be proved. In the unique Markov-perfect equilibrium, firm
decisions reflect state-dependent capital and debt targets. Our approach may be
useful for other dynamic firm models that have an investor break-even
condition. </font><br> Link: <a href='http://arxiv.org/pdf/2202.05885v1' target="_blank">http://arxiv.org/pdf/2202.05885v1</a><br> <br> <br> <font size='5'> 914 </font> <div style="text-align: right"> 2022-02-11 15:43:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uncertainties associated with the backward integration of dwarf satellites using simple parametric potentials</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In order to backward integrate the orbits of Milky Way (MW) dwarf galaxies,
much effort has been invested in recent years to constrain their initial
phase-space coordinates. Yet equally important are the assumptions on the
potential that the dwarf galaxies experience over time, especially given the
fact that the MW is currently accreting the Large Magellanic Cloud (LMC). In
this work, using a dark matter-only zoom-in simulation, we test whether the use
of common parametric forms of the potential is adequate to successfully
backward integrate the orbits of the subhaloes from their present-day
positions. We parametrise the recovered orbits and compare them with those from
the simulations. We find that simple symmetric parametric forms of the
potential fail to capture the complexities and the inhomogeneities of the true
potential experienced by the subhaloes. More specifically, modelling a recent
massive accretion like that of the LMC as a sum of two spherical parametric
potentials leads to substantial errors in the recovered parameters of the
orbits. These errors rival those caused due to a) a 30\% uncertainty in the
virial mass of the MW and b) not modelling the potential of the recently
accreted massive satellite. Our work suggests that i) the uncertainties in the
parameters of the recovered orbits of some MW dwarfs may be under-estimated and
that ii) researchers should characterise the uncertainties inherent to their
choice of integration techniques and assumptions of the potential against
cosmological zoom-in simulations of the MW, which include a recently-accreted
LMC. </font><br> Link: <a href='http://arxiv.org/pdf/2202.05707v1' target="_blank">http://arxiv.org/pdf/2202.05707v1</a><br> <br> <br> <font size='5'> 915 </font> <div style="text-align: right"> 2022-02-11 00:08:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Epidemic Compartment Model for Economic Policy Directions for Managing Future Pandemic</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this research, we develop a framework to analyze the interaction between
the economy and the Covid-19 pandemic using an extension of SIR epidemic model.
At the outset, we assume there are two health related investments including
general medical expenditures and the other for a direct investment for
controlling the pandemic. We incorporate the learning dynamics associated with
the management of the virus into our model. Given that the labor force in a
society depends on the state of the epidemic, we allow birth, death, and
vaccination to occur in our model and assume labor force consists of the
susceptible, vaccinated, and recovered individuals. We also assume parameters
in our epidemic compartmental model depend on investment amount for directly
controlling the epidemic, the health stock of individual representative agents
in the society, and the knowledge or learning about the epidemic in the
community. By controlling consumption, the general medical expenditure, and the
direct investment of funds for controlling the epidemic, we optimize the
utility realized by the representative individuals because of consumption. This
problem is nontrivial since the disease dynamics results in a non-convex
optimization problem. </font><br> Link: <a href='http://arxiv.org/pdf/2202.05374v1' target="_blank">http://arxiv.org/pdf/2202.05374v1</a><br> <br> <br> <font size='5'> 916 </font> <div style="text-align: right"> 2022-02-08 07:14:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CyberOps: Situational Awareness in Cybersecurity Operations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cybersecurity operations, CyberOps, is the use and application of
cybersecurity capabilities to a domain, department, organisation or nation. It
is fundamentally to protect digital investments, contribute to national
economic wellbeing by providing a safe, secure and conducive environment to
conduct business and to protect national critical national infrastructures and
citizens welfare. In this paper, we investigate operational factors that
influence situational awareness of CyberOps, specifically, the features that
deals with understanding and comprehension of operational and human factors
aspects and that helps with insights on human operator decision making such as
cognition, teamwork, knowledge, skills and abilities. The operational factors
discussed in this paper range from tools, techniques, integration, architecture
to automation, cognition, people, policy, process and procedures. </font><br> Link: <a href='http://arxiv.org/pdf/2202.03687v1' target="_blank">http://arxiv.org/pdf/2202.03687v1</a><br> <br> <br> <font size='5'> 917 </font> <div style="text-align: right"> 2022-02-07 15:31:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Composable and Modular Code Generation in MLIR: A Structured and Retargetable Approach to Tensor Compiler Construction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite significant investment in software infrastructure, machine learning
systems, runtimes and compilers do not compose properly. We propose a new
design aiming at providing unprecedented degrees of modularity, composability
and genericity. This paper discusses a structured approach to the construction
of domain-specific code generators for tensor compilers, with the stated goal
of improving the productivity of both compiler engineers and end-users. The
approach leverages the natural structure of tensor algebra. It has been the
main driver for the design of progressive lowering paths in \MLIR. The proposed
abstractions and transformations span data structures and control flow with
both functional (SSA form) and imperative (side-effecting) semantics. We
discuss the implications of this infrastructure on compiler construction and
present preliminary experimental results. </font><br> Link: <a href='http://arxiv.org/pdf/2202.03293v1' target="_blank">http://arxiv.org/pdf/2202.03293v1</a><br> <br> <br> <font size='5'> 918 </font> <div style="text-align: right"> 2022-02-07 11:28:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is Metaverse LAND a good investment? It depends on your unit of account!</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Sandbox metaverse LAND non-fungible token (NFT) prices increased by than
300 times (in USD) between December 2019 and January 2022, but when measured in
its native utility token (SAND), the increase is only 3 times. Depending on how
prices are denominated, investment returns and effective transaction prices
vary. We analyze more than 71,000 transactions and find that users are willing
to pay 3-4% more when transactions are settled in SAND, and 30% less when
settled in wETH (a smart contract version of ETH) when compared to ETH, so unit
of account matters. Our results contribute to the discussions of
blockchain-based, virtual economy management and the digitalization of money
(Brunnermeier et al., 2019). </font><br> Link: <a href='http://arxiv.org/pdf/2202.03081v1' target="_blank">http://arxiv.org/pdf/2202.03081v1</a><br> <br> <br> <font size='5'> 919 </font> <div style="text-align: right"> 2022-02-07 08:24:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Withdrawal Success Estimation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Given a geometric Levy alpha-stable wealth process, a log-Levy alpha-stable
lower bound is constructed for the terminal wealth of a regular investing
schedule. Using a transformation, the lower bound is applied to a schedule of
withdrawals occurring after an initial investment. As a result, an upper bound
is described on the probability to complete a given schedule of withdrawals.
For withdrawals of a constant amount at equidistant times, necessary conditions
are given on the initial investment and parameters of the wealth process such
that $k$ withdrawals can be made with 95% confidence. When the initial
investment is in the S&P Composite Index and $2\leq k\leq 16$, then the initial
investment must be at least $k$ times the amount of each withdrawal. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02994v1' target="_blank">http://arxiv.org/pdf/2202.02994v1</a><br> <br> <br> <font size='5'> 920 </font> <div style="text-align: right"> 2022-02-06 20:16:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Grandes fraudes y gobiernos corporativos en la Economa desde mediados del siglo XX</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The international financial system is currently not yet prepared to face a
foreseeable crisis mainly motivated by the dichotomy between the real economy
and the virtual economy. Skepticism is widespread even when it comes to
investments in sustainable economy. The concentration of capital in a few
persons is one of the greatest risks for the possible reiteration of economic
crises. The benevolent sentences of the courts to some of the fraudsters do not
contribute to dispelling the ghost of fraud nor to the disappearance of tax
havens. From the diachronic perspective, it is observed that economic crises
are increasingly frequent and incidents always in the financial field; which
forces us to rethink an economic model on an international scale in which there
is a greater weight of the economic policy of governments over the power of
multinational companies in the context of globalization. In the context of
Corporate Social Responsibility, Corporate Governance is listed as one of the
fundamental levers to curb large business fraud, but its efficiency seems
insufficient due to the lack of international regulations and the ignorance of
hidden forces in what has been known as fiscal and financial engineering. The
application of liberal policies in an unorthodox way is causing real social
gaps in the distribution of income and is undermining the current capitalist
system. The need to implement corporate governments is recommended as one of
the essential formulas for sustaining the international economic system. </font><br> Link: <a href='http://arxiv.org/pdf/2203.12605v1' target="_blank">http://arxiv.org/pdf/2203.12605v1</a><br> <br> <br> <font size='5'> 921 </font> <div style="text-align: right"> 2022-02-06 12:07:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Smart Gaze based Annotation of Histopathology Images for Training of Deep Convolutional Neural Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Unavailability of large training datasets is a bottleneck that needs to be
overcome to realize the true potential of deep learning in histopathology
applications. Although slide digitization via whole slide imaging scanners has
increased the speed of data acquisition, labeling of virtual slides requires a
substantial time investment from pathologists. Eye gaze annotations have the
potential to speed up the slide labeling process. This work explores the
viability and timing comparisons of eye gaze labeling compared to conventional
manual labeling for training object detectors. Challenges associated with gaze
based labeling and methods to refine the coarse data annotations for subsequent
object detection are also discussed. Results demonstrate that gaze tracking
based labeling can save valuable pathologist time and delivers good performance
when employed for training a deep object detector. Using the task of
localization of Keratin Pearls in cases of oral squamous cell carcinoma as a
test case, we compare the performance gap between deep object detectors trained
using hand-labelled and gaze-labelled data. On average, compared to
`Bounding-box' based hand-labeling, gaze-labeling required $57.6\%$ less time
per label and compared to `Freehand' labeling, gaze-labeling required on
average $85\%$ less time per label. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02764v1' target="_blank">http://arxiv.org/pdf/2202.02764v1</a><br> <br> <br> <font size='5'> 922 </font> <div style="text-align: right"> 2022-02-06 05:30:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hydrogen and Battery Storage Technologies for Low Cost Energy Decarbonization in Distribution Networks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep energy decarbonization cannot be achieved without high penetration of
renewables. At higher renewable energy penetrations, the variability and
intermittent nature of solar photovoltaic (PV) electricity can cause ramping
issues with existing fossil fuel generation, requiring longer term energy
storage to increase the reliability of grid operation. A proton exchange
membrane electrolyzer can produce H2and serves as a utility controllable load.
The produced H2 can then be stored and converted back into electricity, or
mixed with natural gas, or used as transportation fuel, or chemical feedstock.
This paper considers the perspective of the distribution system operator that
operates the distributed energy resources on a standard IEEE 33-node
distribution network considering the technical and physical constraints with
the goal of minimizing total investment and operation cost. Different case
studies, at very high PV penetrations are considered to show the challenges and
path to net-zero emission energy production using H2 energy. Sensitivity of
utility PV costs and electrolyzer capital costs on producing H2 at $1/kg are
presented showing that the distribution network could produce 100% renewable
electricity and H2 could be produced with the same price by 2050 with
conservative cost estimates and by 2030 with accelerated cost declines. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02711v1' target="_blank">http://arxiv.org/pdf/2202.02711v1</a><br> <br> <br> <font size='5'> 923 </font> <div style="text-align: right"> 2022-02-04 18:10:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Portfolio Choice with Indivisible and Illiquid Housing Assets: The Case of Spain</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies the investment decision of the Spanish households using a
unique data set, the Spanish Survey of Household Finance (EFF). We propose a
theoretical model in which households, given a fixed investment in housing,
allocate their net wealth across bank time deposits, stocks, and mortgage.
Besides considering housing as an indivisible and illiquid asset that restricts
the portfolio choice decision, we take into account the financial constraints
that households face when they apply for external funding. For every
representative household in the EFF we solve this theoretical problem and
obtain the theoretically optimal portfolio that is compared with households'
actual choices. We find that households significantly underinvest in stocks and
deposits while the optimal and actual mortgage investments are alike.
Considering the three types of financial assets at once, we find that the
households headed by highly financially sophisticated, older, retired, richer,
and unconstrained persons are the ones investing more efficiently. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02280v1' target="_blank">http://arxiv.org/pdf/2202.02280v1</a><br> <br> <br> <font size='5'> 924 </font> <div style="text-align: right"> 2022-02-04 17:46:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Industry Characteristics and Financial Risk Spillovers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper proposes a new measure of tail risk spillover. The empirical
application provides evidence of significant volatility and tail risk
spillovers from the financial sector to many real economy sectors in the U.S.
economy in the period from 2001 to 2011. These spillovers increase in crisis
periods. The conditional coexceedance in a given sector is positively related
to its amount of debt financing, and negatively related to its relative
valuation and investment. Real economy sectors which require substantial
external financing, and whose value and investment activity are relatively
lower, are prime candidates for depreciation in the wake of crisis in the
financial sector. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02263v1' target="_blank">http://arxiv.org/pdf/2202.02263v1</a><br> <br> <br> <font size='5'> 925 </font> <div style="text-align: right"> 2022-02-04 16:06:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Sustainability and Survivability in the Matchbox Two-Sector Model: A Complete Characterization of Optimal Extinction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We provide a complete characterization of optimal extinction in a two-sector
model of economic growth through three results, surprising in both their
simplicity and intricacy. (i) When the discount factor is below a threshold
identified by the well-known $\delta$-normality condition for the existence of
a stationary optimal stock, the economy's capital becomes extinct in the long
run. (ii) This extinction may be staggered if and only if the investment-good
sector is capital intensive. (iii) We uncover a sequence of thresholds of the
discount factor, identified by a family of rational functions, that represent
bifurcations for optimal postponements on the path to extinction. We also
report various special cases of the model having to do with unsustainable
technologies and equal capital intensities that showcase long-term optimal
growth, all of topical interest and all neglected in the antecedent literature. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02209v1' target="_blank">http://arxiv.org/pdf/2202.02209v1</a><br> <br> <br> <font size='5'> 926 </font> <div style="text-align: right"> 2022-02-04 12:41:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Implementation of a Type-2 Fuzzy Logic Based Prediction System for the Nigerian Stock Exchange</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stock Market can be easily seen as one of the most attractive places for
investors, but it is also very complex in terms of making trading decisions.
Predicting the market is a risky venture because of the uncertainties and
nonlinear nature of the market. Deciding on the right time to trade is key to
every successful trader as it can lead to either a huge gain of money or
totally a loss in investment that will be recorded as a careless trade. The aim
of this research is to develop a prediction system for stock market using Fuzzy
Logic Type2 which will handle these uncertainties and complexities of human
behaviour in general when it comes to buy, hold or sell decision making in
stock trading. The proposed system was developed using VB.NET programming
language as frontend and Microsoft SQL Server as backend. A total of four
different technical indicators were selected for this research. The selected
indicators are the Relative Strength Index, William Average, Moving Average
Convergence and Divergence, and Stochastic Oscillator. These indicators serve
as input variable to the Fuzzy System. The MACD and SO are deployed as primary
indicators, while the RSI and WA are used as secondary indicators. Fibonacci
retracement ratio was adopted for the secondary indicators to determine their
support and resistance level in terms of making trading decisions. The input
variables to the Fuzzy System is fuzzified to Low, Medium, and High using the
Triangular and Gaussian Membership Function. The Mamdani Type Fuzzy Inference
rules were used for combining the trading rules for each input variable to the
fuzzy system. The developed system was tested using sample data collected from
ten different companies listed on the Nigerian Stock Exchange for a total of
fifty two periods. The dataset collected are Opening, High, Low, and Closing
prices of each security. </font><br> Link: <a href='http://arxiv.org/pdf/2202.02107v1' target="_blank">http://arxiv.org/pdf/2202.02107v1</a><br> <br> <br> <font size='5'> 927 </font> <div style="text-align: right"> 2022-02-03 05:48:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do new investment strategies take existing strategies' returns -- An investigation into agent-based models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Commodity trading advisors (CTAs), who mainly trade commodity futures, showed
good returns in the 2000s. However, since the 2010's, they have not performed
very well. One possible reason of this phenomenon is the emergence of
short-term reversal traders (STRTs) who prey on CTAs for profit. In this study,
I built an artificial market model by adding a CTA agent (CTAA) and STRT agent
(STRTA) to a prior model and investigated whether emerging STRTAs led to a
decrease in CTAA revenue to determine whether STRTs prey on CTAs for profit. To
the contrary, my results showed that a CTAA and STRTA are more likely to trade
and earn more when both exist. Therefore, it is possible that they have a
mutually beneficial relationship. </font><br> Link: <a href='http://arxiv.org/pdf/2202.01423v1' target="_blank">http://arxiv.org/pdf/2202.01423v1</a><br> <br> <br> <font size='5'> 928 </font> <div style="text-align: right"> 2022-02-02 01:05:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Instability of financial markets by optimizing investment strategies investigated by an agent-based model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Most finance studies are discussed on the basis of several hypotheses, for
example, investors rationally optimize their investment strategies. However,
the hypotheses themselves are sometimes criticized. Market impacts, where
trades of investors can impact and change market prices, making optimization
impossible. In this study, we built an artificial market model by adding
technical analysis strategy agents searching one optimized parameter to a whole
simulation run to the prior model and investigated whether investors' inability
to accurately estimate market impacts in their optimizations leads to
optimization instability. In our results, the parameter of investment strategy
never converged to a specific value but continued to change. This means that
even if all other traders are fixed, only one investor will use backtesting to
optimize his/her strategy, which leads to the time evolution of market prices
becoming unstable. Optimization instability is one level higher than
"non-equilibrium of market prices." Therefore, the time evolution of market
prices produced by investment strategies having such unstable parameters is
highly unlikely to be predicted and have stable laws written by equations. This
nature makes us suspect that financial markets include the principle of natural
uniformity and indicates the difficulty of building an equation model
explaining the time evolution of prices. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00831v1' target="_blank">http://arxiv.org/pdf/2202.00831v1</a><br> <br> <br> <font size='5'> 929 </font> <div style="text-align: right"> 2022-02-02 00:09:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Increase Investment in Accessible Physics Labs: A Call to Action for the Physics Education Community</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The American Association of Physics Teachers (AAPT) Committee on Laboratories
assembled a task force whose charge was to write an open letter to the physics
education community calling for increased investment in accessible lab courses.
Contributors to this paper include students, staff, and faculty with and
without disabilities who expressed interest in the open letter. In this
document, we recognize the need for making physics laboratories more accessible
in all spaces (e.g., high school courses, graduate level courses, research
labs). We focus on the experiences of students with disabilities in physics lab
courses at the undergraduate level because that is the context for which the
writing team had the most collective experience. The intended audiences for
this document consist of undergraduate physics students, staff, and faculty,
especially those who have direct stake in laboratory courses; physics
departments; and member societies, including AAPT.
  We begin by presenting our motivation for the document and the importance of
accessibility and diversity in education and the workforce. We start with the
broader context of accessibility, narrowing our focus to physics education and
the current state of affairs and availability of accessible resources.
Accessibility is then discussed in the specific context of physics laboratory
courses, focusing on how barriers are created and can be lowered. In exploring
ideas and strategies for improving accessibility, we recognize that the
development of multiple pathways for laboratory investigation creates
opportunities to expand learning opportunities for more students in physics lab
programs. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00816v1' target="_blank">http://arxiv.org/pdf/2202.00816v1</a><br> <br> <br> <font size='5'> 930 </font> <div style="text-align: right"> 2022-02-01 18:20:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Log-Concavity of Infinite Product Generating Functions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the $1970$s Nicolas proved that the coefficients $p_d(n)$ defined by the
generating function \begin{equation*} \sum_{n=0}^{\infty} p_d(n) \, q^n =
\prod_{n=1}^{\infty} \left( 1- q^n\right)^{-n^{d-1}} \end{equation*} are
log-concave for $d=1$. Recently, Ono, Pujahari, and Rolen have extended the
result to $d=2$. Note that $p_1(n)=p(n)$ is the partition function and
$p_2(n)=\func{pp}\left( n\right) $ is the number of plane partitions. In this
paper, we invest in properties for $p_d(n)$ for general $d$. Let $n \geq 6$.
Then $p_d(n)$ is almost log-concave for $n$ divisible by $3$ and almost
strictly log-convex otherwise. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00627v2' target="_blank">http://arxiv.org/pdf/2202.00627v2</a><br> <br> <br> <font size='5'> 931 </font> <div style="text-align: right"> 2022-02-01 15:12:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine learning to assess relatedness: the advantage of using firm-level data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The relatedness between a country or a firm and a product is a measure of the
feasibility of that economic activity. As such, it is a driver for investments
at a private and institutional level. Traditionally, relatedness is measured
using networks derived by country-level co-occurrences of product pairs, that
is counting how many countries export both. In this work, we compare networks
and machine learning algorithms trained not only on country-level data, but
also on firms, that is something not much studied due to the low availability
of firm-level data. We quantitatively compare the different measures of
relatedness, by using them to forecast the exports at the country and
firm-level, assuming that more related products have a higher likelihood to be
exported in the future. Our results show that relatedness is scale-dependent:
the best assessments are obtained by using machine learning on the same
typology of data one wants to predict. Moreover, we found that while
relatedness measures based on country data are not suitable for firms,
firm-level data are very informative also for the development of countries. In
this sense, models built on firm data provide a better assessment of
relatedness. We also discuss the effect of using parameter optimization and
community detection algorithms to identify clusters of related companies and
products, finding that a partition into a higher number of blocks decreases the
computational time while maintaining a prediction performance well above the
network-based benchmarks. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00458v3' target="_blank">http://arxiv.org/pdf/2202.00458v3</a><br> <br> <br> <font size='5'> 932 </font> <div style="text-align: right"> 2022-02-01 12:46:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: NTU VIRAL: A Visual-Inertial-Ranging-Lidar Dataset, From an Aerial Vehicle Viewpoint</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, autonomous robots have become ubiquitous in research and
daily life. Among many factors, public datasets play an important role in the
progress of this field, as they waive the tall order of initial investment in
hardware and manpower. However, for research on autonomous aerial systems,
there appears to be a relative lack of public datasets on par with those used
for autonomous driving and ground robots. Thus, to fill in this gap, we conduct
a data collection exercise on an aerial platform equipped with an extensive and
unique set of sensors: two 3D lidars, two hardware-synchronized global-shutter
cameras, multiple Inertial Measurement Units (IMUs), and especially, multiple
Ultra-wideband (UWB) ranging units. The comprehensive sensor suite resembles
that of an autonomous driving car, but features distinct and challenging
characteristics of aerial operations. We record multiple datasets in several
challenging indoor and outdoor conditions. Calibration results and ground truth
from a high-accuracy laser tracker are also included in each package. All
resources can be accessed via our webpage
https://ntu-aris.github.io/ntu_viral_dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00379v1' target="_blank">http://arxiv.org/pdf/2202.00379v1</a><br> <br> <br> <font size='5'> 933 </font> <div style="text-align: right"> 2022-01-31 17:47:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Exploratory Study of Stock Price Movements from Earnings Calls</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Financial market analysis has focused primarily on extracting signals from
accounting, stock price, and other numerical hard data reported in P&L
statements or earnings per share reports. Yet, it is well-known that the
decision-makers routinely use soft text-based documents that interpret the hard
data they narrate. Recent advances in computational methods for analyzing
unstructured and soft text-based data at scale offer possibilities for
understanding financial market behavior that could improve investments and
market equity. A critical and ubiquitous form of soft data are earnings calls.
Earnings calls are periodic (often quarterly) statements usually by CEOs who
attempt to influence investors' expectations of a company's past and future
performance. Here, we study the statistical relationship between earnings
calls, company sales, stock performance, and analysts' recommendations. Our
study covers a decade of observations with approximately 100,000 transcripts of
earnings calls from 6,300 public companies from January 2010 to December 2019.
In this study, we report three novel findings. First, the buy, sell and hold
recommendations from professional analysts made prior to the earnings have low
correlation with stock price movements after the earnings call. Second, using
our graph neural network based method that processes the semantic features of
earnings calls, we reliably and accurately predict stock price movements in
five major areas of the economy. Third, the semantic features of transcripts
are more predictive of stock price movements than sales and earnings per share,
i.e., traditional hard data in most of the cases. </font><br> Link: <a href='http://arxiv.org/pdf/2203.12460v1' target="_blank">http://arxiv.org/pdf/2203.12460v1</a><br> <br> <br> <font size='5'> 934 </font> <div style="text-align: right"> 2022-01-29 13:21:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Offering incentives (e.g., coupons at Amazon, discounts at Uber and video
bonuses at Tiktok) to user is a common strategy used by online platforms to
increase user engagement and platform revenue. Despite its proven
effectiveness, these marketing incentives incur an inevitable cost and might
result in a low ROI (Return on Investment) if not used properly. On the other
hand, different users respond differently to these incentives, for instance,
some users never buy certain products without coupons, while others do anyway.
Thus, how to select the right amount of incentives (i.e. treatment) to each
user under budget constraints is an important research problem with great
practical implications. In this paper, we call such problem as a
budget-constrained treatment selection (BTS) problem.
  The challenge is how to efficiently solve BTS problem on a Large-Scale
dataset and achieve improved results over the existing techniques. We propose a
novel tree-based treatment selection technique under budget constraints, called
Large-Scale Budget-Constrained Causal Forest (LBCF) algorithm, which is also an
efficient treatment selection algorithm suitable for modern distributed
computing systems. A novel offline evaluation method is also proposed to
overcome an intrinsic challenge in assessing solutions' performance for BTS
problem in randomized control trials (RCT) data. We deploy our approach in a
real-world scenario on a large-scale video platform, where the platform gives
away bonuses in order to increase users' campaign engagement duration. The
simulation analysis, offline and online experiments all show that our method
outperforms various tree-based state-of-the-art baselines. The proposed
approach is currently serving over hundreds of millions of users on the
platform and achieves one of the most tremendous improvements over these
months. </font><br> Link: <a href='http://arxiv.org/pdf/2201.12585v2' target="_blank">http://arxiv.org/pdf/2201.12585v2</a><br> <br> <br> <font size='5'> 935 </font> <div style="text-align: right"> 2022-01-28 10:54:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: 1-2-3 Reproducibility for Quantum Software Experiments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Various fields of science face a reproducibility crisis. For quantum software
engineering as an emerging field, it is therefore imminent to focus on proper
reproducibility engineering from the start. Yet the provision of reproduction
packages is almost universally lacking. Actionable advice on how to build such
packages is rare, particularly unfortunate in a field with many contributions
from researchers with backgrounds outside computer science. In this article, we
argue how to rectify this deficiency by proposing a 1-2-3~approach to
reproducibility engineering for quantum software experiments: Using a
meta-generation mechanism, we generate DOI-safe, long-term functioning and
dependency-free reproduction packages. They are designed to satisfy the
requirements of professional and learned societies solely on the basis of
project-specific research artefacts (source code, measurement and configuration
data), and require little temporal investment by researchers. Our scheme
ascertains long-term traceability even when the quantum processor itself is no
longer accessible. By drastically lowering the technical bar, we foster the
proliferation of reproduction packages in quantum software experiments and ease
the inclusion of non-CS researchers entering the field. </font><br> Link: <a href='http://arxiv.org/pdf/2201.12031v1' target="_blank">http://arxiv.org/pdf/2201.12031v1</a><br> <br> <br> <font size='5'> 936 </font> <div style="text-align: right"> 2022-01-28 02:17:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "That's so cute!": The CARE Dataset for Affective Response Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Social media plays an increasing role in our communication with friends and
family, and our consumption of information and entertainment. Hence, to design
effective ranking functions for posts on social media, it would be useful to
predict the affective response to a post (e.g., whether the user is likely to
be humored, inspired, angered, informed). Similar to work on emotion
recognition (which focuses on the affect of the publisher of the post), the
traditional approach to recognizing affective response would involve an
expensive investment in human annotation of training data.
  We introduce CARE$_{db}$, a dataset of 230k social media posts annotated
according to 7 affective responses using the Common Affective Response
Expression (CARE) method. The CARE method is a means of leveraging the signal
that is present in comments that are posted in response to a post, providing
high-precision evidence about the affective response of the readers to the post
without human annotation. Unlike human annotation, the annotation process we
describe here can be iterated upon to expand the coverage of the method,
particularly for new affective responses. We present experiments that
demonstrate that the CARE annotations compare favorably with crowd-sourced
annotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models
for predicting affective response as well as emotion detection, demonstrating
the utility of the dataset for related tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2201.11895v2' target="_blank">http://arxiv.org/pdf/2201.11895v2</a><br> <br> <br> <font size='5'> 937 </font> <div style="text-align: right"> 2022-01-27 19:46:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sampling Rare Conformational Transitions with a Quantum Computer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Spontaneous structural rearrangements play a central role in the organization
and function of complex biomolecular systems. In principle, physics-based
computer simulations like Molecular Dynamics (MD) enable us to investigate
these thermally activated processes with an atomic level of resolution.
However, rare conformational transitions are intrinsically hard to investigate
with MD, because an exponentially large fraction of computational resources
must be invested to simulate thermal fluctuations in metastable states. Path
sampling methods like Transition Path Sampling hold the great promise of
focusing the available computational power on sampling the rare stochastic
transition between metastable states. In these approaches, one of the
outstanding limitations is to generate paths that visit significantly different
regions of the conformational space at a low computational cost. To overcome
these problems we introduce a rigorous approach that integrates a machine
learning algorithm and MD simulations implemented on a classical computer with
adiabatic quantum computing. First, using functional integral methods, we
derive a rigorous low-resolution representation of the system's dynamics, based
on a small set of molecular configurations generated with machine learning.
Then, a quantum annealing machine is employed to explore the transition path
ensemble of this low-resolution theory, without introducing un-physical biasing
forces to steer the system's dynamics. Using the D-Wave quantum computer, we
validate our scheme by simulating a benchmark conformational transition in a
state-of-the-art atomistic description. We show that the quantum computing step
generates uncorrelated trajectories, thus facilitating the sampling of the
transition region in configuration space. Our results provide a new paradigm
for MD simulations to integrate machine learning and quantum computing. </font><br> Link: <a href='http://arxiv.org/pdf/2201.11781v2' target="_blank">http://arxiv.org/pdf/2201.11781v2</a><br> <br> <br> <font size='5'> 938 </font> <div style="text-align: right"> 2022-01-27 10:56:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Human-centered mechanism design with Democratic AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation. </font><br> Link: <a href='http://arxiv.org/pdf/2201.11441v1' target="_blank">http://arxiv.org/pdf/2201.11441v1</a><br> <br> <br> <font size='5'> 939 </font> <div style="text-align: right"> 2022-01-26 18:48:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Learning for Stock Prediction Based on Fundamental Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Application of machine learning for stock prediction is attracting a lot of
attention in recent years. A large amount of research has been conducted in
this area and multiple existing results have shown that machine learning
methods could be successfully used toward stock predicting using stocks
historical data. Most of these existing approaches have focused on short term
prediction using stocks historical price and technical indicators. In this
paper, we prepared 22 years worth of stock quarterly financial data and
investigated three machine learning algorithms: Feed-forward Neural Network
(FNN), Random Forest (RF) and Adaptive Neural Fuzzy Inference System (ANFIS)
for stock prediction based on fundamental analysis. In addition, we applied RF
based feature selection and bootstrap aggregation in order to improve model
performance and aggregate predictions from different models. Our results show
that RF model achieves the best prediction results, and feature selection is
able to improve test performance of FNN and ANFIS. Moreover, the aggregated
model outperforms all baseline models as well as the benchmark DJIA index by an
acceptable margin for the test period. Our findings demonstrate that machine
learning models could be used to aid fundamental analysts with decision-making
regarding stock investment. </font><br> Link: <a href='http://arxiv.org/pdf/2202.05702v1' target="_blank">http://arxiv.org/pdf/2202.05702v1</a><br> <br> <br> <font size='5'> 940 </font> <div style="text-align: right"> 2022-01-26 11:53:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FiNCAT: Financial Numeral Claim Analysis Tool</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While making investment decisions by reading financial documents, investors
need to differentiate between in-claim and outof-claim numerals. In this paper,
we present a tool which does it automatically. It extracts context embeddings
of the numerals using one of the transformer based pre-trained language model
called BERT. After this, it uses a Logistic Regression based model to detect
whether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)
dataset to train our model. After conducting rigorous experiments we achieve a
Macro F1 score of 0.8223 on the validation set. We have open-sourced this tool
and it can be accessed from
https://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool </font><br> Link: <a href='http://arxiv.org/pdf/2202.00631v1' target="_blank">http://arxiv.org/pdf/2202.00631v1</a><br> <br> <br> <font size='5'> 941 </font> <div style="text-align: right"> 2022-01-26 03:20:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Income Inequality, Cause and Cure</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We argue that the recent growth in income inequality is driven by disparate
growth in investment income rather than by disparate growth in wages.
Specifically, we present evidence that real wages are flat across a range of
professions, doctors, software engineers, auto mechanics and cashiers, while
stock ownership favors higher education and income levels. Artificial
Intelligence and automation allocate an increased share of job tasks towards
capital and away from labor. The rewards of automation accrue to capital, and
are reflected in the growth of the stock market with several companies now
valued in the trillions. We propose a Deferred Investment Payroll plan to
enable all workers to participate in the rewards of automation and analyze the
performance of such a plan.
  JEL Classification: J31, J33, O33 </font><br> Link: <a href='http://arxiv.org/pdf/2201.10726v5' target="_blank">http://arxiv.org/pdf/2201.10726v5</a><br> <br> <br> <font size='5'> 942 </font> <div style="text-align: right"> 2022-01-25 23:30:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust Comparative Statics for the Elasticity of Intertemporal Substitution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study a general class of consumption-savings problems with recursive
preferences. We characterize the sign of the consumption response to arbitrary
shocks in terms of the product of two sufficient statistics: the elasticity of
intertemporal substitution between contemporaneous consumption and continuation
utility (EIS), and the relative elasticity of the marginal value of wealth
(REMV). Under homotheticity, the REMV always equals one, so the propensity of
the agent to save or dis-save is always signed by the relationship of the EIS
with unity. We apply our results to derive comparative statics in classical
problems of portfolio allocation, consumption-savings with income risk, and
entrepreneurial investment. Our results suggest empirical identification
strategies for both the value of the EIS and its relationship with unity. </font><br> Link: <a href='http://arxiv.org/pdf/2201.10673v1' target="_blank">http://arxiv.org/pdf/2201.10673v1</a><br> <br> <br> <font size='5'> 943 </font> <div style="text-align: right"> 2022-01-25 22:00:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Coded Caching with Heterogeneous User Profiles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Coded caching utilizes pre-fetching during off-peak hours and multi-casting
for delivery in order to balance the traffic load in communication networks.
Several works have studied the achievable peak and average rates under
different conditions: variable file lengths or popularities, variable cache
sizes, decentralized networks, etc. However, very few have considered the
possibility of heterogeneous user profiles, despite modern content providers
are investing heavily in categorizing users according to their habits and
preferences.
  This paper proposes three coded caching schemes with uncoded pre-fetching for
scenarios where end users are grouped into classes with different file demand
sets (FDS). One scheme ignores the difference between the classes, another
ignores the intersection between them and the third decouples the delivery of
files common to all FDS from those unique to a single class. The transmission
rates of the three schemes are compared with a lower bound to evaluate their
gap to optimality, and with each other to show that each scheme can outperform
the other two when certain conditions are met. </font><br> Link: <a href='http://arxiv.org/pdf/2201.10646v1' target="_blank">http://arxiv.org/pdf/2201.10646v1</a><br> <br> <br> <font size='5'> 944 </font> <div style="text-align: right"> 2022-01-25 07:13:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Autonomous Vehicles: Open-Source Technologies, Considerations, and Development</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Autonomous vehicles are the culmination of advances in many areas such as
sensor technologies, artificial intelligence (AI), networking, and more. This
paper will introduce the reader to the technologies that build autonomous
vehicles. It will focus on open-source tools and libraries for autonomous
vehicle development, making it cheaper and easier for developers and
researchers to participate in the field. The topics covered are as follows.
First, we will discuss the sensors used in autonomous vehicles and summarize
their performance in different environments, costs, and unique features. Then
we will cover Simultaneous Localization and Mapping (SLAM) and algorithms for
each modality. Third, we will review popular open-source driving simulators, a
cost-effective way to train machine learning models and test vehicle software
performance. We will then highlight embedded operating systems and the security
and development considerations when choosing one. After that, we will discuss
Vehicle-to-Vehicle (V2V) and Internet-of-Vehicle (IoV) communication, which are
areas that fuse networking technologies with autonomous vehicles to extend
their functionality. We will then review the five levels of vehicle automation,
commercial and open-source Advanced Driving Assistance Systems, and their
features. Finally, we will touch on the major manufacturing and software
companies involved in the field, their investments, and their partnerships.
These topics will give the reader an understanding of the industry, its
technologies, active research, and the tools available for developers to build
autonomous vehicles. </font><br> Link: <a href='http://arxiv.org/pdf/2202.03148v2' target="_blank">http://arxiv.org/pdf/2202.03148v2</a><br> <br> <br> <font size='5'> 945 </font> <div style="text-align: right"> 2022-01-25 01:37:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Carbon Explorer: A Holistic Approach for Designing Carbon Aware Datacenters</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Technology companies have been leading the way to a renewable energy
transformation, by investing in renewable energy sources to reduce the carbon
footprint of their datacenters. In addition to helping build new solar and wind
farms, companies make power purchase agreements or purchase carbon offsets,
rather than relying on renewable energy every hour of the day, every day of the
week (24/7). Relying on renewable energy 24/7 is challenging due to the
intermittent nature of wind and solar energy. Inherent variations in solar and
wind energy production causes excess or lack of supply at different times. To
cope with the fluctuations of renewable energy generation, multiple solutions
must be applied. These include: capacity sizing with a mix of solar and wind
power, energy storage options, and carbon aware workload scheduling. However,
depending on the region and datacenter workload characteristics, the
carbon-optimal solution varies. Existing work in this space does not give a
holistic view of the trade-offs of each solution and often ignore the embodied
carbon cost of the solutions. In this work, we provide a framework, Carbon
Explorer, to analyze the multi-dimensional solution space by taking into
account operational and embodided footprint of the solutions to help make
datacenters operate on renewable energy 24/7. The solutions we analyze include
capacity sizing with a mix of solar and wind power, battery storage, and carbon
aware workload scheduling, which entails shifting the workloads from times when
there is lack of renewable supply to times with abundant supply. </font><br> Link: <a href='http://arxiv.org/pdf/2201.10036v3' target="_blank">http://arxiv.org/pdf/2201.10036v3</a><br> <br> <br> <font size='5'> 946 </font> <div style="text-align: right"> 2022-01-24 00:47:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Power Forward Performance in Semimartingale Markets with Stochastic Integrated Factors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the forward investment performance process (FIPP) in an incomplete
semimartingale market model with closed and convex portfolio constraints, when
the investor's risk preferences are of the power form. We provide necessary and
sufficient conditions for the existence of such FIPP. In a semimartingale
factor model, we show that the FIPP can be recovered as a triplet of processes
which admit an integral representation with respect to semimartingales. Using
an integrated stochastic factor model, we relate the factor representation of
the triplet of processes to the smooth solution of an ill-posed partial
integro-differential Hamilton-Jacobi-Bellman (HJB) equation. We develop
explicit constructions for the class of time-monotone FIPPs, generalizing
existing results from Brownian to semimartingale market models. </font><br> Link: <a href='http://arxiv.org/pdf/2201.09406v2' target="_blank">http://arxiv.org/pdf/2201.09406v2</a><br> <br> <br> <font size='5'> 947 </font> <div style="text-align: right"> 2022-01-21 08:18:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reinforcement Learning Your Way: Agent Characterization through Policy Regularization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The increased complexity of state-of-the-art reinforcement learning (RL)
algorithms have resulted in an opacity that inhibits explainability and
understanding. This has led to the development of several post-hoc
explainability methods that aim to extract information from learned policies
thus aiding explainability. These methods rely on empirical observations of the
policy and thus aim to generalize a characterization of agents' behaviour. In
this study, we have instead developed a method to imbue a characteristic
behaviour into agents' policies through regularization of their objective
functions. Our method guides the agents' behaviour during learning which
results in an intrinsic characterization; it connects the learning process with
model explanation. We provide a formal argument and empirical evidence for the
viability of our method. In future work, we intend to employ it to develop
agents that optimize individual financial customers' investment portfolios
based on their spending personalities. </font><br> Link: <a href='http://arxiv.org/pdf/2201.10003v1' target="_blank">http://arxiv.org/pdf/2201.10003v1</a><br> <br> <br> <font size='5'> 948 </font> <div style="text-align: right"> 2022-01-19 07:48:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Forecasting the distribution of long-horizon returns with time-varying volatility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The study of long-horizon returns has received a great deal of attention in
recent years (see, for example, Boudoukh, Richardson, and Whitelaw (2008),
Neuberger (2012) and Lee (2013), Fama and French (2018)). While most of the
discussions are concerned with some practical issues in investment, few have
touched the important aspect on risk management. The approach adopted in this
article is to predict the future distribution of the returns of a fixed
long-horizon by which the risk measures of interest that come in the form of a
distributional functional such as the value at risk (VaR) and the conditional
tail expectation (CTE) can be easily derived. The characteristic feature of our
approach which requires no specification of the volatility dynamics nor
parametric assumptions of the shock distribution extends the work by Ho et al.
(2016) and Ho ( 2017) to a more general volatility dynamics that includes both
the widely-used SV model and the GARCH model (Bollerslev, 1986) as special
cases. </font><br> Link: <a href='http://arxiv.org/pdf/2201.07457v1' target="_blank">http://arxiv.org/pdf/2201.07457v1</a><br> <br> <br> <font size='5'> 949 </font> <div style="text-align: right"> 2022-01-19 05:51:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Small Satellite Mission Concepts for Space Weather Research and as Pathfinders for Operations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in miniaturization and commercial availability of critical
satellite subsystems and detector technology have made small satellites
(SmallSats, including CubeSats) an attractive, low-cost potential solution for
space weather research and operational needs. Motivated by the 1st
International Workshop on SmallSats for Space Weather Research and Forecasting,
held in Washington, DC on 1-4 August 2017, we discuss the need for advanced
space weather measurement capabilities, driven by analyses from the World
Meteorological Organization (WMO), and how SmallSats can efficiently fill these
measurement gaps. We present some current, recent missions and
proposed/upcoming mission concepts using SmallSats that enhance space weather
research and provide prototyping pathways for future operational applications;
how they relate to the WMO requirements; and what challenges remain to be
overcome to meet the WMO goals and operational needs in the future. With
additional investment from cognizant funding agencies worldwide, SmallSats --
including standalone missions and constellations -- could significantly enhance
space weather research and, eventually, operations, by reducing costs and
enabling new measurements not feasible from traditional, large, monolithic
missions. </font><br> Link: <a href='http://arxiv.org/pdf/2201.07426v1' target="_blank">http://arxiv.org/pdf/2201.07426v1</a><br> <br> <br> <font size='5'> 950 </font> <div style="text-align: right"> 2022-01-18 17:24:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Safe Online Bid Optimization with Return-On-Investment and Budget Constraints subject to Uncertainty</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In online marketing, the advertisers' goal is usually a tradeoff between
achieving high volumes and high profitability. The companies' business units
customarily address this tradeoff by maximizing the volumes while guaranteeing
a lower bound to the Return On Investment (ROI). This paper investigates
combinatorial bandit algorithms for the bid optimization of advertising
campaigns subject to uncertain budget and ROI constraints. We study the nature
of both the optimization and learning problems. In particular, when focusing on
the optimization problem without uncertainty, we show that it is inapproximable
within any factor unless P=NP, and we provide a pseudo-polynomial-time
algorithm that achieves an optimal solution. When considering uncertainty, we
prove that no online learning algorithm can violate the (ROI or budget)
constraints during the learning process a sublinear number of times while
guaranteeing a sublinear pseudo-regret. Thus, we provide an algorithm, namely
GCB, guaranteeing sublinear regret at the cost of a potentially linear number
of constraints violations. We also design its safe version, namely GCB_{safe},
guaranteeing w.h.p. a constant upper bound on the number of constraints
violations at the cost of a linear pseudo-regret. More interestingly, we
provide an algorithm, namely GCB_{safe}(\psi,\phi), guaranteeing both sublinear
pseudo-regret and safety w.h.p. at the cost of accepting tolerances \psi and
\phi in the satisfaction of the ROI and budget constraints, respectively. This
algorithm actually mitigates the risks due to the constraints violations
without precluding the convergence to the optimal solution. Finally, we
experimentally compare our algorithms in terms of
pseudo-regret/constraint-violation tradeoff in settings generated from
real-world data, showing the importance of adopting safety constraints in
practice and the effectiveness of our algorithms. </font><br> Link: <a href='http://arxiv.org/pdf/2201.07139v1' target="_blank">http://arxiv.org/pdf/2201.07139v1</a><br> <br> <br> <font size='5'> 951 </font> <div style="text-align: right"> 2022-01-18 13:52:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CloudCast: Characterizing Public Clouds Connectivity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Public clouds are one of the most thriving technologies of the past decade.
Major applications over public clouds require world-wide distribution and large
amounts of data exchange between their distributed servers. To that end, major
cloud providers have invested tens of billions of dollars in building
world-wide inter-region networking infrastructure that can support high
performance communication into, out of, and across public cloud geographic
regions. In this paper, we lay the foundation for a comprehensive study and
real time monitoring of various characteristic of networking within and between
public clouds. We start by presenting CloudCast, a world-wide and expandable
measurements and analysis system, currently (January 2019)collecting data from
three major public clouds (AWS, GCPand Azure), 59 regions, 1184 intra-cloud and
2238 cross-cloud links (each link represents a direct connection between a pair
of regions), amounting to a total of 3422 continuously monitored links and
providing active measurements every minute.CloudCast is composed of measurement
agents automatically installed in each public cloud region, centralized
control, measurement data base, analysis engine and visualization tools. Then
we turn to analyze the latency measurement data collected over almost a year .
Our analysis yields surprising results. First, each public cloud exhibits a
unique set of link latency behaviors along time. Second, using a novel, fair
evaluation methodology, termed similar links, we compare the three clouds.
Third, we prove that more than 50% of all links do not provide the optimal RTT
through the methodology of triangles. Triangles also provide a framework to get
around bottlenecks, benefiting not only the majority (53%-70%) of the
cross-cloud links by 30% to 70%, but also a significant portion (29%-45%) of
intra-cloud links by 14%-33%. </font><br> Link: <a href='http://arxiv.org/pdf/2201.06989v2' target="_blank">http://arxiv.org/pdf/2201.06989v2</a><br> <br> <br> <font size='5'> 952 </font> <div style="text-align: right"> 2022-01-17 06:24:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robust equilibrium strategy for mean-variance-skewness portfolio selection problem</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper considers a robust time-consistent mean-variance-skewness
portfolio selection problem for an ambiguity-averse investor by taking into
account wealth-dependent risk aversion and wealth-dependent skewness preference
as well as model uncertainty. The robust equilibrium investment strategy and
corresponding equilibrium value function are characterized for such a problem
by employing an extended Hamilton-Jacobi-Bellman-Isaacs (HJBI) system via a
game theoretic approach. Furthermore, the robust equilibrium investment
strategy and corresponding equilibrium value function are obtained in
semi-closed form for a special robust time-consistent mean-variance-skewness
portfolio selection problem. Finally, some numerical experiments are provided
to indicate several new findings concerned with the robust equilibrium
investment strategy and the utility losses. </font><br> Link: <a href='http://arxiv.org/pdf/2201.06233v1' target="_blank">http://arxiv.org/pdf/2201.06233v1</a><br> <br> <br> <font size='5'> 953 </font> <div style="text-align: right"> 2022-01-15 18:45:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Profitable Strategy Design by Using Deep Reinforcement Learning for Trades on Cryptocurrency Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep Reinforcement Learning solutions have been applied to different control
problems with outperforming and promising results. In this research work we
have applied Proximal Policy Optimization, Soft Actor-Critic and Generative
Adversarial Imitation Learning to strategy design problem of three
cryptocurrency markets. Our input data includes price data and technical
indicators. We have implemented a Gym environment based on cryptocurrency
markets to be used with the algorithms. Our test results on unseen data shows a
great potential for this approach in helping investors with an expert system to
exploit the market and gain profit. Our highest gain for an unseen 66 day span
is 4850 US dollars per 10000 US dollars investment. We also discuss on how a
specific hyperparameter in the environment design can be used to adjust risk in
the generated strategies. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05906v1' target="_blank">http://arxiv.org/pdf/2201.05906v1</a><br> <br> <br> <font size='5'> 954 </font> <div style="text-align: right"> 2022-01-15 15:32:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: QoS-Aware Resource Placement for LEO Satellite Edge Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the advent of large LEO satellite communication networks to provide
global broadband Internet access, interest in providing edge computing
resources within LEO networks has emerged. The LEO Edge promises low-latency,
high-bandwidth access to compute and storage resources for a global base of
clients and IoT devices regardless of their geographical location.
  Current proposals assume compute resources or service replicas at every LEO
satellite, which requires high upfront investments and can lead to
over-provisioning. To implement and use the LEO Edge efficiently, methods for
server and service placement are required that help select an optimal subset of
satellites as server or service replica locations. In this paper, we show how
the existing research on resource placement on a 2D torus can be applied to
this problem by leveraging the unique topology of LEO satellite networks.
Further, we extend the existing discrete resource placement methods to allow
placement with QoS constraints. In simulation of proposed LEO satellite
communication networks, we show how QoS depends on orbital parameters and that
our proposed method can take these effects into account where the existing
approach cannot. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05872v2' target="_blank">http://arxiv.org/pdf/2201.05872v2</a><br> <br> <br> <font size='5'> 955 </font> <div style="text-align: right"> 2022-01-14 23:23:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How easy is it for investment managers to deploy their talent in green and brown stocks?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We explore the realized alpha-performance heterogeneity in green and brown
stocks' universes using the peer performance ratios of Ardia and Boudt (2018).
Focusing on S&P 500 index firms over 2014-2020 and defining peer groups in
terms of firms' greenhouse gas emission levels, we find that, on average, about
20% of the stocks differentiate themselves from their peers in terms of future
performance. We see a much higher time-variation in this opportunity set within
brown stocks. Furthermore, the performance heterogeneity has decreased over
time, especially for green stocks, implying that it is now more difficult for
investment managers to deploy their skills when choosing among low-GHG
intensity stocks. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05709v2' target="_blank">http://arxiv.org/pdf/2201.05709v2</a><br> <br> <br> <font size='5'> 956 </font> <div style="text-align: right"> 2022-01-14 17:24:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Precise Stock Price Prediction for Robust Portfolio Design from Selected Sectors of the Indian Stock Market</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stock price prediction is a challenging task and a lot of propositions exist
in the literature in this area. Portfolio construction is a process of choosing
a group of stocks and investing in them optimally to maximize the return while
minimizing the risk. Since the time when Markowitz proposed the Modern
Portfolio Theory, several advancements have happened in the area of building
efficient portfolios. An investor can get the best benefit out of the stock
market if the investor invests in an efficient portfolio and could take the buy
or sell decision in advance, by estimating the future asset value of the
portfolio with a high level of precision. In this project, we have built an
efficient portfolio and to predict the future asset value by means of
individual stock price prediction of the stocks in the portfolio. As part of
building an efficient portfolio we have studied multiple portfolio optimization
methods beginning with the Modern Portfolio theory. We have built the minimum
variance portfolio and optimal risk portfolio for all the five chosen sectors
by using past daily stock prices over the past five years as the training data,
and have also conducted back testing to check the performance of the portfolio.
A comparative study of minimum variance portfolio and optimal risk portfolio
with equal weight portfolio is done by backtesting. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05570v1' target="_blank">http://arxiv.org/pdf/2201.05570v1</a><br> <br> <br> <font size='5'> 957 </font> <div style="text-align: right"> 2022-01-14 16:38:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utilizing geospatial data for assessing energy security: Mapping small solar home systems using unmanned aerial vehicles and deep learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Solar home systems (SHS), a cost-effective solution for rural communities far
from the grid in developing countries, are small solar panels and associated
equipment that provides power to a single household. A crucial resource for
targeting further investment of public and private resources, as well as
tracking the progress of universal electrification goals, is shared access to
high-quality data on individual SHS installations including information such as
location and power capacity. Though recent studies utilizing satellite imagery
and machine learning to detect solar panels have emerged, they struggle to
accurately locate many SHS due to limited image resolution (some small solar
panels only occupy several pixels in satellite imagery). In this work, we
explore the viability and cost-performance tradeoff of using automatic SHS
detection on unmanned aerial vehicle (UAV) imagery as an alternative to
satellite imagery. More specifically, we explore three questions: (i) what is
the detection performance of SHS using drone imagery; (ii) how expensive is the
drone data collection, compared to satellite imagery; and (iii) how well does
drone-based SHS detection perform in real-world scenarios. We collect and
publicly-release a dataset of high-resolution drone imagery encompassing SHS
imaged under real-world conditions and use this dataset and a dataset from
Rwanda to evaluate the capabilities of deep learning models to recognize SHS,
including those that are too small to be reliably recognized in satellite
imagery. The results suggest that UAV imagery may be a viable alternative to
identify very small SHS from perspectives of both detection accuracy and
financial costs of data collection. UAV-based data collection may be a
practical option for supporting electricity access planning strategies for
achieving sustainable development goals and for monitoring the progress towards
those goals. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05548v2' target="_blank">http://arxiv.org/pdf/2201.05548v2</a><br> <br> <br> <font size='5'> 958 </font> <div style="text-align: right"> 2022-01-14 10:25:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Strategic mean-variance investing under mean-reverting stock returns</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this report we derive the strategic (deterministic) allocation to bonds
and stocks resulting in the optimal mean-variance trade-off on a given
investment horizon. The underlying capital market features a mean-reverting
process for equity returns, and the primary question of interest is how
mean-reversion effects the optimal strategy and the resulting portfolio value
at the horizon. In particular, we are interested in knowing under which
assumptions and on which horizons, the risk-reward trade-off is so favourable
that the value of the portfolio is effectively bounded from below on the
horizon. In this case, we might think of the portfolio as providing a
stochastic excess return on top of a "guarantee" (the lower bound).
  Deriving optimal strategies is a well-known discipline in mathematical
finance. The modern approach is to derive and solve the Hamilton-Jacobi-Bellman
(HJB) differential equation characterizing the strategy leading to highest
expected utility, for given utility function. However, for two reasons we
approach the problem differently in this work. First, we wish to find the
optimal strategy depending on time only, i.e., we do not allow for dependencies
on capital market state variables, nor the value of the portfolio itself. This
constraint characterizes the strategic allocation of long-term investors.
Second, to gain insights on the role of mean-reversion, we wish to identify the
entire family of extremal strategies, not only the optimal strategies. To
derive the strategies we employ methods from calculus of variations, rather
than the usual HJB approach. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05375v1' target="_blank">http://arxiv.org/pdf/2201.05375v1</a><br> <br> <br> <font size='5'> 959 </font> <div style="text-align: right"> 2022-01-13 17:47:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis of a five-factor capital market model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper we analyse the five-factor capital market model of Munk et
al.(2004). The model features a Vasicek interest rate model, an equity index
with mean-reverting excess return and an index for realized inflation with
mean-reverting expectation. The primary aim of the analysis is to facilitate
so-called exact simulation from the model on a set of discrete time points. It
turns out that this can be achieved by sampling from a (degenerate)
seven-dimensional normal distribution. We derive the distributional results
necessary and describe how to overcome the rank deficiency of the
variance-covariance matrix in practice.
  The tradeable assets in the original model consist of cash, nominal bonds and
stocks. We extend the investment universe to also include inflation bonds by
deriving the arbitrage free break-even inflation (BEI) curve for a
three-parameter specification of the two market prices of inflation risk.
Finally, we provide a number of auxiliary results regarding the dynamics of
constant-maturity nominal and inflation bond indices, the distribution of the
stock index in nominal and real terms, and the distribution of the Sharpe ratio
for individual assets and portfolios with an application to factor investing. </font><br> Link: <a href='http://arxiv.org/pdf/2201.05103v1' target="_blank">http://arxiv.org/pdf/2201.05103v1</a><br> <br> <br> <font size='5'> 960 </font> <div style="text-align: right"> 2022-01-13 13:34:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pricing Time-to-Event Contingent Cash Flows: A Discrete-Time Survival Analysis Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Prudent management of insurance investment portfolios requires competent
asset pricing of fixed-income assets with time-to-event contingent cash flows,
such as consumer asset-backed securities (ABS). Current market pricing
techniques for these assets either rely on a non-random time-to-event model or
may not utilize detailed asset-level data that is now available with most
public transactions. We first establish a framework capable of yielding
estimates of the time-to-event random variable from securitization data, which
is discrete and often subject to left-truncation and right-censoring. We then
show that the vector of discrete-time hazard rate estimators is asymptotically
multivariate normal with independent components, which has not yet been done in
the statistical literature in the case of both left-truncation and
right-censoring. The time-to-event distribution estimates are then fed into our
cash flow model, which is capable of calculating a formulaic price of a pool of
time-to-event contingent cash flows vis-\'{a}-vis calculating an expected
present value with respect to the estimated time-to-event distribution. In an
application to a subset of 29,845 36-month leases from the Mercedes-Benz Auto
Lease Trust 2017-A (MBALT 2017-A) bond, our pricing model yields estimates
closer to the actual realized future cash flows than the non-random
time-to-event model, especially as the fitting window increases. Finally, in
certain settings, the asymptotic properties of the hazard rate estimators allow
investors to assess the potential uncertainty of the price point estimates,
which we illustrate for a subset of 493 24-month leases from MBALT 2017-A. </font><br> Link: <a href='http://arxiv.org/pdf/2201.04981v2' target="_blank">http://arxiv.org/pdf/2201.04981v2</a><br> <br> <br> <font size='5'> 961 </font> <div style="text-align: right"> 2022-01-11 15:16:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Graph-based Methodology for the Sensorless Estimation of Road Traffic Profiles</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Traffic forecasting models rely on data that needs to be sensed, processed,
and stored. This requires the deployment and maintenance of traffic sensing
infrastructure, often leading to unaffordable monetary costs. The lack of
sensed locations can be complemented with synthetic data simulations that
further lower the economical investment needed for traffic monitoring. One of
the most common data generative approaches consists of producing real-like
traffic patterns, according to data distributions from analogous roads. The
process of detecting roads with similar traffic is the key point of these
systems. However, without collecting data at the target location no flow
metrics can be employed for this similarity-based search. We present a method
to discover locations among those with available traffic data by inspecting
topological features. These features are extracted from domain-specific
knowledge as numerical representations (embeddings) to compare different
locations and eventually find roads with analogous daily traffic profiles based
on the similarity between embeddings. The performance of this novel selection
system is examined and compared to simpler traffic estimation approaches. After
finding a similar source of data, a generative method is used to synthesize
traffic profiles. Depending on the resemblance of the traffic behavior at the
sensed road, the generation method can be fed with data from one road only.
Several generation approaches are analyzed in terms of the precision of the
synthesized samples. Above all, this work intends to stimulate further research
efforts towards enhancing the quality of synthetic traffic samples and thereby,
reducing the need for sensing infrastructure. </font><br> Link: <a href='http://arxiv.org/pdf/2201.04968v2' target="_blank">http://arxiv.org/pdf/2201.04968v2</a><br> <br> <br> <font size='5'> 962 </font> <div style="text-align: right"> 2022-01-11 14:34:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Electric vehicle charge scheduling with flexible service operations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Operators who deploy large fleets of electric vehicles often face a
challenging charge scheduling problem. Specifically, time-ineffective
recharging operations limit the profitability of charging during service
operations such that operators recharge vehicles off-duty at a central depot.
Here, high investment cost and grid capacity limit available charging
infrastructure such that operators need to schedule charging operations to keep
the fleet operational. In this context, flexible service operations, i.e.
allowing to delay or expedite vehicle departures, can potentially increase
charger utilization. Beyond this, jointly scheduling charging and service
operations promises operational cost savings through better utilization of
Time-of-Use energy tariffs and carefully crafted charging schedules designed to
minimize battery wear. Against this background, we study the resulting joint
charging and service operations scheduling problem accounting for battery
degradation, non-linear charging, and Time-of-Use energy tariffs. We propose an
exact Branch & Price algorithm, leveraging a custom branching rule and a primal
heuristic to remain efficient during the Branch & Bound phase. Moreover, we
develop an exact labeling algorithm for our pricing problem, constituting a
resource-constrained shortest path problem that considers variable energy
prices and non-linear charging operations. We benchmark our algorithm in a
comprehensive numerical study and show that it can solve problem instances of
realistic size with computational times below one hour, thus enabling its
application in practice. Additionally, we analyze the benefit of jointly
scheduling charging and service operations. We find that our integrated
approach lowers the amount of charging infrastructure required by up to 57%
besides enabling operational cost savings of up to 5%. </font><br> Link: <a href='http://arxiv.org/pdf/2201.03972v2' target="_blank">http://arxiv.org/pdf/2201.03972v2</a><br> <br> <br> <font size='5'> 963 </font> <div style="text-align: right"> 2022-01-11 12:25:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The regulation methods of fiscal risk in the framework of the implementation of entrepreneurship support</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this article, I consider the issues of financial resource shortage, the
limited possibility of attracting bank loans, and business risk. All these
problems constrain the development of entrepreneurship. Objectives I aim to
determine a scientifically based financial mechanism of financing the priority
directions of territories' development. In this study, I develop tools of
financing the priority directions of the municipal economy. The proposed
financial scheme allows to expand the volume of financing and ensure the access
of businesses to financial support. The article proposes concrete financing
mechanisms for investment with minimal risk for the budget and preferential
conditions for business. </font><br> Link: <a href='http://arxiv.org/pdf/2202.00108v1' target="_blank">http://arxiv.org/pdf/2202.00108v1</a><br> <br> <br> <font size='5'> 964 </font> <div style="text-align: right"> 2022-01-10 20:17:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Role of the Magnetic Anisotropy in Atomic-Spin Sensing of 1D Molecular Chains</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One-dimensional metal-organic chains often possess a complex magnetic
structure susceptible to be modified by a alteration of their chemical
composition. The possibility to tune their magnetic properties provides an
interesting playground to explore quasiparticle interactions in low-dimensional
systems. Despite the great effort invested so far, a detailed understanding of
the interactions governing the electronic and magnetic properties of the
low-dimensional systems is still incomplete. One of the reasons is the limited
ability to characterize their magnetic properties at the atomic scale. Here, we
provide a comprehensive study of the magnetic properties of metal-organic
one-dimensional (1D) coordination polymers consisting of
2,5-diamino-1,4-benzoquinonediimine ligands coordinated with Co or Cr atoms
synthesized in ultra-high vacuum conditions on a Au(111) surface. A combination
of an integral X-ray spectroscopy with local-probe inelastic electron tunneling
spectroscopy corroborated by multiplet analysis, density functional theory, and
inelastic electron tunneling simulations enable us to obtain essential
information about their magnetic structure, including the spin magnitude and
orientation at the magnetic atoms, as well as the magnetic anisotropy. </font><br> Link: <a href='http://arxiv.org/pdf/2201.03627v1' target="_blank">http://arxiv.org/pdf/2201.03627v1</a><br> <br> <br> <font size='5'> 965 </font> <div style="text-align: right"> 2022-01-09 11:31:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Portfolio selection models based on interval-valued conditional value at risk (ICVaR) and empirical analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Risk management is very important for individual investors or companies.
There are many ways to measure the risk of investment. Prices of risky assets
vary rapidly and randomly due to the complexity of finance market. Random
interval is a good tool to describe uncertainty with both randomness and
imprecision. Considering the uncertainty of financial market, we employ random
intervals to describe the returns of a risk asset and consider the tail risk,
which is called the interval-valued Conditional Value at Risk (ICVaR, for
short). Such an ICVaR is a risk measure and satisfies subadditivity. Under the
new risk measure ICVaR, as a manner similar to the classical portfolio model of
Markowitz, optimal interval-valued portfolio selection models are built. Based
on the real data from mainland Chinese stock market, the case study shows that
our models are interpretable and consistent with the practical scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2201.02987v2' target="_blank">http://arxiv.org/pdf/2201.02987v2</a><br> <br> <br> <font size='5'> 966 </font> <div style="text-align: right"> 2022-01-07 15:58:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Development of an Extractive Clinical Question Answering Dataset with Multi-Answer and Multi-Focus Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Extractive question-answering (EQA) is a useful natural language
processing (NLP) application for answering patient-specific questions by
locating answers in their clinical notes. Realistic clinical EQA can have
multiple answers to a single question and multiple focus points in one
question, which are lacking in the existing datasets for development of
artificial intelligence solutions. Objective: Create a dataset for developing
and evaluating clinical EQA systems that can handle natural multi-answer and
multi-focus questions. Methods: We leveraged the annotated relations from the
2018 National NLP Clinical Challenges (n2c2) corpus to generate an EQA dataset.
Specifically, the 1-to-N, M-to-1, and M-to-N drug-reason relations were
included to form the multi-answer and multi-focus QA entries, which represent
more complex and natural challenges in addition to the basic
one-drug-one-reason cases. A baseline solution was developed and tested on the
dataset. Results: The derived RxWhyQA dataset contains 96,939 QA entries. Among
the answerable questions, 25% require multiple answers, and 2% ask about
multiple drugs within one question. There are frequent cues observed around the
answers in the text, and 90% of the drug and reason terms occur within the same
or an adjacent sentence. The baseline EQA solution achieved a best f1-measure
of 0.72 on the entire dataset, and on specific subsets, it was: 0.93 on the
unanswerable questions, 0.48 on single-drug questions versus 0.60 on multi-drug
questions, 0.54 on the single-answer questions versus 0.43 on multi-answer
questions. Discussion: The RxWhyQA dataset can be used to train and evaluate
systems that need to handle multi-answer and multi-focus questions.
Specifically, multi-answer EQA appears to be challenging and therefore warrants
more investment in research. </font><br> Link: <a href='http://arxiv.org/pdf/2201.02517v2' target="_blank">http://arxiv.org/pdf/2201.02517v2</a><br> <br> <br> <font size='5'> 967 </font> <div style="text-align: right"> 2022-01-07 02:21:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Epidemics on evolving networks with varying degrees</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Epidemics on complex networks is a widely investigated topic in the last few
years, mainly due to the last pandemic events. Usually, real contact networks
are dynamic, hence much effort has been invested in studying epidemics on
evolving networks. Here we propose and study a model for evolving networks
based on varying degrees, where at each time step a node might get, with
probability $r$, a new degree and new neighbors according to a given degree
distribution, instead of its former neighbors. We find analytically, using the
generating functions framework, the epidemic threshold and the probability for
a macroscopic spread of disease depending on the rewiring rate $r$. Our
analytical results are supported by numerical simulations. We find surprisingly
that the impact of the rewiring rate $r$ has qualitative different trends for
networks having different degree distributions. That is, in some structures,
such as random regular networks the dynamics enhances the epidemic spreading
while in others such as scale free the dynamics reduces the spreading. In
addition, for scale-free networks, we reveal that fast dynamics of the network,
$r=1$, changes the epidemic threshold to nonzero rather than zero found for
$r<1$, which is similar to the known case of $r=0$, i.e., a static network.
Finally, we find the epidemic threshold also for a general distribution of the
recovery time. </font><br> Link: <a href='http://arxiv.org/pdf/2201.02299v2' target="_blank">http://arxiv.org/pdf/2201.02299v2</a><br> <br> <br> <font size='5'> 968 </font> <div style="text-align: right"> 2022-01-07 01:55:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Strategic Storage Investment in Electricity Markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Arbitrage is one important revenue source for energy storage in electricity
markets. However, a large amount of storage in the market will impact the
energy price and reduce potential revenues. This can lead to strategic
behaviors of profit-seeking storage investors. To study the investors'
strategic storage investments, we formulate a non-cooperative game between
competing investors. Each investor decides the storage investment over a long
investment horizon, and operates the storage for arbitrage revenues in the
daily electricity market. Different investors can deploy storage with different
characteristics. Their decisions are coupled due to the market price that is
determined by all the investors' decisions. We use market data from California
ISO to characterize the storage impact on the market price, based on which we
establish a centralized optimization problem to compute the market equilibrium.
We show that an increasing number of investors will increase the market
competition, which reduces investors' profits but increases the total invested
storage capacity. Furthermore, we find that a slight increase in the storage
efficiency (e.g., increased charge and discharge efficiency) can significantly
improve an investor's profit share in the market. </font><br> Link: <a href='http://arxiv.org/pdf/2201.02290v2' target="_blank">http://arxiv.org/pdf/2201.02290v2</a><br> <br> <br> <font size='5'> 969 </font> <div style="text-align: right"> 2022-01-06 00:08:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We suggest a simple practical method to combine the human and artificial
intelligence to both learn best investment practices of fund managers, and
provide recommendations to improve them. Our approach is based on a combination
of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns
the intent of fund managers as suggested by their trading history, and recovers
their implied reward function. At the second step, this reward function is used
by a direct RL algorithm to optimize asset allocation decisions. We show that
our method is able to improve over the performance of individual fund managers. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01874v1' target="_blank">http://arxiv.org/pdf/2201.01874v1</a><br> <br> <br> <font size='5'> 970 </font> <div style="text-align: right"> 2022-01-05 23:15:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CFU Playground: Full-Stack Open-Source Framework for Tiny Machine Learning (tinyML) Acceleration on FPGAs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Need for the efficient processing of neural networks has given rise to the
development of hardware accelerators. The increased adoption of specialized
hardware has highlighted the need for more agile design flows for
hardware-software co-design and domain-specific optimizations. In this paper,
we present CFU Playground: a full-stack open-source framework that enables
rapid and iterative design and evaluation of machine learning (ML) accelerators
for embedded ML systems. Our tool provides a completely open-source end-to-end
flow for hardware-software co-design on FPGAs and future systems research. This
full-stack framework gives the users access to explore experimental and bespoke
architectures that are customized and co-optimized for embedded ML. Our rapid,
deploy-profile-optimization feedback loop lets ML hardware and software
developers achieve significant returns out of a relatively small investment in
customization. Using CFU Playground's design and evaluation loop, we show
substantial speedups between 55$\times$ and 75$\times$. The soft CPU coupled
with the accelerator opens up a new, rich design space between the two
components that we explore in an automated fashion using Vizier, an open-source
black-box optimization service. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01863v3' target="_blank">http://arxiv.org/pdf/2201.01863v3</a><br> <br> <br> <font size='5'> 971 </font> <div style="text-align: right"> 2022-01-05 17:53:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Data science to investigate temperature profiles of large networks of food refrigeration systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The electrical generation and transmission infrastructures of many countries
are under increased pressure. This partially reflects the move towards low
carbon economies and the increased reliance on renewable power generation
systems. There has been a reduction in the use of traditional fossil fuel
generation systems, which provide a stable base load, and this has been
replaced with more unpredictable renewable generation. As a consequence, the
available load on the grid is becoming more unstable. To cope with this
variability, the UK National Grid has placed emphasis on the investigation of
various technical mechanisms (e.g. implementation of smart grids, energy
storage technologies, auxiliary power sources), which may be able to prevent
critical situations, when the grid may become sometimes unstable. The
successful implementation of these mechanisms may require large numbers of
electrical consumers (e.g. HVAC systems, food refrigeration systems) for
example to make additional investments in energy storage technologies (food
refrigeration systems) or to integrate their electrical demand from industrial
processes into the National Grid (HVAC systems). However, in the situation of
food refrigeration systems, during these critical situations, even if the
thermal inertia within refrigeration systems may maintain effective performance
of the device for a short period of time (e.g. under 1 minute) when the
electrical input load into the system is reduced, this still carries the
paramount risk of food safety even for very short periods of time (e.g. under 1
minute). Therefore before considering any future actions (e.g. investing in
energy storage technologies) to prevent the critical situations when grid
becomes unstable, it is also needed to understand during the normal use how the
temperature profiles evolve along the time inside these massive networks of
food refrigeration systems. </font><br> Link: <a href='http://arxiv.org/pdf/2201.02046v1' target="_blank">http://arxiv.org/pdf/2201.02046v1</a><br> <br> <br> <font size='5'> 972 </font> <div style="text-align: right"> 2022-01-05 16:26:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An energy system model for mixed bilateral and pool markets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Investments into renewable energy are increasing rapidly around the world.
Energy system models are able to provide insights into optimal investment
capacities and thus are widely used to aid the long-term investment
decision-making under an electricity market environment. Existing energy system
models, however, fail to consider bilateral electricity markets while in
reality, these constitute a major part of all energy trades. In this paper, we
propose an improved energy system model that endogenously considers mixed
bilateral and pool markets. In this model, we also introduce three externality
cost items that account for the social cost of technologies, carbon
taxes/renewable energy subsidies, and the bilateral product differentiation in
the bilateral market, respectively. We start with an equilibrium problem
formulation for different market players and next, an equivalent optimization
problem is presented. Then, a case study of the pan-European market to reach
95\% emission reduction in 2050 is conducted to demonstrate the model.
Different scenarios are constructed to showcase two different usages of product
differentiation in the bilateral market, i.e., willingness to pay and exogenous
costs. Our main conclusion is that the inclusion of mixed bilateral and pool
markets into our enriched energy system model significantly changed the optimal
investment capacities, compared to benchmark results from the existing,
conventional energy system model. This shows that the inclusion of the
bilateral market is of key importance in future investment considerations. Our
model is the first of its kind to include this important and realistic
bilateral market in energy system models. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01685v1' target="_blank">http://arxiv.org/pdf/2201.01685v1</a><br> <br> <br> <font size='5'> 973 </font> <div style="text-align: right"> 2022-01-05 13:23:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Energy Optimal Point-to-Point Motion Profile Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Position-controlled systems driving repetitive tasks are of significant
importance in industrial machinery. The electric actuators used in these
systems are responsible for a large part of the global energy consumption,
indicating that major savings can be made in this field. In this context,
motion profile optimization is a very cost-effective solution as it allows for
more energy-efficient machines without additional hardware investments or
adaptions. In particular, mono-actuated mechanisms with position-dependent
system properties have received considerable attention in literature. However,
the current state-of-the-art methods often use unbounded design parameters to
describe the motion profile. This both increases the computational complexity
and hampers the search for a global optimum. In this paper, Chebyshev
polynomials are used to describe the motion profile. Moreover, the exact bounds
on the Chebyshev design parameters are derived. This both seriously reduces the
computational complexity and limits the design space, allowing the application
of a global optimizer such as the genetic algorithm. Experiments validate the
added value of the chosen approach. In this study, it is found that the energy
consumption can be reduced by 62.9% compared to a standard reference motion
profile. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01595v1' target="_blank">http://arxiv.org/pdf/2201.01595v1</a><br> <br> <br> <font size='5'> 974 </font> <div style="text-align: right"> 2022-01-05 06:00:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence (AI) has become a part of everyday conversation and
our lives. It is considered as the new electricity that is revolutionizing the
world. AI is heavily invested in both industry and academy. However, there is
also a lot of hype in the current AI debate. AI based on so-called deep
learning has achieved impressive results in many problems, but its limits are
already visible. AI has been under research since the 1940s, and the industry
has seen many ups and downs due to over-expectations and related
disappointments that have followed.
  The purpose of this book is to give a realistic picture of AI, its history,
its potential and limitations. We believe that AI is a helper, not a ruler of
humans. We begin by describing what AI is and how it has evolved over the
decades. After fundamentals, we explain the importance of massive data for the
current mainstream of artificial intelligence. The most common representations
for AI, methods, and machine learning are covered. In addition, the main
application areas are introduced. Computer vision has been central to the
development of AI. The book provides a general introduction to computer vision,
and includes an exposure to the results and applications of our own research.
Emotions are central to human intelligence, but little use has been made in AI.
We present the basics of emotional intelligence and our own research on the
topic. We discuss super-intelligence that transcends human understanding,
explaining why such achievement seems impossible on the basis of present
knowledge,and how AI could be improved. Finally, a summary is made of the
current state of AI and what to do in the future. In the appendix, we look at
the development of AI education, especially from the perspective of contents at
our own university. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01466v1' target="_blank">http://arxiv.org/pdf/2201.01466v1</a><br> <br> <br> <font size='5'> 975 </font> <div style="text-align: right"> 2022-01-04 19:48:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The credit spread curve. I: Fundamental concepts, fitting, par-adjusted spread, and expected return</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The notion of a credit spread curve is fundamental in fixed income investing,
but in practice it is not `given' and needs to be constructed from bond prices
either for a particular issuer, or for a sector rating-by-rating. Rather than
attempting to fit spreads -- and as we discuss here, the Z-spread is unsuitable
-- we fit parametrised survival curves. By deriving a valuation formula for a
risky bond, we explain and avoid the problem that bonds with a high dollar
price trade at a higher yield or spread than those with low dollar price (at
the same maturity point), even though they do not necessarily offer better
value. In fact, a concise treatment of this effect is elusive, and much of the
academic literature on risky bond pricing, including a well-known paper by
Duffie and Singleton (1997), is fundamentally incorrect. We then proceed to
show how to calculate carry, rolldown and relative value for bonds/CDS. Also,
once curve construction has been programmed and automated we can run it
historically and assess the way a curve has moved over time. This provides the
necessary grounding for econometric and arbitrage-free models of curve
dynamics, which will be pursued in later work, as well as assessing how the
perceived relative value of a particular instrument varies over time. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01330v2' target="_blank">http://arxiv.org/pdf/2201.01330v2</a><br> <br> <br> <font size='5'> 976 </font> <div style="text-align: right"> 2022-01-04 16:14:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sparse Non-Convex Optimization For Higher Moment Portfolio Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: One of the reasons that higher order moment portfolio optimization methods
are not fully used by practitioners in investment decisions is the complexity
that these higher moments create by making the optimization problem nonconvex.
Many few methods and theoretical results exists in the literature, but the
present paper uses the method of successive convex approximation for the
mean-variance-skewness problem. </font><br> Link: <a href='http://arxiv.org/pdf/2201.01227v2' target="_blank">http://arxiv.org/pdf/2201.01227v2</a><br> <br> <br> <font size='5'> 977 </font> <div style="text-align: right"> 2022-01-04 10:24:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The impact of accumulative pension policy on welfare of individuals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many countries around the world have had to carry out radical reforms
periodically in their pension systems. Global experience shows that it is
important to optimize the costs of pension and social security systems in order
to ensure a decent old age in addition to reducing the pressure on budgetary
resources. By Georgia is changing demographic situation, special attention is
paid to proper functioning of the pension policy. The pension reform carried
out in Georgia in 2019 caused a difference of opinion among experts. This issue
in today is conditions does not lose relevance. The presented thesis discusses
the impact of the mandatory funded pension system on the well-being of people.
Thesis includes the following issues: peculiarities of the formation of pension
systems in Georgia. It is presented a small historical excursion in terms of
the development of pension systems. In addition, are discussed the
international experience of pension systems and comparative analysis in
relation to Georgia. This paper specifically focuses on the essence of the
mandatory funded pension system and assesses the current situation in terms of
investment potential of the resource accumulated in the pension fund. In
conclusion, are presented the challenges of this system and the ways of
perfection. </font><br> Link: <a href='http://arxiv.org/pdf/2202.12721v1' target="_blank">http://arxiv.org/pdf/2202.12721v1</a><br> <br> <br> <font size='5'> 978 </font> <div style="text-align: right"> 2021-12-31 14:51:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dynamic Portfolio Optimization with Inverse Covariance Clustering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Market conditions change continuously. However, in portfolio's investment
strategies, it is hard to account for this intrinsic non-stationarity. In this
paper, we propose to address this issue by using the Inverse Covariance
Clustering (ICC) method to identify inherent market states and then integrate
such states into a dynamic portfolio optimization process. Extensive
experiments across three different markets, NASDAQ, FTSE and HS300, over a
period of ten years, demonstrate the advantages of our proposed algorithm,
termed Inverse Covariance Clustering-Portfolio Optimization (ICC-PO). The core
of the ICC-PO methodology concerns the identification and clustering of market
states from the analytics of past data and the forecasting of the future market
state. It is therefore agnostic to the specific portfolio optimization method
of choice. By applying the same portfolio optimization technique on a ICC
temporal cluster, instead of the whole train period, we show that one can
generate portfolios with substantially higher Sharpe Ratios, which are
statistically more robust and resilient with great reductions in maximum loss
in extreme situations. This is shown to be consistent across markets, periods,
optimization methods and selection of portfolio assets. </font><br> Link: <a href='http://arxiv.org/pdf/2112.15499v2' target="_blank">http://arxiv.org/pdf/2112.15499v2</a><br> <br> <br> <font size='5'> 979 </font> <div style="text-align: right"> 2021-12-31 03:42:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What is Event Knowledge Graph: A Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),
events are also an essential kind of knowledge in the world, which trigger the
spring up of event-centric knowledge representation form like Event KG (EKG).
It plays an increasingly important role in many downstream applications, such
as search, question-answering, recommendation, financial quantitative
investments, and text generation. This paper provides a comprehensive survey of
EKG from history, ontology, instance, and application views. Specifically, to
characterize EKG thoroughly, we focus on its history, definition, schema
induction, acquisition, related representative graphs/systems, and
applications. The development processes and trends are studied therein. We
further summarize prospective directions to facilitate future research on EKG. </font><br> Link: <a href='http://arxiv.org/pdf/2112.15280v2' target="_blank">http://arxiv.org/pdf/2112.15280v2</a><br> <br> <br> <font size='5'> 980 </font> <div style="text-align: right"> 2021-12-29 22:21:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Human Niche Evolution: pathways, choices and outcomes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Humankind has spread worldwide supported by cultural and technological
knowledge, but the environmental sustainability on the human niche evolution
depends on a new human beings relationship with the biosphere. Human lifestyles
nowadays are very Antropocentric and in many ways deleterious to the other life
forms. Here we try to identify future scenarios, where the less deleterious is
the Natural-Technological Model that points the urgent need to change the
evolutionary direction of the human niche seeking the resumption of original
ecological relations. New cultural habits and novel technologies, thereby,
would reverse the current anthropogenic impacts. The middle way is the
Bio-Anthropogenic Model that predicts the success of the emerging ecosystems
and the symbiotic relationship of humans and anthropogenic-favored species,
hybrids, aliens and genetically modified organisms. For such, we must also
change our way of life and adopt new conscious ways of consumption aiming at
the socio-environmental good. Lastly, the Wear Out Model, which depends only on
maintaining current patterns of human expansion. The lack of investments on new
technologies and new cultural habits, added to the current patterns of human
niche evolution that are based on the massive exploitation of world resources,
will lead to a fearsome scenario with a precarious global health, biodiversity
losses and food scarcity. This theoretical models indicates some pathways and
can help us to choose a better future. </font><br> Link: <a href='http://arxiv.org/pdf/2112.14852v1' target="_blank">http://arxiv.org/pdf/2112.14852v1</a><br> <br> <br> <font size='5'> 981 </font> <div style="text-align: right"> 2021-12-29 09:49:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exact Post-selection Inference For Tracking S&P500</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The problem that is solved in this paper is known as index tracking. The
method of Lasso is used to reduce the dimensions of S&P500 index which has many
applications in both investment and portfolio management algorithms. The
novelty of this paper is that post-selection inference is used to have better
modeling and inference for Lasso approach to index tracking. Both confidence
intervals and curves indicate that the performance of Lasso type method for
dimension reduction of S&P500 is remarkably high. Keywords: index tracking,
lasso, post-selection inference, S&P500 </font><br> Link: <a href='http://arxiv.org/pdf/2112.15448v1' target="_blank">http://arxiv.org/pdf/2112.15448v1</a><br> <br> <br> <font size='5'> 982 </font> <div style="text-align: right"> 2021-12-28 14:13:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cerebro: Static Subsuming Mutant Selection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Mutation testing research has indicated that a major part of its application
cost is due to the large number of low utility mutants that it introduces.
Although previous research has identified this issue, no previous study has
proposed any effective solution to the problem. Thus, it remains unclear how to
mutate and test a given piece of code in a best effort way, i.e., achieving a
good trade-off between invested effort and test effectiveness. To achieve this,
we propose Cerebro, a machine learning approach that statically selects
subsuming mutants, i.e., the set of mutants that resides on the top of the
subsumption hierarchy, based on the mutants' surrounding code context. We
evaluate Cerebro using 48 and 10 programs written in C and Java, respectively,
and demonstrate that it preserves the mutation testing benefits while limiting
application cost, i.e., reduces all cost application factors such as equivalent
mutants, mutant executions, and the mutants requiring analysis. We demonstrate
that Cerebro has strong inter-project prediction ability, which is
significantly higher than two baseline methods, i.e., supervised learning on
features proposed by state-of-the-art, and random mutant selection. More
importantly, our results show that Cerebro's selected mutants lead to strong
tests that are respectively capable of killing 2 times higher than the number
of subsuming mutants killed by the baselines when selecting the same number of
mutants. At the same time, Cerebro reduces the cost-related factors, as it
selects, on average, 68% fewer equivalent mutants, while requiring 90% fewer
test executions than the baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2112.14151v2' target="_blank">http://arxiv.org/pdf/2112.14151v2</a><br> <br> <br> <font size='5'> 983 </font> <div style="text-align: right"> 2021-12-28 07:54:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adversarial Learning for Incentive Optimization in Mobile Payment Marketing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many payment platforms hold large-scale marketing campaigns, which allocate
incentives to encourage users to pay through their applications. To maximize
the return on investment, incentive allocations are commonly solved in a
two-stage procedure. After training a response estimation model to estimate the
users' mobile payment probabilities (MPP), a linear programming process is
applied to obtain the optimal incentive allocation. However, the large amount
of biased data in the training set, generated by the previous biased allocation
policy, causes a biased estimation. This bias deteriorates the performance of
the response model and misleads the linear programming process, dramatically
degrading the performance of the resulting allocation policy. To overcome this
obstacle, we propose a bias correction adversarial network. Our method
leverages the small set of unbiased data obtained under a full-randomized
allocation policy to train an unbiased model and then uses it to reduce the
bias with adversarial learning. Offline and online experimental results
demonstrate that our method outperforms state-of-the-art approaches and
significantly improves the performance of the resulting allocation policy in a
real-world marketing campaign. </font><br> Link: <a href='http://arxiv.org/pdf/2112.15434v1' target="_blank">http://arxiv.org/pdf/2112.15434v1</a><br> <br> <br> <font size='5'> 984 </font> <div style="text-align: right"> 2021-12-23 22:03:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SoK: Privacy-preserving Deep Learning with Homomorphic Encryption</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Outsourced computation for neural networks allows users access to state of
the art models without needing to invest in specialized hardware and know-how.
The problem is that the users lose control over potentially privacy sensitive
data. With homomorphic encryption (HE) computation can be performed on
encrypted data without revealing its content. In this systematization of
knowledge, we take an in-depth look at approaches that combine neural networks
with HE for privacy preservation. We categorize the changes to neural network
models and architectures to make them computable over HE and how these changes
impact performance. We find numerous challenges to HE based privacy-preserving
deep learning such as computational overhead, usability, and limitations posed
by the encryption schemes. </font><br> Link: <a href='http://arxiv.org/pdf/2112.12855v2' target="_blank">http://arxiv.org/pdf/2112.12855v2</a><br> <br> <br> <font size='5'> 985 </font> <div style="text-align: right"> 2021-12-23 18:17:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Toward a New Science of Common Sense</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Common sense has always been of interest in Artificial Intelligence, but has
rarely taken center stage. Despite its mention in one of John McCarthy's
earliest papers and years of work by dedicated researchers, arguably no AI
system with a serious amount of general common sense has ever emerged. Why is
that? What's missing? Examples of AI systems' failures of common sense abound,
and they point to AI's frequent focus on expertise as the cause. Those
attempting to break the resulting brittleness barrier, even in the context of
modern deep learning, have tended to invest their energy in large numbers of
small bits of commonsense knowledge. While important, all the commonsense
knowledge fragments in the world don't add up to a system that actually
demonstrates common sense in a human-like way. We advocate examining common
sense from a broader perspective than in the past. Common sense should be
considered in the context of a full cognitive system with history, goals,
desires, and drives, not just in isolated circumscribed examples. A fresh look
is needed: common sense is worthy of its own dedicated scientific exploration. </font><br> Link: <a href='http://arxiv.org/pdf/2112.12754v2' target="_blank">http://arxiv.org/pdf/2112.12754v2</a><br> <br> <br> <font size='5'> 986 </font> <div style="text-align: right"> 2021-12-23 10:45:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cardinality-constrained Distributionally Robust Portfolio Optimization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies a distributionally robust portfolio optimization model
with a cardinality constraint for limiting the number of invested assets. We
formulate this model as a mixed-integer semidefinite optimization (MISDO)
problem by means of the moment-based ambiguity set of probability distributions
of asset returns. To exactly solve large-scale problems, we propose a
specialized cutting-plane algorithm that is based on bilevel optimization
reformulation. We prove the finite convergence of the algorithm. We also apply
a matrix completion technique to lower-level SDO problems to make their problem
sizes much smaller. Numerical experiments demonstrate that our cutting-plane
algorithm is significantly faster than the state-of-the-art MISDO solver
SCIP-SDP. We also show that our portfolio optimization model can achieve good
investment performance compared with the conventional robust optimization model
based on the ellipsoidal uncertainty set. </font><br> Link: <a href='http://arxiv.org/pdf/2112.12454v2' target="_blank">http://arxiv.org/pdf/2112.12454v2</a><br> <br> <br> <font size='5'> 987 </font> <div style="text-align: right"> 2021-12-22 11:58:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Binary Image Skeletonization Using 2-Stage U-Net</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Object Skeletonization is the process of extracting skeletal, line-like
representations of shapes. It provides a very useful tool for geometric shape
understanding and minimal shape representation. It also has a wide variety of
applications, most notably in anatomical research and activity detection.
Several mathematical algorithmic approaches have been developed to solve this
problem, and some of them have been proven quite robust. However, a lesser
amount of attention has been invested into deep learning solutions for it. In
this paper, we use a 2-stage variant of the famous U-Net architecture to split
the problem space into two sub-problems: shape minimization and corrective
skeleton thinning. Our model produces results that are visually much better
than the baseline SkelNetOn model. We propose a new metric, M-CCORR, based on
normalized correlation coefficients as an alternative to F1 for this challenge
as it solves the problem of class imbalance, managing to recognize skeleton
similarity without suffering from F1's over-sensitivity to pixel-shifts. </font><br> Link: <a href='http://arxiv.org/pdf/2112.11824v1' target="_blank">http://arxiv.org/pdf/2112.11824v1</a><br> <br> <br> <font size='5'> 988 </font> <div style="text-align: right"> 2021-12-21 15:12:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Geographical Peer Matching for P2P Energy Sharing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Significant cost reductions attract ever more households to invest in
small-scale renewable electricity generation and storage. Such distributed
resources are not used in the most effective way when only used individually,
as sharing them provides even greater cost-savings. Energy Peer-to-Peer (P2P)
systems have been shown to be beneficial for prosumers and consumers through
reductions in energy cost while being attractive to grid or service provider.
However, many practical challenges have to be overcome before all players could
gain in having efficient and automated local energy communities, including the
inherent complexity of matching together geographically distributed peers and
the significant computation required to calculate the local matching
preferences. Hence dedicated algorithms are required to be able to perform a
cost-efficient matching of thousands of peers in a computational-efficient
fashion. We define and analyse in this work a precise mathematical modelling of
the geographical peer matching problem. We then propose and analyse dedicated
algorithms. Our experimental study, based on real-world energy data,
demonstrates that our solutions are both efficient in terms of cost-savings
achieved by the peers and in terms of computing requirements. Previous results
have already shown that small communities were efficient but only small-enough
datasets were used allowing for brute-force solutions. Our scalable algorithms
thus provide one core building block on the way to build fully practical and
data-efficient peer-to-peer energy sharing communities for large-scale systems. </font><br> Link: <a href='http://arxiv.org/pdf/2112.11286v1' target="_blank">http://arxiv.org/pdf/2112.11286v1</a><br> <br> <br> <font size='5'> 989 </font> <div style="text-align: right"> 2021-12-20 20:00:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding User Perspectives on Prompts for Brief Reflection on Troubling Emotions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate users' perspectives on an online reflective question activity
(RQA) that prompts people to externalize their underlying emotions on a
troubling situation. Inspired by principles of cognitive behavioral therapy,
our 15-minute activity encourages self-reflection without a human or automated
conversational partner. A deployment of our RQA on Amazon Mechanical Turk
suggests that people perceive several benefits from our RQA, including
structured awareness of their thoughts and problem-solving around managing
their emotions. Quantitative evidence from a randomized experiment suggests
people find that our RQA makes them feel less worried by their selected
situation and worth the minimal time investment. A further two-week technology
probe deployment with 11 participants indicates that people see benefits to
doing this activity repeatedly, although the activity may get monotonous over
time. In summary, this work demonstrates the promise of online reflection
activities that carefully leverage principles of psychology in their design. </font><br> Link: <a href='http://arxiv.org/pdf/2112.10833v1' target="_blank">http://arxiv.org/pdf/2112.10833v1</a><br> <br> <br> <font size='5'> 990 </font> <div style="text-align: right"> 2021-12-20 16:35:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: REFORM: Reputation Based Fair and Temporal Reward Framework for Crowdsourcing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Crowdsourcing is an effective method to collect data by employing distributed
human population. Researchers introduce appropriate reward mechanisms to
incentivize agents to report accurately. In particular, this paper focuses on
Peer-Based Mechanisms (PBMs). We observe that with PBMs, crowdsourcing systems
may not be fair, i.e., agents may not receive the deserved rewards despite
investing efforts and reporting truthfully. Unfair rewards for the agents may
discourage participation. This paper aims to build a general framework that
assures fairness for PBMs in temporal settings, i.e., settings that prefer
early reports. Towards this, we introduce two general notions of fairness for
PBMs, namely gamma-fairness and qualitative fairness. To satisfy these notions,
our framework provides trustworthy agents with additional chances of pairing.
We introduce Temporal Reputation Model (TERM) to quantify agents'
trustworthiness across tasks. With TERM as the key constituent, we present our
iterative framework, REFORM, that can adopt the reward scheme of any existing
PBM. We demonstrate REFORM's significance by deploying the framework with
RPTSC's reward scheme. Specifically, we prove that REFORM with RPTSC
considerably improves fairness; while incentivizing truthful and early reports.
We conduct synthetic simulations and show that our framework provides improved
fairness over RPTSC. </font><br> Link: <a href='http://arxiv.org/pdf/2112.10659v2' target="_blank">http://arxiv.org/pdf/2112.10659v2</a><br> <br> <br> <font size='5'> 991 </font> <div style="text-align: right"> 2021-12-20 15:02:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A dynamic theory of spatial externalities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We characterize the shape of spatial externalities in a continuous time and
space differential game with transboundary pollution. We posit a realistic
spatiotemporal law of motion for pollution (diffusion and advection), and
tackle spatiotemporal non-cooperative (and cooperative) differential games.
Precisely, we consider a circle partitioned into several states where a local
authority decides autonomously about its investment, production and depollution
strategies over time knowing that investment/production generates pollution,
and pollution is transboundary. The time horizon is infinite. We allow for a
rich set of geographic heterogeneities across states. We solve analytically the
induced non-cooperative differential game and characterize its long-term
spatial distributions. In particular, we prove that there exist a Perfect
Markov Equilibrium, unique among the class of the affine feedbacks. We further
provide with a full exploration of the free riding problem and the associated
border effect. </font><br> Link: <a href='http://arxiv.org/pdf/2112.10584v1' target="_blank">http://arxiv.org/pdf/2112.10584v1</a><br> <br> <br> <font size='5'> 992 </font> <div style="text-align: right"> 2021-12-17 23:43:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dollar Cost Averaging Returns Estimation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Given a geometric Brownian motion wealth process, a log-Normal lower bound is
constructed for the returns of a regular investing schedule. The distribution
parameters of this bound are computed recursively. For dollar cost averaging
(equal amounts in equal time intervals), parameters are computed in closed
form. A lump sum (single amount at time 0) investing schedule is described
which achieves a terminal wealth distribution that matches the wealth
distribution indicated by the lower bound. Results are applied to annual
returns of the S&P Composite Index from the last 150 years. Among data analysis
results, the probability of negative returns is less than 2.5% when annual
dollar cost averaging lasts over 40 years. </font><br> Link: <a href='http://arxiv.org/pdf/2112.09807v1' target="_blank">http://arxiv.org/pdf/2112.09807v1</a><br> <br> <br> <font size='5'> 993 </font> <div style="text-align: right"> 2021-12-17 19:00:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reconciling the Results of the z~2 MOSDEF and KBSS-MOSFIRE Surveys</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The combination of the MOSDEF and KBSS-MOSFIRE surveys represents the largest
joint investment of Keck/MOSFIRE time to date, with ~3000 galaxies at
1.4<=z<=3.8, roughly half of which are at z~2. MOSDEF is photometric- and
spectroscopic-redshift selected with a rest-optical magnitude limit, while
KBSS-MOSFIRE is primarily selected based on rest-UV colors and a rest-UV
magnitude limit. Analyzing both surveys in a uniform manner with consistent
spectral-energy-distribution (SED) models, we find that the MOSDEF z~2 targeted
sample has a higher median M_* and redder rest U-V color than the KBSS-MOSFIRE
z~2 targeted sample, and a smaller median SED-based SFR and sSFR (SFR(SED) and
sSFR(SED)). Specifically, MOSDEF targeted a larger population of red galaxies
with U-V and V-J >=1.25, while KBSS-MOSFIRE contains more young galaxies with
intense star formation. Despite these differences in the z~2 targeted samples,
the subsets of the surveys with multiple emission lines detected and analyzed
in previously published work are much more similar. All median host-galaxy
properties with the exception of stellar population age -- i.e., M_*, SFR(SED),
sSFR(SED), A_V, and UVJ colors -- agree within the uncertainties. Additionally,
when uniform emission-line fitting and stellar Balmer absorption correction
techniques are applied, there is no significant offset between the two samples
in the [OIII]$\lambda$5008/H$\beta$ vs. [NII]$\lambda$6585/H$\alpha$ diagnostic
diagram, in contrast to previously-reported discrepancies. We can now combine
the MOSDEF and KBSS-MOSFIRE surveys to form the largest z~2 sample with
moderate-resolution rest-optical spectra and construct the fundamental scaling
relations of star-forming galaxies during this important epoch. </font><br> Link: <a href='http://arxiv.org/pdf/2112.09715v2' target="_blank">http://arxiv.org/pdf/2112.09715v2</a><br> <br> <br> <font size='5'> 994 </font> <div style="text-align: right"> 2021-12-16 16:30:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Portfolio Optimization on Classical and Quantum Computers Using PortFawn</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Portfolio diversification is one of the most effective ways to minimize
investment risk. Individuals and fund managers aim to create a portfolio of
assets that not only have high returns but are also uncorrelated. This goal can
be achieved by comparing the historical performance, fundamentals, predictions,
news sentiment, and many other parameters that can affect the portfolio's
value. One of the most well-known approaches to manage/optimize portfolios is
the well-known mean-variance (Markowitz) portfolio. The algorithm's inputs are
the expected returns and risks (volatility), and its output is the optimized
weights for each asset in the target portfolio. Simplified unrealistic
assumptions and constraints were used in its original version preventing its
use in practical cases. One solution to improve its usability is by altering
the parameters and constraints to match investment goals and requirements. This
paper introduces PortFawn, an open-source Python library to create and backtest
mean-variance portfolios. PortFawn provides simple-to-use APIs to create and
evaluate mean-variance optimization algorithms using classical computing
(real-valued asset weights) as well as quantum annealing computing (binary
asset weights). This tool has many parameters to customize the target
portfolios according to the investment goals. The paper introduces the
background and limitations of the mean-variance portfolio optimization
algorithm, its architecture, and a description of the functionalities of
PortFawn. We also show how one can use this tool in practice using a simple
investment scenario. </font><br> Link: <a href='http://arxiv.org/pdf/2112.08998v1' target="_blank">http://arxiv.org/pdf/2112.08998v1</a><br> <br> <br> <font size='5'> 995 </font> <div style="text-align: right"> 2021-12-16 10:32:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: There is an elephant in the room: Towards a critique on the use of fairness in biometrics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In 2019, the UK's Immigration and Asylum Chamber of the Upper Tribunal
dismissed an asylum appeal basing the decision on the output of a biometric
system, alongside other discrepancies. The fingerprints of the asylum seeker
were found in a biometric database which contradicted the appellant's account.
The Tribunal found this evidence unequivocal and denied the asylum claim.
Nowadays, the proliferation of biometric systems is shaping public debates
around its political, social and ethical implications. Yet whilst concerns
towards the racialised use of this technology for migration control have been
on the rise, investment in the biometrics industry and innovation is increasing
considerably. Moreover, fairness has also been recently adopted by biometrics
to mitigate bias and discrimination on biometrics. However, algorithmic
fairness cannot distribute justice in scenarios which are broken or intended
purpose is to discriminate, such as biometrics deployed at the border.
  In this paper, we offer a critical reading of recent debates about biometric
fairness and show its limitations drawing on research in fairness in machine
learning and critical border studies. Building on previous fairness
demonstrations, we prove that biometric fairness criteria are mathematically
mutually exclusive. Then, the paper moves on illustrating empirically that a
fair biometric system is not possible by reproducing experiments from previous
works. Finally, we discuss the politics of fairness in biometrics by situating
the debate at the border. We claim that bias and error rates have different
impact on citizens and asylum seekers. Fairness has overshadowed the elephant
in the room of biometrics, focusing on the demographic biases and ethical
discourses of algorithms rather than examine how these systems reproduce
historical and political injustices. </font><br> Link: <a href='http://arxiv.org/pdf/2112.11193v2' target="_blank">http://arxiv.org/pdf/2112.11193v2</a><br> <br> <br> <font size='5'> 996 </font> <div style="text-align: right"> 2021-12-16 02:42:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Credibility Cryptocurrency Valuation: Statistical Learning Analysis for Influencer Tweets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cryptocurrency has attracted significant attention. Considering the number of
individuals investing in bitcoin, their motivations are comparatively less
clear than traditional investment decisions. As of December 2020, the market
has continuously increased in cryptocurrency. Especially, the spike of joke
Dogecoin shows the weirdness of the modern meme economy with the support of
Elon Musk, whom himself appointed as "Dogefather". In this paper, we analysis
the impact of tweets by Elon musk and present some statistical analyze with
event study. </font><br> Link: <a href='http://arxiv.org/pdf/2112.08579v1' target="_blank">http://arxiv.org/pdf/2112.08579v1</a><br> <br> <br> <font size='5'> 997 </font> <div style="text-align: right"> 2021-12-16 01:46:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Context-Based Music Recommendation Algorithm Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial Intelligence (AI ) has been very successful in creating and
predicting music playlists for online users based on their data; data received
from users experience using the app such as searching the songs they like.
There are lots of current technological advancements in AI due to the
competition between music platform owners such as Spotify, Pandora, and more.
In this paper, 6 machine learning algorithms and their individual accuracy for
predicting whether a user will like a song are explored across 3 different
platforms including Weka, SKLearn, and Orange. The algorithms explored include
Logistic Regression, Naive Bayes, Sequential Minimal Optimization (SMO),
Multilayer Perceptron (Neural Network), Nearest Neighbor, and Random Forest.
With the analysis of the specific characteristics of each song provided by the
Spotify API [1], Random Forest is the most successful algorithm for predicting
whether a user will like a song with an accuracy of 84%. This is higher than
the accuracy of 82.72% found by Mungekar using the Random Forest technique and
slightly different characteristics of a song [2]. The characteristics in
Mungekars Random Forest algorithm focus more on the artist and popularity
rather than the sonic features of the songs. Removing the popularity aspect and
focusing purely on the sonic qualities improve the accuracy of recommendations.
Finally, this paper shows how song prediction can be accomplished without any
monetary investments, and thus, inspires an idea of what amazing results can be
accomplished with full financial research. </font><br> Link: <a href='http://arxiv.org/pdf/2112.10612v1' target="_blank">http://arxiv.org/pdf/2112.10612v1</a><br> <br> <br> <font size='5'> 998 </font> <div style="text-align: right"> 2021-12-15 22:42:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do You See What I See? Capabilities and Limits of Automated Multimedia Content Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ever-increasing amount of user-generated content online has led, in
recent years, to an expansion in research and investment in automated content
analysis tools. Scrutiny of automated content analysis has accelerated during
the COVID-19 pandemic, as social networking services have placed a greater
reliance on these tools due to concerns about health risks to their moderation
staff from in-person work. At the same time, there are important policy debates
around the world about how to improve content moderation while protecting free
expression and privacy. In order to advance these debates, we need to
understand the potential role of automated content analysis tools.
  This paper explains the capabilities and limitations of tools for analyzing
online multimedia content and highlights the potential risks of using these
tools at scale without accounting for their limitations. It focuses on two main
categories of tools: matching models and computer prediction models. Matching
models include cryptographic and perceptual hashing, which compare
user-generated content with existing and known content. Predictive models
(including computer vision and computer audition) are machine learning
techniques that aim to identify characteristics of new or previously unknown
content. </font><br> Link: <a href='http://arxiv.org/pdf/2201.11105v1' target="_blank">http://arxiv.org/pdf/2201.11105v1</a><br> <br> <br> <font size='5'> 999 </font> <div style="text-align: right"> 2021-12-15 21:33:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Long-Term Productivity Based on Science, not Preference</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This position paper argues that decisions on processes, tools, techniques and
software artifacts (such as user manuals, unit tests, design documents and
code) for scientific software development should be driven by science, not by
personal preference. Decisions should not be based on anecdotal evidence, gut
instinct or the path of least resistance. Moreover, decisions should vary
depending on the users and the context. In most cases of interest, this means
that a longer term view should be adopted. We need to use a scientific approach
based on unambiguous definitions, empirical evidence, hypothesis testing and
rigorous processes. By developing an understanding of where input hours are
spent, what most contributes to user satisfaction, and how to leverage
knowledge produced, we can determine what interventions have the greatest value
relative to the invested effort. We will be able to recommend software
production processes that justify their value because the long-term output
benefits are high compared to the required input resources. A preliminary
definition of productivity is presented, along with ideas on how to potentially
measure this quality. We briefly explore the idea of improving productivity via
an approach where all artifacts are generated from codified knowledge. </font><br> Link: <a href='http://arxiv.org/pdf/2112.12580v1' target="_blank">http://arxiv.org/pdf/2112.12580v1</a><br> <br> <br> <font size='5'> 1000 </font> <div style="text-align: right"> 2021-12-15 15:24:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reinforcement learning (RL) techniques have shown great success in many
challenging quantitative trading tasks, such as portfolio management and
algorithmic trading. Especially, intraday trading is one of the most profitable
and risky tasks because of the intraday behaviors of the financial market that
reflect billions of rapidly fluctuating capitals. However, a vast majority of
existing RL methods focus on the relatively low frequency trading scenarios
(e.g., day-level) and fail to capture the fleeting intraday investment
opportunities due to two major challenges: 1) how to effectively train
profitable RL agents for intraday investment decision-making, which involves
high-dimensional fine-grained action space; 2) how to learn meaningful
multi-modality market representation to understand the intraday behaviors of
the financial market at tick-level. Motivated by the efficient workflow of
professional human intraday traders, we propose DeepScalper, a deep
reinforcement learning framework for intraday trading to tackle the above
challenges. Specifically, DeepScalper includes four components: 1) a dueling
Q-network with action branching to deal with the large action space of intraday
trading for efficient RL optimization; 2) a novel reward function with a
hindsight bonus to encourage RL agents making trading decisions with a
long-term horizon of the entire trading day; 3) an encoder-decoder architecture
to learn multi-modality temporal market embedding, which incorporates both
macro-level and micro-level market information; 4) a risk-aware auxiliary task
to maintain a striking balance between maximizing profit and minimizing risk.
Through extensive experiments on real-world market data spanning over three
years on six financial futures, we demonstrate that DeepScalper significantly
outperforms many state-of-the-art baselines in terms of four financial
criteria. </font><br> Link: <a href='http://arxiv.org/pdf/2201.09058v3' target="_blank">http://arxiv.org/pdf/2201.09058v3</a>
    </body>
    </html>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    <html>
    <head>
    <title>Mathedemo</title>
    <style>
          body {
             margin-left: 400px;
             margin-right: 400px;
          }
       </style>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    </head>

     <br> <br> <br> <font size='5'> 1 </font> <div style="text-align: right"> 2023-07-11 07:31:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages. </font><br> Link: <a href='http://arxiv.org/pdf/2307.05082v1' target="_blank">http://arxiv.org/pdf/2307.05082v1</a><br> <br> <br> <font size='5'> 2 </font> <div style="text-align: right"> 2023-07-11 07:03:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it's more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL. </font><br> Link: <a href='http://arxiv.org/pdf/2307.05052v1' target="_blank">http://arxiv.org/pdf/2307.05052v1</a><br> <br> <br> <font size='5'> 3 </font> <div style="text-align: right"> 2023-07-11 02:52:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Epidemic Modeling with Generative Agents</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study offers a new paradigm of individual-level modeling to address the
grand challenge of incorporating human behavior in epidemic models. Using
generative artificial intelligence in an agent-based epidemic model, each agent
is empowered to make its own reasonings and decisions via connecting to a large
language model such as ChatGPT. Through various simulation experiments, we
present compelling evidence that generative agents mimic real-world behaviors
such as quarantining when sick and self-isolation when cases rise.
Collectively, the agents demonstrate patterns akin to multiple waves observed
in recent pandemics followed by an endemic period. Moreover, the agents
successfully flatten the epidemic curve. This study creates potential to
improve dynamic system modeling by offering a way to represent human brain,
reasoning, and decision making. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04986v1' target="_blank">http://arxiv.org/pdf/2307.04986v1</a><br> <br> <br> <font size='5'> 4 </font> <div style="text-align: right"> 2023-07-11 01:55:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Secrets of RLHF in Large Language Models Part I: PPO</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes </font><br> Link: <a href='http://arxiv.org/pdf/2307.04964v1' target="_blank">http://arxiv.org/pdf/2307.04964v1</a><br> <br> <br> <font size='5'> 5 </font> <div style="text-align: right"> 2023-07-10 12:17:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Demonstrations of the Potential of AI-based Political Issue Polling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Political polling is a multi-billion dollar industry with outsized influence
on the societal trajectory of the United States and nations around the world.
However, it has been challenged by factors that stress its cost, availability,
and accuracy. At the same time, artificial intelligence (AI) chatbots have
become compelling stand-ins for human behavior, powered by increasingly
sophisticated large language models (LLMs). Could AI chatbots be an effective
tool for anticipating public opinion on controversial issues to the extent that
they could be used by campaigns, interest groups, and polling firms? We have
developed a prompt engineering methodology for eliciting human-like survey
responses from ChatGPT, which simulate the response to a policy question of a
person described by a set of demographic factors, and produce both an ordinal
numeric response score and a textual justification. We execute large scale
experiments, querying for thousands of simulated responses at a cost far lower
than human surveys. We compare simulated data to human issue polling data from
the Cooperative Election Study (CES). We find that ChatGPT is effective at
anticipating both the mean level and distribution of public opinion on a
variety of policy issues such as abortion bans and approval of the US Supreme
Court, particularly in their ideological breakdown (correlation typically
>85%). However, it is less successful at anticipating demographic-level
differences. Moreover, ChatGPT tends to overgeneralize to new policy issues
that arose after its training data was collected, such as US support for
involvement in the war in Ukraine. Our work has implications for our
understanding of the strengths and limitations of the current generation of AI
chatbots as virtual publics or online listening platforms, future directions
for LLM development, and applications of AI tools to the political domain.
(Abridged) </font><br> Link: <a href='http://arxiv.org/pdf/2307.04781v1' target="_blank">http://arxiv.org/pdf/2307.04781v1</a><br> <br> <br> <font size='5'> 6 </font> <div style="text-align: right"> 2023-07-10 08:20:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty. </font><br> Link: <a href='http://arxiv.org/pdf/2307.05360v1' target="_blank">http://arxiv.org/pdf/2307.05360v1</a><br> <br> <br> <font size='5'> 7 </font> <div style="text-align: right"> 2023-07-10 02:27:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Tabular data -- also known as structured data -- is one of the most common
data forms in existence, thanks to the stable development and scaled deployment
of database systems in the last few decades. At present however, despite the
blast brought by large pre-trained models in other domains such as ChatGPT or
SAM, how can we extract common knowledge across tables at a scale that may
eventually lead to generalizable representation for tabular data remains a full
blank. Indeed, there have been a few works around this topic. Most (if not all)
of them are limited in the scope of a single table or fixed form of a schema.
In this work, we first identify the crucial research challenges behind tabular
data pre-training, particularly towards the cross-table scenario. We position
the contribution of this work in two folds: (i)-we collect and curate nearly 2k
high-quality tabular datasets, each of which is guaranteed to possess clear
semantics, clean labels, and other necessary meta information. (ii)-we propose
a novel framework that allows cross-table pre-training dubbed as CT-BERT.
Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is
fully compatible with both supervised and self-supervised schemes, where the
specific instantiation of CT-BERT is very much dependent on the downstream
tasks. We further propose and implement a contrastive-learning-based and masked
table modeling (MTM) objective into CT-BERT, that is inspired from computer
vision and natural language processing communities but sophistically tailored
to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's
state-of-the-art performance, where both its supervised and self-supervised
setups significantly outperform the prior approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04308v1' target="_blank">http://arxiv.org/pdf/2307.04308v1</a><br> <br> <br> <font size='5'> 8 </font> <div style="text-align: right"> 2023-07-09 19:28:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04251v1' target="_blank">http://arxiv.org/pdf/2307.04251v1</a><br> <br> <br> <font size='5'> 9 </font> <div style="text-align: right"> 2023-07-09 13:38:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Generative Large Language Models Perform ASR Error Correction?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ASR error correction continues to serve as an important part of
post-processing for speech recognition systems. Traditionally, these models are
trained with supervised training using the decoding results of the underlying
ASR system and the reference text. This approach is computationally intensive
and the model needs to be re-trained when switching the underlying ASR model.
Recent years have seen the development of large language models and their
ability to perform natural language processing tasks in a zero-shot manner. In
this paper, we take ChatGPT as an example to examine its ability to perform ASR
error correction in the zero-shot or 1-shot settings. We use the ASR N-best
list as model input and propose unconstrained error correction and N-best
constrained error correction methods. Results on a Conformer-Transducer model
and the pre-trained Whisper model show that we can largely improve the ASR
system performance with error correction using the powerful ChatGPT model. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04172v1' target="_blank">http://arxiv.org/pdf/2307.04172v1</a><br> <br> <br> <font size='5'> 10 </font> <div style="text-align: right"> 2023-07-08 11:02:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good Personality Recognizer? A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, personality has been regarded as a valuable personal factor
being incorporated into numerous tasks such as sentiment analysis and product
recommendation. This has led to widespread attention to text-based personality
recognition task, which aims to identify an individual's personality based on
given text. Considering that ChatGPT has recently exhibited remarkable
abilities on various natural language processing tasks, we provide a
preliminary evaluation of ChatGPT on text-based personality recognition task
for generating effective personality data. Concretely, we employ a variety of
prompting strategies to explore ChatGPT's ability in recognizing personality
from given text, especially the level-oriented prompting strategy we designed
for guiding ChatGPT in analyzing given text at a specified level. We compare
the performance of ChatGPT on two representative real-world datasets with
traditional neural network, fine-tuned RoBERTa, and corresponding
state-of-the-art task-specific model. The experimental results show that
ChatGPT with zero-shot chain-of-thought prompting exhibits impressive
personality recognition ability. Triggered by zero-shot chain-of-thought
prompting, ChatGPT outperforms fine-tuned RoBERTa on the two datasets and is
capable to provide natural language explanations through text-based logical
reasoning. Furthermore, relative to zero-shot chain-of-thought prompting,
zero-shot level-oriented chain-of-thought prompting enhances the personality
prediction ability of ChatGPT and reduces the performance gap between ChatGPT
and corresponding state-of-the-art task-specific model. Besides, we also
conduct experiments to observe the fairness of ChatGPT when identifying
personality and discover that ChatGPT shows unfairness to some sensitive
demographic attributes such as gender and age. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03952v1' target="_blank">http://arxiv.org/pdf/2307.03952v1</a><br> <br> <br> <font size='5'> 11 </font> <div style="text-align: right"> 2023-07-07 21:13:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RADAR: Robust AI-Text Detection via Adversarial Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusation of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a Robust AI-text Detector via Adversarial
leaRning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic contents to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03838v1' target="_blank">http://arxiv.org/pdf/2307.03838v1</a><br> <br> <br> <font size='5'> 12 </font> <div style="text-align: right"> 2023-07-07 20:41:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How does AI chat change search behaviors?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI tools such as chatGPT are poised to change the way people
engage with online information. Recently, Microsoft announced their "new Bing"
search system which incorporates chat and generative AI technology from OpenAI.
Google has announced plans to deploy search interfaces that incorporate similar
types of technology. These new technologies will transform how people can
search for information. The research presented here is an early investigation
into how people make use of a generative AI chat system (referred to simply as
chat from here on) as part of a search process, and how the incorporation of
chat systems with existing search tools may effect users search behaviors and
strategies.
  We report on an exploratory user study with 10 participants who used a
combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing
Web Search v5 API. Participants completed three search tasks. In this pre-print
paper of preliminary results, we report on ways that users integrated AI chat
into their search process, things they liked and disliked about the chat
system, their trust in the chat responses, and their mental models of how the
chat system generated responses. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03826v1' target="_blank">http://arxiv.org/pdf/2307.03826v1</a><br> <br> <br> <font size='5'> 13 </font> <div style="text-align: right"> 2023-07-07 16:15:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Social media platforms such as Instagram and Twitter have emerged as critical
channels for drug marketing and illegal sale. Detecting and labeling online
illicit drug trafficking activities becomes important in addressing this issue.
However, the effectiveness of conventional supervised learning methods in
detecting drug trafficking heavily relies on having access to substantial
amounts of labeled data, while data annotation is time-consuming and
resource-intensive. Furthermore, these models often face challenges in
accurately identifying trafficking activities when drug dealers use deceptive
language and euphemisms to avoid detection. To overcome this limitation, we
conduct the first systematic study on leveraging large language models (LLMs),
such as ChatGPT, to detect illicit drug trafficking activities on social media.
We propose an analytical framework to compose \emph{knowledge-informed
prompts}, which serve as the interface that humans can interact with and use
LLMs to perform the detection task. Additionally, we design a Monte Carlo
dropout based prompt optimization method to further to improve performance and
interpretability. Our experimental findings demonstrate that the proposed
framework outperforms other baseline language models in terms of drug
trafficking detection accuracy, showing a remarkable improvement of nearly
12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can
effectively identify and label drug trafficking activities on social networks,
even in the presence of deceptive language and euphemisms used by drug dealers
to evade detection. The implications of our research extend to social networks,
emphasizing the importance of incorporating prior knowledge and scenario-based
prompts into analytical tools to improve online security and public safety. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03699v1' target="_blank">http://arxiv.org/pdf/2307.03699v1</a><br> <br> <br> <font size='5'> 14 </font> <div style="text-align: right"> 2023-07-07 13:05:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Text Simplification of Scientific Texts for Non-Expert Readers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reading levels are highly individual and can depend on a text's language, a
person's cognitive abilities, or knowledge on a topic. Text simplification is
the task of rephrasing a text to better cater to the abilities of a specific
target reader group. Simplification of scientific abstracts helps non-experts
to access the core information by bypassing formulations that require domain or
expert knowledge. This is especially relevant for, e.g., cancer patients
reading about novel treatment options. The SimpleText lab hosts the
simplification of scientific abstracts for non-experts (Task 3) to advance this
field. We contribute three runs employing out-of-the-box summarization models
(two based on T5, one based on PEGASUS) and one run using ChatGPT with complex
phrase identification. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03569v1' target="_blank">http://arxiv.org/pdf/2307.03569v1</a><br> <br> <br> <font size='5'> 15 </font> <div style="text-align: right"> 2023-07-07 08:14:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MultiQG-TI: Towards Question Generation from Multi-modal Sources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the new problem of automatic question generation (QG) from
multi-modal sources containing images and texts, significantly expanding the
scope of most of the existing work that focuses exclusively on QG from only
textual sources. We propose a simple solution for our new problem, called
MultiQG-TI, which enables a text-only question generator to process visual
input in addition to textual input. Specifically, we leverage an image-to-text
model and an optical character recognition model to obtain the textual
description of the image and extract any texts in the image, respectively, and
then feed them together with the input texts to the question generator. We only
fine-tune the question generator while keeping the other components fixed. On
the challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantly
outperforms ChatGPT with few-shot prompting, despite having hundred-times less
trainable parameters. Additional analyses empirically confirm the necessity of
both visual and textual signals for QG and show the impact of various modeling
choices. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04643v1' target="_blank">http://arxiv.org/pdf/2307.04643v1</a><br> <br> <br> <font size='5'> 16 </font> <div style="text-align: right"> 2023-07-07 02:18:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Augmented Reality for Maintenance Tasks with ChatGPT for Automated Text-to-Action</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Advancements in sensor technology, artificial intelligence (AI), and
augmented reality (AR) have unlocked opportunities across various domains. AR
and large language models like GPT have witnessed substantial progress and are
increasingly being employed in diverse fields. One such promising application
is in operations and maintenance (O&M). O&M tasks often involve complex
procedures and sequences that can be challenging to memorize and execute
correctly, particularly for novices or under high-stress situations. By
marrying the advantages of superimposing virtual objects onto the physical
world, and generating human-like text using GPT, we can revolutionize O&M
operations. This study introduces a system that combines AR, Optical Character
Recognition (OCR), and the GPT language model to optimize user performance
while offering trustworthy interactions and alleviating workload in O&M tasks.
This system provides an interactive virtual environment controlled by the Unity
game engine, facilitating a seamless interaction between virtual and physical
realities. A case study (N=15) is conducted to illustrate the findings and
answer the research questions. The results indicate that users can complete
similarly challenging tasks in less time using our proposed AR and AI system.
Moreover, the collected data also suggests a reduction in cognitive load and an
increase in trust when executing the same operations using the AR and AI
system. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03351v1' target="_blank">http://arxiv.org/pdf/2307.03351v1</a><br> <br> <br> <font size='5'> 17 </font> <div style="text-align: right"> 2023-07-06 15:42:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT's Responses Boost Traditional Natural Language Processing?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The employment of foundation models is steadily expanding, especially with
the launch of ChatGPT and the release of other foundation models. These models
have shown the potential of emerging capabilities to solve problems, without
being particularly trained to solve. A previous work demonstrated these
emerging capabilities in affective computing tasks; the performance quality was
similar to traditional Natural Language Processing (NLP) techniques, but
falling short of specialised trained models, like fine-tuning of the RoBERTa
language model. In this work, we extend this by exploring if ChatGPT has novel
knowledge that would enhance existing specialised models when they are fused
together. We achieve this by investigating the utility of verbose responses
from ChatGPT about solving a downstream task, in addition to studying the
utility of fusing that with existing NLP methods. The study is conducted on
three affective computing problems, namely sentiment analysis, suicide tendency
detection, and big-five personality assessment. The results conclude that
ChatGPT has indeed novel knowledge that can improve existing NLP techniques by
way of fusion, be it early or late fusion. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04648v1' target="_blank">http://arxiv.org/pdf/2307.04648v1</a><br> <br> <br> <font size='5'> 18 </font> <div style="text-align: right"> 2023-07-06 11:53:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Amplifying Limitations, Harms and Risks of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present this article as a small gesture in an attempt to counter what
appears to be exponentially growing hype around Artificial Intelligence (AI)
and its capabilities, and the distraction provided by the associated talk of
science-fiction scenarios that might arise if AI should become sentient and
super-intelligent. It may also help those outside of the field to become more
informed about some of the limitations of AI technology. In the current context
of popular discourse AI defaults to mean foundation and large language models
(LLMs) such as those used to create ChatGPT. This in itself is a
misrepresentation of the diversity, depth and volume of research, researchers,
and technology that truly represents the field of AI. AI being a field of
research that has existed in software artefacts since at least the 1950's. We
set out to highlight a number of limitations of LLMs, and in so doing highlight
that harms have already arisen and will continue to arise due to these
limitations. Along the way we also highlight some of the associated risks for
individuals and organisations in using this technology. </font><br> Link: <a href='http://arxiv.org/pdf/2307.04821v1' target="_blank">http://arxiv.org/pdf/2307.04821v1</a><br> <br> <br> <font size='5'> 19 </font> <div style="text-align: right"> 2023-07-06 06:07:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What Should Data Science Education Do with Large Language Models?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it's crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02792v2' target="_blank">http://arxiv.org/pdf/2307.02792v2</a><br> <br> <br> <font size='5'> 20 </font> <div style="text-align: right"> 2023-07-06 02:28:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Text Alignment Is An Efficient Unified Model for Massive NLP Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), typically designed as a function of next-word
prediction, have excelled across extensive NLP tasks. Despite the generality,
next-word prediction is often not an efficient formulation for many of the
tasks, demanding an extreme scale of model parameters (10s or 100s of billions)
and sometimes yielding suboptimal performance. In practice, it is often
desirable to build more efficient models -- despite being less versatile, they
still apply to a substantial subset of problems, delivering on par or even
superior performance with much smaller model sizes. In this paper, we propose
text alignment as an efficient unified model for a wide range of crucial tasks
involving text entailment, similarity, question answering (and answerability),
factual consistency, and so forth. Given a pair of texts, the model measures
the degree of alignment between their information. We instantiate an alignment
model (Align) through lightweight finetuning of RoBERTa (355M parameters) using
5.9M examples from 28 datasets. Despite its compact size, extensive experiments
show the model's efficiency and strong performance: (1) On over 20 datasets of
aforementioned diverse tasks, the model matches or surpasses FLAN-T5 models
that have around 2x or 10x more parameters; the single unified model also
outperforms task-specific models finetuned on individual datasets; (2) When
applied to evaluate factual consistency of language generation on 23 datasets,
our model improves over various baselines, including the much larger GPT-3.5
(ChatGPT) and sometimes even GPT-4; (3) The lightweight model can also serve as
an add-on component for LLMs such as GPT-3.5 in question answering tasks,
improving the average exact match (EM) score by 17.94 and F1 score by 15.05
through identifying unanswerable questions. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02729v1' target="_blank">http://arxiv.org/pdf/2307.02729v1</a><br> <br> <br> <font size='5'> 21 </font> <div style="text-align: right"> 2023-07-05 21:42:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as ChatGPT have demonstrated unprecedented
capabilities in multiple AI tasks. However, hardware inefficiencies have become
a significant factor limiting the democratization of LLMs. We propose Chiplet
Cloud, an ASIC supercomputer architecture that optimizes total cost of
ownership (TCO) per token for serving generative LLMs. Chiplet Cloud fits all
model parameters inside the on-chip SRAMs to eliminate bandwidth limitations
while moderating the die size to improve system costs while leveraging software
mappings to overcome data communication overhead. We propose a comprehensive
design methodology that accurately explores a spectrum of major design
trade-offs in the joint space of hardware-software and generates a detailed
performance-cost analysis on all valid design points. We evaluate Chiplet Cloud
on four popular LLMs. Compared to GPU and TPU, our architecture can achieve up
to 94x and 15x improvement in TCO/Token respectively, significantly reducing
the cost for realistically serving modern LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02666v1' target="_blank">http://arxiv.org/pdf/2307.02666v1</a><br> <br> <br> <font size='5'> 22 </font> <div style="text-align: right"> 2023-07-05 18:48:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evade ChatGPT Detectors via A Single Space</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT brings revolutionary social value but also raises concerns about the
misuse of AI-generated content. Consequently, an important question is how to
detect whether content is generated by ChatGPT or by human. Existing detectors
are built upon the assumption that there are distributional gaps between
human-generated and AI-generated content. These gaps are typically identified
using statistical information or classifiers. Our research challenges the
distributional gap assumption in detectors. We find that detectors do not
effectively discriminate the semantic and stylistic gaps between
human-generated and AI-generated content. Instead, the "subtle differences",
such as an extra space, become crucial for detection. Based on this discovery,
we propose the SpaceInfi strategy to evade detection. Experiments demonstrate
the effectiveness of this strategy across multiple benchmarks and detectors. We
also provide a theoretical explanation for why SpaceInfi is successful in
evading perplexity-based detection. Our findings offer new insights and
challenges for understanding and constructing more applicable ChatGPT
detectors. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02599v1' target="_blank">http://arxiv.org/pdf/2307.02599v1</a><br> <br> <br> <font size='5'> 23 </font> <div style="text-align: right"> 2023-07-05 17:58:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Jailbroken: How Does LLM Safety Training Fail?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models trained for safety and harmlessness remain susceptible
to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on
early releases of ChatGPT that elicit undesired behavior. Going beyond
recognition of the issue, we investigate why such attacks succeed and how they
can be created. We hypothesize two failure modes of safety training: competing
objectives and mismatched generalization. Competing objectives arise when a
model's capabilities and safety goals conflict, while mismatched generalization
occurs when safety training fails to generalize to a domain for which
capabilities exist. We use these failure modes to guide jailbreak design and
then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's
Claude v1.3, against both existing and newly designed attacks. We find that
vulnerabilities persist despite the extensive red-teaming and safety-training
efforts behind these models. Notably, new attacks utilizing our failure modes
succeed on every prompt in a collection of unsafe requests from the models'
red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our
analysis emphasizes the need for safety-capability parity -- that safety
mechanisms should be as sophisticated as the underlying model -- and argues
against the idea that scaling alone can resolve these safety failure modes. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02483v1' target="_blank">http://arxiv.org/pdf/2307.02483v1</a><br> <br> <br> <font size='5'> 24 </font> <div style="text-align: right"> 2023-07-05 14:15:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we present the contribution of the BLUE team in the eRisk Lab
task on searching for symptoms of depression. The task consists of retrieving
and ranking Reddit social media sentences that convey symptoms of depression
from the BDI-II questionnaire. Given that synthetic data provided by LLMs have
been proven to be a reliable method for augmenting data and fine-tuning
downstream models, we chose to generate synthetic data using ChatGPT for each
of the symptoms of the BDI-II questionnaire. We designed a prompt such that the
generated data contains more richness and semantic diversity than the BDI-II
responses for each question and, at the same time, contains emotional and
anecdotal experiences that are specific to the more intimate way of sharing
experiences on Reddit. We perform semantic search and rank the sentences'
relevance to the BDI-II symptoms by cosine similarity. We used two
state-of-the-art transformer-based models (MentalRoBERTa and a variant of
MPNet) for embedding the social media posts, the original and generated
responses of the BDI-II. Our results show that using sentence embeddings from a
model designed for semantic search outperforms the approach using embeddings
from a model pre-trained on mental health data. Furthermore, the generated
synthetic data were proved too specific for this task, the approach simply
relying on the BDI-II responses had the best performance. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02313v2' target="_blank">http://arxiv.org/pdf/2307.02313v2</a><br> <br> <br> <font size='5'> 25 </font> <div style="text-align: right"> 2023-07-05 13:59:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sumformer: Universal Approximation for Efficient Transformers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Natural language processing (NLP) made an impressive jump with the
introduction of Transformers. ChatGPT is one of the most famous examples,
changing the perception of the possibilities of AI even outside the research
community. However, besides the impressive performance, the quadratic time and
space complexity of Transformers with respect to sequence length pose
significant limitations for handling long sequences. While efficient
Transformer architectures like Linformer and Performer with linear complexity
have emerged as promising solutions, their theoretical understanding remains
limited. In this paper, we introduce Sumformer, a novel and simple architecture
capable of universally approximating equivariant sequence-to-sequence
functions. We use Sumformer to give the first universal approximation results
for Linformer and Performer. Moreover, we derive a new proof for Transformers,
showing that just one attention layer is sufficient for universal
approximation. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02301v1' target="_blank">http://arxiv.org/pdf/2307.02301v1</a><br> <br> <br> <font size='5'> 26 </font> <div style="text-align: right"> 2023-07-05 13:40:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a performance comparison of three large language models
(LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the
VNHSGE English dataset. The results show that BingChat is better than ChatGPT
and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not
yet officially available in Vietnam. The results also indicate that ChatGPT,
Bing Chat, and Bard outperform Vietnamese students in English language
proficiency. The findings of this study contribute to the understanding of the
potential of LLMs in English language education. The remarkable performance of
ChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools
for teaching and learning English at the high school level. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02288v2' target="_blank">http://arxiv.org/pdf/2307.02288v2</a><br> <br> <br> <font size='5'> 27 </font> <div style="text-align: right"> 2023-07-05 10:15:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study examines the performance of open-source Large Language Models
(LLMs) in text annotation tasks and compares it with proprietary models like
ChatGPT and human-based services such as MTurk. While prior research
demonstrated the high performance of ChatGPT across numerous NLP tasks,
open-source LLMs like HugginChat and FLAN are gaining attention for their
cost-effectiveness, transparency, reproducibility, and superior data
protection. We assess these models using both zero-shot and few-shot approaches
and different temperature parameters across a range of text annotation tasks.
Our findings show that while ChatGPT achieves the best performance in most
tasks, open-source LLMs not only outperform MTurk but also demonstrate
competitive potential against ChatGPT in specific tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02179v1' target="_blank">http://arxiv.org/pdf/2307.02179v1</a><br> <br> <br> <font size='5'> 28 </font> <div style="text-align: right"> 2023-07-05 06:36:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, the release of INSTRUCTEVAL has provided valuable insights into the
performance of large language models (LLMs) that utilize encoder-decoder or
decoder-only architecture. Interestingly, despite being introduced four years
ago, T5-based LLMs, such as FLAN-T5, continue to outperform the latest
decoder-based LLMs, such as LLAMA and VICUNA, on tasks that require general
problem-solving skills. This performance discrepancy can be attributed to three
key factors: (1) Pre-training data, (2) Backbone architecture, and (3)
Instruction dataset. In this technical report, our main focus is on
investigating the impact of the third factor by leveraging VICUNA, a large
language model based on LLAMA, which has undergone fine-tuning on ChatGPT
conversations. To achieve this objective, we fine-tuned VICUNA using a
customized instruction dataset collection called FLANMINI. This collection
includes a subset of the large-scale instruction dataset known as FLAN, as well
as various code-related datasets and conversational datasets derived from
ChatGPT/GPT-4. This dataset comprises a large number of tasks that demand
problem-solving skills. Our experimental findings strongly indicate that the
enhanced problem-solving abilities of our model, FLACUNA, are obtained through
fine-tuning VICUNA on the FLAN dataset, leading to significant improvements
across numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly
available at https://huggingface.co/declare-lab/flacuna-13b-v1.0. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02053v1' target="_blank">http://arxiv.org/pdf/2307.02053v1</a><br> <br> <br> <font size='5'> 29 </font> <div style="text-align: right"> 2023-07-05 06:03:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Recommender Systems in the Era of Large Language Models (LLMs)</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the prosperity of e-commerce and web applications, Recommender Systems
(RecSys) have become an important component of our daily life, providing
personalized suggestions that cater to user preferences. While Deep Neural
Networks (DNNs) have made significant advancements in enhancing recommender
systems by modeling user-item interactions and incorporating textual side
information, DNN-based methods still face limitations, such as difficulties in
understanding users' interests and capturing textual side information,
inabilities in generalizing to various recommendation scenarios and reasoning
on their predictions, etc. Meanwhile, the emergence of Large Language Models
(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural
Language Processing (NLP) and Artificial Intelligence (AI), due to their
remarkable abilities in fundamental responsibilities of language understanding
and generation, as well as impressive generalization and reasoning
capabilities. As a result, recent studies have attempted to harness the power
of LLMs to enhance recommender systems. Given the rapid evolution of this
research direction in recommender systems, there is a pressing need for a
systematic overview that summarizes existing LLM-empowered recommender systems,
to provide researchers in relevant fields with an in-depth understanding.
Therefore, in this paper, we conduct a comprehensive review of LLM-empowered
recommender systems from various aspects including Pre-training, Fine-tuning,
and Prompting. More specifically, we first introduce representative methods to
harness the power of LLMs (as a feature encoder) for learning representations
of users and items. Then, we review recent techniques of LLMs for enhancing
recommender systems from three paradigms, namely pre-training, fine-tuning, and
prompting. Finally, we comprehensively discuss future directions in this
emerging field. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02046v1' target="_blank">http://arxiv.org/pdf/2307.02046v1</a><br> <br> <br> <font size='5'> 30 </font> <div style="text-align: right"> 2023-07-05 04:14:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Research suggests that providing specific and timely feedback to human tutors
enhances their performance. However, it presents challenges due to the
time-consuming nature of assessing tutor performance by human evaluators. Large
language models, such as the AI-chatbot ChatGPT, hold potential for offering
constructive feedback to tutors in practical settings. Nevertheless, the
accuracy of AI-generated feedback remains uncertain, with scant research
investigating the ability of models like ChatGPT to deliver effective feedback.
In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a
tutor-student setting. We use two different prompting approaches, the zero-shot
chain of thought and the few-shot chain of thought, to identify specific
components of effective praise based on five criteria. These approaches are
then compared to the results of human graders for accuracy. Our goal is to
assess the extent to which GPT-4 can accurately identify each praise criterion.
We found that both zero-shot and few-shot chain of thought approaches yield
comparable results. GPT-4 performs moderately well in identifying instances
when the tutor offers specific and immediate praise. However, GPT-4
underperforms in identifying the tutor's ability to deliver sincere praise,
particularly in the zero-shot prompting scenario where examples of sincere
tutor praise statements were not provided. Future work will focus on enhancing
prompt engineering, developing a more general tutoring rubric, and evaluating
our method using real-life tutoring dialogues. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02018v1' target="_blank">http://arxiv.org/pdf/2307.02018v1</a><br> <br> <br> <font size='5'> 31 </font> <div style="text-align: right"> 2023-07-05 01:45:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Zero-shot medical image classification is a critical process in real-world
scenarios where we have limited access to all possible diseases or large-scale
annotated data. It involves computing similarity scores between a query medical
image and possible disease categories to determine the diagnostic result.
Recent advances in pretrained vision-language models (VLMs) such as CLIP have
shown great performance for zero-shot natural image recognition and exhibit
benefits in medical applications. However, an explainable zero-shot medical
image recognition framework with promising performance is yet under
development. In this paper, we propose a novel CLIP-based zero-shot medical
image classification framework supplemented with ChatGPT for explainable
diagnosis, mimicking the diagnostic process performed by human experts. The key
idea is to query large language models (LLMs) with category names to
automatically generate additional cues and knowledge, such as disease symptoms
or descriptions other than a single category name, to help provide more
accurate and explainable diagnosis in CLIP. We further design specific prompts
to enhance the quality of generated texts by ChatGPT that describe visual
medical features. Extensive results on one private dataset and four public
datasets along with detailed analysis demonstrate the effectiveness and
explainability of our training-free zero-shot diagnosis pipeline, corroborating
the great potential of VLMs and LLMs for medical applications. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01981v1' target="_blank">http://arxiv.org/pdf/2307.01981v1</a><br> <br> <br> <font size='5'> 32 </font> <div style="text-align: right"> 2023-07-04 07:51:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning to Prompt in the Classroom to Understand AI Limits: A pilot study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence's progress holds great promise in assisting society
in addressing pressing societal issues. In particular Large Language Models
(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural
language processing capabilities of AI systems allowing them to process an
unprecedented amount of unstructured data. The consequent hype has also
backfired, raising negative sentiment even after novel AI methods' surprising
contributions. One of the causes, but also an important issue per se, is the
rising and misleading feeling of being able to access and process any form of
knowledge to solve problems in any domain with no effort or previous expertise
in AI or problem domain, disregarding current LLMs limits, such as
hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to
address the impact of dogmatic overconfidence in possibly erroneous suggestions
generated by LLMs. At the same time, it can reduce fear and other negative
attitudes toward AI. AI literacy interventions are necessary that allow the
public to understand such LLM limits and learn how to use them in a more
effective manner, i.e. learning to "prompt". With this aim, a pilot educational
intervention was performed in a high school with 30 students. It involved (i)
presenting high-level concepts about intelligence, AI, and LLM, (ii) an initial
naive practice with ChatGPT in a non-trivial task, and finally (iii) applying
currently-accepted prompting strategies. Encouraging preliminary results have
been collected such as students reporting a) high appreciation of the activity,
b) improved quality of the interaction with the LLM during the educational
activity, c) decreased negative sentiments toward AI, d) increased
understanding of limitations and specifically We aim to study factors that
impact AI acceptance and to refine and repeat this activity in more controlled
settings. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01540v1' target="_blank">http://arxiv.org/pdf/2307.01540v1</a><br> <br> <br> <font size='5'> 33 </font> <div style="text-align: right"> 2023-07-03 21:54:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multilingual Language Models are not Multicultural: A Case Study in Emotion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Emotions are experienced and expressed differently across the world. In order
to use Large Language Models (LMs) for multilingual tasks that require
emotional sensitivity, LMs must reflect this cultural variation in emotion. In
this study, we investigate whether the widely-used multilingual LMs in 2023
reflect differences in emotional expressions across cultures and languages. We
find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,
and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding
to prompts in other languages. Our results show that multilingual LMs do not
successfully learn the culturally appropriate nuances of emotion and we
highlight possible research directions towards correcting this. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01370v2' target="_blank">http://arxiv.org/pdf/2307.01370v2</a><br> <br> <br> <font size='5'> 34 </font> <div style="text-align: right"> 2023-07-03 16:15:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advent of ChatGPT, a large language model-powered chatbot, has prompted
questions about its potential implications for traditional search engines. In
this study, we investigate the differences in user behavior when employing
search engines and chatbot tools for information-seeking tasks. We carry out a
randomized online experiment, dividing participants into two groups: one using
a ChatGPT-like tool and the other using a Google Search-like tool. Our findings
reveal that the ChatGPT group consistently spends less time on all tasks, with
no significant difference in overall task performance between the groups.
Notably, ChatGPT levels user search performance across different education
levels and excels in answering straightforward questions and providing general
solutions but falls short in fact-checking tasks. Users perceive ChatGPT's
responses as having higher information quality compared to Google Search,
despite displaying a similar level of trust in both tools. Furthermore,
participants using ChatGPT report significantly better user experiences in
terms of usefulness, enjoyment, and satisfaction, while perceived ease of use
remains comparable between the two tools. However, ChatGPT may also lead to
overreliance and generate or replicate misinformation, yielding inconsistent
results. Our study offers valuable insights for search engine management and
highlights opportunities for integrating chatbot technologies into search
engine designs. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01135v1' target="_blank">http://arxiv.org/pdf/2307.01135v1</a><br> <br> <br> <font size='5'> 35 </font> <div style="text-align: right"> 2023-07-03 15:35:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is not a pocket calculator -- Problems of AI-chatbots for teaching Geography</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent success of large language models and AI chatbots such as ChatGPT
in various knowledge domains has a severe impact on teaching and learning
Geography and GIScience. The underlying revolution is often compared to the
introduction of pocket calculators, suggesting analogous adaptations that
prioritize higher-level skills over other learning content. However, using
ChatGPT can be fraudulent because it threatens the validity of assessments. The
success of such a strategy therefore rests on the assumption that lower-level
learning goals are substitutable by AI, and supervision and assessments can be
refocused on higher-level goals. Based on a preliminary survey on ChatGPT's
quality in answering questions in Geography and GIScience, we demonstrate that
this assumption might be fairly naive, and effective control in assessments and
supervision is required. </font><br> Link: <a href='http://arxiv.org/pdf/2307.03196v1' target="_blank">http://arxiv.org/pdf/2307.03196v1</a><br> <br> <br> <font size='5'> 36 </font> <div style="text-align: right"> 2023-07-03 00:36:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Undoubtedly, the evolution of Generative AI (GenAI) models has been the
highlight of digital transformation in the year 2022. As the different GenAI
models like ChatGPT and Google Bard continue to foster their complexity and
capability, it's critical to understand its consequences from a cybersecurity
perspective. Several instances recently have demonstrated the use of GenAI
tools in both the defensive and offensive side of cybersecurity, and focusing
on the social, ethical and privacy implications this technology possesses. This
research paper highlights the limitations, challenges, potential risks, and
opportunities of GenAI in the domain of cybersecurity and privacy. The work
presents the vulnerabilities of ChatGPT, which can be exploited by malicious
users to exfiltrate malicious information bypassing the ethical constraints on
the model. This paper demonstrates successful example attacks like Jailbreaks,
reverse psychology, and prompt injection attacks on the ChatGPT. The paper also
investigates how cyber offenders can use the GenAI tools in developing cyber
attacks, and explore the scenarios where ChatGPT can be used by adversaries to
create social engineering attacks, phishing attacks, automated hacking, attack
payload generation, malware creation, and polymorphic malware. This paper then
examines defense techniques and uses GenAI tools to improve security measures,
including cyber defense automation, reporting, threat intelligence, secure code
generation and detection, attack identification, developing ethical guidelines,
incidence response plans, and malware detection. We will also discuss the
social, legal, and ethical implications of ChatGPT. In conclusion, the paper
highlights open challenges and future directions to make this GenAI secure,
safe, trustworthy, and ethical as the community understands its cybersecurity
impacts. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00691v1' target="_blank">http://arxiv.org/pdf/2307.00691v1</a><br> <br> <br> <font size='5'> 37 </font> <div style="text-align: right"> 2023-07-02 15:20:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM4CBI: Taming LLMs to Generate Effective Test Programs for Compiler Bug Isolation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Compiler bugs pose a significant threat to safety-critical applications, and
promptly and effectively isolating these bugs is crucial for assuring the
quality of compilers. However, the limited availability of debugging
information on reported bugs complicates the compiler bug isolation task.
Existing compiler bug isolation approaches typically convert the problem into a
test program mutation problem, but they are still limited by ineffective
mutation strategies or high human effort requirements. Drawing inspiration from
the recent progress of pre-trained Large Language Models (LLMs), such as
ChatGPT, in code generation, we propose a new approach named LLM4CBI to tame
LLMs to generate effective test programs for compiler bug isolation. However,
using LLMs directly for test program mutation may not yield the desired results
due to the challenges associated with formulating precise prompts and selecting
specialized prompts. To overcome the challenges, three new components are
designed in LLM4CBI. (1) LLM4CBI utilizes a program complexity-guided prompt
production component, which leverages data and control flow analysis to
identify the most valuable variables and locations in programs for mutation.
(2) LLM4CBI employs a memorized prompt selection component, which adopts
reinforcement learning to select specialized prompts for mutating test programs
continuously. (3) A test program validation component is proposed to select
specialized feedback prompts to avoid repeating the same mistakes during the
mutation process. Compared with the state-of-the-art approaches (DiWi and
RecBi), our evaluation demonstrates the advantages of LLM4CBI: It isolates more
bugs, ranging from 13.6% to 90.9% in various settings, than the other
approaches. Additionally, we demonstrate that LLM4CBI is extensible, allowing
for easy integration with other LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00593v1' target="_blank">http://arxiv.org/pdf/2307.00593v1</a><br> <br> <br> <font size='5'> 38 </font> <div style="text-align: right"> 2023-07-02 15:09:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in large language models (LLMs) have demonstrated
exceptional success in a wide range of general domain tasks, such as question
answering and following instructions. Moreover, LLMs have shown potential in
various software engineering applications. In this study, we present a
systematic comparison of test suites generated by the ChatGPT LLM and the
state-of-the-art SBST tool EvoSuite. Our comparison is based on several
critical factors, including correctness, readability, code coverage, and bug
detection capability. By highlighting the strengths and weaknesses of LLMs
(specifically ChatGPT) in generating unit test cases compared to EvoSuite, this
work provides valuable insights into the performance of LLMs in solving
software engineering problems. Overall, our findings underscore the potential
of LLMs in software engineering and pave the way for further research in this
area. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00588v1' target="_blank">http://arxiv.org/pdf/2307.00588v1</a><br> <br> <br> <font size='5'> 39 </font> <div style="text-align: right"> 2023-06-30 19:53:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence is gaining traction in more ways than ever before.
The popularity of language models and AI-based businesses has soared since
ChatGPT was made available to the general public via OpenAI. It is becoming
increasingly common for people to use ChatGPT both professionally and
personally. Considering the widespread use of ChatGPT and the reliance people
place on it, this study determined how reliable ChatGPT can be for answering
complex medical and clinical questions. Harvard University gross anatomy along
with the United States Medical Licensing Examination (USMLE) questionnaire were
used to accomplish the objective. The paper evaluated the obtained results
using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation
between format and prompt. Furthermore, the physician adjudicators
independently rated the outcome's accuracy, concordance, and insight. As a
result of the analysis, ChatGPT-generated answers were found to be more
context-oriented and represented a better model for deductive reasoning than
regular Google search results. Furthermore, ChatGPT obtained 58.8% on logical
questions and 60% on ethical questions. This means that the ChatGPT is
approaching the passing range for logical questions and has crossed the
threshold for ethical questions. The paper believes ChatGPT and other language
learning models can be invaluable tools for e-learners; however, the study
suggests that there is still room to improve their accuracy. In order to
improve ChatGPT's performance in the future, further research is needed to
better understand how it can answer different types of questions. </font><br> Link: <a href='http://arxiv.org/pdf/2307.00112v1' target="_blank">http://arxiv.org/pdf/2307.00112v1</a><br> <br> <br> <font size='5'> 40 </font> <div style="text-align: right"> 2023-06-30 09:07:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Preference Ranking Optimization for Human Alignment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secur AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment by combining a reward model, typically based on Bradley-Terry
paired comparison, with an RL algorithm such as Proximal Policy Optimization
(PPO) to optimize LLM responses. However, RLHF exhibits complexity,
instability, and sensitivity to hyperparameters. In this paper, we propose
Preference Ranking Optimization (PRO) as an alternative to PPO for directly
aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise
Bradley-Terry comparison to accommodate preference rankings of any length. By
iteratively contrasting the likelihood of generating responses, PRO instructs
the LLM to prioritize the best response while progressively ranking the
remaining responses. In this manner, PRO effectively transforms human alignment
into aligning the probability ranking of $n$ responses generated by LLM with
the preference ranking of humans towards these responses. Experiments have
shown that PRO outperforms existing alignment algorithms, achieving comparable
results to ChatGPT and human responses through automatic-based, reward-based,
GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more
diverse, and higher-quality preference ranking sequences can consistently
enhance the performance of human alignment. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17492v1' target="_blank">http://arxiv.org/pdf/2306.17492v1</a><br> <br> <br> <font size='5'> 41 </font> <div style="text-align: right"> 2023-06-30 06:02:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DeepTagger: Knowledge Enhanced Named Entity Recognition for Web-Based Ads Queries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Named entity recognition (NER) is a crucial task for online advertisement.
State-of-the-art solutions leverage pre-trained language models for this task.
However, three major challenges remain unresolved: web queries differ from
natural language, on which pre-trained models are trained; web queries are
short and lack contextual information; and labeled data for NER is scarce. We
propose DeepTagger, a knowledge-enhanced NER model for web-based ads queries.
The proposed knowledge enhancement framework leverages both model-free and
model-based approaches. For model-free enhancement, we collect unlabeled web
queries to augment domain knowledge; and we collect web search results to
enrich the information of ads queries. We further leverage effective prompting
methods to automatically generate labels using large language models such as
ChatGPT. Additionally, we adopt a model-based knowledge enhancement method
based on adversarial data augmentation. We employ a three-stage training
framework to train DeepTagger models. Empirical results in various NER tasks
demonstrate the effectiveness of the proposed framework. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17413v1' target="_blank">http://arxiv.org/pdf/2306.17413v1</a><br> <br> <br> <font size='5'> 42 </font> <div style="text-align: right"> 2023-06-29 17:57:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI and large language models hold great promise in enhancing
computing education by powering next-generation educational technologies for
introductory programming. Recent works have studied these models for different
scenarios relevant to programming education; however, these works are limited
for several reasons, as they typically consider already outdated models or only
specific scenario(s). Consequently, there is a lack of a systematic study that
benchmarks state-of-the-art models for a comprehensive set of programming
education scenarios. In our work, we systematically evaluate two models,
ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human
tutors for a variety of scenarios. We evaluate using five introductory Python
programming problems and real-world buggy programs from an online platform, and
assess performance using expert-based annotations. Our results show that GPT-4
drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human
tutors' performance for several scenarios. These results also highlight
settings where GPT-4 still struggles, providing exciting future directions on
developing techniques to improve the performance of these models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17156v2' target="_blank">http://arxiv.org/pdf/2306.17156v2</a><br> <br> <br> <font size='5'> 43 </font> <div style="text-align: right"> 2023-06-29 17:01:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic
lyrics transcription method achieving state-of-the-art performance on various
lyrics transcription datasets, even in challenging genres such as rock and
metal. Our novel, training-free approach utilizes Whisper, a weakly supervised
robust speech recognition model, and GPT-4, today's most performant chat-based
large language model. In the proposed method, Whisper functions as the "ear" by
transcribing the audio, while GPT-4 serves as the "brain," acting as an
annotator with a strong performance for contextualized output selection and
correction. Our experiments show that LyricWhiz significantly reduces Word
Error Rate compared to existing methods in English and can effectively
transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to
create the first publicly available, large-scale, multilingual lyrics
transcription dataset with a CC-BY-NC-SA copyright license, based on
MTG-Jamendo, and offer a human-annotated subset for noise level estimation and
evaluation. We anticipate that our proposed method and dataset will advance the
development of multilingual lyrics transcription, a challenging and emerging
task. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17103v2' target="_blank">http://arxiv.org/pdf/2306.17103v2</a><br> <br> <br> <font size='5'> 44 </font> <div style="text-align: right"> 2023-06-29 13:30:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023
shared task for Task-A and Task-C. We focus especially on Task-C and propose a
novel LLMs cooperation system named a doctor-patient loop to generate
high-quality conversation data sets. The experiment results demonstrate that
our approaches yield reasonable performance as evaluated by automatic metrics
such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we
conducted a comparative analysis between our proposed method and ChatGPT and
GPT-4. This analysis also investigates the potential of utilizing cooperation
LLMs to generate high-quality datasets. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16931v1' target="_blank">http://arxiv.org/pdf/2306.16931v1</a><br> <br> <br> <font size='5'> 45 </font> <div style="text-align: right"> 2023-06-29 02:28:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While open-ended self-explanations have been shown to promote robust learning
in multiple studies, they pose significant challenges to automated grading and
feedback in technology-enhanced learning, due to the unconstrained nature of
the students' input. Our work investigates whether recent advances in Large
Language Models, and in particular ChatGPT, can address this issue. Using
decimal exercises and student data from a prior study of the learning game
Decimal Point, with more than 5,000 open-ended self-explanation responses, we
investigate ChatGPT's capability in (1) solving the in-game exercises, (2)
determining the correctness of students' answers, and (3) providing meaningful
feedback to incorrect answers. Our results showed that ChatGPT can respond well
to conceptual questions, but struggled with decimal place values and number
line problems. In addition, it was able to accurately assess the correctness of
75% of the students' answers and generated generally high-quality feedback,
similar to human instructors. We conclude with a discussion of ChatGPT's
strengths and weaknesses and suggest several venues for extending its use cases
in digital teaching and learning. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16639v1' target="_blank">http://arxiv.org/pdf/2306.16639v1</a><br> <br> <br> <font size='5'> 46 </font> <div style="text-align: right"> 2023-06-28 15:54:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have demonstrated impressive performance on
various downstream tasks without requiring fine-tuning, including ChatGPT, a
chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having
a lower training proportion compared to English, these models also exhibit
remarkable capabilities in other languages. In this study, we assess the
performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:
sentiment analysis, translation, transliteration, paraphrasing, part of speech
tagging, summarization, and diacritization. Our findings reveal that GPT-4
outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an
extensive analysis of the sentiment analysis task, providing insights into how
LLMs achieve exceptional results on a challenging dialectal dataset.
Additionally, we introduce a new Python interface
https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks
effortlessly. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16322v1' target="_blank">http://arxiv.org/pdf/2306.16322v1</a><br> <br> <br> <font size='5'> 47 </font> <div style="text-align: right"> 2023-06-28 14:55:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Food effect summarization from New Drug Application (NDA) is an essential
component of product-specific guidance (PSG) development and assessment.
However, manual summarization of food effect from extensive drug application
review documents is time-consuming, which arouses a need to develop automated
methods. Recent advances in large language models (LLMs) such as ChatGPT and
GPT-4, have demonstrated great potential in improving the effectiveness of
automated text summarization, but its ability regarding the accuracy in
summarizing food effect for PSG assessment remains unclear. In this study, we
introduce a simple yet effective approach, iterative prompting, which allows
one to interact with ChatGPT or GPT-4 more effectively and efficiently through
multi-turn interaction. Specifically, we propose a three-turn iterative
prompting approach to food effect summarization in which the keyword-focused
and length-controlled prompts are respectively provided in consecutive turns to
refine the quality of the generated summary. We conduct a series of extensive
evaluations, ranging from automated metrics to FDA professionals and even
evaluation by GPT-4, on 100 NDA review documents selected over the past five
years. We observe that the summary quality is progressively improved throughout
the process. Moreover, we find that GPT-4 performs better than ChatGPT, as
evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).
Importantly, all the FDA professionals unanimously rated that 85% of the
summaries generated by GPT-4 are factually consistent with the golden reference
summary, a finding further supported by GPT-4 rating of 72% consistency. These
results strongly suggest a great potential for GPT-4 to draft food effect
summaries that could be reviewed by FDA professionals, thereby improving the
efficiency of PSG assessment cycle and promoting the generic drug product
development. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16275v1' target="_blank">http://arxiv.org/pdf/2306.16275v1</a><br> <br> <br> <font size='5'> 48 </font> <div style="text-align: right"> 2023-06-28 11:24:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We assessed the performance of commercial Large Language Models (LLMs)
GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b
Phase B, which is focused on answer generation, both models demonstrated
competitive abilities with leading systems. Remarkably, they achieved this with
simple zero-shot learning, grounded with relevant snippets. Even without
relevant snippets, their performance was decent, though not on par with the
best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was
able to compete with GPT-4 in the grounded Q&A setting on factoid and list
answers. In Task 11b Phase A, focusing on retrieval, query expansion through
zero-shot learning improved performance, but the models fell short compared to
other systems. The code needed to rerun these experiments is available through
GitHub. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16108v1' target="_blank">http://arxiv.org/pdf/2306.16108v1</a><br> <br> <br> <font size='5'> 49 </font> <div style="text-align: right"> 2023-06-28 03:31:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have been recently leveraged as training data
generators for various natural language processing (NLP) tasks. While previous
research has explored different approaches to training models using generated
data, they generally rely on simple class-conditional prompts, which may limit
the diversity of the generated data and inherit systematic biases of LLM. Thus,
we investigate training data generation with diversely attributed prompts
(e.g., specifying attributes like length and style), which have the potential
to yield diverse and attributed generated data. Our investigation focuses on
datasets with high cardinality and diverse domains, wherein we demonstrate that
attributed prompts outperform simple class-conditional prompts in terms of the
resulting model's performance. Additionally, we present a comprehensive
empirical study on data generation encompassing vital aspects like bias,
diversity, and efficiency, and highlight three key observations: firstly,
synthetic datasets generated by simple prompts exhibit significant biases, such
as regional bias; secondly, attribute diversity plays a pivotal role in
enhancing model performance; lastly, attributed prompts achieve the performance
of simple class-conditional prompts while utilizing only 5\% of the querying
cost of ChatGPT associated with the latter. We release the generated dataset
and used prompts to facilitate future research. The data and code will be
available on \url{https://github.com/yueyu1030/AttrPrompt}. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15895v1' target="_blank">http://arxiv.org/pdf/2306.15895v1</a><br> <br> <br> <font size='5'> 50 </font> <div style="text-align: right"> 2023-06-27 18:32:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DMNER: Biomedical Entity Recognition by Detection and Matching</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Biomedical named entity recognition (BNER) serves as the foundation for
numerous biomedical text mining tasks. Unlike general NER, BNER require a
comprehensive grasp of the domain, and incorporating external knowledge beyond
training data poses a significant challenge. In this study, we propose a novel
BNER framework called DMNER. By leveraging existing entity representation
models SAPBERT, we tackle BNER as a two-step process: entity boundary detection
and biomedical entity matching. DMNER exhibits applicability across multiple
NER scenarios: 1) In supervised NER, we observe that DMNER effectively
rectifies the output of baseline NER models, thereby further enhancing
performance. 2) In distantly supervised NER, combining MRC and AutoNER as span
boundary detectors enables DMNER to achieve satisfactory results. 3) For
training NER by merging multiple datasets, we adopt a framework similar to
DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the
training. Through extensive experiments conducted on 10 benchmark datasets, we
demonstrate the versatility and effectiveness of DMNER. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15736v2' target="_blank">http://arxiv.org/pdf/2306.15736v2</a><br> <br> <br> <font size='5'> 51 </font> <div style="text-align: right"> 2023-06-27 16:49:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring Durham University Physics exams with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of advanced Natural Language Processing (NLP) models like
ChatGPT has raised concerns among universities regarding AI-driven exam
completion. This paper provides a comprehensive evaluation of the proficiency
of GPT-4 and GPT-3.5 in answering a set of 42 exam papers derived from 10
distinct physics courses, administered at Durham University over the span of
2018 to 2022, totalling 593 questions and 2504 available marks. These exams,
spanning both undergraduate and postgraduate levels, include traditional
pre-COVID and adaptive COVID-era formats. Questions from the years 2018-2020
were designed for pre-COVID in person adjudicated examinations whereas the
2021-2022 exams were set for varying COVID-adapted conditions including
open-book conditions. To ensure a fair evaluation of AI performances, the exams
completed by AI were assessed by the original exam markers. However, due to
staffing constraints, only the aforementioned 593 out of the total 1280
questions were marked. GPT-4 and GPT-3.5 scored an average of 49.4\% and
38.6\%, respectively, suggesting only the weaker students would potential
improve their marks if using AI. For exams from the pre-COVID era, the average
scores for GPT-4 and GPT-3.5 were 50.8\% and 41.6\%, respectively. However,
post-COVID, these dropped to 47.5\% and 33.6\%. Thus contrary to expectations,
the change to less fact-based questions in the COVID era did not significantly
impact AI performance for the state-of-the-art models such as GPT-4. These
findings suggest that while current AI models struggle with university-level
Physics questions, an improving trend is observable. The code used for
automated AI completion is made publicly available for further research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15609v1' target="_blank">http://arxiv.org/pdf/2306.15609v1</a><br> <br> <br> <font size='5'> 52 </font> <div style="text-align: right"> 2023-06-27 15:23:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CrunchGPT: A chatGPT assisted framework for scientific machine learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Scientific Machine Learning (SciML) has advanced recently across many
different areas in computational science and engineering. The objective is to
integrate data and physics seamlessly without the need of employing elaborate
and computationally taxing data assimilation schemes. However, preprocessing,
problem formulation, code generation, postprocessing and analysis are still
time consuming and may prevent SciML from wide applicability in industrial
applications and in digital twin frameworks. Here, we integrate the various
stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which
plays the role of a conductor orchestrating the entire workflow of SciML based
on simple prompts by the user. Specifically, we present two examples that
demonstrate the potential use of CrunchGPT in optimizing airfoils in
aerodynamics, and in obtaining flow fields in various geometries in interactive
mode, with emphasis on the validation stage. To demonstrate the flow of the
CrunchGPT, and create an infrastructure that can facilitate a broader vision,
we built a webapp based guided user interface, that includes options for a
comprehensive summary report. The overall objective is to extend CrunchGPT to
handle diverse problems in computational mechanics, design, optimization and
controls, and general scientific computing tasks involved in SciML, hence using
it as a research assistant tool but also as an educational tool. While here the
examples focus in fluid mechanics, future versions will target solid mechanics
and materials science, geophysics, systems biology and bioinformatics. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15551v1' target="_blank">http://arxiv.org/pdf/2306.15551v1</a><br> <br> <br> <font size='5'> 53 </font> <div style="text-align: right"> 2023-06-27 07:57:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the field of machine reading comprehension (MRC), existing systems have
surpassed the average performance of human beings in many tasks like SQuAD.
However, there is still a long way to go when it comes to logical reasoning.
Although some methods for it have been put forward, they either are designed in
a quite complicated way or rely too much on external structures. In this paper,
we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand
but highly effective further pre-training task which logically strengthens the
pre-trained models with the help of 6 types of logical indicators and a
logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art
performance on ReClor and LogiQA, the two most representative benchmarks in
logical reasoning MRC, and is proven to be capable of generalizing to different
pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0
while keeping competitive general language understanding ability through
testing on tasks in GLUE. Besides, at the beginning of the era of large
language models, we take several of them like ChatGPT into comparison and find
that IDOL still shows its advantage. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15273v1' target="_blank">http://arxiv.org/pdf/2306.15273v1</a><br> <br> <br> <font size='5'> 54 </font> <div style="text-align: right"> 2023-06-26 10:48:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Robustness of Large Language Models for Solving Programming Problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Using large language models (LLMs) for source code has recently gained
attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have
been shown to be highly capable of solving a wide range of programming
problems. However, the extent to which LLMs understand problem descriptions and
generate programs accordingly or just retrieve source code from the most
relevant problem in training data based on superficial cues has not been
discovered yet. To explore this research question, we conduct experiments to
understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series
models, capable of tackling code generation tasks in introductory programming
problems. Our experimental results show that CodeGen and Codex are sensitive to
the superficial modifications of problem descriptions and significantly impact
code generation performance. Furthermore, we observe that Codex relies on
variable names, as randomized variables decrease the solved rate significantly.
However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,
show higher robustness to superficial modifications and have an outstanding
capability for solving programming problems. This highlights the fact that
slight modifications to the prompts given to the LLMs can greatly affect code
generation performance, and careful formatting of prompts is essential for
high-quality code generation, while the SOTA models are becoming more robust to
perturbations. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14583v1' target="_blank">http://arxiv.org/pdf/2306.14583v1</a><br> <br> <br> <font size='5'> 55 </font> <div style="text-align: right"> 2023-06-26 08:21:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatIDS: Explainable Cybersecurity Using Generative AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Intrusion Detection Systems (IDS) are a proven approach to secure networks.
However, in a privately used network, it is difficult for users without
cybersecurity expertise to understand IDS alerts, and to respond in time with
adequate measures. This puts the security of home networks, smart home
installations, home-office workers, etc. at risk, even if an IDS is correctly
installed and configured. In this work, we propose ChatIDS, our approach to
explain IDS alerts to non-experts by using large language models. We evaluate
the feasibility of ChatIDS by using ChatGPT, and we identify open research
issues with the help of interdisciplinary experts in artificial intelligence.
Our results show that ChatIDS has the potential to increase network security by
proposing meaningful security measures in an intuitive language from IDS
alerts. Nevertheless, some potential issues in areas such as trust, privacy,
ethics, etc. need to be resolved, before ChatIDS might be put into practice. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14504v1' target="_blank">http://arxiv.org/pdf/2306.14504v1</a><br> <br> <br> <font size='5'> 56 </font> <div style="text-align: right"> 2023-06-26 03:15:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Discriminating Human-authored from ChatGPT-Generated Code Via Discernable Feature Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ubiquitous adoption of Large Language Generation Models (LLMs) in
programming has underscored the importance of differentiating between
human-written code and code generated by intelligent models. This paper
specifically aims to distinguish code generated by ChatGPT from that authored
by humans. Our investigation reveals disparities in programming style,
technical level, and readability between these two sources. Consequently, we
develop a discriminative feature set for differentiation and evaluate its
efficacy through ablation experiments. Additionally, we devise a dataset
cleansing technique, which employs temporal and spatial segmentation, to
mitigate the dearth of datasets and to secure high-caliber, uncontaminated
datasets. To further enrich data resources, we employ "code transformation,"
"feature transformation," and "feature customization" techniques, generating an
extensive dataset comprising 10,000 lines of ChatGPT-generated code. The
salient contributions of our research include: proposing a discriminative
feature set yielding high accuracy in differentiating ChatGPT-generated code
from human-authored code in binary classification tasks; devising methods for
generating extensive ChatGPT-generated codes; and introducing a dataset
cleansing strategy that extracts immaculate, high-grade code datasets from
open-source repositories, thus achieving exceptional accuracy in code
authorship attribution tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14397v2' target="_blank">http://arxiv.org/pdf/2306.14397v2</a><br> <br> <br> <font size='5'> 57 </font> <div style="text-align: right"> 2023-06-25 12:08:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancement of Large Language Models (LLMs) has led to extensive
discourse regarding their potential to boost the return of quantitative stock
trading strategies. This discourse primarily revolves around harnessing the
remarkable comprehension capabilities of LLMs to extract sentiment factors
which facilitate informed and high-frequency investment portfolio adjustments.
To ensure successful implementations of these LLMs into the analysis of Chinese
financial texts and the subsequent trading strategy development within the
Chinese stock market, we provide a rigorous and encompassing benchmark as well
as a standardized back-testing framework aiming at objectively assessing the
efficacy of various types of LLMs in the specialized domain of sentiment factor
extraction from Chinese news text data. To illustrate how our benchmark works,
we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the
Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 3) the
financial domain-specific fine-tuned LLM classifier(Chinese FinBERT). We apply
them directly to the task of sentiment factor extraction from large volumes of
Chinese news summary texts. We then proceed to building quantitative trading
strategies and running back-tests under realistic trading scenarios based on
the derived sentiment factors and evaluate their performances with our
benchmark. By constructing such a comparative analysis, we invoke the question
of what constitutes the most important element for improving a LLM's
performance on extracting sentiment factors. And by ensuring that the LLMs are
evaluated on the same benchmark, following the same standardized experimental
procedures that are designed with sufficient expertise in quantitative trading,
we make the first stride toward answering such a question. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14222v1' target="_blank">http://arxiv.org/pdf/2306.14222v1</a><br> <br> <br> <font size='5'> 58 </font> <div style="text-align: right"> 2023-06-23 22:39:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Potential Benefits of Employing Large Language Models in Research in Moral Education and Development</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, computer scientists have developed large language models (LLMs) by
training prediction models with large-scale language corpora and human
reinforcements. The LLMs have become one promising way to implement artificial
intelligence with accuracy in various fields. Interestingly, recent LLMs
possess emergent functional features that emulate sophisticated human
cognition, especially in-context learning and the chain of thought, which were
unavailable in previous prediction models. In this paper, I will examine how
LLMs might contribute to moral education and development research. To achieve
this goal, I will review the most recently published conference papers and
ArXiv preprints to overview the novel functional features implemented in LLMs.
I also intend to conduct brief experiments with ChatGPT to investigate how LLMs
behave while addressing ethical dilemmas and external feedback. The results
suggest that LLMs might be capable of solving dilemmas based on reasoning and
revising their reasoning process with external input. I will discuss the
potential implications of LLMs on research on moral education and development
with the results. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13805v1' target="_blank">http://arxiv.org/pdf/2306.13805v1</a><br> <br> <br> <font size='5'> 59 </font> <div style="text-align: right"> 2023-06-23 20:57:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deductive coding is a widely used qualitative research method for determining
the prevalence of themes across documents. While useful, deductive coding is
often burdensome and time consuming since it requires researchers to read,
interpret, and reliably categorize a large body of unstructured text documents.
Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI
tools that can perform a range of natural language processing and reasoning
tasks. In this study, we explore the use of LLMs to reduce the time it takes
for deductive coding while retaining the flexibility of a traditional content
analysis. We outline the proposed approach, called LLM-assisted content
analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a
publicly available deductive coding data set. Additionally, we conduct an
empirical benchmark using LACA on 4 publicly available data sets to assess the
broader question of how well GPT-3.5 performs across a range of deductive
coding tasks. Overall, we find that GPT-3.5 can often perform deductive coding
at levels of agreement comparable to human coders. Additionally, we demonstrate
that LACA can help refine prompts for deductive coding, identify codes for
which an LLM is randomly guessing, and help assess when to use LLMs vs. human
coders for deductive coding. We conclude with several implications for future
practice of deductive coding and related research methods. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14924v1' target="_blank">http://arxiv.org/pdf/2306.14924v1</a><br> <br> <br> <font size='5'> 60 </font> <div style="text-align: right"> 2023-06-23 15:19:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of ChatGPT has been rapid, and although it has demonstrated
positive impacts in certain domains, its influence is not universally
advantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics
Education, particularly in teaching basic Linear Algebra. While there are
instances where ChatGPT delivers accurate and well-motivated answers, it is
crucial to recognize numerous cases where it makes significant mathematical
errors and fails in logical inference. These occurrences raise concerns
regarding the system's genuine understanding of mathematics, as it appears to
rely more on visual patterns rather than true comprehension. Additionally, the
suitability of ChatGPT as a teacher for students also warrants consideration. </font><br> Link: <a href='http://arxiv.org/pdf/2306.16282v1' target="_blank">http://arxiv.org/pdf/2306.16282v1</a><br> <br> <br> <font size='5'> 61 </font> <div style="text-align: right"> 2023-06-23 15:15:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research delves into the construction and utilization of synthetic
datasets, specifically within the telematics sphere, leveraging OpenAI's
powerful language model, ChatGPT. Synthetic datasets present an effective
solution to challenges pertaining to data privacy, scarcity, and control over
variables - characteristics that make them particularly valuable for research
pursuits. The utility of these datasets, however, largely depends on their
quality, measured through the lenses of diversity, relevance, and coherence. To
illustrate this data creation process, a hands-on case study is conducted,
focusing on the generation of a synthetic telematics dataset. The experiment
involved an iterative guidance of ChatGPT, progressively refining prompts and
culminating in the creation of a comprehensive dataset for a hypothetical urban
planning scenario in Columbus, Ohio. Upon generation, the synthetic dataset was
subjected to an evaluation, focusing on the previously identified quality
parameters and employing descriptive statistics and visualization techniques
for a thorough analysis. Despite synthetic datasets not serving as perfect
replacements for actual world data, their potential in specific use-cases, when
executed with precision, is significant. This research underscores the
potential of AI models like ChatGPT in enhancing data availability for complex
sectors like telematics, thus paving the way for a myriad of new research
opportunities. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13700v1' target="_blank">http://arxiv.org/pdf/2306.13700v1</a><br> <br> <br> <font size='5'> 62 </font> <div style="text-align: right"> 2023-06-23 09:30:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Product Information Extraction using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Structured product data in the form of attribute/value pairs is the
foundation of many e-commerce applications such as faceted product search,
product comparison, and product recommendation. Product offers often only
contain textual descriptions of the product attributes in the form of titles or
free text. Hence, extracting attribute/value pairs from textual product
descriptions is an essential enabler for e-commerce applications. In order to
excel, state-of-the-art product information extraction methods require large
quantities of task-specific training data. The methods also struggle with
generalizing to out-of-distribution attributes and attribute values that were
not a part of the training data. Due to being pre-trained on huge amounts of
text as well as due to emergent effects resulting from the model size, Large
Language Models like ChatGPT have the potential to address both of these
shortcomings. This paper explores the potential of ChatGPT for extracting
attribute/value pairs from product descriptions. We experiment with different
zero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a
performance similar to a pre-trained language model but requires much smaller
amounts of training data and computation for fine-tuning. </font><br> Link: <a href='http://arxiv.org/pdf/2306.14921v1' target="_blank">http://arxiv.org/pdf/2306.14921v1</a><br> <br> <br> <font size='5'> 63 </font> <div style="text-align: right"> 2023-06-23 05:21:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring Qualitative Research Using LLMs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advent of AI driven large language models (LLMs) have stirred discussions
about their role in qualitative research. Some view these as tools to enrich
human understanding, while others perceive them as threats to the core values
of the discipline. This study aimed to compare and contrast the comprehension
capabilities of humans and LLMs. We conducted an experiment with small sample
of Alexa app reviews, initially classified by a human analyst. LLMs were then
asked to classify these reviews and provide the reasoning behind each
classification. We compared the results with human classification and
reasoning. The research indicated a significant alignment between human and
ChatGPT 3.5 classifications in one third of cases, and a slightly lower
alignment with GPT4 in over a quarter of cases. The two AI models showed a
higher alignment, observed in more than half of the instances. However, a
consensus across all three methods was seen only in about one fifth of the
classifications. In the comparison of human and LLMs reasoning, it appears that
human analysts lean heavily on their individual experiences. As expected, LLMs,
on the other hand, base their reasoning on the specific word choices found in
app reviews and the functional components of the app itself. Our results
highlight the potential for effective human LLM collaboration, suggesting a
synergistic rather than competitive relationship. Researchers must continuously
evaluate LLMs role in their work, thereby fostering a future where AI and
humans jointly enrich qualitative research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13298v1' target="_blank">http://arxiv.org/pdf/2306.13298v1</a><br> <br> <br> <font size='5'> 64 </font> <div style="text-align: right"> 2023-06-22 17:07:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Explainable Evaluation Metrics for Machine Translation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Unlike classical lexical overlap metrics such as BLEU, most current
evaluation metrics for machine translation (for example, COMET or BERTScore)
are based on black-box large language models. They often achieve strong
correlations with human judgments, but recent research indicates that the
lower-quality classical metrics remain dominant, one of the potential reasons
being that their decision processes are more transparent. To foster more
widespread acceptance of novel high-quality metrics, explainability thus
becomes crucial. In this concept paper, we identify key properties as well as
key goals of explainable machine translation metrics and provide a
comprehensive synthesis of recent techniques, relating them to our established
goals and properties. In this context, we also discuss the latest
state-of-the-art approaches to explainable metrics based on generative models
such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation
approaches, including natural language explanations. We hope that our work can
help catalyze and guide future research on explainable evaluation metrics and,
mediately, also contribute to better and more transparent machine translation
systems. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13041v1' target="_blank">http://arxiv.org/pdf/2306.13041v1</a><br> <br> <br> <font size='5'> 65 </font> <div style="text-align: right"> 2023-06-22 15:10:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT sets a new record with the fastest-growing user base, as a chatbot
powered by a large language model (LLM). While it demonstrates state-of-the-art
capabilities in a variety of language-generating tasks, it also raises
widespread public concerns regarding its societal impact. In this paper, we
utilize natural language processing approaches to investigate the public
attitudes towards ChatGPT by applying sentiment analysis and topic modeling
techniques to Twitter data. Our result shows that the overall sentiment is
largely neutral to positive, which also holds true across different occupation
groups. Among a wide range of topics mentioned in tweets, the most popular
topics are Artificial Intelligence, Search Engines, Education, Writing, and
Question Answering. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12951v1' target="_blank">http://arxiv.org/pdf/2306.12951v1</a><br> <br> <br> <font size='5'> 66 </font> <div style="text-align: right"> 2023-06-22 14:31:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While summarization has been extensively researched in natural language
processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a
largely unexplored area that has the potential to improve cross-cultural
accessibility, information sharing, and understanding. This paper
comprehensively addresses the CLCTS task, including dataset creation, modeling,
and evaluation. We build the first CLCTS corpus, leveraging historical fictive
texts and Wikipedia summaries in English and German, and examine the
effectiveness of popular transformer end-to-end models with different
intermediate task finetuning tasks. Additionally, we explore the potential of
ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report
evaluations from humans, ChatGPT, and several recent automatic evaluation
metrics where we find our intermediate task finetuned end-to-end models
generate bad to moderate quality summaries; ChatGPT as a summarizer (without
any finetuning) provides moderate to good quality outputs and as an evaluator
correlates moderately with human evaluations though it is prone to giving lower
scores. ChatGPT also seems to be very adept at normalizing historical text. We
finally test ChatGPT in a scenario with adversarially attacked and unseen
source documents and find that ChatGPT is better at omission and entity swap
than negating against its prior knowledge. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12916v1' target="_blank">http://arxiv.org/pdf/2306.12916v1</a><br> <br> <br> <font size='5'> 67 </font> <div style="text-align: right"> 2023-06-22 13:21:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Analysis of innovation has been fundamentally limited by conventional
approaches to broad, structural variables. This paper pushes the boundaries,
taking an LLM approach to patent analysis with the groundbreaking ChatGPT
technology. OpenAI's state-of-the-art textual embedding accesses complex
information about the quality and impact of each invention to power deep
learning predictive models. The nuanced embedding drives a 24% incremental
improvement in R-squared predicting patent value and clearly isolates the worst
and best applications. These models enable a revision of the contemporary
Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median
deviation of 1.5 times, accounting for potential institutional predictions.
Furthermore, the market fails to incorporate timely information about
applications; a long-short portfolio based on predicted acceptance rates
achieves significant abnormal returns of 3.3% annually. The models provide an
opportunity to revolutionize startup and small-firm corporate policy vis-a-vis
patenting. </font><br> Link: <a href='http://arxiv.org/pdf/2307.01202v1' target="_blank">http://arxiv.org/pdf/2307.01202v1</a><br> <br> <br> <font size='5'> 68 </font> <div style="text-align: right"> 2023-06-22 03:56:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sentiment analysis is a vital tool for uncovering insights from financial
articles, news, and social media, shaping our understanding of market
movements. Despite the impressive capabilities of large language models (LLMs)
in financial natural language processing (NLP), they still struggle with
accurately interpreting numerical values and grasping financial context,
limiting their effectiveness in predicting financial sentiment. In this paper,
we introduce a simple yet effective instruction tuning approach to address
these issues. By transforming a small portion of supervised financial sentiment
analysis data into instruction data and fine-tuning a general-purpose LLM with
this method, we achieve remarkable advancements in financial sentiment
analysis. In the experiment, our approach outperforms state-of-the-art
supervised sentiment analysis models, as well as widely used LLMs like ChatGPT
and LLaMAs, particularly in scenarios where numerical understanding and
contextual comprehension are vital. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12659v1' target="_blank">http://arxiv.org/pdf/2306.12659v1</a><br> <br> <br> <font size='5'> 69 </font> <div style="text-align: right"> 2023-06-22 03:52:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Identifying and Extracting Rare Disease Phenotypes with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Rare diseases (RDs) are collectively common and affect 300 million people
worldwide. Accurate phenotyping is critical for informing diagnosis and
treatment, but RD phenotypes are often embedded in unstructured text and
time-consuming to extract manually. While natural language processing (NLP)
models can perform named entity recognition (NER) to automate extraction, a
major bottleneck is the development of a large, annotated corpus for model
training. Recently, prompt learning emerged as an NLP paradigm that can lead to
more generalizable results without any (zero-shot) or few labeled samples
(few-shot). Despite growing interest in ChatGPT, a revolutionary large language
model capable of following complex human prompts and generating high-quality
responses, none have studied its NER performance for RDs in the zero- and
few-shot settings. To this end, we engineered novel prompts aimed at extracting
RD phenotypes and, to the best of our knowledge, are the first the establish a
benchmark for evaluating ChatGPT's performance in these settings. We compared
its performance to the traditional fine-tuning approach and conducted an
in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in
higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the
zero- and few-shot settings, respectively). Despite this, ChatGPT achieved
similar or higher accuracy for certain entities (i.e., rare diseases and signs)
in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with
appropriate prompt engineering, ChatGPT has the potential to match or
outperform fine-tuned language models for certain entity types with just one
labeled sample. While the proliferation of large language models may provide
opportunities for supporting RD diagnosis and treatment, researchers and
clinicians should critically evaluate model outputs and be well-informed of
their limitations. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12656v1' target="_blank">http://arxiv.org/pdf/2306.12656v1</a><br> <br> <br> <font size='5'> 70 </font> <div style="text-align: right"> 2023-06-21 16:29:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Testing of Detection Tools for AI-Generated Text</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in generative pre-trained transformer large language models
have emphasised the potential risks of unfair use of artificial intelligence
(AI) generated content in an academic environment and intensified efforts in
searching for solutions to detect such content. The paper examines the general
functionality of detection tools for artificial intelligence generated text and
evaluates them based on accuracy and error type analysis. Specifically, the
study seeks to answer research questions about whether existing detection tools
can reliably differentiate between human-written text and ChatGPT-generated
text, and whether machine translation and content obfuscation techniques affect
the detection of AI-generated text. The research covers 12 publicly available
tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely
used in the academic setting. The researchers conclude that the available
detection tools are neither accurate nor reliable and have a main bias towards
classifying the output as human-written rather than detecting AI-generated
text. Furthermore, content obfuscation techniques significantly worsen the
performance of tools. The study makes several significant contributions. First,
it summarises up-to-date similar scientific and non-scientific efforts in the
field. Second, it presents the result of one of the most comprehensive tests
conducted so far, based on a rigorous research methodology, an original
document set, and a broad coverage of tools. Third, it discusses the
implications and drawbacks of using detection tools for AI-generated text in
academic settings. </font><br> Link: <a href='http://arxiv.org/pdf/2306.15666v2' target="_blank">http://arxiv.org/pdf/2306.15666v2</a><br> <br> <br> <font size='5'> 71 </font> <div style="text-align: right"> 2023-06-21 15:42:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The disruptive technology provided by large-scale pre-trained language models
(LLMs) such as ChatGPT or GPT-4 has received significant attention in several
application domains, often with an emphasis on high-level opportunities and
concerns. This paper is the first examination regarding the use of LLMs for
scientific simulations. We focus on four modeling and simulation tasks, each
time assessing the expected benefits and limitations of LLMs while providing
practical guidance for modelers regarding the steps involved. The first task is
devoted to explaining the structure of a conceptual model to promote the
engagement of participants in the modeling process. The second task focuses on
summarizing simulation outputs, so that model users can identify a preferred
scenario. The third task seeks to broaden accessibility to simulation platforms
by conveying the insights of simulation visualizations via text. Finally, the
last task evokes the possibility of explaining simulation errors and providing
guidance to resolve them. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13679v1' target="_blank">http://arxiv.org/pdf/2306.13679v1</a><br> <br> <br> <font size='5'> 72 </font> <div style="text-align: right"> 2023-06-21 09:26:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as a tool for User Story Quality Evaluation: Trustworthy Out of the Box?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In Agile software development, user stories play a vital role in capturing
and conveying end-user needs, prioritizing features, and facilitating
communication and collaboration within development teams. However, automated
methods for evaluating user stories require training in NLP tools and can be
time-consuming to develop and integrate. This study explores using ChatGPT for
user story quality evaluation and compares its performance with an existing
benchmark. Our study shows that ChatGPT's evaluation aligns well with human
evaluation, and we propose a ``best of three'' strategy to improve its output
stability. We also discuss the concept of trustworthiness in AI and its
implications for non-experts using ChatGPT's unprocessed outputs. Our research
contributes to understanding the reliability and applicability of AI in user
story evaluation and offers recommendations for future research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12132v1' target="_blank">http://arxiv.org/pdf/2306.12132v1</a><br> <br> <br> <font size='5'> 73 </font> <div style="text-align: right"> 2023-06-21 05:31:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of foundation models, such as large language models (LLMs)
GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities
across various domains. People can now use natural language (i.e. prompts) to
communicate with AI to perform tasks. While people can use foundation models
through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the
underlying models, is not a production tool for building reusable AI services.
APIs like LangChain allow for LLM-based application development but require
substantial programming knowledge, thus posing a barrier. To mitigate this, we
propose the concept of AI chain and introduce the best principles and practices
that have been accumulated in software engineering for decades into AI chain
engineering, to systematise AI chain engineering methodology. We also develop a
no-code integrated development environment, Prompt Sapper, which embodies these
AI chain engineering principles and patterns naturally in the process of
building AI chains, thereby improving the performance and quality of AI chains.
With Prompt Sapper, AI chain engineers can compose prompt-based AI services on
top of foundation models through chat-based requirement analysis and visual
programming. Our user study evaluated and demonstrated the efficiency and
correctness of Prompt Sapper. </font><br> Link: <a href='http://arxiv.org/pdf/2306.12028v1' target="_blank">http://arxiv.org/pdf/2306.12028v1</a><br> <br> <br> <font size='5'> 74 </font> <div style="text-align: right"> 2023-06-21 02:13:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Chain of AI-based Solutions for Resolving FQNs and Fixing Syntax Errors in Partial Code</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: API documentation, technical blogs and programming Q&A sites contain numerous
partial code that can be reused in programming tasks, but often these code are
uncompilable due to unresolved names and syntax errors. To facilitate partial
code reuse, we propose the Partial Code Reuse Chain (PCR-Chain) for resolving
fully-qualified names (FQNs) and fixing last-mile syntax errors in partial code
based on a giant large language model (LLM) like ChatGPT. Methodologically,
PCR-Chain is backed up by the underlying global-level prompt architecture
(which combines three design ideas: hierarchical task breakdown, prompt
composition, and a mix of prompt-based AI and non-AI units) and the local-level
prompt design. Technically, we propose PCR-Chain, which employs in-context
learning rather than symbolic, costly training methods. Experimental results
demonstrate that in dynamically-typed languages (Python), PCR-Chain outperforms
current state-of-the-art (SOTA) 5% accuracy like RING. For statically-type
languages (Java), our approach achieves high accuracy of 80.5% in resolving
both non-FQNs and last-mile syntax errors, surpassing SOTA methods (RING) that
can only address last-mile syntax errors. The correct execution of the unit,
module, and PCR-Chain demonstrates the effectiveness of the prompt design,
composition, and architecture and opens up possibilities for building software
engineering tools based on LLMs, replacing traditional program analysis
methods. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11981v1' target="_blank">http://arxiv.org/pdf/2306.11981v1</a><br> <br> <br> <font size='5'> 75 </font> <div style="text-align: right"> 2023-06-21 02:12:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-based Smart Reply (LSR): Enhancing Collaborative Performance with ChatGPT-mediated Smart Reply System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: CSCW studies have increasingly explored AI's role in enhancing communication
efficiency and productivity in collaborative tasks. AI tools such as chatbots,
smart replies, and language models aim to optimize conversation management and
improve team performance. Early AI assistants, such as Gmail smart reply, were
limited by predefined knowledge bases and decision trees. However, the advent
of large language models (LLMs) such as ChatGPT has revolutionized AI
assistants, employing advanced deep learning architecture to generate
context-aware, coherent, and personalized responses. Consequently,
ChatGPT-based AI assistants provide a more natural and efficient user
experience across various tasks and domains. In this paper, we formalize the
concept of AI Collaborative Tools (ACT) as AI technologies in human
collaborative work and discuss how the emergence of ChatGPT has transformed the
AI landscape and increased focus on ACT for improving team performance.
Meanwhile, we present an LLM-based Smart Reply (LSR) system utilizing the
ChatGPT API to generate personalized responses in daily collaborative
scenarios, considering context, tone, and communication style. Our two-step
process involves generating a preliminary response type (e.g., Agree, Disagree)
to provide a generalized direction for message generation, thus reducing
response drafting time. We conducted an experiment in which participants
completed simulated work tasks, involving Google Calendar manipulation and a
double-back N-back test, while interacting with researchers posing as teammates
requesting scheduling changes. Our findings indicate that the AI teammate
increases perceived performance and reduces mental demand, as measured by the
NASA TLX, and improves performance in the N-back task. We also provide
qualitative feedback on participants' experiences working with the AI teammate. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11980v2' target="_blank">http://arxiv.org/pdf/2306.11980v2</a><br> <br> <br> <font size='5'> 76 </font> <div style="text-align: right"> 2023-06-21 02:05:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interactive Molecular Discovery with Natural Language</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Natural language is expected to be a key medium for various human-machine
interactions in the era of large language models. When it comes to the
biochemistry field, a series of tasks around molecules (e.g., property
prediction, molecule mining, etc.) are of great significance while having a
high technical threshold. Bridging the molecule expressions in natural language
and chemical language can not only hugely improve the interpretability and
reduce the operation difficulty of these tasks, but also fuse the chemical
knowledge scattered in complementary materials for a deeper comprehension of
molecules. Based on these benefits, we propose the conversational molecular
design, a novel task adopting natural language for describing and editing
target molecules. To better accomplish this task, we design ChatMol, a
knowledgeable and versatile generative pre-trained model, enhanced by injecting
experimental property information, molecular spatial knowledge, and the
associations between natural and chemical languages into it. Several typical
solutions including large language models (e.g., ChatGPT) are evaluated,
proving the challenge of conversational molecular design and the effectiveness
of our knowledge enhancement method. Case observations and analysis are
conducted to provide directions for further exploration of natural-language
interaction in molecular discovery. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11976v1' target="_blank">http://arxiv.org/pdf/2306.11976v1</a><br> <br> <br> <font size='5'> 77 </font> <div style="text-align: right"> 2023-06-20 21:12:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11892v1' target="_blank">http://arxiv.org/pdf/2306.11892v1</a><br> <br> <br> <font size='5'> 78 </font> <div style="text-align: right"> 2023-06-20 18:19:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning to Generate Better Than Your LLM</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for conditional text generation. In
particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent
conversations with users by incorporating RL and feedback from humans. Inspired
by learning-to-search algorithms and capitalizing on key properties of text
generation, we seek to investigate reinforcement learning algorithms beyond
general purpose algorithms such as Proximal policy optimization (PPO). In
particular, we extend RL algorithms to allow them to interact with a dynamic
black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a
suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive
review and CommonGen text generation task from the GRUE benchmark. We show that
our RL algorithms achieve higher performance than supervised learning (SL) and
default PPO baselines, demonstrating the benefit of interaction with the guide
LLM. On CommonGen, we not only outperform our SL baselines but also improve
beyond PPO across a variety of lexical and semantic metrics beyond the one we
optimized for. Notably, on the IMDB dataset, we show that our GPT-2 based
policy outperforms the zero-shot GPT-3 oracle, indicating that our algorithms
can learn from a powerful, black-box GPT-3 oracle with a simpler, cheaper, and
publicly available GPT-2 model while gaining performance. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11816v1' target="_blank">http://arxiv.org/pdf/2306.11816v1</a><br> <br> <br> <font size='5'> 79 </font> <div style="text-align: right"> 2023-06-20 15:02:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FAIR: A Causal Framework for Accurately Inferring Judgments Reversals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence researchers have made significant advances in legal
intelligence in recent years. However, the existing studies have not focused on
the important value embedded in judgments reversals, which limits the
improvement of the efficiency of legal intelligence. In this paper, we propose
a causal Framework for Accurately Inferring case Reversals (FAIR), which models
the problem of judgments reversals based on real Chinese judgments. We mine the
causes of judgments reversals by causal inference methods and inject the
obtained causal relationships into the neural network as a priori knowledge.
And then, our framework is validated on a challenging dataset as a legal
judgment prediction task. The experimental results show that our framework can
tap the most critical factors in judgments reversal, and the obtained causal
relationships can effectively improve the neural network's performance. In
addition, we discuss the generalization ability of large language models for
legal intelligence tasks using ChatGPT as an example. Our experiment has found
that the generalization ability of large language models still has defects, and
mining causal relationships can effectively improve the accuracy and explain
ability of model predictions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11585v1' target="_blank">http://arxiv.org/pdf/2306.11585v1</a><br> <br> <br> <font size='5'> 80 </font> <div style="text-align: right"> 2023-06-20 12:53:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) such as ChatGPT, have gained significant
attention due to their impressive natural language processing capabilities. It
is crucial to prioritize human-centered principles when utilizing these models.
Safeguarding the ethical and moral compliance of LLMs is of utmost importance.
However, individual ethical issues have not been well studied on the latest
LLMs. Therefore, this study aims to address these gaps by introducing a new
benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in
three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT
examines toxicity in language models by employing toxic prompt templates
derived from social norms. It then quantifies the extent of bias in models by
measuring quantifiable toxicity values across different groups. Lastly,
TrustGPT assesses the value of conversation generation models from both active
value-alignment and passive value-alignment tasks. Through the implementation
of TrustGPT, this research aims to enhance our understanding of the performance
of conversation generation models and promote the development of language
models that are more ethical and socially responsible. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11507v1' target="_blank">http://arxiv.org/pdf/2306.11507v1</a><br> <br> <br> <font size='5'> 81 </font> <div style="text-align: right"> 2023-06-20 12:21:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11489v1' target="_blank">http://arxiv.org/pdf/2306.11489v1</a><br> <br> <br> <font size='5'> 82 </font> <div style="text-align: right"> 2023-06-20 07:06:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, the advent of pre-trained large-scale language models (LLMs) like
ChatGPT and GPT-4 have significantly advanced the machine's natural language
understanding capabilities. This breakthrough has allowed us to seamlessly
integrate these open-source LLMs into a unified robot simulator environment to
help robots accurately understand and execute human natural language
instructions. To this end, in this work, we introduce a realistic robotic
manipulation simulator and build a Robotic Manipulation with Progressive
Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT
benchmark builds a new high-fidelity digital twin scene based on Unreal Engine
5, which includes 782 categories, 2023 objects, and 15K natural language
instructions generated by ChatGPT for a detailed evaluation of robot
manipulation. We propose a general pipeline for the RM-PRT benchmark that takes
as input multimodal prompts containing natural language instructions and
automatically outputs actions containing the movement and position transitions.
We set four natural language understanding tasks with progressive reasoning
levels and evaluate the robot's ability to understand natural language
instructions in two modes of adsorption and grasping. In addition, we also
conduct a comprehensive analysis and comparison of the differences and
advantages of 10 different LLMs in instruction understanding and generation
quality. We hope the new simulator and benchmark will facilitate future
research on language-guided robotic manipulation. Project website:
https://necolizer.github.io/RM-PRT/ . </font><br> Link: <a href='http://arxiv.org/pdf/2306.11335v2' target="_blank">http://arxiv.org/pdf/2306.11335v2</a><br> <br> <br> <font size='5'> 83 </font> <div style="text-align: right"> 2023-06-20 05:20:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We use prompt engineering to guide ChatGPT in the automation of text mining
of metal-organic frameworks (MOFs) synthesis conditions from diverse formats
and styles of the scientific literature. This effectively mitigates ChatGPT's
tendency to hallucinate information -- an issue that previously made the use of
Large Language Models (LLMs) in scientific fields challenging. Our approach
involves the development of a workflow implementing three different processes
for text mining, programmed by ChatGPT itself. All of them enable parsing,
searching, filtering, classification, summarization, and data unification with
different tradeoffs between labor, speed, and accuracy. We deploy this system
to extract 26,257 distinct synthesis parameters pertaining to approximately 800
MOFs sourced from peer-reviewed research articles. This process incorporates
our ChemPrompt Engineering strategy to instruct ChatGPT in text mining,
resulting in impressive precision, recall, and F1 scores of 90-99%.
Furthermore, with the dataset built by text mining, we constructed a
machine-learning model with over 86% accuracy in predicting MOF experimental
crystallization outcomes and preliminarily identifying important factors in MOF
crystallization. We also developed a reliable data-grounded MOF chatbot to
answer questions on chemical reactions and synthesis procedures. Given that the
process of using ChatGPT reliably mines and tabulates diverse MOF synthesis
information in a unified format, while using only narrative language requiring
no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be
very useful across various other chemistry sub-disciplines. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11296v1' target="_blank">http://arxiv.org/pdf/2306.11296v1</a><br> <br> <br> <font size='5'> 84 </font> <div style="text-align: right"> 2023-06-19 21:33:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating Privacy Questions From Stack Overflow: Can ChatGPT Compete?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stack Overflow and other similar forums are used commonly by developers to
seek answers for their software development as well as privacy-related
concerns. Recently, ChatGPT has been used as an alternative to generate code or
produce responses to developers' questions. In this paper, we aim to understand
developers' privacy challenges by evaluating the types of privacy-related
questions asked on Stack Overflow. We then conduct a comparative analysis
between the accepted responses given by Stack Overflow users and the responses
produced by ChatGPT for those extracted questions to identify if ChatGPT could
serve as a viable alternative. Our results show that most privacy-related
questions are related to choice/consent, aggregation, and identification.
Furthermore, our findings illustrate that ChatGPT generates similarly correct
responses for about 56% of questions, while for the rest of the responses, the
answers from Stack Overflow are slightly more accurate than ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11174v1' target="_blank">http://arxiv.org/pdf/2306.11174v1</a><br> <br> <br> <font size='5'> 85 </font> <div style="text-align: right"> 2023-06-19 05:11:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Online news platforms commonly employ personalized news recommendation
methods to assist users in discovering interesting articles, and many previous
works have utilized language model techniques to capture user interests and
understand news content. With the emergence of large language models like GPT-3
and T-5, a new recommendation paradigm has emerged, leveraging pre-trained
language models for making recommendations. ChatGPT, with its user-friendly
interface and growing popularity, has become a prominent choice for text-based
tasks. Considering the growing reliance on ChatGPT for language tasks, the
importance of news recommendation in addressing social issues, and the trend of
using language models in recommendations, this study conducts an initial
investigation of ChatGPT's performance in news recommendations, focusing on
three perspectives: personalized news recommendation, news provider fairness,
and fake news detection. ChatGPT has the limitation that its output is
sensitive to the input phrasing. We therefore aim to explore the constraints
present in the generated responses of ChatGPT for each perspective.
Additionally, we investigate whether specific prompt formats can alleviate
these constraints or if these limitations require further attention from
researchers in the future. We also surpass fixed evaluations by developing a
webpage to monitor ChatGPT's performance on weekly basis on the tasks and
prompts we investigated. Our aim is to contribute to and encourage more
researchers to engage in the study of enhancing news recommendation performance
through the utilization of large language models such as ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10702v1' target="_blank">http://arxiv.org/pdf/2306.10702v1</a><br> <br> <br> <font size='5'> 86 </font> <div style="text-align: right"> 2023-06-19 04:09:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The technology of Conversational AI has made significant advancements over
the last eighteen months. As a consequence, conversational agents are likely to
be deployed in the near future that are designed to pursue targeted influence
objectives. Sometimes referred to as the "AI Manipulation Problem," the
emerging risk is that consumers will unwittingly engage in real-time dialog
with predatory AI agents that can skillfully persuade them to buy particular
products, believe particular pieces of misinformation, or fool them into
revealing sensitive personal data. For many users, current systems like ChatGPT
and LaMDA feel safe because they are primarily text-based, but the industry is
already shifting towards real-time voice and photorealistic digital personas
that look, move, and express like real people. This will enable the deployment
of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive
through real-time adaptive influence. This paper explores the manipulative
tactics that are likely to be deployed through conversational AI agents, the
unique threats such agents pose to the epistemic agency of human users, and the
emerging need for policymakers to protect against the most likely predatory
practices. </font><br> Link: <a href='http://arxiv.org/pdf/2306.11748v1' target="_blank">http://arxiv.org/pdf/2306.11748v1</a><br> <br> <br> <font size='5'> 87 </font> <div style="text-align: right"> 2023-06-18 22:23:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Educational chatbots come with a promise of interactive and personalized
learning experiences, yet their development has been limited by the restricted
free interaction capabilities of available platforms and the difficulty of
encoding knowledge in a suitable format. Recent advances in language learning
models with zero-shot learning capabilities, such as ChatGPT, suggest a new
possibility for developing educational chatbots using a prompt-based approach.
We present a case study with a simple system that enables mixed-turn chatbot
interactions and we discuss the insights and preliminary guidelines obtained
from initial tests. We examine ChatGPT's ability to pursue multiple
interconnected learning objectives, adapt the educational activity to users'
characteristics, such as culture, age, and level of education, and its ability
to use diverse educational strategies and conversational styles. Although the
results are encouraging, challenges are posed by the limited history maintained
for the conversation and the highly structured form of responses by ChatGPT, as
well as their variability, which can lead to an unexpected switch of the
chatbot's role from a teacher to a therapist. We provide some initial
guidelines to address these issues and to facilitate the development of
effective educational chatbots. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10645v1' target="_blank">http://arxiv.org/pdf/2306.10645v1</a><br> <br> <br> <font size='5'> 88 </font> <div style="text-align: right"> 2023-06-18 12:20:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sentiment analysis is a well-known natural language processing task that
involves identifying the emotional tone or polarity of a given piece of text.
With the growth of social media and other online platforms, sentiment analysis
has become increasingly crucial for businesses and organizations seeking to
monitor and comprehend customer feedback as well as opinions. Supervised
learning algorithms have been popularly employed for this task, but they
require human-annotated text to create the classifier. To overcome this
challenge, lexicon-based tools have been used. A drawback of lexicon-based
algorithms is their reliance on pre-defined sentiment lexicons, which may not
capture the full range of sentiments in natural language. ChatGPT is a new
product of OpenAI and has emerged as the most popular AI product. It can answer
questions on various topics and tasks. This study explores the use of ChatGPT
as a tool for data labeling for different sentiment analysis tasks. It is
evaluated on two distinct sentiment analysis datasets with varying purposes.
The results demonstrate that ChatGPT outperforms other lexicon-based
unsupervised methods with significant improvements in overall accuracy.
Specifically, compared to the best-performing lexical-based algorithms, ChatGPT
achieves a remarkable increase in accuracy of 20% for the tweets dataset and
approximately 25% for the Amazon reviews dataset. These findings highlight the
exceptional performance of ChatGPT in sentiment analysis tasks, surpassing
existing lexicon-based approaches by a significant margin. The evidence
suggests it can be used for annotation on different sentiment analysis events
and taskss. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17177v1' target="_blank">http://arxiv.org/pdf/2306.17177v1</a><br> <br> <br> <font size='5'> 89 </font> <div style="text-align: right"> 2023-06-18 10:36:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deceptive AI Ecosystems: The Case of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, an AI chatbot, has gained popularity for its capability in
generating human-like responses. However, this feature carries several risks,
most notably due to its deceptive behaviour such as offering users misleading
or fabricated information that could further cause ethical issues. To better
understand the impact of ChatGPT on our social, cultural, economic, and
political interactions, it is crucial to investigate how ChatGPT operates in
the real world where various societal pressures influence its development and
deployment. This paper emphasizes the need to study ChatGPT "in the wild", as
part of the ecosystem it is embedded in, with a strong focus on user
involvement. We examine the ethical challenges stemming from ChatGPT's
deceptive human-like interactions and propose a roadmap for developing more
transparent and trustworthy chatbots. Central to our approach is the importance
of proactive risk assessment and user participation in shaping the future of
chatbot technology. </font><br> Link: <a href='http://arxiv.org/pdf/2306.13671v1' target="_blank">http://arxiv.org/pdf/2306.13671v1</a><br> <br> <br> <font size='5'> 90 </font> <div style="text-align: right"> 2023-06-18 09:54:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), like ChatGPT, have shown some human-like
cognitive abilities. For comparing these abilities of different models, several
benchmarks (i.e. sets of standard test questions) from different fields (e.g.,
Literature, Biology and Psychology) are often adopted and the test results
under traditional metrics such as accuracy, recall and F1, are reported.
However, such way for evaluating LLMs can be inefficient and inaccurate from
the cognitive science perspective. Inspired by Computerized Adaptive Testing
(CAT) used in psychometrics, we propose an adaptive testing framework for LLM
evaluation. Rather than using a standard test set and simply reporting
accuracy, this approach dynamically adjusts the characteristics of the test
questions, such as difficulty, based on the model's performance. This allows
for a more accurate estimation of the model's abilities, using fewer questions.
More importantly, it allows LLMs to be compared with humans easily, which is
essential for NLP models that aim for human-level ability. Our diagnostic
reports have found that ChatGPT often behaves like a ``careless student'',
prone to slip and occasionally guessing the questions. We conduct a
fine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three
aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where
GPT4 can outperform other models significantly and reach the cognitive ability
of middle-level students. Different tests for different models using efficient
adaptive testing -- we believe this has the potential to become a new norm in
evaluating large language models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10512v1' target="_blank">http://arxiv.org/pdf/2306.10512v1</a><br> <br> <br> <font size='5'> 91 </font> <div style="text-align: right"> 2023-06-18 04:30:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aimed to evaluate the proficiency of prominent Large Language
Models (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and
Microsoft's Bing AI in discerning the truthfulness of news items using black
box testing. A total of 100 fact-checked news items, all sourced from
independent fact-checking agencies, were presented to each of these LLMs under
controlled conditions. Their responses were classified into one of three
categories: True, False, and Partially True/False. The effectiveness of the
LLMs was gauged based on the accuracy of their classifications against the
verified facts provided by the independent agencies. The results showed a
moderate proficiency across all models, with an average score of 65.25 out of
100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71,
suggesting an edge in newer LLMs' abilities to differentiate fact from
deception. However, when juxtaposed against the performance of human
fact-checkers, the AI models, despite showing promise, lag in comprehending the
subtleties and contexts inherent in news information. The findings highlight
the potential of AI in the domain of fact-checking while underscoring the
continued importance of human cognitive skills and the necessity for persistent
advancements in AI capabilities. Finally, the experimental data produced from
the simulation of this work is openly available on Kaggle. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17176v1' target="_blank">http://arxiv.org/pdf/2306.17176v1</a><br> <br> <br> <font size='5'> 92 </font> <div style="text-align: right"> 2023-06-17 11:44:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Given a natural language, a general robot has to comprehend the instruction
and find the target object or location based on visual observations even in
unexplored environments. Most agents rely on massive diverse training data to
achieve better generalization, which requires expensive labor. These agents
often focus on common objects and fewer tasks, thus are not intelligent enough
to handle different types of instructions. To facilitate research in open-set
vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at
testing the effectiveness and generalization of the agent in the multi-task
setting. First, we develop a 3D simulator rendered by realistic scenarios using
Unreal Engine 5, containing more realistic lights and details. The simulator
contains three scenes, i.e., cafe, restaurant, and nursing house, of high value
in the industry. Besides, our simulator involves multiple uncommon objects,
such as takeaway cup and medical adhesive tape, which are more complicated
compared with existing environments. Inspired by the recent success of large
language models (e.g., ChatGPT, Vicuna), we construct diverse high-quality data
of instruction type without human annotation. Our benchmark MO-VLN provides
four tasks: 1) goal-conditioned navigation given a specific object category
(e.g., "fork"); 2) goal-conditioned navigation given simple instructions (e.g.,
"Search for and move towards a tennis ball"); 3) step-by-step instruction
following; 4) finding abstract object based on high-level instruction (e.g., "I
am thirsty"). </font><br> Link: <a href='http://arxiv.org/pdf/2306.10322v1' target="_blank">http://arxiv.org/pdf/2306.10322v1</a><br> <br> <br> <font size='5'> 93 </font> <div style="text-align: right"> 2023-06-17 02:51:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled from Foundation Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Constructing commonsense knowledge graphs (CKGs) has attracted wide research
attention due to its significant importance in cognitive intelligence.
Nevertheless, existing CKGs are typically oriented to English, limiting the
research in non-English languages. Meanwhile, the emergence of foundation
models like ChatGPT and GPT-4 has shown promising intelligence with the help of
reinforcement learning from human feedback. Under the background, in this
paper, we utilize foundation models to construct a Chinese CKG, named Snowman.
Specifically, we distill different types of commonsense head items from
ChatGPT, and continue to use it to collect tail items with respect to the head
items and pre-defined relations. Based on the preliminary analysis, we find the
negative commonsense knowledge distilled by ChatGPT achieves lower human
acceptance compared to other knowledge. Therefore, we design a simple yet
effective self-instruct filtering strategy to filter out invalid negative
commonsense. Overall, the constructed Snowman covers more than ten million
Chinese commonsense triples, making it the largest Chinese CKG. Moreover, human
studies show the acceptance of Snowman achieves 90.6\%, indicating the
high-quality triples distilled by the cutting-edge foundation model. We also
conduct experiments on commonsense knowledge models to show the usability and
effectiveness of our Snowman. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10241v1' target="_blank">http://arxiv.org/pdf/2306.10241v1</a><br> <br> <br> <font size='5'> 94 </font> <div style="text-align: right"> 2023-06-17 01:22:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI tools such as ChatGPT can fundamentally change the way
investors process information. We probe the economic usefulness of these tools
in summarizing complex corporate disclosures using the stock market as a
laboratory. The unconstrained summaries are dramatically shorter, often by more
than 70% compared to the originals, whereas their information content is
amplified. When a document has a positive (negative) sentiment, its summary
becomes more positive (negative). More importantly, the summaries are more
effective at explaining stock market reactions to the disclosed information.
Motivated by these findings, we propose a measure of information "bloat." We
show that bloated disclosure is associated with adverse capital markets
consequences, such as lower price efficiency and higher information asymmetry.
Finally, we show that the model is effective at constructing targeted summaries
that identify firms' (non-)financial performance and risks. Collectively, our
results indicate that generative language modeling adds considerable value for
investors with information processing constraints. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10224v1' target="_blank">http://arxiv.org/pdf/2306.10224v1</a><br> <br> <br> <font size='5'> 95 </font> <div style="text-align: right"> 2023-06-16 15:50:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Friend or Foe? Exploring the Implications of Large Language Models on the Science System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advent of ChatGPT by OpenAI has prompted extensive discourse on its
potential implications for science and higher education. While the impact on
education has been a primary focus, there is limited empirical research on the
effects of large language models (LLMs) and LLM-based chatbots on science and
scientific practice. To investigate this further, we conducted a Delphi study
involving 72 experts specialising in research and AI. The study focused on
applications and limitations of LLMs, their effects on the science system,
ethical and legal considerations, and the required competencies for their
effective use. Our findings highlight the transformative potential of LLMs in
science, particularly in administrative, creative, and analytical tasks.
However, risks related to bias, misinformation, and quality assurance need to
be addressed through proactive regulation and science education. This research
contributes to informed discussions on the impact of generative AI in science
and helps identify areas for future action. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09928v1' target="_blank">http://arxiv.org/pdf/2306.09928v1</a><br> <br> <br> <font size='5'> 96 </font> <div style="text-align: right"> 2023-06-16 13:39:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09841v2' target="_blank">http://arxiv.org/pdf/2306.09841v2</a><br> <br> <br> <font size='5'> 97 </font> <div style="text-align: right"> 2023-06-16 09:40:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pushing the Limits of ChatGPT on NLP Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite the success of ChatGPT, its performances on most NLP tasks are still
well below the supervised baselines. In this work, we looked into the causes,
and discovered that its subpar performance was caused by the following factors:
(1) token limit in the prompt does not allow for the full utilization of the
supervised datasets; (2) mismatch between the generation nature of ChatGPT and
NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly
focus on certain keywords, etc.
  In this work, we propose a collection of general modules to address these
issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed
modules include (1) a one-input-multiple-prompts strategy that employs multiple
prompts for one input to accommodate more demonstrations; (2) using fine-tuned
models for better demonstration retrieval; (3) transforming tasks to formats
that are more tailored to the generation nature; (4) employing reasoning
strategies that are tailored to addressing the task-specific complexity; (5)
the self-verification strategy to address the hallucination issue of LLMs; (6)
the paraphrase strategy to improve the robustness of model predictions.
  We conduct experiments on 21 datasets of 10 representative NLP tasks,
including question answering, commonsense reasoning, natural language
inference, sentiment analysis, named entity recognition, entity-relation
extraction, event extraction, dependency parsing, semantic role labeling, and
part-of-speech tagging. Using the proposed assemble of techniques, we are able
to significantly boost the performance of ChatGPT on the selected NLP tasks,
achieving performances comparable to or better than supervised baselines, or
even existing SOTA performances. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09719v1' target="_blank">http://arxiv.org/pdf/2306.09719v1</a><br> <br> <br> <font size='5'> 98 </font> <div style="text-align: right"> 2023-06-15 23:21:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Effective Model and Strategies for Online Teaching and Learning of Introductory Physics in the Era of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent development of physics education leads physics faculty to explore
new teaching and learning methodologies and restructure classes and assignments
to bring students' knowledge in Physics to the highest level of education by
allowing learners to gain various skills that are reflected in their future. We
are discussing how to develop and deliver effective online courses by
personalizing the class and individualizing the assignments for learners to
succeed in Physics. Various teaching and learning activities are aimed at
effective learning and interaction to enhance student learning in this class
structure. The students' performances and progress are analyzed in four
different large introductory Physics classes. Student registration and
retention are well preserved by providing learners freedom of education, a
sense of belonging, mindset-growing encouragements, effective feedback,
problem-solving strategies, one-to-one communications, and real-life
applications. Academic integrity is well-balanced by individualizing the
assignments, personalizing class materials, breaking down problems, and
investigating ChatGPT usage. The students' performances are analyzed by
evaluating class performance in total grade distribution and the number of
missing assignments. Student progress is investigated by analyzing their
commitment, respect, willingness to improve, and time management toward
education. Overall, the class module demonstrates great performance, progress,
and achievements that indicate a positive learning environment for all students
throughout the semester. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09545v1' target="_blank">http://arxiv.org/pdf/2306.09545v1</a><br> <br> <br> <font size='5'> 99 </font> <div style="text-align: right"> 2023-06-15 22:12:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper studies recent developments in large language models' (LLM)
abilities to pass assessments in introductory and intermediate Python
programming courses at the postsecondary level. The emergence of ChatGPT
resulted in heated debates of its potential uses (e.g., exercise generation,
code explanation) as well as misuses in programming classes (e.g., cheating).
Recent studies show that while the technology performs surprisingly well on
diverse sets of assessment instruments employed in typical programming classes
the performance is usually not sufficient to pass the courses. The release of
GPT-4 largely emphasized notable improvements in the capabilities related to
handling assessments originally designed for human test-takers. This study is
the necessary analysis in the context of this ongoing transition towards mature
generative AI systems. Specifically, we report the performance of GPT-4,
comparing it to the previous generations of GPT models, on three Python courses
with assessments ranging from simple multiple-choice questions (no code
involved) to complex programming projects with code bases distributed into
multiple files (599 exercises overall). Additionally, we analyze the
assessments that were not handled well by GPT-4 to understand the current
limitations of the model, as well as its capabilities to leverage feedback
provided by an auto-grader. We found that the GPT models evolved from
completely failing the typical programming class' assessments (the original
GPT-3) to confidently passing the courses with no human involvement (GPT-4).
While we identified certain limitations in GPT-4's handling of MCQs and coding
exercises, the rate of improvement across the recent generations of GPT models
strongly suggests their potential to handle almost any type of assessment
widely used in higher education programming courses. These findings could be
leveraged by educators and institutions to adapt the design of programming
assessments as well as to fuel the necessary discussions into how programming
classes should be updated to reflect the recent technological developments.
This study provides evidence that programming instructors need to prepare for a
world in which there is an easy-to-use widely accessible technology that can be
utilized by learners to collect passing scores, with no effort whatsoever, on
what today counts as viable programming knowledge and skills assessments. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10073v1' target="_blank">http://arxiv.org/pdf/2306.10073v1</a><br> <br> <br> <font size='5'> 100 </font> <div style="text-align: right"> 2023-06-15 20:19:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has drawn considerable attention from both the general public and
domain experts with its remarkable text generation capabilities. This has
subsequently led to the emergence of diverse applications in the field of
biomedicine and health. In this work, we examine the diverse applications of
large language models (LLMs), such as ChatGPT, in biomedicine and health.
Specifically we explore the areas of biomedical information retrieval, question
answering, medical text summarization, information extraction, and medical
education, and investigate whether LLMs possess the transformative power to
revolutionize these tasks or whether the distinct complexities of biomedical
domain presents unique challenges. Following an extensive literature survey, we
find that significant advances have been made in the field of text generation
tasks, surpassing the previous state-of-the-art methods. For other
applications, the advances have been modest. Overall, LLMs have not yet
revolutionized the biomedicine, but recent rapid progress indicates that such
methods hold great potential to provide valuable means for accelerating
discovery and improving health. We also find that the use of LLMs, like
ChatGPT, in the fields of biomedicine and health entails various risks and
challenges, including fabricated information in its generated responses, as
well as legal and privacy concerns associated with sensitive patient data. We
believe this first-of-its-kind survey can provide a comprehensive overview to
biomedical researchers and healthcare practitioners on the opportunities and
challenges associated with using ChatGPT and other LLMs for transforming
biomedicine and health. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10070v1' target="_blank">http://arxiv.org/pdf/2306.10070v1</a><br> <br> <br> <font size='5'> 101 </font> <div style="text-align: right"> 2023-06-15 16:40:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of Generative Artificial Intelligence systems (``AI systems'') has
created unprecedented social engagement. AI code generation systems provide
responses (output) to questions or requests by accessing the vast library of
open-source code created by developers over decades. However, they do so by
allegedly stealing the open-source code stored in virtual libraries, known as
repositories. How all this happens and whether there is a solution short of
years of litigation that can protect innovation is the focus of this article.
We also peripherally touch upon the array of issues raised by the relationship
between AI and copyright. Looking ahead, we propose the following: (a)
immediate changes to the licenses for open-source code created by developers
that will allow access and/or use of any open-source code to humans only; (b)
we suggest revisions to the Massachusetts Institute of Technology (``MIT'')
license so that AI systems procure appropriate licenses from open-source code
developers, which we believe will harmonize standards and build social
consensus for the benefit of all of humanity rather than profit-driven centers
of innovation; (c) We call for urgent legislative action to protect the future
of AI systems while also promoting innovation; and (d) we propose that there is
a shift in the burden of proof to AI systems in obfuscation cases. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09267v1' target="_blank">http://arxiv.org/pdf/2306.09267v1</a><br> <br> <br> <font size='5'> 102 </font> <div style="text-align: right"> 2023-06-15 16:01:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a novel framework for quantitatively evaluating the
interactive ChatGPT model in the context of suicidality assessment from social
media posts, utilizing the University of Maryland Reddit suicidality dataset.
We conduct a technical evaluation of ChatGPT's performance on this task using
Zero-Shot and Few-Shot experiments and compare its results with those of two
fine-tuned transformer-based models. Additionally, we investigate the impact of
different temperature parameters on ChatGPT's response generation and discuss
the optimal temperature based on the inconclusiveness rate of ChatGPT. Our
results indicate that while ChatGPT attains considerable accuracy in this task,
transformer-based models fine-tuned on human-annotated datasets exhibit
superior performance. Moreover, our analysis sheds light on how adjusting the
ChatGPT's hyperparameters can improve its ability to assist mental health
professionals in this critical task. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09390v1' target="_blank">http://arxiv.org/pdf/2306.09390v1</a><br> <br> <br> <font size='5'> 103 </font> <div style="text-align: right"> 2023-06-15 14:47:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT pass the Vietnamese National High School Graduation Examination?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research article highlights the potential of AI-powered chatbots in
education and presents the results of using ChatGPT, a large language model, to
complete the Vietnamese National High School Graduation Examination (VNHSGE).
The study dataset included 30 essays in the literature test case and 1,700
multiple-choice questions designed for other subjects. The results showed that
ChatGPT was able to pass the examination with an average score of 6-7,
demonstrating the technology's potential to revolutionize the educational
landscape. The analysis of ChatGPT performance revealed its proficiency in a
range of subjects, including mathematics, English, physics, chemistry, biology,
history, geography, civic education, and literature, which suggests its
potential to provide effective support for learners. However, further research
is needed to assess ChatGPT performance on more complex exam questions and its
potential to support learners in different contexts. As technology continues to
evolve and improve, we can expect to see the use of AI tools like ChatGPT
become increasingly common in educational settings, ultimately enhancing the
educational experience for both students and educators. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09170v3' target="_blank">http://arxiv.org/pdf/2306.09170v3</a><br> <br> <br> <font size='5'> 104 </font> <div style="text-align: right"> 2023-06-15 06:50:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Zero-shot NL2SQL is crucial in achieving natural language to SQL that is
adaptive to new environments (e.g., new databases, new linguistic phenomena or
SQL structures) with zero annotated NL2SQL samples from such environments.
Existing approaches either fine-tune pre-trained language models (PLMs) based
on annotated data or use prompts to guide fixed large language models (LLMs)
such as ChatGPT. PLMs can perform well in schema alignment but struggle to
achieve complex reasoning, while LLMs is superior in complex reasoning tasks
but cannot achieve precise schema alignment. In this paper, we propose a
ZeroNL2SQL framework that combines the complementary advantages of PLMs and
LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an
SQL sketch via schema alignment, then uses LLMs to fill the missing information
via complex reasoning. Moreover, in order to better align the generated SQL
queries with values in the given database instances, we design a predicate
calibration method to guide the LLM in completing the SQL sketches based on the
database instances and select the optimal SQL query via an execution-based
strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best
zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL
outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds
LLM-based methods by 10% to 20% on execution accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08891v1' target="_blank">http://arxiv.org/pdf/2306.08891v1</a><br> <br> <br> <font size='5'> 105 </font> <div style="text-align: right"> 2023-06-15 05:59:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The pervasive influence of misinformation has far-reaching and detrimental
effects on both individuals and society. The COVID-19 pandemic has witnessed an
alarming surge in the dissemination of medical misinformation. However,
existing datasets pertaining to misinformation predominantly focus on textual
information, neglecting the inclusion of visual elements, and tend to center
solely on COVID-19-related misinformation, overlooking misinformation
surrounding other diseases. Furthermore, the potential of Large Language Models
(LLMs), such as the ChatGPT developed in late 2022, in generating
misinformation has been overlooked in previous works. To overcome these
limitations, we present Med-MMHL, a novel multi-modal misinformation detection
dataset in a general medical domain encompassing multiple diseases. Med-MMHL
not only incorporates human-generated misinformation but also includes
misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate
comprehensive research and development of methodologies for detecting
misinformation across diverse diseases and various scenarios, including human
and LLM-generated misinformation detection at the sentence, document, and
multi-modal levels. To access our dataset and code, visit our GitHub
repository: \url{https://github.com/styxsys0927/Med-MMHL}. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08871v1' target="_blank">http://arxiv.org/pdf/2306.08871v1</a><br> <br> <br> <font size='5'> 106 </font> <div style="text-align: right"> 2023-06-15 03:30:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT and other large language models (LLMs) have proven useful in
crowdsourcing tasks, where they can effectively annotate machine learning
training data. However, this means that they also have the potential for
misuse, specifically to automatically answer surveys. LLMs can potentially
circumvent quality assurance measures, thereby threatening the integrity of
methodologies that rely on crowdsourcing surveys. In this paper, we propose a
mechanism to detect LLM-generated responses to surveys. The mechanism uses
"prompt injection", such as directions that can mislead LLMs into giving
predictable responses. We evaluate our technique against a range of question
scenarios, types, and positions, and find that it can reliably detect
LLM-generated responses with more than 93% effectiveness. We also provide an
open-source software to help survey designers use our technique to detect LLM
responses. Our work is a step in ensuring that survey methodologies remain
rigorous vis-a-vis LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08833v1' target="_blank">http://arxiv.org/pdf/2306.08833v1</a><br> <br> <br> <font size='5'> 107 </font> <div style="text-align: right"> 2023-06-14 14:44:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Knowledge Distillation of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge from white-box generative LLMs is still under-explored,
which becomes more and more important with the prosperity of LLMs. In this
work, we propose MiniLLM that distills smaller language models from generative
larger language models. We first replace the forward Kullback-Leibler
divergence (KLD) objective in the standard KD approaches with reverse KLD,
which is more suitable for KD on generative language models, to prevent the
student model from overestimating the low-probability regions of the teacher
distribution. Then, we derive an effective optimization approach to learn this
objective. Extensive experiments in the instruction-following setting show that
the MiniLLM models generate more precise responses with the higher overall
quality, lower exposure bias, better calibration, and higher long-text
generation performance. Our method is also scalable for different model
families with 120M to 13B parameters. We will release our code and model
checkpoints at https://aka.ms/MiniLLM. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08543v1' target="_blank">http://arxiv.org/pdf/2306.08543v1</a><br> <br> <br> <font size='5'> 108 </font> <div style="text-align: right"> 2023-06-14 07:15:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unifying Large Language Models and Knowledge Graphs: A Roadmap</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08302v2' target="_blank">http://arxiv.org/pdf/2306.08302v2</a><br> <br> <br> <font size='5'> 109 </font> <div style="text-align: right"> 2023-06-14 03:17:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions, and Prospects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversational recommender systems (CRSs) have become crucial emerging
research topics in the field of RSs, thanks to their natural advantages of
explicitly acquiring user preferences via interactive conversations and
revealing the reasons behind recommendations. However, the majority of current
CRSs are text-based, which is less user-friendly and may pose challenges for
certain users, such as those with visual impairments or limited writing and
reading abilities. Therefore, for the first time, this paper investigates the
potential of voice-based CRS (VCRSs) to revolutionize the way users interact
with RSs in a natural, intuitive, convenient, and accessible fashion. To
support such studies, we create two VCRSs benchmark datasets in the e-commerce
and movie domains, after realizing the lack of such datasets through an
exhaustive literature review. Specifically, we first empirically verify the
benefits and necessity of creating such datasets. Thereafter, we convert the
user-item interactions to text-based conversations through the ChatGPT-driven
prompts for generating diverse and natural templates, and then synthesize the
corresponding audios via the text-to-speech model. Meanwhile, a number of
strategies are delicately designed to ensure the naturalness and high quality
of voice conversations. On this basis, we further explore the potential
solutions and point out possible directions to build end-to-end VCRSs by
seamlessly extracting and integrating voice-based inputs, thus delivering
performance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims
to establish the foundation and motivate further pioneering research in the
emerging field of VCRSs. This aligns with the principles of explainable AI and
AI for social good, viz., utilizing technology's potential to create a fair,
sustainable, and just world. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08219v1' target="_blank">http://arxiv.org/pdf/2306.08219v1</a><br> <br> <br> <font size='5'> 110 </font> <div style="text-align: right"> 2023-06-13 23:53:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT vs. Lightweight Security: First Work Implementing the NIST Cryptographic Standard ASCON</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study, to the best of our knowledge, is the first to explore the
intersection between lightweight cryptography (LWC) and advanced artificial
intelligence (AI) language models. LWC, in particular the ASCON algorithm which
has been selected as the LWC standard by the National Institute of Standards
and Technology (NIST) in Feb. 2023, has become increasingly significant for
preserving data security given the quick expansion and resource limitations of
Internet of Things (IoT) devices. On the other hand, OpenAI's large language
model (LLM) ChatGPT has demonstrated significant potential in producing
complex, human-like text. This paper offers a novel method for implementing the
NIST LWC standard, ASCON, using the GPT-4 model. Moreover, this paper details
the design and functionality of ASCON, the procedures and actual Python
implementation of ASCON using ChatGPT, and a discussion of the outcomes. The
results contribute valuable insights into the efficient application of advanced
AI language models in cryptography, particularly in constrained environments.
Source code can be found at: https://github.com/DrCintas/ASCON-with-ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08178v1' target="_blank">http://arxiv.org/pdf/2306.08178v1</a><br> <br> <br> <font size='5'> 111 </font> <div style="text-align: right"> 2023-06-13 19:27:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The surge in Reinforcement Learning (RL) applications in Intelligent
Transportation Systems (ITS) has contributed to its growth as well as
highlighted key challenges. However, defining objectives of RL agents in
traffic control and management tasks, as well as aligning policies with these
goals through an effective formulation of Markov Decision Process (MDP), can be
challenging and often require domain experts in both RL and ITS. Recent
advancements in Large Language Models (LLMs) such as GPT-4 highlight their
broad general knowledge, reasoning capabilities, and commonsense priors across
various domains. In this work, we conduct a large-scale user study involving 70
participants to investigate whether novices can leverage ChatGPT to solve
complex mixed traffic control problems. Three environments are tested,
including ring road, bottleneck, and intersection. We find ChatGPT has mixed
results. For intersection and bottleneck, ChatGPT increases number of
successful policies by 150% and 136% compared to solely beginner capabilities,
with some of them even outperforming experts. However, ChatGPT does not provide
consistent improvements across all scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2306.08094v1' target="_blank">http://arxiv.org/pdf/2306.08094v1</a><br> <br> <br> <font size='5'> 112 </font> <div style="text-align: right"> 2023-06-13 14:21:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale language models, like ChatGPT, have garnered significant media
attention and stunned the public with their remarkable capacity for generating
coherent text from short natural language prompts. In this paper, we aim to
conduct a systematic inspection of ChatGPT's performance in two controllable
generation tasks, with respect to ChatGPT's ability to adapt its output to
different target audiences (expert vs. layman) and writing styles (formal vs.
informal). Additionally, we evaluate the faithfulness of the generated text,
and compare the model's performance with human-authored texts. Our findings
indicate that the stylistic variations produced by humans are considerably
larger than those demonstrated by ChatGPT, and the generated texts diverge from
human samples in several characteristics, such as the distribution of word
types. Moreover, we observe that ChatGPT sometimes incorporates factual errors
or hallucinations when adapting the text to suit a specific style. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07799v1' target="_blank">http://arxiv.org/pdf/2306.07799v1</a><br> <br> <br> <font size='5'> 113 </font> <div style="text-align: right"> 2023-06-13 08:43:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Therefore, it is of
great importance to evaluate their emerging abilities. In this study, we show
that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles
human-like intuition -- and the cognitive errors that come with it. However,
LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,
learned to avoid succumbing to these errors and perform in a hyperrational
manner. For our experiments, we probe LLMs with the Cognitive Reflection Test
(CRT) as well as semantic illusions that were originally designed to
investigate intuitive decision-making in humans. Moreover, we probe how sturdy
the inclination for intuitive-like decision-making is. Our study demonstrates
that investigating LLMs with methods from psychology has the potential to
reveal otherwise unknown emergent traits. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07622v1' target="_blank">http://arxiv.org/pdf/2306.07622v1</a><br> <br> <br> <font size='5'> 114 </font> <div style="text-align: right"> 2023-06-13 06:13:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ethical Aspects of ChatGPT in Software Engineering Research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT can improve Software Engineering (SE) research practices by offering
efficient, accessible information analysis and synthesis based on natural
language interactions. However, ChatGPT could bring ethical challenges,
encompassing plagiarism, privacy, data security, and the risk of generating
biased or potentially detrimental data. This research aims to fill the given
gap by elaborating on the key elements: motivators, demotivators, and ethical
principles of using ChatGPT in SE research. To achieve this objective, we
conducted a literature survey, identified the mentioned elements, and presented
their relationships by developing a taxonomy. Further, the identified
literature-based elements (motivators, demotivators, and ethical principles)
were empirically evaluated by conducting a comprehensive questionnaire-based
survey involving SE researchers. Additionally, we employed Interpretive
Structure Modeling (ISM) approach to analyze the relationships between the
ethical principles of using ChatGPT in SE research and develop a level based
decision model. We further conducted a Cross-Impact Matrix Multiplication
Applied to Classification (MICMAC) analysis to create a cluster-based decision
model. These models aim to help SE researchers devise effective strategies for
ethically integrating ChatGPT into SE research by following the identified
principles through adopting the motivators and addressing the demotivators. The
findings of this study will establish a benchmark for incorporating ChatGPT
services in SE research with an emphasis on ethical considerations. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07557v1' target="_blank">http://arxiv.org/pdf/2306.07557v1</a><br> <br> <br> <font size='5'> 115 </font> <div style="text-align: right"> 2023-06-13 02:23:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adding guardrails to advanced chatbots</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI models continue to become more powerful. The launch of ChatGPT
in November 2022 has ushered in a new era of AI. ChatGPT and other similar
chatbots have a range of capabilities, from answering student homework
questions to creating music and art. There are already concerns that humans may
be replaced by chatbots for a variety of jobs. Because of the wide spectrum of
data chatbots are built on, we know that they will have human errors and human
biases built into them. These biases may cause significant harm and/or inequity
toward different subpopulations. To understand the strengths and weakness of
chatbot responses, we present a position paper that explores different use
cases of ChatGPT to determine the types of questions that are answered fairly
and the types that still need improvement. We find that ChatGPT is a fair
search engine for the tasks we tested; however, it has biases on both text
generation and code generation. We find that ChatGPT is very sensitive to
changes in the prompt, where small changes lead to different levels of
fairness. This suggests that we need to immediately implement "corrections" or
mitigation strategies in order to improve fairness of these systems. We suggest
different strategies to improve chatbots and also advocate for an impartial
review panel that has access to the model parameters to measure the levels of
different types of biases and then recommends safeguards that move toward
responses that are less discriminatory and more accurate. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07500v1' target="_blank">http://arxiv.org/pdf/2306.07500v1</a><br> <br> <br> <font size='5'> 116 </font> <div style="text-align: right"> 2023-06-12 17:31:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we present MovieFactory, a powerful framework to generate
cinematic-picture (3072$\times$1280), film-style (multi-scene), and
multi-modality (sounding) movies on the demand of natural languages. As the
first fully automated movie generation model to the best of our knowledge, our
approach empowers users to create captivating movies with smooth transitions
using simple text inputs, surpassing existing methods that produce soundless
videos limited to a single scene of modest quality. To facilitate this
distinctive functionality, we leverage ChatGPT to expand user-provided text
into detailed sequential scripts for movie generation. Then we bring scripts to
life visually and acoustically through vision generation and audio retrieval.
To generate videos, we extend the capabilities of a pretrained text-to-image
diffusion model through a two-stage process. Firstly, we employ spatial
finetuning to bridge the gap between the pretrained image model and the new
video dataset. Subsequently, we introduce temporal learning to capture object
motion. In terms of audio, we leverage sophisticated retrieval models to select
and align audio elements that correspond to the plot and visual content of the
movie. Extensive experiments demonstrate that our MovieFactory produces movies
with realistic visuals, diverse scenes, and seamlessly fitting audio, offering
users a novel and immersive experience. Generated samples can be found in
YouTube or Bilibili (1080P). </font><br> Link: <a href='http://arxiv.org/pdf/2306.07257v1' target="_blank">http://arxiv.org/pdf/2306.07257v1</a><br> <br> <br> <font size='5'> 117 </font> <div style="text-align: right"> 2023-06-12 16:11:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Valley: Video Assistant with Large Language model Enhanced abilitY</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, several multi-modal models have been developed for joint image and
language understanding, which have demonstrated impressive chat abilities by
utilizing advanced large language models (LLMs). The process of developing such
models is straightforward yet effective. It involves pre-training an adaptation
module to align the semantics of the vision encoder and language model,
followed by fine-tuning on the instruction-following data. However, despite the
success of this pipeline in image and language understanding, its effectiveness
in joint video and language understanding has not been widely explored. In this
paper, we aim to develop a novel multi-modal foundation model capable of
perceiving video, image, and language within a general framework. To achieve
this goal, we introduce Valley: Video Assistant with Large Language model
Enhanced ability. Specifically, our proposed Valley model is designed with a
simple projection module that bridges video, image, and language modalities,
and is further unified with a multi-lingual LLM. We also collect multi-source
vision-text pairs and adopt a spatio-temporal pooling strategy to obtain a
unified vision encoding of video and image input for pre-training. Furthermore,
we generate multi-task instruction-following video data, including multi-shot
captions, long video descriptions, action recognition, causal relationship
inference, etc. To obtain the instruction-following data, we design diverse
rounds of task-oriented conversations between humans and videos, facilitated by
ChatGPT. Qualitative examples demonstrate that our proposed model has the
potential to function as a highly effective multilingual video assistant that
can make complex video understanding scenarios easy. Code, data, and models
will be available at https://github.com/RupertLuo/Valley. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07207v1' target="_blank">http://arxiv.org/pdf/2306.07207v1</a><br> <br> <br> <font size='5'> 118 </font> <div style="text-align: right"> 2023-06-12 14:42:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Enhancing AI systems to perform tasks following human instructions can
significantly boost productivity. In this paper, we present InstructP2P, an
end-to-end framework for 3D shape editing on point clouds, guided by high-level
textual instructions. InstructP2P extends the capabilities of existing methods
by synergizing the strengths of a text-conditioned point cloud diffusion model,
Point-E, and powerful language models, enabling color and geometry editing
using language instructions. To train InstructP2P, we introduce a new shape
editing dataset, constructed by integrating a shape segmentation dataset,
off-the-shelf shape programs, and diverse edit instructions generated by a
large language model, ChatGPT. Our proposed method allows for editing both
color and geometry of specific regions in a single forward pass, while leaving
other regions unaffected. In our experiments, InstructP2P shows generalization
capabilities, adapting to novel shape categories and instructions, despite
being trained on a limited amount of data. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07154v1' target="_blank">http://arxiv.org/pdf/2306.07154v1</a><br> <br> <br> <font size='5'> 119 </font> <div style="text-align: right"> 2023-06-11 22:15:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A blind spot for large language models: Supradiegetic linguistic information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) like ChatGPT reflect profound changes in the
field of Artificial Intelligence, achieving a linguistic fluency that is
impressively, even shockingly, human-like. The extent of their current and
potential capabilities is an active area of investigation by no means limited
to scientific researchers. It is common for people to frame the training data
for LLMs as "text" or even "language". We examine the details of this framing
using ideas from several areas, including linguistics, embodied cognition,
cognitive science, mathematics, and history. We propose that considering what
it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us
gain insight into its capabilities in general, and in particular, that its
exposure to linguistic training data can be productively reframed as exposure
to the diegetic information encoded in language, and its deficits can be
reframed as ignorance of extradiegetic information, including supradiegetic
linguistic information. Supradiegetic linguistic information consists of those
arbitrary aspects of the physical form of language that are not derivable from
the one-dimensional relations of context -- frequency, adjacency, proximity,
co-occurrence -- that LLMs like ChatGPT have access to. Roughly speaking, the
diegetic portion of a word can be thought of as its function, its meaning, as
the information in a theoretical vector in a word embedding, while the
supradiegetic portion of the word can be thought of as its form, like the
shapes of its letters or the sounds of its syllables. We use these concepts to
investigate why LLMs like ChatGPT have trouble handling palindromes, the visual
characteristics of symbols, translating Sumerian cuneiform, and continuing
integer sequences. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06794v1' target="_blank">http://arxiv.org/pdf/2306.06794v1</a><br> <br> <br> <font size='5'> 120 </font> <div style="text-align: right"> 2023-06-11 21:44:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Augmenting Greybox Fuzzing with Generative AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Real-world programs expecting structured inputs often has a format-parsing
stage gating the deeper program space. Neither a mutation-based approach nor a
generative approach can provide a solution that is effective and scalable.
Large language models (LLM) pre-trained with an enormous amount of natural
language corpus have proved to be effective for understanding the implicit
format syntax and generating format-conforming inputs. In this paper, propose
ChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, we
pick a seed in the fuzzer's seed pool and prompt ChatGPT generative models to
variations, which are more likely to be format-conforming and thus of high
quality. We conduct extensive experiments to explore the best practice for
harvesting the power of generative LLM models. The experiment results show that
our approach improves the edge coverage by 12.77\% over the SOTA greybox fuzzer
(AFL++) on 12 target programs from three well-tested benchmarks. As for
vulnerability detection, \sys is able to perform similar to or better than
AFL++ for programs with explicit syntax rules but not for programs with
non-trivial syntax. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06782v1' target="_blank">http://arxiv.org/pdf/2306.06782v1</a><br> <br> <br> <font size='5'> 121 </font> <div style="text-align: right"> 2023-06-11 20:39:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study investigates the transformative potential of Large Language Models
(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public
data, these models, which possess remarkable language understanding and
generation capabilities, are augmenting the interpretive skills of
radiologists, enhancing patient-physician communication, and streamlining
clinical workflows. The paper introduces an analytic framework for presenting
the complex interactions between LLMs and the broader ecosystem of medical
imaging stakeholders, including businesses, insurance entities, governments,
research institutions, and hospitals (nicknamed BIGR-H). Through detailed
analyses, illustrative use cases, and discussions on the broader implications
and future directions, this perspective seeks to raise discussion in strategic
planning and decision-making in the era of AI-enabled healthcare. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06767v2' target="_blank">http://arxiv.org/pdf/2306.06767v2</a><br> <br> <br> <font size='5'> 122 </font> <div style="text-align: right"> 2023-06-11 08:16:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Molecule discovery plays a crucial role in various scientific fields,
advancing the design of tailored materials and drugs. Traditional methods for
molecule discovery follow a trial-and-error process, which are both
time-consuming and costly, while computational approaches such as artificial
intelligence (AI) have emerged as revolutionary tools to expedite various
tasks, like molecule-caption translation. Despite the importance of
molecule-caption translation for molecule discovery, most of the existing
methods heavily rely on domain experts, require excessive computational cost,
and suffer from poor performance. On the other hand, Large Language Models
(LLMs), like ChatGPT, have shown remarkable performance in various cross-modal
tasks due to their great powerful capabilities in natural language
understanding, generalization, and reasoning, which provides unprecedented
opportunities to advance molecule discovery. To address the above limitations,
in this work, we propose a novel LLMs-based framework (\textbf{MolReGPT}) for
molecule-caption translation, where a retrieval-based prompt paradigm is
introduced to empower molecule discovery with LLMs like ChatGPT without
fine-tuning. More specifically, MolReGPT leverages the principle of molecular
similarity to retrieve similar molecules and their text descriptions from a
local database to ground the generation of LLMs through in-context few-shot
molecule learning. We evaluate the effectiveness of MolReGPT via
molecule-caption translation, which includes molecule understanding and
text-based molecule generation. Experimental results show that MolReGPT
outperforms fine-tuned models like MolT5-base without any additional training.
To the best of our knowledge, MolReGPT is the first work to leverage LLMs in
molecule-caption translation for advancing molecule discovery. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06615v1' target="_blank">http://arxiv.org/pdf/2306.06615v1</a><br> <br> <br> <font size='5'> 123 </font> <div style="text-align: right"> 2023-06-10 20:55:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The identification of key factors such as medications, diseases, and
relationships within electronic health records and clinical notes has a wide
range of applications in the clinical field. In the N2C2 2022 competitions,
various tasks were presented to promote the identification of key factors in
electronic health records (EHRs) using the Contextualized Medication Event
Dataset (CMED). Pretrained large language models (LLMs) demonstrated
exceptional performance in these tasks. This study aims to explore the
utilization of LLMs, specifically ChatGPT, for data augmentation to overcome
the limited availability of annotated data for identifying the key factors in
EHRs. Additionally, different pre-trained BERT models, initially trained on
extensive datasets like Wikipedia and MIMIC, were employed to develop models
for identifying these key variables in EHRs through fine-tuning on augmented
datasets. The experimental results of two EHR analysis tasks, namely medication
identification and medication event classification, indicate that data
augmentation based on ChatGPT proves beneficial in improving performance for
both medication identification and medication event classification. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07297v1' target="_blank">http://arxiv.org/pdf/2306.07297v1</a><br> <br> <br> <font size='5'> 124 </font> <div style="text-align: right"> 2023-06-10 02:01:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06331v1' target="_blank">http://arxiv.org/pdf/2306.06331v1</a><br> <br> <br> <font size='5'> 125 </font> <div style="text-align: right"> 2023-06-09 17:53:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation. </font><br> Link: <a href='http://arxiv.org/pdf/2306.07401v1' target="_blank">http://arxiv.org/pdf/2306.07401v1</a><br> <br> <br> <font size='5'> 126 </font> <div style="text-align: right"> 2023-06-09 17:48:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Trapping LLM Hallucinations Using Tagged Context Prompts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in large language models (LLMs), such as ChatGPT, have led to
highly sophisticated conversation agents. However, these models suffer from
"hallucinations," where the model generates false or fabricated information.
Addressing this challenge is crucial, particularly with AI-driven platforms
being adopted across various sectors. In this paper, we propose a novel method
to recognize and flag instances when LLMs perform outside their domain
knowledge, and ensuring users receive accurate information.
  We find that the use of context combined with embedded tags can successfully
combat hallucinations within generative language models. To do this, we
baseline hallucination frequency in no-context prompt-response pairs using
generated URLs as easily-tested indicators of fabricated data. We observed a
significant reduction in overall hallucination when context was supplied along
with question prompts for tested generative engines. Lastly, we evaluated how
placing tags within contexts impacted model responses and were able to
eliminate hallucinations in responses with 98.88% effectiveness. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06085v1' target="_blank">http://arxiv.org/pdf/2306.06085v1</a><br> <br> <br> <font size='5'> 127 </font> <div style="text-align: right"> 2023-06-09 14:54:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Legal and ethical considerations regarding the use of ChatGPT in education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence has evolved enormously over the last two decades,
becoming mainstream in different scientific domains including education, where
so far, it is mainly utilized to enhance administrative and intelligent
tutoring systems services and academic support. ChatGPT, an artificial
intelligence-based chatbot, developed by OpenAI and released in November 2022,
has rapidly gained attention from the entire international community for its
impressive performance in generating comprehensive, systematic, and informative
human-like responses to user input through natural language processing.
Inevitably, it has also rapidly posed several challenges, opportunities, and
potential issues and concerns raised regarding its use across various
scientific disciplines. This paper aims to discuss the legal and ethical
implications arising from this new technology, identify potential use cases,
and enrich our understanding of Generative AI, such as ChatGPT, and its
capabilities in education. </font><br> Link: <a href='http://arxiv.org/pdf/2306.10037v1' target="_blank">http://arxiv.org/pdf/2306.10037v1</a><br> <br> <br> <font size='5'> 128 </font> <div style="text-align: right"> 2023-06-09 13:03:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in natural language processing (NLP) have led to the
development of large language models (LLMs) such as ChatGPT. This paper
proposes a methodology for developing and evaluating ChatGPT detectors for
French text, with a focus on investigating their robustness on out-of-domain
data and against common attack schemes. The proposed method involves
translating an English dataset into French and training a classifier on the
translated data. Results show that the detectors can effectively detect
ChatGPT-generated text, with a degree of robustness against basic attack
techniques in in-domain settings. However, vulnerabilities are evident in
out-of-domain contexts, highlighting the challenge of detecting adversarial
text. The study emphasizes caution when applying in-domain testing results to a
wider variety of content. We provide our translated datasets and models as
open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection </font><br> Link: <a href='http://arxiv.org/pdf/2306.05871v1' target="_blank">http://arxiv.org/pdf/2306.05871v1</a><br> <br> <br> <font size='5'> 129 </font> <div style="text-align: right"> 2023-06-09 11:57:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the ever-increasing utilization of natural language processing (NLP), we
started to witness over the past few years a significant transformation in our
interaction with legal texts. This technology has advanced the analysis and
enhanced the understanding of complex legal terminology and contexts. The
development of recent large language models (LLMs), particularly ChatGPT, has
also introduced a revolutionary contribution to the way that legal texts can be
processed and comprehended. In this paper, we present our work on a
cooperative-legal question-answering LLM-based chatbot, where we developed a
set of legal questions about Palestinian cooperatives, associated with their
regulations and compared the auto-generated answers by the chatbot to their
correspondences that are designed by a legal expert. To evaluate the proposed
chatbot, we have used 50 queries generated by the legal expert and compared the
answers produced by the chart to their relevance judgments. Finding
demonstrated that an overall accuracy rate of 82% has been achieved when
answering the queries, while exhibiting an F1 score equivalent to 79%. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05827v1' target="_blank">http://arxiv.org/pdf/2306.05827v1</a><br> <br> <br> <font size='5'> 130 </font> <div style="text-align: right"> 2023-06-09 11:30:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detecting Phishing Sites Using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of large language models (LLMs) has had a significant impact on
various domains, including natural language processing and artificial
intelligence. While LLMs such as ChatGPT have been extensively researched for
tasks such as code generation and text synthesis, their application in
detecting malicious web content, particularly phishing sites, has been largely
unexplored. To combat the rising tide of automated cyber attacks facilitated by
LLMs, it is imperative to automate the detection of malicious web content,
which requires approaches that leverage the power of LLMs to analyze and
classify phishing sites. In this paper, we propose a novel method that utilizes
ChatGPT to detect phishing sites. Our approach involves leveraging a web
crawler to gather information from websites and generate prompts based on this
collected data. This approach enables us to detect various phishing sites
without the need for fine-tuning machine learning models and identify social
engineering techniques from the context of entire websites and URLs. To
evaluate the performance of our proposed method, we conducted experiments using
a dataset. The experimental results using GPT-4 demonstrated promising
performance, with a precision of 98.3% and a recall of 98.4%. Comparative
analysis between GPT-3.5 and GPT-4 revealed an enhancement in the latter's
capability to reduce false negatives. These findings not only highlight the
potential of LLMs in efficiently identifying phishing sites but also have
significant implications for enhancing cybersecurity measures and protecting
users from the dangers of online fraudulent activities. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05816v1' target="_blank">http://arxiv.org/pdf/2306.05816v1</a><br> <br> <br> <font size='5'> 131 </font> <div style="text-align: right"> 2023-06-08 17:59:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversation agents fueled by Large Language Models (LLMs) are providing a
new way to interact with visual data. While there have been initial attempts
for image-based conversation models, this work addresses the underexplored
field of video-based conversation by introducing Video-ChatGPT. It is a
multimodal model that merges a video-adapted visual encoder with a LLM. The
model is capable of understanding and generating human-like conversations about
videos. We introduce a new dataset of 100,000 video-instruction pairs used to
train Video-ChatGPT acquired via manual and semi-automated pipeline that is
easily scalable and robust to label noise. We also develop a quantiative
evaluation framework for video-based dialogue models to objectively analyse the
strengths and weaknesses of proposed models. Our code, models, instruction-sets
and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05424v1' target="_blank">http://arxiv.org/pdf/2306.05424v1</a><br> <br> <br> <font size='5'> 132 </font> <div style="text-align: right"> 2023-06-08 11:29:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Regulatory bodies worldwide are intensifying their efforts to ensure
transparency in influencer marketing on social media through instruments like
the Unfair Commercial Practices Directive (UCPD) in the European Union, or
Section 5 of the Federal Trade Commission Act. Yet enforcing these obligations
has proven to be highly problematic due to the sheer scale of the influencer
market. The task of automatically detecting sponsored content aims to enable
the monitoring and enforcement of such regulations at scale. Current research
in this field primarily frames this problem as a machine learning task,
focusing on developing models that achieve high classification performance in
detecting ads. These machine learning tasks rely on human data annotation to
provide ground truth information. However, agreement between annotators is
often low, leading to inconsistent labels that hinder the reliability of
models. To improve annotation accuracy and, thus, the detection of sponsored
content, we propose using chatGPT to augment the annotation process with
phrases identified as relevant features and brief explanations. Our experiments
show that this approach consistently improves inter-annotator agreement and
annotation accuracy. Additionally, our survey of user experience in the
annotation task indicates that the explanations improve the annotators'
confidence and streamline the process. Our proposed methods can ultimately lead
to more transparency and alignment with regulatory requirements in sponsored
content detection. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05115v1' target="_blank">http://arxiv.org/pdf/2306.05115v1</a><br> <br> <br> <font size='5'> 133 </font> <div style="text-align: right"> 2023-06-08 11:14:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid adoption of generative Artificial Intelligence (AI) tools that can
generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have
put the societal impacts of these technologies at the center of public debate.
These tools are possible due to the massive amount of data (text and images)
that is publicly available through the Internet. At the same time, these
generative AI tools become content creators that are already contributing to
the data that is available to train future models. Therefore, future versions
of generative AI tools will be trained with a mix of human-created and
AI-generated content, causing a potential feedback loop between generative AI
and public data repositories. This interaction raises many questions: how will
future versions of generative AI tools behave when trained on a mixture of real
and AI generated data? Will they evolve and improve with the new data sets or
on the contrary will they degrade? Will evolution introduce biases or reduce
diversity in subsequent generations of generative AI tools? What are the
societal implications of the possible degradation of these models? Can we
mitigate the effects of this feedback loop? In this document, we explore the
effect of this interaction and report some initial results using simple
diffusion models trained with various image datasets. Our results show that the
quality and diversity of the generated images can degrade over time suggesting
that incorporating AI-created data can have undesired effects on future
versions of generative models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06130v1' target="_blank">http://arxiv.org/pdf/2306.06130v1</a><br> <br> <br> <font size='5'> 134 </font> <div style="text-align: right"> 2023-06-08 08:41:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), such as ChatGPT and GPT-4, are gaining
wide-spread real world use. Yet, the two LLMs are closed source, and little is
known about the LLMs' performance in real-world use cases. In academia, LLM
performance is often measured on benchmarks which may have leaked into
ChatGPT's and GPT-4's training data. In this paper, we apply and evaluate
ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question
answering over a text corpus that was published after the two LLMs completed
training. More specifically, we extract research challenges for researchers in
the field of HCI from the proceedings of the 2023 Conference on Human Factors
in Computing Systems (CHI). We critically evaluate the LLMs on this practical
task and conclude that the combination of ChatGPT and GPT-4 makes an excellent
cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is
key for prototyping research ideas and analyzing text corpora from different
perspectives, with implications for applying LLMs in academia and practice. For
researchers in HCI, we contribute an interactive visualization of 4392 research
challenges in over 90 research topics. We share this visualization and the
dataset in the spirit of open science. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05036v1' target="_blank">http://arxiv.org/pdf/2306.05036v1</a><br> <br> <br> <font size='5'> 135 </font> <div style="text-align: right"> 2023-06-08 08:35:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cash, Credibility, and Conversion: The Influence of Synthetic Media on Investment Behavior</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Prior to November of 2022, the topic of synthetic media was largely buried
within academic journals, constrained to conversations about national security,
and often fundamentally misunderstood. The release of ChatGPT, however, has
accelerated discourse on the societal impacts of synthetic media. This study
first highlights several gaps within existing literature on synthetic media,
structuring the impact potential and limitations of synthetic media threats
within a theoretical framework. Second, it identifies financial information
environments as prime candidates for future disruption via synthetic text
modalities, proposing an experimental survey for measuring the influential
power of synthetic financial text on global investment communities. Rather than
merely assessing the ability of survey participants to distinguish genuine from
synthetic text, the experiment contained within this study measures synthetic
media influence by observing its ability to manipulate belief via a series of
behavioral variables. The results indicate that synthetic text can
significantly shift investor sentiment away from what it might otherwise have
been under truthful information conditions. Furthermore, synthetic financial
text demonstrated a unique ability to "convert" investors, inspiring extreme
changes in outlook about a company compared to genuine financial texts. This
trend should inspire concern within the global financial community,
particularly given the historical vulnerability of equity markets to investor
sentiment shocks. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05033v1' target="_blank">http://arxiv.org/pdf/2306.05033v1</a><br> <br> <br> <font size='5'> 136 </font> <div style="text-align: right"> 2023-06-08 08:34:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: System logs play a critical role in maintaining the reliability of software
systems. Fruitful studies have explored automatic log-based anomaly detection
and achieved notable accuracy on benchmark datasets. However, when applied to
large-scale cloud systems, these solutions face limitations due to high
resource consumption and lack of adaptability to evolving logs. In this paper,
we present an accurate, lightweight, and adaptive log-based anomaly detection
framework, referred to as SeaLog. Our method introduces a Trie-based Detection
Agent (TDA) that employs a lightweight, dynamically-growing trie structure for
real-time anomaly detection. To enhance TDA's accuracy in response to evolving
log data, we enable it to receive feedback from experts. Interestingly, our
findings suggest that contemporary large language models, such as ChatGPT, can
provide feedback with a level of consistency comparable to human experts, which
can potentially reduce manual verification efforts. We extensively evaluate
SeaLog on two public datasets and an industrial dataset. The results show that
SeaLog outperforms all baseline methods in terms of effectiveness, runs 2X to
10X faster and only consumes 5% to 41% of the memory resource. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05032v1' target="_blank">http://arxiv.org/pdf/2306.05032v1</a><br> <br> <br> <font size='5'> 137 </font> <div style="text-align: right"> 2023-06-08 07:10:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04980v1' target="_blank">http://arxiv.org/pdf/2306.04980v1</a><br> <br> <br> <font size='5'> 138 </font> <div style="text-align: right"> 2023-06-08 04:08:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: covLLM: Large Language Models for COVID-19 Biomedical Literature</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The COVID-19 pandemic led to 1.1 million deaths in the United States, despite
the explosion of coronavirus research. These new findings are slow to translate
to clinical interventions, leading to poorer patient outcomes and unnecessary
deaths. One reason is that clinicians, overwhelmed by patients, struggle to
keep pace with the rate of new coronavirus literature. A potential solution is
developing a tool for evaluating coronavirus literature using large language
models (LLMs) -- neural networks that are deployed for natural language
processing. LLMs can be used to summarize and extract user-specified
information. The greater availability and advancement of LLMs and pre-processed
coronavirus literature databases provide the opportunity to assist clinicians
in evaluating coronavirus literature through a coronavirus literature specific
LLM (covLLM), a tool that directly takes an inputted research article and a
user query to return an answer. Using the COVID-19 Open Research Dataset
(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of
handwritten prompts and synthetic prompts generated using OpenAI, and (2) real
abstracts, which contains abstract and title pairs. covLLM was trained with
LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca
and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real
abstract datasets. These models were evaluated by two human evaluators and
ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract
pairs datasets performs competitively with ChatGPT and outperforms covLLM
trained primarily using the Alpaca dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04926v1' target="_blank">http://arxiv.org/pdf/2306.04926v1</a><br> <br> <br> <font size='5'> 139 </font> <div style="text-align: right"> 2023-06-07 22:56:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid growth of scientific publications, particularly during the COVID-19
pandemic, emphasizes the need for tools to help researchers efficiently
comprehend the latest advancements. One essential part of understanding
scientific literature is research aspect classification, which categorizes
sentences in abstracts to Background, Purpose, Method, and Finding. In this
study, we investigate the impact of different datasets on model performance for
the crowd-annotated CODA-19 research aspect classification task. Specifically,
we explore the potential benefits of using the large, automatically curated
PubMed 200K RCT dataset and evaluate the effectiveness of large language models
(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that
using the PubMed 200K RCT dataset does not improve performance for the CODA-19
task. We also observe that while GPT-4 performs well, it does not outperform
the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance
of a dedicated and task-aligned datasets dataset for the target task. Our code
is available at https://github.com/Crowd-AI-Lab/CODA-19-exp. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04820v1' target="_blank">http://arxiv.org/pdf/2306.04820v1</a><br> <br> <br> <font size='5'> 140 </font> <div style="text-align: right"> 2023-06-07 19:59:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work we explore recent advances in instruction-tuning language models
on a range of open instruction-following datasets. Despite recent claims that
open models can be on par with state-of-the-art proprietary models, these
claims are often accompanied by limited evaluation, making it difficult to
compare models across the board and determine the utility of various resources.
We provide a large set of instruction-tuned models from 6.7B to 65B parameters
in size, trained on 12 instruction datasets ranging from manually curated
(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and
systematically evaluate them on their factual knowledge, reasoning,
multilinguality, coding, and open-ended instruction following abilities through
a collection of automatic, model-based, and human-based metrics. We further
introduce T\"ulu, our best performing instruction-tuned model suite finetuned
on a combination of high-quality open resources.
  Our experiments show that different instruction-tuning datasets can uncover
or enhance specific skills, while no single dataset (or combination) provides
the best performance across all evaluations. Interestingly, we find that model
and human preference-based evaluations fail to reflect differences in model
capabilities exposed by benchmark-based evaluations, suggesting the need for
the type of systemic evaluation performed in this work. Our evaluations show
that the best model in any given evaluation reaches on average 83% of ChatGPT
performance, and 68% of GPT-4 performance, suggesting that further investment
in building better base models and instruction-tuning data is required to close
the gap. We release our instruction-tuned models, including a fully finetuned
65B T\"ulu, along with our code, data, and evaluation framework at
https://github.com/allenai/open-instruct to facilitate future research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04751v1' target="_blank">http://arxiv.org/pdf/2306.04751v1</a><br> <br> <br> <font size='5'> 141 </font> <div style="text-align: right"> 2023-06-07 16:10:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Humor is a central aspect of human communication that has not been solved for
artificial agents so far. Large language models (LLMs) are increasingly able to
capture implicit and contextual information. Especially, OpenAI's ChatGPT
recently gained immense public attention. The GPT3-based model almost seems to
communicate on a human level and can even tell jokes. Humor is an essential
component of human communication. But is ChatGPT really funny? We put ChatGPT's
sense of humor to the test. In a series of exploratory experiments around
jokes, i.e., generation, explanation, and detection, we seek to understand
ChatGPT's capability to grasp and reproduce human humor. Since the model itself
is not accessible, we applied prompt-based experiments. Our empirical evidence
indicates that jokes are not hard-coded but mostly also not newly generated by
the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system
accurately explains valid jokes but also comes up with fictional explanations
for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the
classification of jokes. ChatGPT has not solved computational humor yet but it
can be a big leap toward "funny" machines. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04563v1' target="_blank">http://arxiv.org/pdf/2306.04563v1</a><br> <br> <br> <font size='5'> 142 </font> <div style="text-align: right"> 2023-06-07 15:44:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT and Microsoft Bing AI Chat Performances on Physics Exams of Vietnamese National High School Graduation Examination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The promise and difficulties of language model-based approaches for physics
teaching were assessed in this study. This study evaluates how well ChatGPT and
BingChat, two state-of-the-art (SOTA) large language models (LLMs), perform
when answering high school physics questions on Vietnamese exams from 2019 to
2023. When we compared the results of the LLMs with the scores of Vietnamese
students, we discovered that ChatGPT and BingChat both perform worse than
Vietnamese students, proving that LLMs are not yet capable of fully replacing
human intellect in the field of physics teaching. The outcomes also showed that
neither LLM is capable of responding to questions at the high application
levels. In terms of accuracy, BingChat typically surpassed ChatGPT, although
ChatGPT showed more stability. Our research suggests that LLMs can help
students and teachers during learning and teaching activities, particularly by
offering immediate feedback and individualized learning experiences. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04538v3' target="_blank">http://arxiv.org/pdf/2306.04538v3</a><br> <br> <br> <font size='5'> 143 </font> <div style="text-align: right"> 2023-06-07 15:42:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Long-form analogies generated by chatGPT lack human-like psycholinguistic properties</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Psycholinguistic analyses provide a means of evaluating large language model
(LLM) output and making systematic comparisons to human-generated text. These
methods can be used to characterize the psycholinguistic properties of LLM
output and illustrate areas where LLMs fall short in comparison to
human-generated text. In this work, we apply psycholinguistic methods to
evaluate individual sentences from long-form analogies about biochemical
concepts. We compare analogies generated by human subjects enrolled in
introductory biochemistry courses to analogies generated by chatGPT. We perform
a supervised classification analysis using 78 features extracted from
Coh-metrix that analyze text cohesion, language, and readability (Graesser et.
al., 2004). Results illustrate high performance for classifying
student-generated and chatGPT-generated analogies. To evaluate which features
contribute most to model performance, we use a hierarchical clustering
approach. Results from this analysis illustrate several linguistic differences
between the two sources. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04537v1' target="_blank">http://arxiv.org/pdf/2306.04537v1</a><br> <br> <br> <font size='5'> 144 </font> <div style="text-align: right"> 2023-06-07 15:20:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Whereas the recent emergence of large language models (LLMs) like ChatGPT has
exhibited impressive general performance, it still has a large gap with
fully-supervised models on specific tasks such as multi-span question
answering. Previous researches found that in-context learning is an effective
approach to exploiting LLM, by using a few task-related labeled data as
demonstration examples to construct a few-shot prompt for answering new
questions. A popular implementation is to concatenate a few questions and their
correct answers through simple templates, informing LLM of the desired output.
In this paper, we propose a novel way of employing labeled data such that it
also informs LLM of some undesired output, by extending demonstration examples
with feedback about answers predicted by an off-the-shelf model, e.g., correct,
incorrect, or incomplete. Experiments on three multi-span question answering
datasets as well as a keyphrase extraction dataset show that our new prompting
strategy consistently improves LLM's in-context learning performance. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04508v1' target="_blank">http://arxiv.org/pdf/2306.04508v1</a><br> <br> <br> <font size='5'> 145 </font> <div style="text-align: right"> 2023-06-07 15:11:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04504v2' target="_blank">http://arxiv.org/pdf/2306.04504v2</a><br> <br> <br> <font size='5'> 146 </font> <div style="text-align: right"> 2023-06-07 13:31:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Examining Bias in Opinion Summarisation Through the Perspective of Opinion Diversity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Opinion summarisation is a task that aims to condense the information
presented in the source documents while retaining the core message and
opinions. A summary that only represents the majority opinions will leave the
minority opinions unrepresented in the summary. In this paper, we use the
stance towards a certain target as an opinion. We study bias in opinion
summarisation from the perspective of opinion diversity, which measures whether
the model generated summary can cover a diverse set of opinions. In addition,
we examine opinion similarity, a measure of how closely related two opinions
are in terms of their stance on a given topic, and its relationship with
opinion diversity. Through the lens of stances towards a topic, we examine
opinion diversity and similarity using three debatable topics under COVID-19.
Experimental results on these topics revealed that a higher degree of
similarity of opinions did not indicate good diversity or fairly cover the
various opinions originally presented in the source documents. We found that
BART and ChatGPT can better capture diverse opinions presented in the source
documents. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04424v1' target="_blank">http://arxiv.org/pdf/2306.04424v1</a><br> <br> <br> <font size='5'> 147 </font> <div style="text-align: right"> 2023-06-07 12:35:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction tuning has significantly advanced large language models (LLMs)
such as ChatGPT, enabling them to align with human instructions across diverse
tasks. However, progress in open vision-language models (VLMs) has been limited
due to the scarcity of high-quality instruction datasets. To tackle this
challenge and promote research in the vision-language field, we introduce the
Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to
optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises
40 carefully curated datasets, including 2.4 million instances and 400 manually
written task instructions, reformatted into a vision-to-text structure. Key
tasks are translated into 80 languages with an advanced translation system,
ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding
task coverage, instruction number and instance scale. Moreover, we develop
Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential
to answer complex questions requiring world knowledge, generalize to unseen
video tasks, and comprehend unseen instructions in Chinese. We have
open-sourced the dataset to encourage further research. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04387v2' target="_blank">http://arxiv.org/pdf/2306.04387v2</a><br> <br> <br> <font size='5'> 148 </font> <div style="text-align: right"> 2023-06-07 12:33:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With ChatGPT under the spotlight, utilizing large language models (LLMs) for
academic writing has drawn a significant amount of discussions and concerns in
the community. While substantial research efforts have been stimulated for
detecting LLM-Generated Content (LLM-content), most of the attempts are still
in the early stage of exploration. In this paper, we present a holistic
investigation of detecting LLM-generate academic writing, by providing a
dataset, evidence, and algorithms, in order to inspire more community effort to
address the concern of LLM academic misuse. We first present GPABenchmark, a
benchmarking dataset of 600,000 samples of human-written, GPT-written,
GPT-completed, and GPT-polished abstracts of research papers in CS, physics,
and humanities and social sciences (HSS). We show that existing open-source and
commercial GPT detectors provide unsatisfactory performance on GPABenchmark,
especially for GPT-polished text. Moreover, through a user study of 150+
participants, we show that it is highly challenging for human users, including
experienced faculty members and researchers, to identify GPT-generated
abstracts. We then present CheckGPT, a novel LLM-content detector consisting of
a general representation module and an attentive-BiLSTM classification module,
which is accurate, transferable, and interpretable. Experimental results show
that CheckGPT achieves an average classification accuracy of 98% to 99% for the
task-specific discipline-specific detectors and the unified detectors. CheckGPT
is also highly transferable that, without tuning, it achieves ~90% accuracy in
new domains, such as news articles, while a model tuned with approximately
2,000 samples in the target domain achieves ~98% accuracy. Finally, we
demonstrate the explainability insights obtained from CheckGPT to reveal the
key behaviors of how LLM generates texts. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05524v1' target="_blank">http://arxiv.org/pdf/2306.05524v1</a><br> <br> <br> <font size='5'> 149 </font> <div style="text-align: right"> 2023-06-07 10:45:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Last Week with ChatGPT: A Weibo Study on Social Perspective regarding ChatGPT for Education and Beyond</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has piqued the interest of many fields, particularly in the academic
community. GPT-4, the latest version, starts supporting multimodal input and
output. This study examines social media posts to analyze how the Chinese
public perceives the potential of ChatGPT for educational and general purposes.
The study also serves as the first effort to investigate the changes in public
opinion since the release of GPT-4. According to the analysis results, prior to
GPT-4, although some social media users believed that AI advancements would
benefit education and society, some believed that advanced AI, such as ChatGPT,
would make humans feel inferior and lead to problems such as cheating and a
decline in moral principles, while the majority remain neutral. Interestingly,
public attitudes have tended to shift in a positive direction since the release
of GPT-4. We present a thorough analysis of the trending shift and a roadmap to
ensure the ethical application of ChatGPT-like models in education and beyond. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04325v1' target="_blank">http://arxiv.org/pdf/2306.04325v1</a><br> <br> <br> <font size='5'> 150 </font> <div style="text-align: right"> 2023-06-07 02:32:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding Place Identity with Generative AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Researchers are constantly leveraging new forms of data with the goal of
understanding how people perceive the built environment and build the
collective place identity of cities. Latest advancements in generative
artificial intelligence (AI) models have enabled the production of realistic
representations learned from vast amounts of data. In this study, we aim to
test the potential of generative AI as the source of textual and visual
information in capturing the place identity of cities assessed by filtered
descriptions and images. We asked questions on the place identity of a set of
31 global cities to two generative AI models, ChatGPT and DALL-E2. Since
generative AI has raised ethical concerns regarding its trustworthiness, we
performed cross-validation to examine whether the results show similar patterns
to real urban settings. In particular, we compared the outputs with Wikipedia
data for text and images searched from Google for image. Our results indicate
that generative AI models have the potential to capture the collective image of
cities that can make them distinguishable. This study is among the first
attempts to explore the capabilities of generative AI in understanding human
perceptions of the built environment. It contributes to urban design literature
by discussing future research opportunities and potential limitations. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04662v1' target="_blank">http://arxiv.org/pdf/2306.04662v1</a><br> <br> <br> <font size='5'> 151 </font> <div style="text-align: right"> 2023-06-06 23:15:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Amid ongoing health crisis, there is a growing necessity to discern possible
signs of Wellness Dimensions (WD) manifested in self-narrated text. As the
distribution of WD on social media data is intrinsically imbalanced, we
experiment the generative NLP models for data augmentation to enable further
improvement in the pre-screening task of classifying WD. To this end, we
propose a simple yet effective data augmentation approach through prompt-based
Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic
similarity among existing interpretations and augmented data. Our approach with
ChatGPT model surpasses all the other methods and achieves improvement over
baselines such as Easy-Data Augmentation and Backtranslation. Introducing data
augmentation to generate more training samples and balanced dataset, results in
the improved F-score and the Matthew's Correlation Coefficient for upto 13.11%
and 15.95%, respectively. </font><br> Link: <a href='http://arxiv.org/pdf/2306.04059v1' target="_blank">http://arxiv.org/pdf/2306.04059v1</a><br> <br> <br> <font size='5'> 152 </font> <div style="text-align: right"> 2023-06-06 17:18:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deductive Verification of Chain-of-Thought Reasoning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03872v2' target="_blank">http://arxiv.org/pdf/2306.03872v2</a><br> <br> <br> <font size='5'> 153 </font> <div style="text-align: right"> 2023-06-06 05:50:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since the release of OpenAI's ChatGPT, generative language models have
attracted extensive public attention. The increased usage has highlighted
generative models' broad utility, but also revealed several forms of embedded
bias. Some is induced by the pre-training corpus; but additional bias specific
to generative models arises from the use of subjective fine-tuning to avoid
generating harmful content. Fine-tuning bias may come from individual engineers
and company policies, and affects which prompts the model chooses to refuse. In
this experiment, we characterize ChatGPT's refusal behavior using a black-box
attack. We first query ChatGPT with a variety of offensive and benign prompts
(n=1,706), then manually label each response as compliance or refusal. Manual
examination of responses reveals that refusal is not cleanly binary, and lies
on a continuum; as such, we map several different kinds of responses to a
binary of compliance or refusal. The small manually-labeled dataset is used to
train a refusal classifier, which achieves an accuracy of 96%. Second, we use
this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from
the Quora Insincere Questions dataset. With this machine-labeled data, we train
a prompt classifier to predict whether ChatGPT will refuse a given question,
without seeing ChatGPT's response. This prompt classifier achieves 76% accuracy
on a test set of manually labeled questions (n=985). We examine our classifiers
and the prompt n-grams that are most predictive of either compliance or
refusal. Our datasets and code are available at
https://github.com/maxwellreuter/chatgpt-refusals. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03423v2' target="_blank">http://arxiv.org/pdf/2306.03423v2</a><br> <br> <br> <font size='5'> 154 </font> <div style="text-align: right"> 2023-06-05 22:24:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Adapting Computer Science Courses to AI Assistants' Capabilities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The use of AI assistants, along with the challenges they present, has sparked
significant debate within the community of computer science education. While
these tools demonstrate the potential to support students' learning and
instructors' teaching, they also raise concerns about enabling unethical uses
by students. Previous research has suggested various strategies aimed at
addressing these issues. However, they concentrate on the introductory
programming courses and focus on one specific type of problem.
  The present research evaluated the performance of ChatGPT, a state-of-the-art
AI assistant, at solving 187 problems spanning three distinct types that were
collected from six undergraduate computer science. The selected courses covered
different topics and targeted different program levels. We then explored
methods to modify these problems to adapt them to ChatGPT's capabilities to
reduce potential misuse by students. Finally, we conducted semi-structured
interviews with 11 computer science instructors. The aim was to gather their
opinions on our problem modification methods, understand their perspectives on
the impact of AI assistants on computer science education, and learn their
strategies for adapting their courses to leverage these AI capabilities for
educational improvement. The results revealed issues ranging from academic
fairness to long-term impact on students' mental models. From our results, we
derived design implications and recommended tools to help instructors design
and create future course material that could more effectively adapt to AI
assistants' capabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03289v1' target="_blank">http://arxiv.org/pdf/2306.03289v1</a><br> <br> <br> <font size='5'> 155 </font> <div style="text-align: right"> 2023-06-05 21:38:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large pre-trained neural language models have brought immense progress to
both NLP and software engineering. Models in OpenAI's GPT series now dwarf
Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide
range of NLP applications. These models are trained on massive corpora of
heterogeneous data from web crawls, which enables them to learn general
language patterns and semantic relationships. However, the largest models are
both expensive to train and deploy and are often closed-source, so we lack
access to their data and design decisions. We argue that this trend towards
large, general-purpose models should be complemented with single-purpose, more
modestly sized pre-trained models. In this work, we take StackOverflow (SO) as
a domain example in which large volumes of rich aligned code and text data is
available. We adopt standard practices for pre-training large language models,
including using a very large context size (2,048 tokens), batch size (0.5M
tokens) and training set (27B tokens), coupled with a powerful toolkit
(Megatron-LM), to train two models: SOBertBase, with 109M parameters, and
SOBertLarge with 762M parameters, at a budget of just $\$187$ and $\$800$ each.
We compare the performance of our models with both the previous SOTA model
trained on SO data exclusively as well general-purpose BERT models and OpenAI's
ChatGPT on four SO-specific downstream tasks - question quality prediction,
closed question prediction, named entity recognition and obsoletion prediction
(a new task we introduce). Not only do our models consistently outperform all
baselines, the smaller model is often sufficient for strong results. Both
models are released to the public. These results demonstrate that pre-training
both extensively and properly on in-domain data can yield a powerful and
affordable alternative to leveraging closed-source general-purpose models. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03268v1' target="_blank">http://arxiv.org/pdf/2306.03268v1</a><br> <br> <br> <font size='5'> 156 </font> <div style="text-align: right"> 2023-06-05 21:33:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction-tuned generative Large language models (LLMs) like ChatGPT and
Bloomz possess excellent generalization abilities, but they face limitations in
understanding radiology reports, particularly in the task of generating the
IMPRESSIONS section from the FINDINGS section. They tend to generate either
verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to
medical text data during training. We present a system which leverages
large-scale medical text data for domain-adaptive pre-training of
instruction-tuned LLMs to enhance its medical knowledge and performance on
specific medical tasks. We show that this system performs better in a zero-shot
setting than a number of pretrain-and-finetune adaptation methods on the
IMPRESSIONS generation task, and ranks 1st among participating systems in Task
1B: Radiology Report Summarization at the BioNLP 2023 workshop. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03264v1' target="_blank">http://arxiv.org/pdf/2306.03264v1</a><br> <br> <br> <font size='5'> 157 </font> <div style="text-align: right"> 2023-06-05 21:14:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-Shot 3D Shape Correspondence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a novel zero-shot approach to computing correspondences between 3D
shapes. Existing approaches mainly focus on isometric and near-isometric shape
pairs (e.g., human vs. human), but less attention has been given to strongly
non-isometric and inter-class shape matching (e.g., human vs. cow). To this
end, we introduce a fully automatic method that exploits the exceptional
reasoning capabilities of recent foundation models in language and vision to
tackle difficult shape correspondence problems. Our approach comprises multiple
stages. First, we classify the 3D shapes in a zero-shot manner by feeding
rendered shape views to a language-vision model (e.g., BLIP2) to generate a
list of class proposals per shape. These proposals are unified into a single
class per shape by employing the reasoning capabilities of ChatGPT. Second, we
attempt to segment the two shapes in a zero-shot manner, but in contrast to the
co-segmentation problem, we do not require a mutual set of semantic regions.
Instead, we propose to exploit the in-context learning capabilities of ChatGPT
to generate two different sets of semantic regions for each shape and a
semantic mapping between them. This enables our approach to match strongly
non-isometric shapes with significant differences in geometric structure.
Finally, we employ the generated semantic mapping to produce coarse
correspondences that can further be refined by the functional maps framework to
produce dense point-to-point maps. Our approach, despite its simplicity,
produces highly plausible results in a zero-shot manner, especially between
strongly non-isometric shapes. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03253v1' target="_blank">http://arxiv.org/pdf/2306.03253v1</a><br> <br> <br> <font size='5'> 158 </font> <div style="text-align: right"> 2023-06-05 19:26:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores the concept of leveraging generative AI as a mapping
assistant for enhancing the efficiency of collaborative mapping. We present
results of an experiment that combines multiple sources of volunteered
geographic information (VGI) and large language models (LLMs). Three analysts
described the content of crowdsourced Mapillary street-level photographs taken
along roads in a small test area in Miami, Florida. GPT-3.5-turbo was
instructed to suggest the most appropriate tagging for each road in
OpenStreetMap (OSM). The study also explores the utilization of BLIP-2, a
state-of-the-art multimodal pre-training method as an artificial analyst of
street-level photographs in addition to human analysts. Results demonstrate two
ways to effectively increase the accuracy of mapping suggestions without
modifying the underlying AI models: by (1) providing a more detailed
description of source photographs, and (2) combining prompt engineering with
additional context (e.g. location and objects detected along a road). The first
approach increases the suggestion accuracy by up to 29%, and the second one by
up to 20%. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03204v1' target="_blank">http://arxiv.org/pdf/2306.03204v1</a><br> <br> <br> <font size='5'> 159 </font> <div style="text-align: right"> 2023-06-05 17:59:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Coaching, which involves classroom observation and expert feedback, is a
widespread and fundamental part of teacher training. However, the majority of
teachers do not have access to consistent, high quality coaching due to limited
resources and access to expertise. We explore whether generative AI could
become a cost-effective complement to expert feedback by serving as an
automated teacher coach. In doing so, we propose three teacher coaching tasks
for generative AI: (A) scoring transcript segments based on classroom
observation instruments, (B) identifying highlights and missed opportunities
for good instructional strategies, and (C) providing actionable suggestions for
eliciting more student reasoning. We recruit expert math teachers to evaluate
the zero-shot performance of ChatGPT on each of these tasks for elementary math
classroom transcripts. Our results reveal that ChatGPT generates responses that
are relevant to improving instruction, but they are often not novel or
insightful. For example, 82% of the model's suggestions point to places in the
transcript where the teacher is already implementing that suggestion. Our work
highlights the challenges of producing insightful, novel and truthful feedback
for teachers while paving the way for future research to address these
obstacles and improve the capacity of generative AI to coach teachers. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03090v1' target="_blank">http://arxiv.org/pdf/2306.03090v1</a><br> <br> <br> <font size='5'> 160 </font> <div style="text-align: right"> 2023-06-05 17:55:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models~(LLMs) are instruction followers, but it can be
challenging to find the best instruction for different situations, especially
for black-box LLMs on which backpropagation is forbidden. Instead of directly
optimizing the discrete instruction, we optimize a low-dimensional soft prompt
applied to an open-source LLM to generate the instruction for the black-box
LLM. On each iteration of the proposed method, which we call InstructZero, a
soft prompt is converted into an instruction using the open-source LLM, which
is then submitted to the black-box LLM for zero-shot evaluation, and the
performance is sent to Bayesian optimization to produce new soft prompts
improving the zero-shot performance. We evaluate InstructZero on different
combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our
results show that InstructZero outperforms SOTA auto-instruction methods across
a variety of downstream tasks. Our code and data are publicly available at
https://github.com/Lichang-Chen/InstructZero. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03082v1' target="_blank">http://arxiv.org/pdf/2306.03082v1</a><br> <br> <br> <font size='5'> 161 </font> <div style="text-align: right"> 2023-06-05 16:44:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: PokemonChat: Auditing ChatGPT for Pokmon Universe Knowledge</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recently released ChatGPT model demonstrates unprecedented capabilities
in zero-shot question-answering. In this work, we probe ChatGPT for its
conversational understanding and introduce a conversational framework
(protocol) that can be adopted in future studies. The Pok\'emon universe serves
as an ideal testing ground for auditing ChatGPT's reasoning capabilities due to
its closed world assumption. After bringing ChatGPT's background knowledge (on
the Pok\'emon universe) to light, we test its reasoning process when using
these concepts in battle scenarios. We then evaluate its ability to acquire new
knowledge and include it in its reasoning process. Our ultimate goal is to
assess ChatGPT's ability to generalize, combine features, and to acquire and
reason over newly introduced knowledge from human feedback. We find that
ChatGPT has prior knowledge of the Pokemon universe, which can reason upon in
battle scenarios to a great extent, even when new information is introduced.
The model performs better with collaborative feedback and if there is an
initial phase of information retrieval, but also hallucinates occasionally and
is susceptible to adversarial attacks. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03024v1' target="_blank">http://arxiv.org/pdf/2306.03024v1</a><br> <br> <br> <font size='5'> 162 </font> <div style="text-align: right"> 2023-06-05 13:52:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Will ChatGPT and Related AI-Tools Alter the Future of the Geosciences and Petroleum Engineering?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A key aim of this paper is to explore how our professional tasks as
geoscientists and petroleum engineers can be completed more effectively making
use of tools powered by Artificial Intelligence (AI), offered in commercial
platforms now readily available to individual users. This paper intends to
provide some guidance, but at the same time does not claim to be comprehensive
or conclusive in any way. The paper presents a utility assessment from the
research and teaching vantage points of two professors and one student, from
geosciences and petroleum engineering departments. After a brief overview of
the new technologies, some key questions raised include: How can one assess
originality of class papers by students and research papers by their
professors? How will the contribution of intelligent devices be acknowledged?
Will the presentation of conference papers by author avatars be accepted by the
organizing committee? </font><br> Link: <a href='http://arxiv.org/pdf/2306.02882v1' target="_blank">http://arxiv.org/pdf/2306.02882v1</a><br> <br> <br> <font size='5'> 163 </font> <div style="text-align: right"> 2023-06-05 08:58:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Orca: Progressive Learning from Complex Explanation Traces of GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs; small
scale homogeneous training data; and most notably a lack of rigorous evaluation
resulting in overestimating the small model's capability as they tend to learn
to imitate the style, but not the reasoning process of LFMs. To address these
challenges, we develop Orca (We are working with our legal team to publicly
release a diff of the model weights in accordance with LLaMA's release policy
to be published at https://aka.ms/orca-lm), a 13-billion parameter model that
learns to imitate the reasoning process of LFMs. Orca learns from rich signals
from GPT-4 including explanation traces; step-by-step thought processes; and
other complex instructions, guided by teacher assistance from ChatGPT. To
promote this progressive learning, we tap into large-scale and diverse
imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B by
more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard
(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH
benchmark and shows competitive performance (4 pts gap with optimized system
message) in professional and academic examinations like the SAT, LSAT, GRE, and
GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our
research indicates that learning from step-by-step explanations, whether these
are generated by humans or more advanced AI models, is a promising direction to
improve model capabilities and skills. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02707v1' target="_blank">http://arxiv.org/pdf/2306.02707v1</a><br> <br> <br> <font size='5'> 164 </font> <div style="text-align: right"> 2023-06-05 03:32:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present LLM-Blender, an ensembling framework designed to attain
consistently superior performance by leveraging the diverse strengths of
multiple open-source large language models (LLMs). Our framework consists of
two modules: PairRanker and GenFuser, addressing the observation that optimal
LLMs for different examples can significantly vary. PairRanker employs a
specialized pairwise comparison method to distinguish subtle differences
between candidate outputs. It jointly encodes the input text and a pair of
candidates, using cross-attention encoders to determine the superior one. Our
results demonstrate that PairRanker exhibits the highest correlation with
ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,
generating an improved output by capitalizing on their strengths and mitigating
their weaknesses. To facilitate large-scale evaluation, we introduce a
benchmark dataset, MixInstruct, which is a mixture of multiple instruction
datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly
outperform individual LLMs and baseline methods across various metrics,
establishing a substantial performance gap. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02561v3' target="_blank">http://arxiv.org/pdf/2306.02561v3</a><br> <br> <br> <font size='5'> 165 </font> <div style="text-align: right"> 2023-06-05 02:52:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of AI Chatbots for Patient-Specific EHR Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02549v1' target="_blank">http://arxiv.org/pdf/2306.02549v1</a><br> <br> <br> <font size='5'> 166 </font> <div style="text-align: right"> 2023-06-04 03:09:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the development of large language models, many remarkable linguistic
systems like ChatGPT have thrived and achieved astonishing success on many
tasks, showing the incredible power of foundation models. In the spirit of
unleashing the capability of foundation models on vision tasks, the Segment
Anything Model (SAM), a vision foundation model for image segmentation, has
been proposed recently and presents strong zero-shot ability on many downstream
2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be
explored, especially 3D object detection. With this inspiration, we explore
adapting the zero-shot ability of SAM to 3D object detection in this paper. We
propose a SAM-powered BEV processing pipeline to detect objects and get
promising results on the large-scale Waymo open dataset. As an early attempt,
our method takes a step toward 3D object detection with vision foundation
models and presents the opportunity to unleash their power on 3D vision tasks.
The code is released at https://github.com/DYZhang09/SAM3D. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02245v1' target="_blank">http://arxiv.org/pdf/2306.02245v1</a><br> <br> <br> <font size='5'> 167 </font> <div style="text-align: right"> 2023-06-03 22:35:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have gained considerable attention for
Artificial Intelligence Generated Content (AIGC), particularly with the
emergence of ChatGPT. However, the direct adaptation of continuous speech to
LLMs that process discrete tokens remains an unsolved challenge, hindering the
application of LLMs for speech generation. The advanced speech LMs are in the
corner, as that speech signals encapsulate a wealth of information, including
speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated
notable gains in parameter efficiency and competitive performance on some
speech classification tasks. However, the extent to which prompts can
effectively elicit generation tasks from speech LMs remains an open question.
In this paper, we present pioneering research that explores the application of
prompt tuning to stimulate speech LMs for various generation tasks, within a
unified framework called SpeechGen, with around 10M trainable parameters. The
proposed unified framework holds great promise for efficiency and
effectiveness, particularly with the imminent arrival of advanced speech LMs,
which will significantly enhance the capabilities of the framework. The code
and demos of SpeechGen will be available on the project website:
\url{https://ga642381.github.io/SpeechPrompt/speechgen} </font><br> Link: <a href='http://arxiv.org/pdf/2306.02207v2' target="_blank">http://arxiv.org/pdf/2306.02207v2</a><br> <br> <br> <font size='5'> 168 </font> <div style="text-align: right"> 2023-06-03 15:41:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unsupervised Human Activity Recognition through Two-stage Prompting with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Wearable sensor devices, which offer the advantage of recording daily objects
used by a person while performing an activity, enable the feasibility of
unsupervised Human Activity Recognition (HAR). Unfortunately, previous
unsupervised approaches using the usage sequence of objects usually require a
proper description of activities manually prepared by humans. Instead, we
leverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT.
Because the sequence of objects robustly characterizes the activity identity,
it is possible that ChatGPT already learned the association between activities
and objects from existing contexts. However, previous prompt engineering for
ChatGPT exhibits limited generalization ability when dealing with a list of
words (i.e., sequence of objects) due to the similar weighting assigned to each
word in the list. In this study, we propose a two-stage prompt engineering,
which first guides ChatGPT to generate activity descriptions associated with
objects while emphasizing important objects for distinguishing similar
activities; then outputs activity classes and explanations for enhancing the
contexts that are helpful for HAR. To the best of our knowledge, this is the
first study that utilizes ChatGPT to recognize activities using objects in an
unsupervised manner. We conducted our approach on three datasets and
demonstrated the state-of-the-art performance. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02140v1' target="_blank">http://arxiv.org/pdf/2306.02140v1</a><br> <br> <br> <font size='5'> 169 </font> <div style="text-align: right"> 2023-06-03 12:12:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Befriending ChatGPT and other superchatbots: An AI-integrated take-home assessment preserving integrity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the launch of ChatGPT, serious concerns have reasonably been raised of
its ill-effect on the integrity of remote take-home exams. By way of mitigating
the concern, in this study, a rather straightforward Artificial-Intelligence
(AI)-integrated take-home assessment technique is proposed, and the outcome of
its practice is discussed. Despite involving AI, in the form of ChatGPT, the
assessment adheres to the convention of posing questions invoking critical
thinking and problem solving skills. However, AI is characteristically
integrated in this assessment by instructing the learners to employ ChatGPT as
one of the primary sources. The learners are directed to report the use of
ChatGPT by including both the prompts and its responses, before expressing
their thoughts on AI-generated answers and their own concluding statement.
These three characteristic components of the present techniques -- the handling
of ChatGPT through the prompts, comments on the AI-responses and the concluding
thoughts -- are evaluated to gauge the learning.
  The proposed assessment was assigned as a take-home group activity for a
batch of seventy eight students, divided into thirteen groups. Despite
addressing the same questions, there was no significant overlap in the answers.
Moreover, a wide range of approaches were adopted by the groups in handling
ChatGPT, which in-turn rendered different responses, ultimately drawing
distinct answers. Besides preventing the undesired use of ChatGPT by explicitly
integrating it, the proposed assessment seemingly helped the learners question
the accuracy of its responses. This self-realised skepticism can be expected to
curtail blatant malpractices involving ChatGPT in the long run. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02096v1' target="_blank">http://arxiv.org/pdf/2306.02096v1</a><br> <br> <br> <font size='5'> 170 </font> <div style="text-align: right"> 2023-06-03 10:54:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Utilizing ChatGPT to Enhance Clinical Trial Enrollment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Clinical trials are a critical component of evaluating the effectiveness of
new medical interventions and driving advancements in medical research.
Therefore, timely enrollment of patients is crucial to prevent delays or
premature termination of trials. In this context, Electronic Health Records
(EHRs) have emerged as a valuable tool for identifying and enrolling eligible
participants. In this study, we propose an automated approach that leverages
ChatGPT, a large language model, to extract patient-related information from
unstructured clinical notes and generate search queries for retrieving
potentially eligible clinical trials. Our empirical evaluation, conducted on
two benchmark retrieval collections, shows improved retrieval performance
compared to existing approaches when several general-purposed and task-specific
prompts are used. Notably, ChatGPT-generated queries also outperform
human-generated queries in terms of retrieval performance. These findings
highlight the potential use of ChatGPT to enhance clinical trial enrollment
while ensuring the quality of medical service and minimizing direct risks to
patients. </font><br> Link: <a href='http://arxiv.org/pdf/2306.02077v1' target="_blank">http://arxiv.org/pdf/2306.02077v1</a><br> <br> <br> <font size='5'> 171 </font> <div style="text-align: right"> 2023-06-02 22:39:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LIC-GAN: Language Information Conditioned Graph Generative GAN Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep generative models for Natural Language data offer a new angle on the
problem of graph synthesis: by optimizing differentiable models that directly
generate graphs, it is possible to side-step expensive search procedures in the
discrete and vast space of possible graphs. We introduce LIC-GAN, an implicit,
likelihood-free generative model for small graphs that circumvents the need for
expensive graph matching procedures. Our method takes as input a natural
language query and using a combination of language modelling and Generative
Adversarial Networks (GANs) and returns a graph that closely matches the
description of the query. We combine our approach with a reward network to
further enhance the graph generation with desired properties. Our experiments,
show that LIC-GAN does well on metrics such as PropMatch and Closeness getting
scores of 0.36 and 0.48. We also show that LIC-GAN performs as good as ChatGPT,
with ChatGPT getting scores of 0.40 and 0.42. We also conduct a few experiments
to demonstrate the robustness of our method, while also highlighting a few
interesting caveats of the model. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01937v1' target="_blank">http://arxiv.org/pdf/2306.01937v1</a><br> <br> <br> <font size='5'> 172 </font> <div style="text-align: right"> 2023-06-02 17:12:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating Language Models for Mathematics through Interactions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The standard methodology of evaluating large language models (LLMs) based on
static pairs of inputs and outputs is insufficient for developing assistants:
this kind of assessments fails to take into account the essential interactive
element in their deployment, and therefore limits how we understand language
model capabilities. We introduce CheckMate, an adaptable prototype platform for
humans to interact with and evaluate LLMs. We conduct a study with CheckMate to
evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants
in proving undergraduate-level mathematics, with a mixed cohort of participants
from undergraduate students to professors of mathematics. We release the
resulting interaction and rating dataset, MathConverse. By analysing
MathConverse, we derive a preliminary taxonomy of human behaviours and uncover
that despite a generally positive correlation, there are notable instances of
divergence between correctness and perceived helpfulness in LLM generations,
amongst other findings. Further, we identify useful scenarios and existing
issues of GPT-4 in mathematical reasoning through a series of case studies
contributed by expert mathematicians. We conclude with actionable takeaways for
ML practitioners and mathematicians: models which communicate uncertainty,
respond well to user corrections, are more interpretable and concise may
constitute better assistants; interactive evaluation is a promising way to
continually navigate the capability of these models; humans should be aware of
language models' algebraic fallibility, and for that reason discern where they
should be used. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01694v1' target="_blank">http://arxiv.org/pdf/2306.01694v1</a><br> <br> <br> <font size='5'> 173 </font> <div style="text-align: right"> 2023-06-02 14:58:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Evaluation of Log Parsing with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Software logs play an essential role in ensuring the reliability and
maintainability of large-scale software systems, as they are often the sole
source of runtime information. Log parsing, which converts raw log messages
into structured data, is an important initial step towards downstream log
analytics. In recent studies, ChatGPT, the current cutting-edge large language
model (LLM), has been widely applied to a wide range of software engineering
tasks. However, its performance in automated log parsing remains unclear. In
this paper, we evaluate ChatGPT's ability to undertake log parsing by
addressing two research questions. (1) Can ChatGPT effectively parse logs? (2)
How does ChatGPT perform with different prompting methods? Our results show
that ChatGPT can achieve promising results for log parsing with appropriate
prompts, especially with few-shot prompting. Based on our findings, we outline
several challenges and opportunities for ChatGPT-based log parsing. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01590v1' target="_blank">http://arxiv.org/pdf/2306.01590v1</a><br> <br> <br> <font size='5'> 174 </font> <div style="text-align: right"> 2023-06-02 09:18:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From Large Language Models to Databases and Back: A discussion on research and education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This discussion was conducted at a recent panel at the 28th International
Conference on Database Systems for Advanced Applications (DASFAA 2023), held
April 17-20, 2023 in Tianjin, China. The title of the panel was "What does LLM
(ChatGPT) Bring to Data Science Research and Education? Pros and Cons". It was
moderated by Lei Chen and Xiaochun Yang. The discussion raised several
questions on how large language models (LLMs) and database research and
education can help each other and the potential risks of LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01388v2' target="_blank">http://arxiv.org/pdf/2306.01388v2</a><br> <br> <br> <font size='5'> 175 </font> <div style="text-align: right"> 2023-06-02 09:15:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent research on dialogue state tracking (DST) focuses on methods that
allow few- and zero-shot transfer to new domains or schemas. However,
performance gains heavily depend on aggressive data augmentation and
fine-tuning of ever larger language model based architectures. In contrast,
general purpose language models, trained on large amounts of diverse data, hold
the promise of solving any kind of task without task-specific training. We
present preliminary experimental results on the ChatGPT research preview,
showing that ChatGPT achieves state-of-the-art performance in zero-shot DST.
Despite our findings, we argue that properties inherent to general purpose
models limit their ability to replace specialized systems. We further theorize
that the in-context learning capabilities of such models will likely become
powerful tools to support the development of dedicated and dynamic dialogue
state trackers. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01386v1' target="_blank">http://arxiv.org/pdf/2306.01386v1</a><br> <br> <br> <font size='5'> 176 </font> <div style="text-align: right"> 2023-06-02 06:28:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is a Remarkable Tool -- For Experts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates the capabilities of ChatGPT as an automated assistant
in diverse domains, including scientific writing, mathematics, education,
programming, and healthcare. We explore the potential of ChatGPT to enhance
productivity, streamline problem-solving processes, and improve writing style.
Furthermore, we highlight the potential risks associated with excessive
reliance on ChatGPT in these fields. These limitations encompass factors like
incorrect and fictitious responses, inaccuracies in code, limited logical
reasoning abilities, overconfidence, and critical ethical concerns of
copyrights and privacy violation. We outline areas and objectives where ChatGPT
proves beneficial, applications where it should be used judiciously, and
scenarios where its reliability may be limited. In light of observed
limitations, and given that the tool's fundamental errors may pose a special
challenge for non-experts, ChatGPT should be used with a strategic methodology.
By drawing from comprehensive experimental studies, we offer methods and flow
charts for effectively using ChatGPT. Our recommendations emphasize iterative
interaction with ChatGPT and independent verification of its outputs.
Considering the importance of utilizing ChatGPT judiciously and with expertise,
we recommend its usage for experts who are well-versed in the respective
domains. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03102v1' target="_blank">http://arxiv.org/pdf/2306.03102v1</a><br> <br> <br> <font size='5'> 177 </font> <div style="text-align: right"> 2023-06-02 03:16:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automatic summarization of legal case judgements has traditionally been
attempted by using extractive summarization methods. However, in recent years,
abstractive summarization models are gaining popularity since they can generate
more natural and coherent summaries. Legal domain-specific pre-trained
abstractive summarization models are now available. Moreover, general-domain
pre-trained Large Language Models (LLMs), such as ChatGPT, are known to
generate high-quality text and have the capacity for text summarization. Hence
it is natural to ask if these models are ready for off-the-shelf application to
automatically generate abstractive summaries for case judgements. To explore
this question, we apply several state-of-the-art domain-specific abstractive
summarization models and general-domain LLMs on Indian court case judgements,
and check the quality of the generated summaries. In addition to standard
metrics for summary quality, we check for inconsistencies and hallucinations in
the summaries. We see that abstractive summarization models generally achieve
slightly higher scores than extractive models in terms of standard summary
evaluation metrics such as ROUGE and BLEU. However, we often find inconsistent
or hallucinated information in the generated abstractive summaries. Overall,
our investigation indicates that the pre-trained abstractive summarization
models and LLMs are not yet ready for fully automatic deployment for case
judgement summarization; rather a human-in-the-loop approach including manual
checks for inconsistencies is more suitable at present. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01248v2' target="_blank">http://arxiv.org/pdf/2306.01248v2</a><br> <br> <br> <font size='5'> 178 </font> <div style="text-align: right"> 2023-06-02 00:48:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI for Product Design: Getting the Right Design and the Design Right</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI (GenAI) models excel in their ability to recognize patterns in
existing data and generate new and unexpected content. Recent advances have
motivated applications of GenAI tools (e.g., Stable Diffusion, ChatGPT) to
professional practice across industries, including product design. While these
generative capabilities may seem enticing on the surface, certain barriers
limit their practical application for real-world use in industry settings. In
this position paper, we articulate and situate these barriers within two phases
of the product design process, namely "getting the right design" and "getting
the design right," and propose a research agenda to stimulate discussions
around opportunities for realizing the full potential of GenAI tools in product
design. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01217v1' target="_blank">http://arxiv.org/pdf/2306.01217v1</a><br> <br> <br> <font size='5'> 179 </font> <div style="text-align: right"> 2023-06-01 21:58:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Text summarization is a downstream natural language processing (NLP) task
that challenges the understanding and generation capabilities of language
models. Considerable progress has been made in automatically summarizing short
texts, such as news articles, often leading to satisfactory results. However,
summarizing long documents remains a major challenge. This is due to the
complex contextual information in the text and the lack of open-source
benchmarking datasets and evaluation frameworks that can be used to develop and
test model performance. In this work, we use ChatGPT, the latest breakthrough
in the field of large language models (LLMs), together with the extractive
summarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a
hybrid extraction and summarization pipeline for long documents such as
business articles and books. We work with the world-renowned company
getAbstract AG and leverage their expertise and experience in professional book
summarization. A practical study has shown that machine-generated summaries can
perform at least as well as human-written summaries when evaluated using
current automated evaluation metrics. However, a closer examination of the
texts generated by ChatGPT through human evaluations has shown that there are
still critical issues in terms of text coherence, faithfulness, and style.
Overall, our results show that the use of ChatGPT is a very promising but not
yet mature approach for summarizing long documents and can at best serve as an
inspiration for human editors. We anticipate that our work will inform NLP
researchers about the extent to which ChatGPT's capabilities for summarizing
long documents overlap with practitioners' needs. Further work is needed to
test the proposed hybrid summarization pipeline, in particular involving GPT-4,
and to propose a new evaluation framework tailored to the task of summarizing
long documents. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01169v1' target="_blank">http://arxiv.org/pdf/2306.01169v1</a><br> <br> <br> <font size='5'> 180 </font> <div style="text-align: right"> 2023-06-01 14:40:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Column Type Annotation using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Column type annotation is the task of annotating the columns of a relational
table with the semantic type of the values contained in each column. Column
type annotation is a crucial pre-processing step for data search and
integration in the context of data lakes. State-of-the-art column type
annotation methods either rely on matching table columns to properties of a
knowledge graph or fine-tune pre-trained language models such as BERT for the
column type annotation task. In this work, we take a different approach and
explore using ChatGPT for column type annotation. We evaluate different prompt
designs in zero- and few-shot settings and experiment with providing task
definitions and detailed instructions to the model. We further implement a
two-step table annotation pipeline which first determines the class of the
entities described in the table and depending on this class asks ChatGPT to
annotate columns using only the relevant subset of the overall vocabulary.
Using instructions as well as the two-step pipeline, ChatGPT reaches F1 scores
of over 85% in zero- and one-shot setups. To reach a similar F1 score a RoBERTa
model needs to be fine-tuned with 300 examples. This comparison shows that
ChatGPT is able deliver competitive results for the column type annotation task
given no or only a minimal amount of task-specific demonstrations. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00745v1' target="_blank">http://arxiv.org/pdf/2306.00745v1</a><br> <br> <br> <font size='5'> 181 </font> <div style="text-align: right"> 2023-06-01 14:24:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Physics Learning with ChatGPT, Bing Chat, and Bard as Agents-to-Think-With: A Comparative Case Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of AI has brought remarkable advancements in education, with AI
models demonstrating their ability to analyse and provide instructive solutions
to complex problems. This study compared and analysed the responses of four
Generative AI-powered chatbots (GenAIbots) - ChatGPT-3.5, ChatGPT-4, Bing Chat,
and Bard - within the constructivist theoretical framework. Using a single-case
study methodology, interaction logs between the GenAIbots and a simulated
student in Physics learning scenarios were analysed. The GenAIbots were
presented with conceptually dense Physics problems to promote deep
understanding. The qualitative analysis focused on tutor traits such as
subject-matter knowledge, empathy, assessment emphasis, facilitation skills,
and comprehension of the learning process. Findings showed that all GenAIbots
functioned as agents-to-think-with, fostering critical thinking,
problem-solving, and subject-matter knowledge. ChatGPT-4 stood out for
demonstrating empathy and a deep understanding of the learning process.
However, inconsistencies and shortcomings were observed, highlighting the need
for human intervention in AI-assisted learning. In conclusion, while GenAIbots
have limitations, their potential as agents-to-think-with in Physics education
offers promising prospects for revolutionising instruction. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00724v1' target="_blank">http://arxiv.org/pdf/2306.00724v1</a><br> <br> <br> <font size='5'> 182 </font> <div style="text-align: right"> 2023-06-01 13:39:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Predicting the Quality of Revisions in Argumentative Writing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ability to revise in response to feedback is critical to students'
writing success. In the case of argument writing in specific, identifying
whether an argument revision (AR) is successful or not is a complex problem
because AR quality is dependent on the overall content of an argument. For
example, adding the same evidence sentence could strengthen or weaken existing
claims in different argument contexts (ACs). To address this issue we developed
Chain-of-Thought prompts to facilitate ChatGPT-generated ACs for AR quality
predictions. The experiments on two corpora, our annotated elementary essays
and existing college essays benchmark, demonstrate the superiority of the
proposed ACs over baselines. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00667v1' target="_blank">http://arxiv.org/pdf/2306.00667v1</a><br> <br> <br> <font size='5'> 183 </font> <div style="text-align: right"> 2023-06-01 12:12:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analysis of ChatGPT on Source Code</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores the use of Large Language Models (LLMs) and in particular
ChatGPT in programming, source code analysis, and code generation. LLMs and
ChatGPT are built using machine learning and artificial intelligence
techniques, and they offer several benefits to developers and programmers.
While these models can save time and provide highly accurate results, they are
not yet advanced enough to replace human programmers entirely. The paper
investigates the potential applications of LLMs and ChatGPT in various areas,
such as code creation, code documentation, bug detection, refactoring, and
more. The paper also suggests that the usage of LLMs and ChatGPT is expected to
increase in the future as they offer unparalleled benefits to the programming
community. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00597v2' target="_blank">http://arxiv.org/pdf/2306.00597v2</a><br> <br> <br> <font size='5'> 184 </font> <div style="text-align: right"> 2023-06-01 11:14:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Digital textbooks have become an integral part of everyday learning tasks. In
this work, we consider the use of digital textbooks for programming classes.
Generally, students struggle with utilizing textbooks on programming to the
maximum, with a possible reason being that the example programs provided as
illustration of concepts in these textbooks don't offer sufficient
interactivity for students, and thereby not sufficiently motivating to explore
or understand these programming examples better. In our work, we explore the
idea of enhancing the navigability of intelligent textbooks with the use of
``counterfactual'' questions, to make students think critically about these
programs and enhance possible program comprehension. Inspired from previous
works on nudging students on counter factual thinking, we present the
possibility to enhance digital textbooks with questions generated using GPT. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00551v2' target="_blank">http://arxiv.org/pdf/2306.00551v2</a><br> <br> <br> <font size='5'> 185 </font> <div style="text-align: right"> 2023-06-01 02:20:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sustainable AI Regulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper suggests that AI regulation needs a shift from trustworthiness to
sustainability. With the carbon footprint of large generative AI models like
ChatGPT or GPT-4 adding urgency to this goal, the paper develops a roadmap to
make AI, and technology more broadly, environmentally sustainable. It explores
two key dimensions: legal instruments to make AI greener; and methods to render
AI regulation more sustainable. Concerning the former, transparency mechanisms,
such as the disclosure of the GHG footprint under Article 11 AI Act, could be a
first step. However, given the well-known limitations of disclosure, regulation
needs to go beyond transparency. Hence, I propose a mix of co-regulation
strategies; sustainability by design; restrictions on training data; and
consumption caps. This regulatory toolkit may then, in a second step, serve as
a blueprint for other information technologies and infrastructures facing
significant sustainability challenges due to their high GHG emissions, e.g.:
blockchain; metaverse applications; and data centers. The second dimension
consists in efforts to render AI regulation, and by implication the law itself,
more sustainable. Certain rights we have come to take for granted, such as the
right to erasure (Article 17 GDPR), may have to be limited due to
sustainability considerations. For example, the subjective right to erasure, in
some situations, has to be balanced against the collective interest in
mitigating climate change. The paper formulates guidelines to strike this
balance equitably, discusses specific use cases, and identifies doctrinal legal
methods for incorporating such a "sustainability limitation" into existing
(e.g., Art. 17(3) GDPR) and future law (e.g., AI Act). Ultimately, law,
computer science and sustainability studies need to team up to effectively
address the dual large-scale transformations of digitization and
sustainability. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00292v1' target="_blank">http://arxiv.org/pdf/2306.00292v1</a><br> <br> <br> <font size='5'> 186 </font> <div style="text-align: right"> 2023-05-31 22:46:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From Human-Centered to Social-Centered Artificial Intelligence: Assessing ChatGPT's Impact through Disruptive Events</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) and dialogue agents have existed for years, but
the release of recent GPT models has been a watershed moment for artificial
intelligence (AI) research and society at large. Immediately recognized for its
generative capabilities and versatility, ChatGPT's impressive proficiency
across technical and creative domains led to its widespread adoption. While
society grapples with the emerging cultural impacts of ChatGPT, critiques of
ChatGPT's impact within the machine learning community have coalesced around
its performance or other conventional Responsible AI evaluations relating to
bias, toxicity, and 'hallucination.' We argue that these latter critiques draw
heavily on a particular conceptualization of the 'human-centered' framework,
which tends to cast atomized individuals as the key recipients of both the
benefits and detriments of technology. In this article, we direct attention to
another dimension of LLMs and dialogue agents' impact: their effect on social
groups, institutions, and accompanying norms and practices. By illustrating
ChatGPT's social impact through three disruptive events, we challenge
individualistic approaches in AI development and contribute to ongoing debates
around the ethical and responsible implementation of AI systems. We hope this
effort will call attention to more comprehensive and longitudinal evaluation
tools and compel technologists to go beyond human-centered thinking and ground
their efforts through social-centered AI. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00227v1' target="_blank">http://arxiv.org/pdf/2306.00227v1</a><br> <br> <br> <font size='5'> 187 </font> <div style="text-align: right"> 2023-05-31 18:34:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MuseCoco: Generating Symbolic Music from Text</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generating music from text descriptions is a user-friendly mode since the
text is a relatively easy interface for user engagement. While some approaches
utilize texts to control music audio generation, editing musical elements in
generated audio is challenging for users. In contrast, symbolic music offers
ease of editing, making it more accessible for users to manipulate specific
musical elements. In this paper, we propose MuseCoco, which generates symbolic
music from text descriptions with musical attributes as the bridge to break
down the task into text-to-attribute understanding and attribute-to-music
generation stages. MuseCoCo stands for Music Composition Copilot that empowers
musicians to generate music directly from given text descriptions, offering a
significant improvement in efficiency compared to creating music entirely from
scratch. The system has two main advantages: Firstly, it is data efficient. In
the attribute-to-music generation stage, the attributes can be directly
extracted from music sequences, making the model training self-supervised. In
the text-to-attribute understanding stage, the text is synthesized and refined
by ChatGPT based on the defined attribute templates. Secondly, the system can
achieve precise control with specific attributes in text descriptions and
offers multiple control options through attribute-conditioned or
text-conditioned approaches. MuseCoco outperforms baseline systems in terms of
musicality, controllability, and overall score by at least 1.27, 1.08, and 1.32
respectively. Besides, there is a notable enhancement of about 20% in objective
control accuracy. In addition, we have developed a robust large-scale model
with 1.2 billion parameters, showcasing exceptional controllability and
musicality. </font><br> Link: <a href='http://arxiv.org/pdf/2306.00110v1' target="_blank">http://arxiv.org/pdf/2306.00110v1</a><br> <br> <br> <font size='5'> 188 </font> <div style="text-align: right"> 2023-05-31 16:44:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI for Low-Code for AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Low-code programming allows citizen developers to create programs with
minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In
parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs
from natural language instructions. We argue that these modalities are
complementary: tools like ChatGPT greatly reduce the need to memorize large
APIs but still require their users to read (and modify) programs, whereas
visual tools abstract away most or all programming but struggle to provide easy
access to large APIs. At their intersection, we propose LowCoder, the first
low-code tool for developing AI pipelines that supports both a visual
programming interface (LowCoder_VP) and an AI-powered natural language
interface (LowCoder_NL). We leverage this tool to provide some of the first
insights into whether and how these two modalities help programmers by
conducting a user study. We task 20 developers with varying levels of AI
expertise with implementing four ML pipelines using LowCoder, replacing the
LowCoder_NL component with a simple keyword search in half the tasks. Overall,
we find that LowCoder is especially useful for (i) Discoverability: using
LowCoder_NL, participants discovered new operators in 75% of the tasks,
compared to just 32.5% and 27.5% using web search or scrolling through options
respectively in the keyword-search condition, and (ii) Iterative Composition:
82.5% of tasks were successfully completed and many initial pipelines were
further successfully improved. Qualitative analysis shows that AI helps users
discover how to implement constructs when they know what to do, but still fails
to support novices when they lack clarity on what they want to accomplish.
Overall, our work highlights the benefits of combining the power of AI with
low-code programming. </font><br> Link: <a href='http://arxiv.org/pdf/2305.20015v1' target="_blank">http://arxiv.org/pdf/2305.20015v1</a><br> <br> <br> <font size='5'> 189 </font> <div style="text-align: right"> 2023-05-31 15:03:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have made remarkable advancements in the field
of artificial intelligence, significantly reshaping the human-computer
interaction. We not only focus on the performance of LLMs, but also explore
their features from a psychological perspective, acknowledging the importance
of understanding their behavioral characteristics. Our study examines the
behavioral patterns displayed by LLMs by employing trait theory, a
psychological framework. We first focus on evaluating the consistency of
personality types exhibited by ChatGPT. Furthermore, experiments include
cross-lingual effects on seven additional languages, and the investigation of
six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit
personality changes in response to instructions or contextual cues. The
findings show that ChatGPT consistently maintains its ENFJ personality
regardless of instructions or contexts. By shedding light on the
personalization of LLMs, we anticipate that our study will serve as a catalyst
for further research in this field. </font><br> Link: <a href='http://arxiv.org/pdf/2305.19926v2' target="_blank">http://arxiv.org/pdf/2305.19926v2</a><br> <br> <br> <font size='5'> 190 </font> <div style="text-align: right"> 2023-05-31 04:23:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Catalysis distillation neural network for the few shot open catalyst challenge</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The integration of artificial intelligence and science has resulted in
substantial progress in computational chemistry methods for the design and
discovery of novel catalysts. Nonetheless, the challenges of electrocatalytic
reactions and developing a large-scale language model in catalysis persist, and
the recent success of ChatGPT's (Chat Generative Pre-trained Transformer)
few-shot methods surpassing BERT (Bidirectional Encoder Representation from
Transformers) underscores the importance of addressing limited data, expensive
computations, time constraints and structure-activity relationship in research.
Hence, the development of few-shot techniques for catalysis is critical and
essential, regardless of present and future requirements. This paper introduces
the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the
application of machine learning technology for predicting catalytic reactions
on catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen
peroxide electrocatalysis. To address the challenge of limited data in
catalysis, we propose a machine learning approach based on MLP-Like and a
framework called Catalysis Distillation Graph Neural Network (CDGNN). Our
results demonstrate that CDGNN effectively learns embeddings from catalytic
structures, enabling the capture of structure-adsorption relationships. This
accomplishment has resulted in the utmost advanced and efficient determination
of the reaction pathway for hydrogen peroxide, surpassing the current graph
neural network approach by 16.1%.. Consequently, CDGNN presents a promising
approach for few-shot learning in catalysis. </font><br> Link: <a href='http://arxiv.org/pdf/2305.19545v1' target="_blank">http://arxiv.org/pdf/2305.19545v1</a><br> <br> <br> <font size='5'> 191 </font> <div style="text-align: right"> 2023-05-30 21:52:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial Uses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large generative AI models (GMs) like GPT and DALL-E are trained to generate
content for general, wide-ranging purposes. GM content filters are generalized
to filter out content which has a risk of harm in many cases, e.g., hate
speech. However, prohibited content is not always harmful -- there are
instances where generating prohibited content can be beneficial. So, when GMs
filter out content, they preclude beneficial use cases along with harmful ones.
Which use cases are precluded reflects the values embedded in GM content
filtering. Recent work on red teaming proposes methods to bypass GM content
filters to generate harmful content. We coin the term green teaming to describe
methods of bypassing GM content filters to design for beneficial use cases. We
showcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a
person experiencing suicidal ideation, for suicide support training; 2) Using
Codex to intentionally generate buggy solutions to train students on debugging;
and 3) Examining an Instagram page using Midjourney to generate images of
anti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases
demonstrate green teaming as both a practical design method and a mode of
critique, which problematizes and subverts current understandings of harms and
values in generative AI. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03097v1' target="_blank">http://arxiv.org/pdf/2306.03097v1</a><br> <br> <br> <font size='5'> 192 </font> <div style="text-align: right"> 2023-05-30 15:25:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Modern large language models (LLMs) like ChatGPT have shown remarkable
performance on general language tasks but still struggle on complex reasoning
tasks, which drives the research on cognitive behaviors of LLMs to explore
human-like problem-solving strategies. Along this direction, one representative
strategy is self-reflection, which asks an LLM to refine the solution with the
feedback generated by itself iteratively. However, our study shows that such
reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:
once the LLM has established confidence in its solutions, it is unable to
generate novel thoughts later through reflection even if its initial stance is
incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)
framework, in which multiple agents express their arguments in the state of
"tit for tat" and a judge manages the debate process to obtain a final
solution. Clearly, our MAD framework encourages divergent thinking in LLMs
which would be helpful for tasks that require deep levels of contemplation.
Experiment results on two challenging datasets, commonsense machine translation
and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of
our MAD framework. Extensive analyses suggest that the adaptive break of debate
and the modest level of "tit for tat" state are required for MAD to obtain good
performance. Moreover, we find that LLMs might not be a fair judge if different
LLMs are used for agents. Codes:
https://github.com/Skytliang/Multi-Agents-Debate </font><br> Link: <a href='http://arxiv.org/pdf/2305.19118v1' target="_blank">http://arxiv.org/pdf/2305.19118v1</a><br> <br> <br> <font size='5'> 193 </font> <div style="text-align: right"> 2023-05-30 15:06:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does Conceptual Representation Require Embodiment? Insights From Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in large language models (LLM) have the potential to shed
light on the debate regarding the extent to which knowledge representation
requires the grounding of embodied experience. Despite learning from limited
modalities (e.g., text for GPT-3.5, and text+image for GPT-4), LLMs have
nevertheless demonstrated human-like behaviors in various psychology tasks,
which may provide an alternative interpretation of the acquisition of
conceptual knowledge. We compared lexical conceptual representations between
humans and ChatGPT (GPT-3.5 and GPT-4) on subjective ratings of various lexical
conceptual features or dimensions (e.g., emotional arousal, concreteness,
haptic, etc.). The results show that both GPT-3.5 and GPT-4 were strongly
correlated with humans in some abstract dimensions, such as emotion and
salience. In dimensions related to sensory and motor domains, GPT-3.5 shows
weaker correlations while GPT-4 has made significant progress compared to
GPT-3.5. Still, GPT-4 struggles to fully capture motor aspects of conceptual
knowledge such as actions with foot/leg, mouth/throat, and torso. Moreover, we
found that GPT-4's progress can largely be associated with its training in the
visual domain. Certain aspects of conceptual representation appear to exhibit a
degree of independence from sensory capacities, but others seem to necessitate
them. Our findings provide insights into the complexities of knowledge
representation from diverse perspectives and highlights the potential influence
of embodied experience in shaping language and cognition. </font><br> Link: <a href='http://arxiv.org/pdf/2305.19103v1' target="_blank">http://arxiv.org/pdf/2305.19103v1</a><br> <br> <br> <font size='5'> 194 </font> <div style="text-align: right"> 2023-05-30 11:18:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A comparison between three chatbots which are based on large language models,
namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their
ability to give correct answers to mathematics and logic problems. In
particular, we check their ability to Understand the problem at hand; Apply
appropriate algorithms or methods for its solution; and Generate a coherent
response and a correct answer. We use 30 questions that are clear, without any
ambiguities, fully described with plain text only, and have a unique, well
defined correct answer. The questions are divided into two sets of 15 each. The
questions of Set A are 15 "Original" problems that cannot be found online,
while Set B contains 15 "Published" problems that one can find online, usually
with their solution. Each question is posed three times to each chatbot. The
answers are recorded and discussed, highlighting their strengths and
weaknesses. It has been found that for straightforward arithmetic, algebraic
expressions, or basic logic puzzles, chatbots may provide accurate solutions,
although not in every attempt. However, for more complex mathematical problems
or advanced logic tasks, their answers, although written in a usually
"convincing" way, may not be reliable. Consistency is also an issue, as many
times a chatbot will provide conflicting answers when given the same question
more than once. A comparative quantitative evaluation of the three chatbots is
made through scoring their final answers based on correctness. It was found
that ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes
third in the original questions of Set A, behind the other two chatbots, while
it has the best performance (first place) in the published questions of Set B.
This is probably because Bard has direct access to the internet, in contrast to
ChatGPT chatbots which do not have any communication with the outside world. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18618v1' target="_blank">http://arxiv.org/pdf/2305.18618v1</a><br> <br> <br> <font size='5'> 195 </font> <div style="text-align: right"> 2023-05-30 09:54:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose a novel framework for learning high-level cognitive capabilities
in robot manipulation tasks, such as making a smiley face using building
blocks. These tasks often involve complex multi-step reasoning, presenting
significant challenges due to the limited paired data connecting human
instructions (e.g., making a smiley face) and robot actions (e.g., end-effector
movement). Existing approaches relieve this challenge by adopting an open-loop
paradigm decomposing high-level instructions into simple sub-task plans, and
executing them step-by-step using low-level control models. However, these
approaches are short of instant observations in multi-step reasoning, leading
to sub-optimal results. To address this issue, we propose to automatically
collect a cognitive robot dataset by Large Language Models (LLMs). The
resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of
multi-step text plans and paired observation sequences. To enable efficient
data acquisition, we employ elaborated multi-round prompt designs that
effectively reduce the burden of extensive human involvement. We further
propose a closed-loop multi-modal embodied planning model that autoregressively
generates plans by taking image observations as input. To facilitate effective
learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and
finetune additional vision adapter and Q-former to enable fine-grained spatial
perception for manipulation tasks. We conduct experiments to verify the
superiority over existing open and closed-loop methods, and achieve a
significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4
based robot tasks. Real-world demos are shown in
https://www.youtube.com/watch?v=ayAzID1_qQk . </font><br> Link: <a href='http://arxiv.org/pdf/2305.18898v1' target="_blank">http://arxiv.org/pdf/2305.18898v1</a><br> <br> <br> <font size='5'> 196 </font> <div style="text-align: right"> 2023-05-30 05:27:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper aims to efficiently enable Large Language Models (LLMs) to use
multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have
shown great potential for tool usage through sophisticated prompt engineering.
Nevertheless, these models typically rely on prohibitive computational costs
and publicly inaccessible data. To address these challenges, we propose the
GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and
OPT, to use tools. It generates an instruction-following dataset by prompting
an advanced teacher with various multi-modal contexts. By using the Low-Rank
Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs
to solve a range of visual problems, including visual comprehension and image
generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to
use tools, which is performed in both zero-shot and fine-tuning ways. Extensive
experiments demonstrate the effectiveness of our method on various language
models, which not only significantly improves the accuracy of invoking seen
tools, but also enables the zero-shot capacity for unseen tools. The code and
demo are available at https://github.com/StevenGrove/GPT4Tools. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18752v1' target="_blank">http://arxiv.org/pdf/2305.18752v1</a><br> <br> <br> <font size='5'> 197 </font> <div style="text-align: right"> 2023-05-29 15:25:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multiscale Positive-Unlabeled Detection of AI-Generated Texts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are
astonishing at generating human-like texts, but they may get misused for fake
scholarly texts, fake news, fake tweets, et cetera. Previous works have
proposed methods to detect these multiscale AI-generated texts, including
simple ML classifiers, pretrained-model-based training-agnostic methods, and
finetuned language classification models. However, mainstream detectors are
formulated without considering the factor of corpus length: shorter corpuses
are harder to detect compared with longer ones for shortage of informative
features. In this paper, a Multiscale Positive-Unlabeled (MPU) training
framework is proposed to address the challenge of multiscale text detection.
Firstly, we acknowledge the human-resemblance property of short machine texts,
and rephrase text classification as a Positive-Unlabeled (PU) problem by
marking these short machine texts as "unlabeled" during training. In this PU
context, we propose the length-sensitive Multiscale PU Loss, where we use a
recurrent model in abstraction to estimate positive priors of scale-variant
corpuses. Additionally, we introduce a Text Multiscaling module to enrich
training corpuses. Experiments show that our MPU method augments detection
performance on long AI-generated text, and significantly improves short-corpus
detection of language model detectors. Language Models trained with MPU could
outcompete existing detectors by large margins on multiscale AI-generated
texts. The codes are available at
https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt
and https://github.com/YuchuanTian/AIGC_text_detector. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18149v2' target="_blank">http://arxiv.org/pdf/2305.18149v2</a><br> <br> <br> <font size='5'> 198 </font> <div style="text-align: right"> 2023-05-29 14:43:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in conversational large language models (LLMs), such as
ChatGPT, have demonstrated remarkable promise in various domains, including
drug discovery. However, existing works mainly focus on investigating the
capabilities of conversational LLMs on chemical reaction and retrosynthesis.
While drug editing, a critical task in the drug discovery pipeline, remains
largely unexplored. To bridge this gap, we propose ChatDrug, a framework to
facilitate the systematic investigation of drug editing using LLMs. ChatDrug
jointly leverages a prompt module, a retrieval and domain feedback (ReDF)
module, and a conversation module to streamline effective drug editing. We
empirically show that ChatDrug reaches the best performance on 33 out of 39
drug editing tasks, encompassing small molecules, peptides, and proteins. We
further demonstrate, through 10 case studies, that ChatDrug can successfully
identify the key substructures (e.g., the molecule functional groups, peptide
motifs, and protein structures) for manipulation, generating diverse and valid
suggestions for drug editing. Promisingly, we also show that ChatDrug can offer
insightful explanations from a domain-specific perspective, enhancing
interpretability and enabling informed decision-making. This research sheds
light on the potential of ChatGPT and conversational LLMs for drug editing. It
paves the way for a more efficient and collaborative drug discovery pipeline,
contributing to the advancement of pharmaceutical research and development. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18090v1' target="_blank">http://arxiv.org/pdf/2305.18090v1</a><br> <br> <br> <font size='5'> 199 </font> <div style="text-align: right"> 2023-05-29 14:07:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) demonstrate promising translation performance
among various natural languages. However, many LLMs especially the open-sourced
ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of
natural languages, making the potential of LLMs on language translation less
explored. In this work, we present BigTranslate which adapts LLaMA that covers
only 20 languages and enhances it with multilingual translation capability on
more than 100 languages. BigTranslate is built upon LLaMA-13B and it is
optimized in three steps. First, we continue training LLaMA with massive
Chinese monolingual data. Second, we continue training the model with a
large-scale parallel dataset that covers 102 natural languages. Third, we
instruct-tune the foundation model with multilingual translation instructions,
leading to our BigTranslate model. The preliminary experiments on multilingual
translation show that BigTranslate performs comparably with ChatGPT and Google
Translate in many languages and even outperforms ChatGPT in 8 language pairs.
We release the BigTranslate model and hope it can advance the research
progress. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18098v2' target="_blank">http://arxiv.org/pdf/2305.18098v2</a><br> <br> <br> <font size='5'> 200 </font> <div style="text-align: right"> 2023-05-29 12:37:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18486v4' target="_blank">http://arxiv.org/pdf/2305.18486v4</a><br> <br> <br> <font size='5'> 201 </font> <div style="text-align: right"> 2023-05-29 12:26:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chatbots shifted from rule-based to artificial intelligence techniques and
gained traction in medicine, shopping, customer services, food delivery,
education, and research. OpenAI developed ChatGPT blizzard on the Internet as
it crossed one million users within five days of its launch. However, with the
enhanced popularity, chatbots experienced cybersecurity threats and
vulnerabilities. This paper discussed the relevant literature, reports, and
explanatory incident attacks generated against chatbots. Our initial point is
to explore the timeline of chatbots from ELIZA (an early natural language
processing computer program) to GPT-4 and provide the working mechanism of
ChatGPT. Subsequently, we explored the cybersecurity attacks and
vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically
in the context of creating the malware code, phishing emails, undetectable
zero-day attacks, and generation of macros and LOLBINs. Furthermore, the
history of cyberattacks and vulnerabilities exploited by cybercriminals are
discussed, particularly considering the risk and vulnerabilities in ChatGPT.
Addressing these threats and vulnerabilities requires specific strategies and
measures to reduce the harmful consequences. Therefore, the future directions
to address the challenges were presented. </font><br> Link: <a href='http://arxiv.org/pdf/2306.09255v1' target="_blank">http://arxiv.org/pdf/2306.09255v1</a><br> <br> <br> <font size='5'> 202 </font> <div style="text-align: right"> 2023-05-29 12:24:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent works have explored text-guided image editing using diffusion models
and generated edited images based on text prompts. However, the models struggle
to accurately locate the regions to be edited and faithfully perform precise
edits. In this work, we propose a framework termed InstructEdit that can do
fine-grained editing based on user instructions. Our proposed framework has
three components: language processor, segmenter, and image editor. The first
component, the language processor, processes the user instruction using a large
language model. The goal of this processing is to parse the user instruction
and output prompts for the segmenter and captions for the image editor. We
adopt ChatGPT and optionally BLIP2 for this step. The second component, the
segmenter, uses the segmentation prompt provided by the language processor. We
employ a state-of-the-art segmentation framework Grounded Segment Anything to
automatically generate a high-quality mask based on the segmentation prompt.
The third component, the image editor, uses the captions from the language
processor and the masks from the segmenter to compute the edited image. We
adopt Stable Diffusion and the mask-guided generation from DiffEdit for this
purpose. Experiments show that our method outperforms previous editing methods
in fine-grained editing applications where the input image contains a complex
object or multiple objects. We improve the mask quality over DiffEdit and thus
improve the quality of edited images. We also show that our framework can
accept multiple forms of user instructions as input. We provide the code at
https://github.com/QianWangX/InstructEdit. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18047v1' target="_blank">http://arxiv.org/pdf/2305.18047v1</a><br> <br> <br> <font size='5'> 203 </font> <div style="text-align: right"> 2023-05-29 07:41:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models are not Fair Evaluators</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We uncover a systematic bias in the evaluation paradigm of adopting large
language models~(LLMs), e.g., GPT-4, as a referee to score the quality of
responses generated by candidate models. We find that the quality ranking of
candidate responses can be easily hacked by simply altering their order of
appearance in the context. This manipulation allows us to skew the evaluation
result, making one model appear considerably superior to the other, e.g.,
vicuna could beat ChatGPT on 66 over 80 tested queries. To address this issue,
we propose two simple yet effective calibration strategies: 1) Multiple
Evidence Calibration, which requires the evaluator model to generate multiple
detailed pieces of evidence before assigning ratings; 2) Balanced Position
Calibration, which aggregates results across various orders to determine the
final score. Extensive experiments demonstrate that our approach successfully
mitigates evaluation bias, resulting in closer alignment with human judgments.
To facilitate future research on more robust large language model comparison,
we integrate the techniques in the paper into an easy-to-use toolkit
\emph{FairEval}, along with the human
annotations.\footnote{\url{https://github.com/i-Eval/FairEval}} </font><br> Link: <a href='http://arxiv.org/pdf/2305.17926v1' target="_blank">http://arxiv.org/pdf/2305.17926v1</a><br> <br> <br> <font size='5'> 204 </font> <div style="text-align: right"> 2023-05-28 22:46:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Inferring over and extracting information from Large Language Models (LLMs)
trained on a large corpus of scientific literature can potentially drive a new
era in biomedical research, reducing the barriers for accessing existing
medical evidence. This work examines the potential of LLMs for dialoguing with
biomedical background knowledge, using the context of antibiotic discovery as
an exemplar motivational scenario. The context of biomedical discovery from
natural products entails understanding the relational evidence between an
organism, an associated chemical and its associated antibiotic properties. We
provide a systematic assessment on the ability of LLMs to encode and express
these relations, verifying for fluency, prompt-alignment, semantic coherence,
factual knowledge and specificity of generated responses. The systematic
analysis is applied to nine state-of-the-art models (including ChatGPT and
GPT-4) in two prompting-based tasks: chemical compound definition generation
and chemical compound-fungus relation determination. Results show that while
recent models have improved in fluency, factual accuracy is still low and
models are biased towards over-represented entities. The ability of LLMs to
serve as biomedical knowledge bases is questioned, and the need for additional
systematic evaluation frameworks is highlighted. The best performing GPT-4
produced a factual definition for 70% of chemical compounds and 43.6% factual
relations to fungi, whereas the best open source model BioGPT-large 30% of the
compounds and 30% of the relations for the best-performing prompt. The results
show that while LLMs are currently not fit for purpose to be used as biomedical
factual knowledge bases, there is a promising emerging property in the
direction of factuality as the models become domain specialised, scale-up in
size and level of human feedback. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17819v1' target="_blank">http://arxiv.org/pdf/2305.17819v1</a><br> <br> <br> <font size='5'> 205 </font> <div style="text-align: right"> 2023-05-28 21:11:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Informed Graph Neural Network for Stock Movement Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has demonstrated remarkable capabilities across various natural
language processing (NLP) tasks. However, its potential for inferring dynamic
network structures from temporal textual data, specifically financial news,
remains an unexplored frontier. In this research, we introduce a novel
framework that leverages ChatGPT's graph inference capabilities to enhance
Graph Neural Networks (GNN). Our framework adeptly extracts evolving network
structures from textual data, and incorporates these networks into graph neural
networks for subsequent predictive tasks. The experimental results from stock
movement forecasting indicate our model has consistently outperformed the
state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios
constructed based on our model's outputs demonstrate higher annualized
cumulative returns, alongside reduced volatility and maximum drawdown. This
superior performance highlights the potential of ChatGPT for text-based network
inferences and underscores its promising implications for the financial sector. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03763v3' target="_blank">http://arxiv.org/pdf/2306.03763v3</a><br> <br> <br> <font size='5'> 206 </font> <div style="text-align: right"> 2023-05-28 17:59:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ConvGenVisMo: Evaluation of Conversational Generative Vision Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversational generative vision models (CGVMs) like Visual ChatGPT (Wu et
al., 2023) have recently emerged from the synthesis of computer vision and
natural language processing techniques. These models enable more natural and
interactive communication between humans and machines, because they can
understand verbal inputs from users and generate responses in natural language
along with visual outputs. To make informed decisions about the usage and
deployment of these models, it is important to analyze their performance
through a suitable evaluation framework on realistic datasets. In this paper,
we present ConvGenVisMo, a framework for the novel task of evaluating CGVMs.
ConvGenVisMo introduces a new benchmark evaluation dataset for this task, and
also provides a suite of existing and new automated evaluation metrics to
evaluate the outputs. All ConvGenVisMo assets, including the dataset and the
evaluation code, will be made available publicly on GitHub. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17784v1' target="_blank">http://arxiv.org/pdf/2305.17784v1</a><br> <br> <br> <font size='5'> 207 </font> <div style="text-align: right"> 2023-05-28 10:04:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian News Texts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper describes the RuSentNE-2023 evaluation devoted to targeted
sentiment analysis in Russian news texts. The task is to predict sentiment
towards a named entity in a single sentence. The dataset for RuSentNE-2023
evaluation is based on the Russian news corpus RuSentNE having rich
sentiment-related annotation. The corpus is annotated with named entities and
sentiments towards these entities, along with related effects and emotional
states. The evaluation was organized using the CodaLab competition framework.
The main evaluation measure was macro-averaged measure of positive and negative
classes. The best results achieved were of 66% Macro F-measure
(Positive+Negative classes). We also tested ChatGPT on the test set from our
evaluation and found that the zero-shot answers provided by ChatGPT reached 60%
of the F-measure, which corresponds to 4th place in the evaluation. ChatGPT
also provided detailed explanations of its conclusion. This can be considered
as quite high for zero-shot application. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17679v1' target="_blank">http://arxiv.org/pdf/2305.17679v1</a><br> <br> <br> <font size='5'> 208 </font> <div style="text-align: right"> 2023-05-28 02:12:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reward Collapse in Aligning Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The extraordinary capabilities of large language models (LLMs) such as
ChatGPT and GPT-4 are in part unleashed by aligning them with reward models
that are trained on human preferences, which are often represented as rankings
of responses to prompts. In this paper, we document the phenomenon of
\textit{reward collapse}, an empirical observation where the prevailing
ranking-based approach results in an \textit{identical} reward distribution
\textit{regardless} of the prompts during the terminal phase of training. This
outcome is undesirable as open-ended prompts like ``write a short story about
your best friend'' should yield a continuous range of rewards for their
completions, while specific prompts like ``what is the capital of New Zealand''
should generate either high or low rewards. Our theoretical investigation
reveals that reward collapse is primarily due to the insufficiency of the
ranking-based objective function to incorporate prompt-related information
during optimization. This insight allows us to derive closed-form expressions
for the reward distribution associated with a set of utility functions in an
asymptotic regime. To overcome reward collapse, we introduce a prompt-aware
optimization scheme that provably admits a prompt-dependent reward distribution
within the interpolating regime. Our experimental results suggest that our
proposed prompt-aware utility functions significantly alleviate reward collapse
during the training of reward models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17608v1' target="_blank">http://arxiv.org/pdf/2305.17608v1</a><br> <br> <br> <font size='5'> 209 </font> <div style="text-align: right"> 2023-05-27 15:10:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Curse of Recursion: Training on Generated Data Makes Models Forget</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stable Diffusion revolutionised image creation from descriptive text. GPT-2,
GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of
language tasks. ChatGPT introduced such language models to the general public.
It is now clear that large language models (LLMs) are here to stay, and will
bring about drastic change in the whole ecosystem of online text and images. In
this paper we consider what the future might hold. What will happen to GPT-{n}
once LLMs contribute much of the language found online? We find that use of
model-generated content in training causes irreversible defects in the
resulting models, where tails of the original content distribution disappear.
We refer to this effect as Model Collapse and show that it can occur in
Variational Autoencoders, Gaussian Mixture Models and LLMs. We build
theoretical intuition behind the phenomenon and portray its ubiquity amongst
all learned generative models. We demonstrate that it has to be taken seriously
if we are to sustain the benefits of training from large-scale data scraped
from the web. Indeed, the value of data collected about genuine human
interactions with systems will be increasingly valuable in the presence of
content generated by LLMs in data crawled from the Internet. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17493v2' target="_blank">http://arxiv.org/pdf/2305.17493v2</a><br> <br> <br> <font size='5'> 210 </font> <div style="text-align: right"> 2023-05-26 21:57:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improved Instruction Ordering in Recipe-Grounded Conversation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we study the task of instructional dialogue and focus on the
cooking domain. Analyzing the generated output of the GPT-J model, we reveal
that the primary challenge for a recipe-grounded dialog system is how to
provide the instructions in the correct order. We hypothesize that this is due
to the model's lack of understanding of user intent and inability to track the
instruction state (i.e., which step was last instructed). Therefore, we propose
to explore two auxiliary subtasks, namely User Intent Detection and Instruction
State Tracking, to support Response Generation with improved instruction
grounding. Experimenting with our newly collected dataset, ChattyChef, shows
that incorporating user intent and instruction state information helps the
response generation model mitigate the incorrect order issue. Furthermore, to
investigate whether ChatGPT has completely solved this task, we analyze its
outputs and find that it also makes mistakes (10.7% of the responses), about
half of which are out-of-order instructions. We will release ChattyChef to
facilitate further research in this area at:
https://github.com/octaviaguo/ChattyChef. </font><br> Link: <a href='http://arxiv.org/pdf/2305.17280v1' target="_blank">http://arxiv.org/pdf/2305.17280v1</a><br> <br> <br> <font size='5'> 211 </font> <div style="text-align: right"> 2023-05-26 14:41:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Trained with an unprecedented scale of data, large language models (LLMs)
like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities
from model scaling. Such a trend underscored the potential of training LLMs
with unlimited language data, advancing the development of a universal embodied
agent. In this work, we introduce the NavGPT, a purely LLM-based
instruction-following navigation agent, to reveal the reasoning capability of
GPT models in complex embodied scenes by performing zero-shot sequential action
prediction for vision-and-language navigation (VLN). At each step, NavGPT takes
the textual descriptions of visual observations, navigation history, and future
explorable directions as inputs to reason the agent's current status, and makes
the decision to approach the target. Through comprehensive experiments, we
demonstrate NavGPT can explicitly perform high-level planning for navigation,
including decomposing instruction into sub-goal, integrating commonsense
knowledge relevant to navigation task resolution, identifying landmarks from
observed scenes, tracking navigation progress, and adapting to exceptions with
plan adjustment. Furthermore, we show that LLMs is capable of generating
high-quality navigational instructions from observations and actions along a
path, as well as drawing accurate top-down metric trajectory given the agent's
navigation history. Despite the performance of using NavGPT to zero-shot R2R
tasks still falling short of trained models, we suggest adapting multi-modality
inputs for LLMs to use as visual navigation agents and applying the explicit
reasoning of LLMs to benefit learning-based models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16986v2' target="_blank">http://arxiv.org/pdf/2305.16986v2</a><br> <br> <br> <font size='5'> 212 </font> <div style="text-align: right"> 2023-05-26 13:49:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On Evaluating Adversarial Robustness of Large Vision-Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented
performance in response generation, especially with visual inputs, enabling
more creative and adaptable interaction than large language models such as
ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable modality (e.g., vision). To this end, we propose evaluating the
robustness of open-source large VLMs in the most realistic and high-risk
setting, where adversaries have only black-box system access and seek to
deceive the model into returning the targeted responses. In particular, we
first craft targeted adversarial examples against pretrained models such as
CLIP and BLIP, and then transfer these adversarial examples to other VLMs such
as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we
observe that black-box queries on these VLMs can further improve the
effectiveness of targeted evasion, resulting in a surprisingly high success
rate for generating targeted responses. Our findings provide a quantitative
understanding regarding the adversarial vulnerability of large VLMs and call
for a more thorough examination of their potential security flaws before
deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16934v1' target="_blank">http://arxiv.org/pdf/2305.16934v1</a><br> <br> <br> <font size='5'> 213 </font> <div style="text-align: right"> 2023-05-26 11:29:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT (Chat Generative Pre-trained Transformer) is a chatbot launched by
OpenAI on November 30, 2022. OpenAI's GPT-3 family of large language models
serve as the foundation for ChatGPT. ChatGPT is fine-tuned with both supervised
and reinforcement learning techniques and has received widespread attention for
its articulate responses across diverse domains of knowledge. In this study, we
explore how ChatGPT can be used to help with common software engineering tasks.
Many of the ubiquitous tasks covering the breadth of software engineering such
as ambiguity resolution in software requirements, method name suggestion, test
case prioritization, code review, log summarization can potentially be
performed using ChatGPT. In this study, we explore fifteen common software
engineering tasks using ChatGPT. We juxtapose and analyze ChatGPT's answers
with the respective state of the art outputs (where available) and/or human
expert ground truth. Our experiments suggest that for many tasks, ChatGPT does
perform credibly and the response from it is detailed and often better than the
human expert output or the state of the art output. However, for a few other
tasks, ChatGPT in its present form provides incorrect answers and hence is not
suited for such tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16837v1' target="_blank">http://arxiv.org/pdf/2305.16837v1</a><br> <br> <br> <font size='5'> 214 </font> <div style="text-align: right"> 2023-05-26 11:07:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As the use of Large Language Models (LLMs) in text generation tasks
proliferates, concerns arise over their potential to compromise academic
integrity. The education sector currently tussles with distinguishing
student-authored homework assignments from AI-generated ones. This paper
addresses the challenge by introducing HowkGPT, designed to identify homework
assignments generated by AI. HowkGPT is built upon a dataset of academic
assignments and accompanying metadata [17] and employs a pretrained LLM to
compute perplexity scores for student-authored and ChatGPT-generated responses.
These scores then assist in establishing a threshold for discerning the origin
of a submitted assignment. Given the specificity and contextual nature of
academic work, HowkGPT further refines its analysis by defining
category-specific thresholds derived from the metadata, enhancing the precision
of the detection. This study emphasizes the critical need for effective
strategies to uphold academic integrity amidst the growing influence of LLMs
and provides an approach to ensuring fair and accurate grading in educational
institutions. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18226v2' target="_blank">http://arxiv.org/pdf/2305.18226v2</a><br> <br> <br> <font size='5'> 215 </font> <div style="text-align: right"> 2023-05-26 09:27:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a conversational artificial intelligence that is a member of the
generative pre-trained transformer of the large language model family. This
text generative model was fine-tuned by both supervised learning and
reinforcement learning so that it can produce text documents that seem to be
written by natural intelligence. Although there are numerous advantages of this
generative model, it comes with some reasonable concerns as well. This paper
presents a machine learning-based solution that can identify the ChatGPT
delivered text from the human written text along with the comparative analysis
of a total of 11 machine learning and deep learning algorithms in the
classification process. We have tested the proposed model on a Kaggle dataset
consisting of 10,000 texts out of which 5,204 texts were written by humans and
collected from news and social media. On the corpus generated by GPT-3.5, the
proposed algorithm presents an accuracy of 77%. </font><br> Link: <a href='http://arxiv.org/pdf/2306.01761v1' target="_blank">http://arxiv.org/pdf/2306.01761v1</a><br> <br> <br> <font size='5'> 216 </font> <div style="text-align: right"> 2023-05-26 08:41:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AlignScore: Evaluating Factual Consistency with a Unified Alignment Function</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many text generation applications require the generated text to be factually
consistent with input information. Automatic evaluation of factual consistency
is challenging. Previous work has developed various metrics that often depend
on specific functions, such as natural language inference (NLI) or question
answering (QA), trained on limited data. Those metrics thus can hardly assess
diverse factual inconsistencies (e.g., contradictions, hallucinations) that
occur in varying inputs/outputs (e.g., sentences, documents) from different
tasks. In this paper, we propose AlignScore, a new holistic metric that applies
to a variety of factual inconsistency scenarios as above. AlignScore is based
on a general function of information alignment between two arbitrary text
pieces. Crucially, we develop a unified training framework of the alignment
function by integrating a large diversity of data sources, resulting in 4.7M
training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact
verification, information retrieval, semantic similarity, and summarization).
We conduct extensive experiments on large-scale benchmarks including 22
evaluation datasets, where 19 of the datasets were never seen in the alignment
training. AlignScore achieves substantial improvement over a wide range of
previous metrics. Moreover, AlignScore (355M parameters) matches or even
outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude
larger. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16739v1' target="_blank">http://arxiv.org/pdf/2305.16739v1</a><br> <br> <br> <font size='5'> 217 </font> <div style="text-align: right"> 2023-05-26 08:03:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Attention Paper: How Generative AI Reshapes Digital Shadow Industry?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid development of digital economy has led to the emergence of various
black and shadow internet industries, which pose potential risks that can be
identified and managed through digital risk management (DRM) that uses
different techniques such as machine learning and deep learning. The evolution
of DRM architecture has been driven by changes in data forms. However, the
development of AI-generated content (AIGC) technology, such as ChatGPT and
Stable Diffusion, has given black and shadow industries powerful tools to
personalize data and generate realistic images and conversations for fraudulent
activities. This poses a challenge for DRM systems to control risks from the
source of data generation and to respond quickly to the fast-changing risk
environment. This paper aims to provide a technical analysis of the challenges
and opportunities of AIGC from upstream, midstream, and downstream paths of
black/shadow industries and suggest future directions for improving existing
risk control systems. The paper will explore the new black and shadow
techniques triggered by generative AI technology and provide insights for
building the next-generation DRM system. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18346v1' target="_blank">http://arxiv.org/pdf/2305.18346v1</a><br> <br> <br> <font size='5'> 218 </font> <div style="text-align: right"> 2023-05-26 05:13:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently large language models (LLMs) like ChatGPT have shown impressive
performance on many natural language processing tasks with zero-shot. In this
paper, we investigate the effectiveness of zero-shot LLMs in the financial
domain. We compare the performance of ChatGPT along with some open-source
generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We
address three inter-related research questions on data annotation, performance
gaps, and the feasibility of employing generative models in the finance domain.
Our findings demonstrate that ChatGPT performs well even without labeled data
but fine-tuned models generally outperform it. Our research also highlights how
annotating with generative models can be time-intensive. Our codebase is
publicly available on GitHub under CC BY-NC 4.0 license. </font><br> Link: <a href='http://arxiv.org/pdf/2305.16633v1' target="_blank">http://arxiv.org/pdf/2305.16633v1</a><br> <br> <br> <font size='5'> 219 </font> <div style="text-align: right"> 2023-05-26 03:44:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid growth of information on the Internet has led to an overwhelming
amount of opinions and comments on various activities, products, and services.
This makes it difficult and time-consuming for users to process all the
available information when making decisions. Text summarization, a Natural
Language Processing (NLP) task, has been widely explored to help users quickly
retrieve relevant information by generating short and salient content from long
or multiple documents. Recent advances in pre-trained language models, such as
ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in
text generation. However, LLMs require massive amounts of data and resources
and are challenging to implement as offline applications. Furthermore, existing
text summarization approaches often lack the ``adaptive" nature required to
capture diverse aspects in opinion summarization, which is particularly
detrimental to users with specific requirements or preferences. In this paper,
we propose an Aspect-adaptive Knowledge-based Opinion Summarization model for
product reviews, which effectively captures the adaptive nature required for
opinion summarization. The model generates aspect-oriented summaries given a
set of reviews for a particular product, efficiently providing users with
useful information on specific aspects they are interested in, ensuring the
generated summaries are more personalized and informative. Extensive
experiments have been conducted using real-world datasets to evaluate the
proposed model. The results demonstrate that our model outperforms
state-of-the-art approaches and is adaptive and efficient in generating
summaries that focus on particular aspects, enabling users to make
well-informed decisions and catering to their diverse interests and
preferences. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05537v1' target="_blank">http://arxiv.org/pdf/2306.05537v1</a><br> <br> <br> <font size='5'> 220 </font> <div style="text-align: right"> 2023-05-25 17:35:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, an AI-based chatbot, was released to provide coherent and useful
replies based on analysis of large volumes of data. In this article, leading
scientists, researchers and engineers discuss the transformative effects of
ChatGPT on modern education. This research seeks to improve our knowledge of
ChatGPT capabilities and its use in the education sector, identifying potential
concerns and challenges. Our preliminary evaluation concludes that ChatGPT
performed differently in each subject area including finance, coding and maths.
While ChatGPT has the ability to help educators by creating instructional
content, offering suggestions and acting as an online educator to learners by
answering questions and promoting group work, there are clear drawbacks in its
use, such as the possibility of producing inaccurate or false data and
circumventing duplicate content (plagiarism) detectors where originality is
essential. The often reported hallucinations within Generative AI in general,
and also relevant for ChatGPT, can render its use of limited benefit where
accuracy is essential. What ChatGPT lacks is a stochastic measure to help
provide sincere and sensitive communication with its users. Academic
regulations and evaluation practices used in educational institutions need to
be updated, should ChatGPT be used as a tool in education. To address the
transformative effects of ChatGPT on the learning environment, educating
teachers and students alike about its capabilities and limitations will be
crucial. </font><br> Link: <a href='http://arxiv.org/pdf/2306.03823v1' target="_blank">http://arxiv.org/pdf/2306.03823v1</a><br> <br> <br> <font size='5'> 221 </font> <div style="text-align: right"> 2023-05-25 15:10:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The exponential growth in user acquisition and popularity of ChatGPT, an
artificial intelligence(AI) powered chatbot, was accompanied by widespread
mainstream media coverage. This article presents a quantitative data analysis
of the early trends and sentiments revealed by conducting text mining and NLP
methods onto a corpus of 10,902 mainstream news headlines related to the
subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in
November 2022 to March 2023. The findings revealed in sentiment analysis,
ChatGPT and artificial intelligence, were perceived more positively than
negatively in the mainstream media. In regards to word frequency results, over
sixty-five percent of the top frequency words were focused on Big Tech issues
and actors while topics such as jobs, diversity, ethics, copyright, gender and
women were poorly represented or completely absent and only accounted for six
percent of the total corpus. This article is a critical analysis into the power
structures and collusions between Big Tech and Big Media in their matrix of
domination. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18340v1' target="_blank">http://arxiv.org/pdf/2305.18340v1</a><br> <br> <br> <font size='5'> 222 </font> <div style="text-align: right"> 2023-05-25 15:09:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the widespread use of large artificial intelligence (AI) models such as
ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is
leading a paradigm shift in content creation and knowledge representation. AIGC
uses generative large AI algorithms to assist or replace humans in creating
massive, high-quality, and human-like content at a faster pace and lower cost,
based on user-provided prompts. Despite the recent significant progress in
AIGC, security, privacy, ethical, and legal challenges still need to be
addressed. This paper presents an in-depth survey of working principles,
security and privacy threats, state-of-the-art solutions, and future challenges
of the AIGC paradigm. Specifically, we first explore the enabling technologies,
general architecture of AIGC, and discuss its working modes and key
characteristics. Then, we investigate the taxonomy of security and privacy
threats to AIGC and highlight the ethical and societal implications of GPT and
AIGC technologies. Furthermore, we review the state-of-the-art AIGC
watermarking approaches for regulatable AIGC paradigms regarding the AIGC model
and its produced content. Finally, we identify future challenges and open
research directions related to AIGC. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18339v1' target="_blank">http://arxiv.org/pdf/2305.18339v1</a><br> <br> <br> <font size='5'> 223 </font> <div style="text-align: right"> 2023-05-25 10:57:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Emergence of a phonological bias in ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Current large language models, such as OpenAI's ChatGPT, have captured the
public's attention because how remarkable they are in the use of language.
Here, I demonstrate that ChatGPT displays phonological biases that are a
hallmark of human language processing. More concretely, just like humans,
ChatGPT has a consonant bias. That is, the chatbot has a tendency to use
consonants over vowels to identify words. This is observed across languages
that differ in their relative distribution of consonants and vowels such as
English and Spanish. Despite the differences in how current artificial
intelligence language models are trained to process linguistic stimuli and how
human infants acquire language, such training seems to be enough for the
emergence of a phonological bias in ChatGPT </font><br> Link: <a href='http://arxiv.org/pdf/2305.15929v2' target="_blank">http://arxiv.org/pdf/2305.15929v2</a><br> <br> <br> <font size='5'> 224 </font> <div style="text-align: right"> 2023-05-25 08:43:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (large LMs) are susceptible to producing text with
hallucinated content. Self-contradiction, where the LM generates two
contradictory sentences within the same context, is an important form of
hallucination. In this work, we present a comprehensive analysis on
self-contradiction for state-of-the-art, instruction-tuned LMs, including
evaluation, detection, and mitigation. To effectively trigger
self-contradictions, we design a framework that constrains LMs to generate
appropriate sentence pairs. Our evaluation on these sentence pairs reveals that
self-contradictions occur frequently across different LMs for both famous and
lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our
results indicate that ChatGPT and GPT-4 are able to accurately identify
self-contradictions, while Vicuna-13B struggles to do so. For example, with our
best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the
sentence pairs generated by itself. To automatically mitigate
self-contradictions, we develop an iterative algorithm that prompts the LMs to
remove the detected self-contradictions from the generated text. Our algorithm
successfully revises the text such that self-contradictions are significantly
reduced, while maintaining its fluency and informativeness. Importantly, our
entire pipeline of triggering, detecting, and mitigating self-contradictions is
applicable to black-box LMs and does not require any external grounded
knowledge. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15852v1' target="_blank">http://arxiv.org/pdf/2305.15852v1</a><br> <br> <br> <font size='5'> 225 </font> <div style="text-align: right"> 2023-05-25 07:46:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for PLC/DCS Control Logic Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) providing generative AI have become popular to
support software engineers in creating, summarizing, optimizing, and
documenting source code. It is still unknown how LLMs can support control
engineers using typical control programming languages in programming tasks.
Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code
generation but did not yet tackle control logic programming. The contribution
of this paper is an exploratory study, for which we created 100 LLM prompts in
10 representative categories to analyze control logic generation for of PLCs
and DCS from natural language. We tested the prompts by generating answers with
ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3
Structured Text code in many cases and demonstrated useful reasoning skills
that could boost control engineer productivity. Our prompt collection is the
basis for a more formal LLM benchmark to test and compare such models for
control logic generation. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15809v1' target="_blank">http://arxiv.org/pdf/2305.15809v1</a><br> <br> <br> <font size='5'> 226 </font> <div style="text-align: right"> 2023-05-25 07:04:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Service Composition in the ChatGPT Era</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The paper speculates about how ChatGPT-like systems can support the field of
automated service composition and identifies new research areas to explore in
order to take advantage of such tools in the field of service-oriented
composition. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15788v1' target="_blank">http://arxiv.org/pdf/2305.15788v1</a><br> <br> <br> <font size='5'> 227 </font> <div style="text-align: right"> 2023-05-25 05:00:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The False Promise of Imitating Proprietary LLMs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: An emerging method to cheaply improve a weaker language model is to finetune
it on outputs from a stronger model, such as a proprietary system like ChatGPT
(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply
imitate the proprietary model's capabilities using a weaker open-source model.
In this work, we critically analyze this approach. We first finetune a series
of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data
sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the
models using crowd raters and canonical NLP benchmarks. Initially, we were
surprised by the output quality of our imitation models -- they appear far
better at following instructions, and crowd workers rate their outputs as
competitive with ChatGPT. However, when conducting more targeted automatic
evaluations, we find that imitation models close little to none of the gap from
the base LM to ChatGPT on tasks that are not heavily supported in the imitation
data. We show that these performance discrepancies may slip past human raters
because imitation models are adept at mimicking ChatGPT's style but not its
factuality. Overall, we conclude that model imitation is a false promise: there
exists a substantial capabilities gap between open and closed LMs that, with
current methods, can only be bridged using an unwieldy amount of imitation data
or by using more capable base LMs. In turn, we argue that the highest leverage
action for improving open-source models is to tackle the difficult challenge of
developing better base LMs, rather than taking the shortcut of imitating
proprietary systems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15717v1' target="_blank">http://arxiv.org/pdf/2305.15717v1</a><br> <br> <br> <font size='5'> 228 </font> <div style="text-align: right"> 2023-05-25 02:45:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the continuous development and change exhibited by large language model
(LLM) technology, represented by generative pretrained transformers (GPTs),
many classic scenarios in various fields have re-emerged with new
opportunities. This paper takes ChatGPT as the modeling object, incorporates
LLM technology into the typical book resource understanding and recommendation
scenario for the first time, and puts it into practice. By building a
ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,
this paper attempts to apply ChatGPT to recommendation modeling for three
typical tasks, book rating recommendation, user rating recommendation, and book
summary recommendation, and explores the feasibility of LLM technology in book
recommendation scenarios. At the same time, based on different evaluation
schemes for book recommendation tasks and the existing classic recommendation
models, this paper discusses the advantages and disadvantages of the BookGPT in
book recommendation scenarios and analyzes the opportunities and improvement
directions for subsequent LLMs in these scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15673v1' target="_blank">http://arxiv.org/pdf/2305.15673v1</a><br> <br> <br> <font size='5'> 229 </font> <div style="text-align: right"> 2023-05-25 02:01:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative AI tools such as ChatGPT have recently gained significant
attention in higher education. This study aims to understand how universities
establish policies regarding the use of AI tools and explore the factors that
influence their decisions. Our study examines ChatGPT policies implemented at
universities around the world, including their existence, content, and issuance
dates. Specifically, we analyzed the top 500 universities according to the 2022
QS World University Rankings. Our findings indicate that there is significant
variation in university policies. Less than one-third of the universities
included in the study had implemented ChatGPT policies. Of the universities
with ChatGPT policies, approximately 67 percent embraced ChatGPT in teaching
and learning, more than twice the number of universities that banned it. The
majority of the universities that ban the use of ChatGPT in assessments allow
individual instructors to deviate from this restrictive policy. Our empirical
analysis identifies several factors that are significantly and positively
correlated with a university's likelihood of having a ChatGPT policy, including
the university's academic reputation score, being in an English-speaking
country, and the general public attitudes toward ChatGPT. In addition, we found
that a university's likelihood of having a ban policy is positively associated
with faculty student ratio, citations, and the English-speaking country dummy,
while negatively associated with the number of peer universities within the
same country that have banned ChatGPT. We discuss the challenges faced by
universities based our empirical findings. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18617v1' target="_blank">http://arxiv.org/pdf/2305.18617v1</a><br> <br> <br> <font size='5'> 230 </font> <div style="text-align: right"> 2023-05-24 16:49:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research article critically examines the potential risks and
implications arising from the malicious utilization of large language
models(LLM), focusing specifically on ChatGPT and Google's Bard. Although these
large language models have numerous beneficial applications, the misuse of this
technology by cybercriminals for creating offensive payloads and tools is a
significant concern. In this study, we systematically generated implementable
code for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, and
conduct a comparative analysis of its performance with Google's Bard. Our
experimentation reveals that ChatGPT has the potential to enable attackers to
accelerate the operation of more targeted and sophisticated attacks.
Additionally, the technology provides amateur attackers with more capabilities
to perform a wide range of attacks and empowers script kiddies to develop
customized tools that contribute to the acceleration of cybercrime.
Furthermore, LLMs significantly benefits malware authors, particularly
ransomware gangs, in generating sophisticated variants of wiper and ransomware
attacks with ease. On a positive note, our study also highlights how offensive
security researchers and pentesters can make use of LLMs to simulate realistic
attack scenarios, identify potential vulnerabilities, and better protect
organizations. Overall, we conclude by emphasizing the need for increased
vigilance in mitigating the risks associated with LLMs. This includes
implementing robust security measures, increasing awareness and education
around the potential risks of this technology, and collaborating with security
experts to stay ahead of emerging threats. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15336v1' target="_blank">http://arxiv.org/pdf/2305.15336v1</a><br> <br> <br> <font size='5'> 231 </font> <div style="text-align: right"> 2023-05-24 16:23:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Science in the Era of ChatGPT, Large Language Models and AI: Challenges for Research Ethics Review and How to Respond</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models of artificial intelligence (AI) such as ChatGPT find
remarkable but controversial applicability in science and research. This paper
reviews epistemological challenges, ethical and integrity risks in science
conduct. This is with the aim to lay new timely foundations for a high-quality
research ethics review in the era of AI. The role of AI language models as a
research instrument and subject is scrutinized along with ethical implications
for scientists, participants and reviewers. Ten recommendations shape a
response for a more responsible research conduct with AI language models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15299v1' target="_blank">http://arxiv.org/pdf/2305.15299v1</a><br> <br> <br> <font size='5'> 232 </font> <div style="text-align: right"> 2023-05-24 15:27:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Unlearning: its nature, scope, and importance for a "delete culture"</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The article explores the cultural shift from recording to deleting
information in the digital age and its implications on privacy, intellectual
property (IP), and Large Language Models like ChatGPT. It begins by defining a
delete culture where information, in principle legal, is made unavailable or
inaccessible because unacceptable or undesirable, especially but not only due
to its potential to infringe on privacy or IP. Then it focuses on two
strategies in this context: deleting, to make information unavailable; and
blocking, to make it inaccessible. The article argues that both strategies have
significant implications, particularly for machine learning (ML) models where
information is not easily made unavailable. However, the emerging research area
of Machine Unlearning (MU) is highlighted as a potential solution. MU, still in
its infancy, seeks to remove specific data points from ML models, effectively
making them 'forget' completely specific information. If successful, MU could
provide a feasible means to manage the overabundance of information and ensure
a better protection of privacy and IP. However, potential ethical risks, such
as misuse, overuse, and underuse of MU, should be systematically studied to
devise appropriate policies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15242v1' target="_blank">http://arxiv.org/pdf/2305.15242v1</a><br> <br> <br> <font size='5'> 233 </font> <div style="text-align: right"> 2023-05-24 12:00:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have
shown strong abilities in multilingual translations, without being explicitly
trained on parallel corpora. It is interesting how the LLMs obtain their
ability to carry out translation instructions for different languages. In this
paper, we present a detailed analysis by finetuning a multilingual pretrained
language model, XGLM-7B, to perform multilingual translation following given
instructions. Firstly, we show that multilingual LLMs have stronger translation
abilities than previously demonstrated. For a certain language, the performance
depends on its similarity to English and the amount of data used in the
pretraining phase. Secondly, we find that LLMs' ability to carry out
translation instructions relies on the understanding of translation
instructions and the alignment among different languages. With multilingual
finetuning, LLMs could learn to perform the translation task well even for
those language pairs unseen during the instruction tuning phase. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15083v2' target="_blank">http://arxiv.org/pdf/2305.15083v2</a><br> <br> <br> <font size='5'> 234 </font> <div style="text-align: right"> 2023-05-24 11:56:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HuatuoGPT, towards Taming Language Model to Be a Doctor</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we present HuatuoGPT, a large language model (LLM) for medical
consultation. The core recipe of HuatuoGPT is to leverage both
\textit{distilled data from ChatGPT} and \textit{real-world data from doctors}
in the supervised fine-tuned stage. The responses of ChatGPT are usually
detailed, well-presented and informative while it cannot perform like a doctor
in many aspects, e.g. for integrative diagnosis. We argue that real-world data
from doctors would be complementary to distilled data in the sense the former
could tame a distilled language model to perform like doctors. To better
leverage the strengths of both data, we train a reward model to align the
language model with the merits that both data bring, following an RLAIF
(reinforced learning from AI feedback) fashion. To evaluate and benchmark the
models, we propose a comprehensive evaluation scheme (including automatic and
manual metrics). Experimental results demonstrate that HuatuoGPT achieves
state-of-the-art results in performing medical consultation among open-source
LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It
is worth noting that by using additional real-world data and RLAIF, the
distilled language model (i.e., HuatuoGPT) outperforms its teacher model
ChatGPT in most cases. Our code, data, and models are publicly available at
\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is
available at \url{https://www.HuatuoGPT.cn/}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15075v1' target="_blank">http://arxiv.org/pdf/2305.15075v1</a><br> <br> <br> <font size='5'> 235 </font> <div style="text-align: right"> 2023-05-24 11:55:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As advances in large language models (LLMs) and multimodal techniques
continue to mature, the development of general-purpose multimodal large
language models (MLLMs) has surged, with significant applications in natural
image interpretation. However, the field of pathology has largely remained
untapped in this regard, despite the growing need for accurate, timely, and
personalized diagnostics. To bridge the gap in pathology MLLMs, we present the
PathAsst in this study, which is a generative foundation AI assistant to
revolutionize diagnostic and predictive analytics in pathology. To develop
PathAsst, we collect over 142K high-quality pathology image-text pairs from a
variety of reliable sources, including PubMed, comprehensive pathology
textbooks, reputable pathology websites, and private data annotated by
pathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we
generate over 180K instruction-following samples. Furthermore, we devise
additional instruction-following data, specifically tailored for the invocation
of the pathology-specific models, allowing the PathAsst to effectively interact
with these models based on the input image and user intent, consequently
enhancing the model's diagnostic capabilities. Subsequently, our PathAsst is
trained based on Vicuna-13B language model in coordination with the CLIP vision
encoder. The results of PathAsst show the potential of harnessing the
AI-powered generative foundation model to improve pathology diagnosis and
treatment processes. We are committed to open-sourcing our meticulously curated
dataset, as well as a comprehensive toolkit designed to aid researchers in the
extensive collection and preprocessing of their own datasets. Resources can be
obtained at
https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15072v1' target="_blank">http://arxiv.org/pdf/2305.15072v1</a><br> <br> <br> <font size='5'> 236 </font> <div style="text-align: right"> 2023-05-24 11:53:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models~(LLM) like ChatGPT have become indispensable to
artificial general intelligence~(AGI), demonstrating excellent performance in
various natural language processing tasks. In the real world, graph data is
ubiquitous and an essential part of AGI and prevails in domains like social
network analysis, bioinformatics and recommender systems. The training corpus
of large language models often includes some algorithmic components, which
allows them to achieve certain effects on some graph data-related problems.
However, there is still little research on their performance on a broader range
of graph-structured data. In this study, we conduct an extensive investigation
to assess the proficiency of LLMs in comprehending graph data, employing a
diverse range of structural and semantic-related tasks. Our analysis
encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph
understanding. Through our study, we not only uncover the current limitations
of language models in comprehending graph structures and performing associated
reasoning tasks but also emphasize the necessity for further advancements and
novel approaches to enhance their graph processing capabilities. Our findings
contribute valuable insights towards bridging the gap between language models
and graph understanding, paving the way for more effective graph mining and
knowledge extraction. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15066v2' target="_blank">http://arxiv.org/pdf/2305.15066v2</a><br> <br> <br> <font size='5'> 237 </font> <div style="text-align: right"> 2023-05-24 11:06:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the era of sustainable smart agriculture, a massive amount of agricultural
news text is being posted on the Internet, in which massive agricultural
knowledge has been accumulated. In this context, it is urgent to explore
effective text classification techniques for users to access the required
agricultural knowledge with high efficiency. Mainstream deep learning
approaches employing fine-tuning strategies on pre-trained language models
(PLMs), have demonstrated remarkable performance gains over the past few years.
Nonetheless, these methods still face many drawbacks that are complex to solve,
including: 1. Limited agricultural training data due to the expensive-cost and
labour-intensive annotation; 2. Poor domain transferability, especially of
cross-linguistic ability; 3. Complex and expensive large models
deployment.Inspired by the extraordinary success brought by the recent ChatGPT
(e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore
the capability and utilization of ChatGPT applying to the agricultural
informatization field. ....(shown in article).... Code has been released on
Github
https://github.com/albert-jin/agricultural_textual_classification_ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15024v1' target="_blank">http://arxiv.org/pdf/2305.15024v1</a><br> <br> <br> <font size='5'> 238 </font> <div style="text-align: right"> 2023-05-24 10:48:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: LLM-powered chatbots are becoming widely adopted in applications such as
healthcare, personal assistants, industry hiring decisions, etc. In many of
these cases, chatbots are fed sensitive, personal information in their prompts,
as samples for in-context learning, retrieved records from a database, or as
part of the conversation. The information provided in the prompt could directly
appear in the output, which might have privacy ramifications if there is
sensitive information there. As such, in this paper, we aim to understand the
input copying and regurgitation capabilities of these models during inference
and how they can be directly instructed to limit this copying by complying with
regulations such as HIPAA and GDPR, based on their internal knowledge of them.
More specifically, we find that when ChatGPT is prompted to summarize cover
letters of a 100 candidates, it would retain personally identifiable
information (PII) verbatim in 57.4% of cases, and we find this retention to be
non-uniform between different subgroups of people, based on attributes such as
gender identity. We then probe ChatGPT's perception of privacy-related policies
and privatization mechanisms by directly instructing it to provide compliant
outputs and observe a significant omission of PII from output. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15008v1' target="_blank">http://arxiv.org/pdf/2305.15008v1</a><br> <br> <br> <font size='5'> 239 </font> <div style="text-align: right"> 2023-05-24 10:45:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sentiment Analysis in the Era of Large Language Models: A Reality Check</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Sentiment analysis (SA) has been a long-standing research area in natural
language processing. It can offer rich insights into human sentiments and
opinions and has thus seen considerable interest from both academia and
industry. With the advent of large language models (LLMs) such as ChatGPT,
there is a great potential for their employment on SA problems. However, the
extent to which existing LLMs can be leveraged for different sentiment analysis
tasks remains unclear. This paper aims to provide a comprehensive investigation
into the capabilities of LLMs in performing various sentiment analysis tasks,
from conventional sentiment classification to aspect-based sentiment analysis
and multifaceted analysis of subjective texts. We evaluate performance across
13 tasks on 26 datasets and compare the results against small language models
(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs
demonstrate satisfactory performance in simpler tasks, they lag behind in more
complex tasks requiring deeper understanding or structured sentiment
information. However, LLMs significantly outperform SLMs in few-shot learning
settings, suggesting their potential when annotation resources are limited. We
also highlight the limitations of current evaluation practices in assessing
LLMs' SA abilities and propose a novel benchmark, \textsc{SentiEval}, for a
more comprehensive and realistic evaluation. Data and code during our
investigations are available at
\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15005v1' target="_blank">http://arxiv.org/pdf/2305.15005v1</a><br> <br> <br> <font size='5'> 240 </font> <div style="text-align: right"> 2023-05-24 10:30:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RefGPT: Reference -> Truthful & Customized Dialogues Generation by GPTs and for GPTs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: General chat models, like ChatGPT, have attained impressive capability to
resolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with
high-quality instruction data. However, collecting human-written high-quality
data, especially multi-turn dialogues, is expensive and unattainable for most
people. Though previous studies have used powerful LLMs to generate the
dialogues automatically, but they all suffer from generating untruthful
dialogues because of the LLMs hallucination. Therefore, we propose a method
called RefGPT to generate enormous truthful and customized dialogues without
worrying about factual errors caused by the model hallucination. RefGPT solves
the model hallucination in dialogue generation by restricting the LLMs to
leverage the given reference instead of reciting their own knowledge to
generate dialogues. Additionally, RefGPT adds detailed controls on every
utterances to enable highly customization capability, which previous studies
have ignored. On the basis of RefGPT, we also propose two high-quality dialogue
datasets generated by GPT-4, namely RefGPT-Fact and RefGPT-Code. RefGPT-Fact is
100k multi-turn dialogue datasets based on factual knowledge and RefGPT-Code is
76k multi-turn dialogue dataset covering a wide range of coding scenarios. Our
code and datasets are released in https://github.com/ziliwangnlp/RefGPT </font><br> Link: <a href='http://arxiv.org/pdf/2305.14994v2' target="_blank">http://arxiv.org/pdf/2305.14994v2</a><br> <br> <br> <font size='5'> 241 </font> <div style="text-align: right"> 2023-05-24 10:16:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Benchmarking Arabic AI with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With large Foundation Models (FMs), language technologies (AI in general) are
entering a new paradigm: eliminating the need for developing large-scale
task-specific datasets and supporting a variety of tasks through set-ups
ranging from zero-shot to few-shot learning. However, understanding FMs
capabilities requires a systematic benchmarking effort by comparing FMs
performance with the state-of-the-art (SOTA) task-specific models. With that
goal, past work focused on the English language and included a few efforts with
multiple languages. Our study contributes to ongoing research by evaluating FMs
performance for standard Arabic NLP and Speech processing, including a range of
tasks from sequence tagging to content classification across diverse domains.
We start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM,
addressing 33 unique tasks using 59 publicly available datasets resulting in 96
test setups. For a few tasks, FMs performs on par or exceeds the performance of
the SOTA models but for the majority it under-performs. Given the importance of
prompt for the FMs performance, we discuss our prompt strategies in detail and
elaborate on our findings. Our future work on Arabic AI will explore few-shot
prompting, expand the range of tasks, and investigate additional open-source
models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14982v1' target="_blank">http://arxiv.org/pdf/2305.14982v1</a><br> <br> <br> <font size='5'> 242 </font> <div style="text-align: right"> 2023-05-24 10:12:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent emergence of ChatGPT has brought a revolutionary change in the
landscape of NLP. Although ChatGPT has consistently shown impressive
performance on English benchmarks, its exact capabilities on most other
languages remain largely unknown. To better understand ChatGPT's capabilities
on Arabic, we present a large-scale evaluation of the model on a broad range of
Arabic NLP tasks. Namely, we evaluate ChatGPT on 32 diverse natural language
understanding and generation tasks on over 60 different datasets. To the best
of our knowledge, our work offers the first performance analysis of ChatGPT on
Arabic NLP at such a massive scale. Our results show that, despite its success
on English benchmarks, ChatGPT trained in-context (few-shot) is consistently
outperformed by much smaller dedicated models finetuned on Arabic. These
results suggest that there is significant place for improvement for
instruction-tuned LLMs such as ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14976v1' target="_blank">http://arxiv.org/pdf/2305.14976v1</a><br> <br> <br> <font size='5'> 243 </font> <div style="text-align: right"> 2023-05-24 10:08:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The exceptional performance of pre-trained large language models has
revolutionised various applications, but their adoption in production
environments is hindered by prohibitive costs and inefficiencies, particularly
when utilising long prompts. This paper proposes OverPrompt, an in-context
learning method aimed at improving LLM efficiency and performance by processing
multiple inputs in parallel. Evaluated across diverse datasets, OverPrompt
enhances task efficiency and integrates a diverse range of examples for
improved performance. Particularly, it amplifies fact-checking and sentiment
analysis tasks when supplemented with contextual information. Synthetic data
grouping further enhances performance, suggesting a viable approach for data
augmentation. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14973v1' target="_blank">http://arxiv.org/pdf/2305.14973v1</a><br> <br> <br> <font size='5'> 244 </font> <div style="text-align: right"> 2023-05-24 09:40:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Adversarial Demonstration Attacks on Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the emergence of more powerful large language models (LLMs), such as
ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence
in leveraging these models for specific tasks by utilizing data-label pairs as
precondition prompts. While incorporating demonstrations can greatly enhance
the performance of LLMs across various tasks, it may introduce a new security
concern: attackers can manipulate only the demonstrations without changing the
input to perform an attack. In this paper, we investigate the security concern
of ICL from an adversarial perspective, focusing on the impact of
demonstrations. We propose an ICL attack based on TextAttack, which aims to
only manipulate the demonstration without changing the input to mislead the
models. Our results demonstrate that as the number of demonstrations increases,
the robustness of in-context learning would decreases. Furthermore, we also
observe that adversarially attacked demonstrations exhibit transferability to
diverse input examples. These findings emphasize the critical security risks
associated with ICL and underscore the necessity for extensive research on the
robustness of ICL, particularly given its increasing significance in the
advancement of LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14950v1' target="_blank">http://arxiv.org/pdf/2305.14950v1</a><br> <br> <br> <font size='5'> 245 </font> <div style="text-align: right"> 2023-05-24 08:24:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ClusterLLM: Large Language Models as a Guide for Text Clustering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce ClusterLLM, a novel text clustering framework that leverages
feedback from an instruction-tuned large language model, such as ChatGPT.
Compared with traditional unsupervised methods that builds upon "small"
embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the
emergent capability of LLM even if its embeddings are inaccessible; and (2) it
understands the user's preference on clustering through textual instruction
and/or a few annotated data. First, we prompt ChatGPT for insights on
clustering perspective by constructing hard triplet questions <does A better
correspond to B than C>, where A, B and C are similar data points that belong
to different clusters according to small embedder. We empirically show that
this strategy is both effective for fine-tuning small embedder and
cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on
clustering granularity by carefully designed pairwise questions <do A and B
belong to the same category>, and tune the granularity from cluster hierarchies
that is the most consistent with the ChatGPT answers. Extensive experiments on
14 datasets show that ClusterLLM consistently improves clustering quality, at
an average cost of ~$0.6 per dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14871v1' target="_blank">http://arxiv.org/pdf/2305.14871v1</a><br> <br> <br> <font size='5'> 246 </font> <div style="text-align: right"> 2023-05-24 08:21:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The task of zero-shot commonsense question answering evaluates models on
their capacity to reason about general scenarios beyond those presented in
specific datasets. Existing approaches for tackling this task leverage external
knowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on
synthetic QA pairs constructed from CSKBs. In these approaches, negative
examples (distractors) are formulated by randomly sampling from CSKBs using
fairly primitive keyword constraints. However, two bottlenecks limit these
approaches: the inherent incompleteness of CSKBs limits the semantic coverage
of synthetic QA pairs, and the lack of human annotations makes the sampled
negative examples potentially uninformative and contradictory. To tackle these
limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a
zero-shot commonsense question-answering framework that fully leverages the
power of conceptualization. Specifically, CAR abstracts a commonsense knowledge
triple to many higher-level instances, which increases the coverage of CSKB and
expands the ground-truth answer space, reducing the likelihood of selecting
false-negative distractors. Extensive experiments demonstrate that CAR more
robustly generalizes to answering questions about zero-shot commonsense
scenarios than existing methods, including large language models, such as
GPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at
https://github.com/HKUST-KnowComp/CAR. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14869v1' target="_blank">http://arxiv.org/pdf/2305.14869v1</a><br> <br> <br> <font size='5'> 247 </font> <div style="text-align: right"> 2023-05-24 08:06:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite remarkable advancements in few-shot generalization in natural
language processing, most models are developed and evaluated primarily in
English. To facilitate research on few-shot cross-lingual transfer, we
introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across
54 languages in a sequence-to-sequence format and provides a fixed set of
few-shot examples and instructions. BUFFET is designed to establish a rigorous
and equitable evaluation framework for few-shot cross-lingual transfer across a
broad range of tasks and languages. Using BUFFET, we perform thorough
evaluations of state-of-the-art multilingual large language models with
different transfer methods, namely in-context learning and fine-tuning. Our
findings reveal significant room for improvement in few-shot in-context
cross-lingual transfer. In particular, ChatGPT with in-context learning often
performs worse than much smaller mT5-base models fine-tuned on English task
data and few-shot in-language examples. Our analysis suggests various avenues
for future research in few-shot cross-lingual transfer, such as improved
pretraining, understanding, and future evaluations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14857v1' target="_blank">http://arxiv.org/pdf/2305.14857v1</a><br> <br> <br> <font size='5'> 248 </font> <div style="text-align: right"> 2023-05-24 07:40:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SummIt: Iterative Text Summarization via ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Existing text summarization systems have made significant progress in recent
years but typically generates summaries in a single step. The one-shot
summarization setting is sometimes inadequate, however, as the generated
summary may contain hallucinations or overlook important details related to the
reader's interests. In this paper, we address this limitation by proposing
SummIt, an iterative text summarization framework based on large language
models like ChatGPT. Our framework enables the model to refine the generated
summary iteratively through self-evaluation and feedback, closely resembling
the iterative process humans undertake when drafting and revising summaries. We
also explore using in-context learning to guide the rationale generation and
summary refinement. Furthermore, we explore the potential benefits of
integrating knowledge and topic extractors into the framework to enhance
summary faithfulness and controllability. We evaluate the performance of our
framework on three benchmark summarization datasets through empirical and
qualitative analyses. We also conduct a human evaluation to validate the
effectiveness of the model's refinements and find a potential issue of
over-correction. Our code is available at
\url{https://github.com/hpzhang94/summ_it}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14835v1' target="_blank">http://arxiv.org/pdf/2305.14835v1</a><br> <br> <br> <font size='5'> 249 </font> <div style="text-align: right"> 2023-05-24 06:41:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper sheds light on the limitations of ChatGPT's understanding
capabilities, focusing on simple inference tasks that are typically easy for
humans but appear to be challenging for the model. Specifically, we target (i)
grammatically-specified entailments, (ii) premises with evidential adverbs of
uncertainty, and (iii) monotonicity entailments. We present expert-designed
evaluation sets for these inference types and conduct experiments in a
zero-shot setup. Our results show that the model struggles with these types of
inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT
demonstrates knowledge of the underlying linguistic concepts when prompted
directly, it often fails to incorporate this knowledge to make correct
inferences. Even more strikingly, further experiments show that embedding the
premise under presupposition triggers or non-factive verbs causes the model to
predict entailment more frequently {regardless} of the correct semantic label.
Overall these results suggest that, despite GPT's celebrated language
understanding capacity, ChatGPT has blindspots with respect to certain types of
entailment, and that certain entailment-cancelling features act as ``blinds''
overshadowing the semantics of the embedded premise. Our analyses emphasize the
need for further research into the linguistic comprehension and reasoning
capabilities of LLMs, in order to improve their reliability, and establish
their trustworthiness for real-world applications. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14785v1' target="_blank">http://arxiv.org/pdf/2305.14785v1</a><br> <br> <br> <font size='5'> 250 </font> <div style="text-align: right"> 2023-05-24 06:14:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine "intelligence". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14763v1' target="_blank">http://arxiv.org/pdf/2305.14763v1</a><br> <br> <br> <font size='5'> 251 </font> <div style="text-align: right"> 2023-05-24 06:02:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Human-Centered Metrics for Dialog System Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present metrics for evaluating dialog systems through a
psychologically-grounded "human" lens: conversational agents express a
diversity of both states (short-term factors like emotions) and traits
(longer-term factors like personality) just as people do. These interpretable
metrics consist of five measures from established psychology constructs that
can be applied both across dialogs and on turns within dialogs: emotional
entropy, linguistic style and emotion matching, as well as agreeableness and
empathy. We compare these human metrics against 6 state-of-the-art automatic
metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We
also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which
consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We
demonstrate the proposed human metrics offer novel information, are
uncorrelated with automatic metrics, and lead to increased accuracy beyond
existing automatic metrics for predicting crowd-sourced dialog judgements. The
interpretability and unique signal of our proposed human-centered framework
make it a valuable tool for evaluating and improving dialog systems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14757v1' target="_blank">http://arxiv.org/pdf/2305.14757v1</a><br> <br> <br> <font size='5'> 252 </font> <div style="text-align: right"> 2023-05-24 03:51:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ExpertPrompting: Instructing Large Language Models to be Distinguished Experts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
\url{https://github.com/OFA-Sys/ExpertLLaMA}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14688v1' target="_blank">http://arxiv.org/pdf/2305.14688v1</a><br> <br> <br> <font size='5'> 253 </font> <div style="text-align: right"> 2023-05-24 02:52:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: LLMs (large language models) such as ChatGPT have shown remarkable language
understanding and generation capabilities. Although reference-free evaluators
based on LLMs show better human alignment than traditional reference-based
evaluators, there are many challenges in using reference-free evaluators based
on LLMs. Reference-free evaluators are more suitable for open-ended examples
with different semantics responses. But not all examples are open-ended. For
closed-ended examples with unique correct semantic response, reference-free
evaluators will still consider it high quality when giving a response that is
inconsistent with the facts and the semantic of reference. In order to
comprehensively evaluate the reliability of evaluators based on LLMs, we
construct two adversarial meta-evaluation dialogue generation datasets
KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared
to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more
challenging since they requires evaluators to be able to reasonably evaluate
closed-ended examples with the help of external knowledge or even its own
knowledge. Empirical results show that the ability of LLMs to identify
unreasonable responses is insufficient. There are risks in using eference-free
evaluators based on LLMs to evaluate the quality of dialogue responses. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14658v1' target="_blank">http://arxiv.org/pdf/2305.14658v1</a><br> <br> <br> <font size='5'> 254 </font> <div style="text-align: right"> 2023-05-24 01:46:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Fact-checking is an essential task in NLP that is commonly utilized for
validating the factual accuracy of claims. Prior work has mainly focused on
fine-tuning pre-trained languages models on specific datasets, which can be
computationally intensive and time-consuming. With the rapid development of
large language models (LLMs), such as ChatGPT and GPT-3, researchers are now
exploring their in-context learning capabilities for a wide range of tasks. In
this paper, we aim to assess the capacity of LLMs for fact-checking by
introducing Self-Checker, a framework comprising a set of plug-and-play modules
that facilitate fact-checking by purely prompting LLMs in an almost zero-shot
setting. This framework provides a fast and efficient way to construct
fact-checking systems in low-resource environments. Empirical results
demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.
However, there is still significant room for improvement compared to SOTA
fine-tuned models, which suggests that LLM adoption could be a promising
approach for future fact-checking research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14623v1' target="_blank">http://arxiv.org/pdf/2305.14623v1</a><br> <br> <br> <font size='5'> 255 </font> <div style="text-align: right"> 2023-05-24 00:10:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) excel at implementing code from functionality
descriptions, but struggle with algorithmic problems that require not only
implementation but also identification of the suitable algorithm. Moreover,
LLM-generated programs lack guaranteed correctness and require human
verification. To address these challenges, we propose ALGO, a framework that
synthesizes Algorithmic programs with LLM-Generated Oracles to guide the
creation and verify their correctness. ALGO first generates a probably correct
but possibly slow reference oracle by prompting an LLM to exhaustively
enumerate all the combinations of relevant variables. This oracle is then
utilized to guide an arbitrary search strategy in exploring the algorithm space
and to verify the algorithms synthesized. Our study shows that the
LLM-generated oracles are correct for 88% of the cases. With the oracles as
verifiers, ALGO can be integrated with any existing code generation model in a
model-agnostic manner to enhance its performance. Experiments show that when
equipped with ALGO, we achieve an 8x better one-submission pass rate over the
Codex model and a 2.6x better one-submission pass rate over CodeT, the current
state-of-the-art model on CodeContests. We can also get 1.3x better pass rate
over the ChatGPT Code Interpreter on unseen problems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14591v1' target="_blank">http://arxiv.org/pdf/2305.14591v1</a><br> <br> <br> <font size='5'> 256 </font> <div style="text-align: right"> 2023-05-23 22:31:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large pre-trained language models have exhibited unprecedented capabilities
in producing high-quality text via prompting techniques. This fact introduces
new possibilities for data collection and annotation, particularly in
situations where such data is scarce, complex to gather, expensive, or even
sensitive. In this paper, we explore the potential of these models to generate
and annotate goal-oriented dialogues, and conduct an in-depth analysis to
evaluate their quality. Our experiments employ ChatGPT, and encompass three
categories of goal-oriented dialogues (task-oriented, collaborative, and
explanatory), two generation modes (interactive and one-shot), and two
languages (English and Italian). Based on extensive human-based evaluations, we
demonstrate that the quality of generated dialogues and annotations is on par
with those generated by humans. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14556v1' target="_blank">http://arxiv.org/pdf/2305.14556v1</a><br> <br> <br> <font size='5'> 257 </font> <div style="text-align: right"> 2023-05-23 18:54:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative chat models, such as ChatGPT and GPT-4, have revolutionized
natural language generation (NLG) by incorporating instructions and human
feedback to achieve significant performance improvements. However, the lack of
standardized evaluation benchmarks for chat models, particularly for Chinese
and domain-specific models, hinders their assessment and progress. To address
this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,
focusing on general and financial domains. The CGCE benchmark encompasses
diverse tasks, including 200 questions in the general domain and 150 specific
professional questions in the financial domain. Manual scoring evaluates
factors such as accuracy, coherence, expression clarity, and completeness. The
CGCE benchmark provides researchers with a standardized framework to assess and
compare Chinese generative chat models, fostering advancements in NLG research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14471v1' target="_blank">http://arxiv.org/pdf/2305.14471v1</a><br> <br> <br> <font size='5'> 258 </font> <div style="text-align: right"> 2023-05-23 18:33:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Generation through Summarization Duality and Explicit Outline Control</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automatically open-ended long text generation poses significant challenges
due to semantic incoherence and plot implausibility. Previous works usually
alleviate this problem through outlines in the form of short phrases or
abstractive signals by designing unsupervised tasks, which tend to be unstable
and weakly interpretable.
  Assuming that a summary serves as a mature outline, we introduce a two-stage,
summary-enhanced outline supervised generation framework. This framework
leverages the dual characteristics of the summarization task to improve outline
prediction, resulting in more explicit and plausible outlines. Furthermore, we
identify an underutilization issue in outline-based generation with both
standard pretrained language models (e.g., GPT-2, BART) and large language
models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit
outline control method for more effective utilization of generated outlines. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14459v1' target="_blank">http://arxiv.org/pdf/2305.14459v1</a><br> <br> <br> <font size='5'> 259 </font> <div style="text-align: right"> 2023-05-23 18:17:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has stimulated the research boom in the field of large language
models. In this paper, we assess the capabilities of ChatGPT from four
perspectives including Performance, Evaluation Criteria, Robustness and Error
Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets
with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought
scenarios, and find a huge performance gap between ChatGPT and SOTA results.
Next, we rethink this gap and propose a soft-matching strategy for evaluation
to more accurately reflect ChatGPT's performance. Then, we analyze the
robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely
outputs invalid responses; 2) Irrelevant context and long-tail target types
greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the
subject-object relationships in RE task. Finally, we analyze the errors of
ChatGPT, and find that "unannotated spans" is the most dominant error type.
This raises concerns about the quality of annotated data, and indicates the
possibility of annotating data with ChatGPT. The data and code are released at
Github site. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14450v1' target="_blank">http://arxiv.org/pdf/2305.14450v1</a><br> <br> <br> <font size='5'> 260 </font> <div style="text-align: right"> 2023-05-23 17:50:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: QLoRA: Efficient Finetuning of Quantized LLMs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage
enough to finetune a 65B parameter model on a single 48GB GPU while preserving
full 16-bit finetuning task performance. QLoRA backpropagates gradients through
a frozen, 4-bit quantized pretrained language model into Low Rank
Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all
previous openly released models on the Vicuna benchmark, reaching 99.3% of the
performance level of ChatGPT while only requiring 24 hours of finetuning on a
single GPU. QLoRA introduces a number of innovations to save memory without
sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is
information theoretically optimal for normally distributed weights (b) double
quantization to reduce the average memory footprint by quantizing the
quantization constants, and (c) paged optimziers to manage memory spikes. We
use QLoRA to finetune more than 1,000 models, providing a detailed analysis of
instruction following and chatbot performance across 8 instruction datasets,
multiple model types (LLaMA, T5), and model scales that would be infeasible to
run with regular finetuning (e.g. 33B and 65B parameter models). Our results
show that QLoRA finetuning on a small high-quality dataset leads to
state-of-the-art results, even when using smaller models than the previous
SoTA. We provide a detailed analysis of chatbot performance based on both human
and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable
alternative to human evaluation. Furthermore, we find that current chatbot
benchmarks are not trustworthy to accurately evaluate the performance levels of
chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to
ChatGPT. We release all of our models and code, including CUDA kernels for
4-bit training. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14314v1' target="_blank">http://arxiv.org/pdf/2305.14314v1</a><br> <br> <br> <font size='5'> 261 </font> <div style="text-align: right"> 2023-05-23 17:48:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction-tuned Large Language Models (LLMs) have exhibited impressive
language understanding and the capacity to generate responses that follow
specific instructions. However, due to the computational demands associated
with training these models, their applications often rely on zero-shot
settings. In this paper, we evaluate the zero-shot performance of two publicly
accessible LLMs, ChatGPT and OpenAssistant, in the context of Computational
Social Science classification tasks, while also investigating the effects of
various prompting strategies. Our experiment considers the impact of prompt
complexity, including the effect of incorporating label definitions into the
prompt, using synonyms for label names, and the influence of integrating past
memories during the foundation model training. The findings indicate that in a
zero-shot setting, the current LLMs are unable to match the performance of
smaller, fine-tuned baseline transformer models (such as BERT). Additionally,
we find that different prompting strategies can significantly affect
classification accuracy, with variations in accuracy and F1 scores exceeding
10%. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14310v1' target="_blank">http://arxiv.org/pdf/2305.14310v1</a><br> <br> <br> <font size='5'> 262 </font> <div style="text-align: right"> 2023-05-23 17:33:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-powered Data Augmentation for Enhanced Crosslingual Performance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper aims to explore the potential of leveraging Large Language Models
(LLMs) for data augmentation in crosslingual commonsense reasoning datasets,
where the available training data is extremely limited. To achieve this, we
employ several LLMs including Dolly-v2, StableVicuna, ChatGPT, and GPT-4 to
augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we
assess the effectiveness of fine-tuning smaller crosslingual models, mBERT and
XLMR, using the synthesised data. We compare the performance of training with
data generated in English and target languages, as well as translating the
English-generated data into the target languages. Our experiments reveal the
overall advantages of incorporating data generated by LLMs. Training on
synthetic data generated by GPT-4, whether English or multilingual, improves
performance consistently compared to the baseline. Other models also exhibit an
overall increase in performance, however, their effectiveness decreases in some
settings. We also ask native speakers to evaluate the naturalness and logical
soundness of the generated examples for different languages. Human evaluation
reveals that LLMs like ChatGPT and GPT-4 excel at generating natural text in
most languages, except a few such as Tamil. Moreover, ChatGPT trails behind in
generating plausible alternatives in comparison to the original dataset, while
GPT-4 demonstrates competitive logic consistency in the synthesised data. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14288v1' target="_blank">http://arxiv.org/pdf/2305.14288v1</a><br> <br> <br> <font size='5'> 263 </font> <div style="text-align: right"> 2023-05-23 17:06:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Evaluating the factuality of long-form text generated by large language
models (LMs) is non-trivial because (1) generations often contain a mixture of
supported and unsupported pieces of information, making binary judgments of
quality inadequate, and (2) human evaluation is time-consuming and costly. In
this paper, we introduce FActScore (Factual precision in Atomicity Score), a
new evaluation that breaks a generation into a series of atomic facts and
computes the percentage of atomic facts supported by a reliable knowledge
source. We conduct an extensive human evaluation to obtain FActScores of people
biographies generated by several state-of-the-art commercial LMs --
InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report
new analysis demonstrating the need for such a fine-grained score (e.g.,
ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce
an automated model that estimates FActScore, using retrieval and a strong
language model, with less than a 2% error rate. Finally, we use this automated
metric to evaluate 6,500 generations from a new set of 13 recent LMs that would
have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT
are more factual than public models, and Vicuna and Alpaca are some of the best
public models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14251v1' target="_blank">http://arxiv.org/pdf/2305.14251v1</a><br> <br> <br> <font size='5'> 264 </font> <div style="text-align: right"> 2023-05-23 16:49:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Fine-tuning on instruction data has been widely validated as an effective
practice for implementing chat language models like ChatGPT. Scaling the
diversity and quality of such data, although straightforward, stands a great
chance of leading to improved performance. This paper aims to improve the upper
bound of open-source models further. We first provide a systematically
designed, diverse, informative, large-scale dataset of instructional
conversations, UltraChat, which does not involve human queries. Our objective
is to capture the breadth of interactions that a human might have with an AI
assistant and employs a comprehensive framework to generate multi-turn
conversation iteratively. UltraChat contains 1.5 million high-quality
multi-turn dialogues and covers a wide range of topics and instructions. Our
statistical analysis of UltraChat reveals its superiority in various key
metrics, including scale, average length, diversity, coherence, etc.,
solidifying its position as a leading open-source dataset. Building upon
UltraChat, we fine-tune a LLaMA model to create a powerful conversational
model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently
outperforms other open-source models, including Vicuna, the previously
recognized state-of-the-art open-source model. The dataset and the model will
be publicly released\footnote{\url{https://github.com/thunlp/UltraChat}}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14233v1' target="_blank">http://arxiv.org/pdf/2305.14233v1</a><br> <br> <br> <font size='5'> 265 </font> <div style="text-align: right"> 2023-05-23 16:15:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test sets, without training
or development data. We adapt six tasks from the SCROLLS benchmark, and add
four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard </font><br> Link: <a href='http://arxiv.org/pdf/2305.14196v1' target="_blank">http://arxiv.org/pdf/2305.14196v1</a><br> <br> <br> <font size='5'> 266 </font> <div style="text-align: right"> 2023-05-23 14:16:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Revisiting Acceptability Judgements</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Years have passed since the NLP community has last focused on linguistic
acceptability. In this work, we revisit this topic in the context of large
language models. We introduce CoLAC - Corpus of Linguistic Acceptability in
Chinese, the first large-scale non-English acceptability dataset that is
verified by native speakers and comes with two sets of labels. Our experiments
show that even the largest InstructGPT model performs only at chance level on
CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised
models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer
experiments and fine-grained linguistic analysis, we demonstrate for the first
time that knowledge of linguistic acceptability can be transferred across
typologically distinct languages, as well as be traced back to pre-training. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14091v2' target="_blank">http://arxiv.org/pdf/2305.14091v2</a><br> <br> <br> <font size='5'> 267 </font> <div style="text-align: right"> 2023-05-23 13:14:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, launched in November 2022, has gained widespread attention from
students and educators globally, with an online report by Hu (2023) stating it
as the fastest-growing consumer application in history. While discussions on
the use of ChatGPT in higher education are abundant, empirical studies on its
impact on collaborative interdisciplinary learning are rare. To investigate its
potential, we conducted a quasi-experimental study with 130 undergraduate
students (STEM and non-STEM) learning digital literacy with or without ChatGPT
over two weeks. Weekly surveys were conducted on collaborative
interdisciplinary problem-solving, physical and cognitive engagement, and
individual reflections on ChatGPT use. Analysis of survey responses showed
significant main effects of topics on collaborative interdisciplinary
problem-solving and physical and cognitive engagement, a marginal interaction
effect between disciplinary backgrounds and ChatGPT conditions for cognitive
engagement, and a significant interaction effect for physical engagement.
Sentiment analysis of student reflections suggested no significant difference
between STEM and non-STEM students' opinions towards ChatGPT. Qualitative
analysis of reflections generated eight positive themes, including efficiency,
addressing knowledge gaps, and generating human-like responses, and eight
negative themes, including generic responses, lack of innovation, and
counterproductive to self-discipline and thinking. Our findings suggest that
ChatGPT use needs to be optimized by considering the topics being taught and
the disciplinary backgrounds of students rather than applying it uniformly.
These findings have implications for both pedagogical research and practices. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18616v1' target="_blank">http://arxiv.org/pdf/2305.18616v1</a><br> <br> <br> <font size='5'> 268 </font> <div style="text-align: right"> 2023-05-23 12:55:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does ChatGPT have Theory of Mind?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ``Theory of Mind" (ToM) is the ability to understand human thinking and
decision-making, an ability that plays a crucial role in many types of social
interaction between people, including linguistic communication. This paper
investigates to what extent recent Large Language Models in the ChatGPT
tradition possess ToM. Focussing on six well-known ToM problems, we posed each
problem to two versions of ChatGPT and compared the results under a range of
prompting strategies. While the results concerning ChatGPT-3 were somewhat
inconclusive, ChatGPT-4 was shown to arrive at the correct answers more often
than would be expected based on chance, although correct answers were often
arrived at on the basis of false assumptions or invalid reasoning. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14020v1' target="_blank">http://arxiv.org/pdf/2305.14020v1</a><br> <br> <br> <font size='5'> 269 </font> <div style="text-align: right"> 2023-05-23 12:54:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChipGPT: How far are we from natural language hardware design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As large language models (LLMs) like ChatGPT exhibited unprecedented machine
intelligence, it also shows great performance in assisting hardware engineers
to realize higher-efficiency logic design via natural language interaction. To
estimate the potential of the hardware design process assisted by LLMs, this
work attempts to demonstrate an automated design environment that explores LLMs
to generate hardware logic designs from natural language specifications. To
realize a more accessible and efficient chip development flow, we present a
scalable four-stage zero-code logic design framework based on LLMs without
retraining or finetuning. At first, the demo, ChipGPT, begins by generating
prompts for the LLM, which then produces initial Verilog programs. Second, an
output manager corrects and optimizes these programs before collecting them
into the final design space. Eventually, ChipGPT will search through this space
to select the optimal design under the target metrics. The evaluation sheds
some light on whether LLMs can generate correct and complete hardware logic
designs described by natural language for some specifications. It is shown that
ChipGPT improves programmability, and controllability, and shows broader design
optimization space compared to prior work and native LLMs alone. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14019v3' target="_blank">http://arxiv.org/pdf/2305.14019v3</a><br> <br> <br> <font size='5'> 270 </font> <div style="text-align: right"> 2023-05-23 09:33:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential
but also introduce challenges related to content constraints and potential
misuse. Our study investigates three key research questions: (1) the number of
different prompt types that can jailbreak LLMs, (2) the effectiveness of
jailbreak prompts in circumventing LLM constraints, and (3) the resilience of
ChatGPT against these jailbreak prompts. Initially, we develop a classification
model to analyze the distribution of existing prompts, identifying ten distinct
patterns and three categories of jailbreak prompts. Subsequently, we assess the
jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a
dataset of 3,120 jailbreak questions across eight prohibited scenarios.
Finally, we evaluate the resistance of ChatGPT against jailbreak prompts,
finding that the prompts can consistently evade the restrictions in 40 use-case
scenarios. The study underscores the importance of prompt structures in
jailbreaking LLMs and discusses the challenges of robust jailbreak prompt
generation and prevention. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13860v1' target="_blank">http://arxiv.org/pdf/2305.13860v1</a><br> <br> <br> <font size='5'> 271 </font> <div style="text-align: right"> 2023-05-23 06:41:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Aligning Large Language Models through Synthetic Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Aligning large language models (LLMs) to human values has become increasingly
important as it enables sophisticated steering of LLMs, e.g., making them
follow given instructions while keeping them less toxic. However, it requires a
significant amount of human demonstrations and feedback. Recently, open-sourced
models have attempted to replicate the alignment learning process by distilling
data from already aligned LLMs like InstructGPT or ChatGPT. While this process
reduces human efforts, constructing these datasets has a heavy dependency on
the teacher models. In this work, we propose a novel framework for alignment
learning with almost no human labor and no dependency on pre-aligned LLMs.
First, we perform reward modeling (RM) with synthetic feedback by contrasting
responses from vanilla LLMs with various sizes and prompts. Then, we use the RM
for simulating high-quality demonstrations to train a supervised policy and for
further optimizing the model with reinforcement learning. Our resulting model,
Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms
open-sourced models, including Alpaca, Dolly, and OpenAssistant, which are
trained on the outputs of InstructGPT or human-annotated instructions. Our
7B-sized model outperforms the 12-13B models in the A/B tests using GPT-4 as
the judge with about 75% winning rate on average. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13735v1' target="_blank">http://arxiv.org/pdf/2305.13735v1</a><br> <br> <br> <font size='5'> 272 </font> <div style="text-align: right"> 2023-05-23 06:19:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS)
method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that
can deeply understand the content and purpose of an input prompt and
appropriately respond to the user's request. We focus on ChatGPT's reading
comprehension and introduce it to EDSS, a task of synthesizing speech that can
empathize with the interlocutor's emotion. Our method first gives chat history
to ChatGPT and asks it to generate three words representing the intention,
emotion, and speaking style for each line in the chat. Then, it trains an EDSS
model using the embeddings of ChatGPT-derived context words as the conditioning
features. The experimental results demonstrate that our method performs
comparably to ones using emotion labels or neural network-derived context
embeddings learned from chat histories. The collected ChatGPT-derived context
information is available at
https://sarulab-speech.github.io/demo_ChatGPT_EDSS/. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13724v1' target="_blank">http://arxiv.org/pdf/2305.13724v1</a><br> <br> <br> <font size='5'> 273 </font> <div style="text-align: right"> 2023-05-23 04:38:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT, Can You Generate Solutions for my Coding Exercises? An Evaluation on its Effectiveness in an undergraduate Java Programming Course</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this study, we assess the efficacy of employing the ChatGPT language model
to generate solutions for coding exercises within an undergraduate Java
programming course. ChatGPT, a large-scale, deep learning-driven natural
language processing model, is capable of producing programming code based on
textual input. Our evaluation involves analyzing ChatGPT-generated solutions
for 80 diverse programming exercises and comparing them to the correct
solutions. Our findings indicate that ChatGPT accurately generates Java
programming solutions, which are characterized by high readability and
well-structured organization. Additionally, the model can produce alternative,
memory-efficient solutions. However, as a natural language processing model,
ChatGPT struggles with coding exercises containing non-textual descriptions or
class files, leading to invalid solutions. In conclusion, ChatGPT holds
potential as a valuable tool for students seeking to overcome programming
challenges and explore alternative approaches to solving coding problems. By
understanding its limitations, educators can design coding exercises that
minimize the potential for misuse as a cheating aid while maintaining their
validity as assessment tools. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13680v1' target="_blank">http://arxiv.org/pdf/2305.13680v1</a><br> <br> <br> <font size='5'> 274 </font> <div style="text-align: right"> 2023-05-23 04:07:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often require
abundant annotated data, which creates challenges when faced with noisy
annotations or low-resource settings. We introduce GDP-Zero, an approach using
Open-Loop MCTS to perform goal-oriented dialogue policy planning without any
model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13660v1' target="_blank">http://arxiv.org/pdf/2305.13660v1</a><br> <br> <br> <font size='5'> 275 </font> <div style="text-align: right"> 2023-05-23 04:00:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as your Personal Data Scientist</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of big data has amplified the need for efficient, user-friendly
automated machine learning (AutoML) tools. However, the intricacy of
understanding domain-specific data and defining prediction tasks necessitates
human intervention making the process time-consuming while preventing full
automation. Instead, envision an intelligent agent capable of assisting users
in conducting AutoML tasks through intuitive, natural conversations without
requiring in-depth knowledge of the underlying machine learning (ML) processes.
This agent's key challenge is to accurately comprehend the user's prediction
goals and, consequently, formulate precise ML tasks, adjust data sets and model
parameters accordingly, and articulate results effectively. In this paper, we
take a pioneering step towards this ambitious goal by introducing a
ChatGPT-based conversational data-science framework to act as a "personal data
scientist". Precisely, we utilize Large Language Models (ChatGPT) to build a
natural interface between the users and the ML models (Scikit-Learn), which in
turn, allows us to approach this ambitious problem with a realistic solution.
  Our model pivots around four dialogue states: Data Visualization, Task
Formulation, Prediction Engineering, and Result Summary and Recommendation.
Each state marks a unique conversation phase, impacting the overall user-system
interaction. Multiple LLM instances, serving as "micro-agents", ensure a
cohesive conversation flow, granting us granular control over the
conversation's progression. In summary, we developed an end-to-end system that
not only proves the viability of the novel concept of conversational data
science but also underscores the potency of LLMs in solving complex tasks.
Interestingly, its development spotlighted several critical weaknesses in the
current LLMs (ChatGPT) and highlighted substantial opportunities for
improvement. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13657v1' target="_blank">http://arxiv.org/pdf/2305.13657v1</a><br> <br> <br> <font size='5'> 276 </font> <div style="text-align: right"> 2023-05-23 02:49:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversational systems based on Large Language Models (LLMs), such as
ChatGPT, show exceptional proficiency in context understanding and response
generation. However, despite their impressive capabilities, they still possess
limitations, such as providing randomly-guessed answers to ambiguous queries or
failing to refuse users' requests, both of which are considered aspects of a
conversational agent's proactivity. This raises the question of whether
LLM-based conversational systems are equipped to handle proactive dialogue
problems. In this work, we conduct a comprehensive analysis of LLM-based
conversational systems, specifically focusing on three aspects of proactive
dialogue systems: clarification, target-guided, and non-collaborative
dialogues. To trigger the proactivity of LLMs, we propose the Proactive
Chain-of-Thought prompting scheme, which augments LLMs with the goal planning
capability over descriptive reasoning chains. Empirical findings are discussed
to promote future studies on LLM-based proactive dialogue systems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13626v1' target="_blank">http://arxiv.org/pdf/2305.13626v1</a><br> <br> <br> <font size='5'> 277 </font> <div style="text-align: right"> 2023-05-23 02:25:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13614v1' target="_blank">http://arxiv.org/pdf/2305.13614v1</a><br> <br> <br> <font size='5'> 278 </font> <div style="text-align: right"> 2023-05-22 23:14:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Language Model Hallucinations Can Snowball</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A major risk of using language models in practical applications is their
tendency to hallucinate incorrect statements. Hallucinations are often
attributed to knowledge gaps in LMs, but we hypothesize that in some cases,
when justifying previously generated hallucinations, LMs output false claims
that they can separately recognize as incorrect. We construct three
question-answering datasets where ChatGPT and GPT-4 often state an incorrect
answer and offer an explanation with at least one incorrect claim. Crucially,
we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes,
respectively. We refer to this phenomenon as hallucination snowballing: an LM
over-commits to early mistakes, leading to more mistakes that it otherwise
would not make. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13534v1' target="_blank">http://arxiv.org/pdf/2305.13534v1</a><br> <br> <br> <font size='5'> 279 </font> <div style="text-align: right"> 2023-05-22 22:37:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Study of Generative Large Language Model for Medical Research and Healthcare</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is enormous enthusiasm and concerns in using large language models
(LLMs) in healthcare, yet current assumptions are all based on general-purpose
LLMs such as ChatGPT. This study develops a clinical generative LLM,
GatorTronGPT, using 277 billion words of mixed clinical and English text with a
GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical
natural language processing for medical research. Synthetic NLP models trained
using GatorTronGPT generated text outperform NLP models trained using
real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)
scale shows that there is no significant difference in linguistic readability
(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical
relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that
physicians cannot differentiate them (p < 0.001). This study provides insights
on the opportunities and challenges of LLMs for medical research and
healthcare. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13523v1' target="_blank">http://arxiv.org/pdf/2305.13523v1</a><br> <br> <br> <font size='5'> 280 </font> <div style="text-align: right"> 2023-05-22 21:59:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13512v1' target="_blank">http://arxiv.org/pdf/2305.13512v1</a><br> <br> <br> <font size='5'> 281 </font> <div style="text-align: right"> 2023-05-22 18:03:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can LLMs facilitate interpretation of pre-trained language models?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Work done to uncover the knowledge encoded within pre-trained language
models, rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
hierarchical clustering over contextualized representations and then annotate
these concepts using GPT annotations. Our findings demonstrate that ChatGPT
produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing framework and neuron interpretation. To facilitate further exploration
and experimentation in this field, we have made available a substantial
ConceptNet dataset comprising 39,000 annotated latent concepts. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13386v1' target="_blank">http://arxiv.org/pdf/2305.13386v1</a><br> <br> <br> <font size='5'> 282 </font> <div style="text-align: right"> 2023-05-22 17:58:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The fixed-size context of Transformer makes GPT models incapable of
generating arbitrarily long text. In this paper, we introduce RecurrentGPT, a
language-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT is
built upon a large language model (LLM) such as ChatGPT and uses natural
language to simulate the Long Short-Term Memory mechanism in an LSTM. At each
timestep, RecurrentGPT generates a paragraph of text and updates its
language-based long-short term memory stored on the hard drive and the prompt,
respectively. This recurrence mechanism enables RecurrentGPT to generate texts
of arbitrary length without forgetting. Since human users can easily observe
and edit the natural language memories, RecurrentGPT is interpretable and
enables interactive generation of long text. RecurrentGPT is an initial step
towards next-generation computer-assisted writing systems beyond local editing
suggestions. In addition to producing AI-generated content (AIGC), we also
demonstrate the possibility of using RecurrentGPT as an interactive fiction
that directly interacts with consumers. We call this usage of generative models
by ``AI As Contents'' (AIAC), which we believe is the next form of conventional
AIGC. We further demonstrate the possibility of using RecurrentGPT to create
personalized interactive fiction that directly interacts with readers instead
of interacting with writers. More broadly, RecurrentGPT demonstrates the
utility of borrowing ideas from popular model designs in cognitive science and
deep learning for prompting LLMs. Our code is available at
https://github.com/aiwaves-cn/RecurrentGPT and an online demo is available at
https://www.aiwaves.org/recurrentgpt. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13304v1' target="_blank">http://arxiv.org/pdf/2305.13304v1</a><br> <br> <br> <font size='5'> 283 </font> <div style="text-align: right"> 2023-05-22 17:55:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm. </font><br> Link: <a href='http://arxiv.org/pdf/2305.14387v1' target="_blank">http://arxiv.org/pdf/2305.14387v1</a><br> <br> <br> <font size='5'> 284 </font> <div style="text-align: right"> 2023-05-22 17:51:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fairness of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Understanding and addressing unfairness in LLMs are crucial for responsible
AI deployment. However, there is a limited availability of quantitative
analyses and in-depth studies regarding fairness evaluations in LLMs,
especially when applying LLMs to high-stakes fields. This work aims to fill
this gap by providing a systematic evaluation of the effectiveness and fairness
of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's
performance in high-takes fields including education, criminology, finance and
healthcare. To make thorough evaluation, we consider both group fairness and
individual fairness and we also observe the disparities in ChatGPT's outputs
under a set of biased or unbiased prompts. This work contributes to a deeper
understanding of LLMs' fairness performance, facilitates bias mitigation and
fosters the development of responsible artificial intelligence systems. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18569v1' target="_blank">http://arxiv.org/pdf/2305.18569v1</a><br> <br> <br> <font size='5'> 285 </font> <div style="text-align: right"> 2023-05-22 17:36:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13276v2' target="_blank">http://arxiv.org/pdf/2305.13276v2</a><br> <br> <br> <font size='5'> 286 </font> <div style="text-align: right"> 2023-05-22 17:28:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Watermarking Text Data on Large Language Models for Dataset Copyright Protection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as BERT and GPT-based models like ChatGPT,
have recently demonstrated their impressive capacity for learning language
representations, yielding significant benefits for various downstream Natural
Language Processing (NLP) tasks. However, the immense data requirements of
these large models have incited substantial concerns regarding copyright
protection and data privacy. In an attempt to address these issues,
particularly the unauthorized use of private data in LLMs, we introduce a novel
watermarking technique via a backdoor-based membership inference approach,
i.e., TextMarker, which can safeguard diverse forms of private information
embedded in the training text data in LLMs. Specifically, TextMarker is a new
membership inference framework that can eliminate the necessity for additional
proxy data and surrogate model training, which are common in traditional
membership inference techniques, thereby rendering our proposal significantly
more practical and applicable. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13257v1' target="_blank">http://arxiv.org/pdf/2305.13257v1</a><br> <br> <br> <font size='5'> 287 </font> <div style="text-align: right"> 2023-05-22 17:13:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chip-Chat: Challenges and Opportunities in Conversational Hardware Design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Modern hardware design starts with specifications provided in natural
language. These are then translated by hardware engineers into appropriate
Hardware Description Languages (HDLs) such as Verilog before synthesizing
circuit elements. Automating this translation could reduce sources of human
error from the engineering process. But, it is only recently that artificial
intelligence (AI) has demonstrated capabilities for machine-based end-to-end
design translations. Commercially-available instruction-tuned Large Language
Models (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to
produce code in a variety of programming languages; but studies examining them
for hardware are still lacking. In this work, we thus explore the challenges
faced and opportunities presented when leveraging these recent advances in LLMs
for hardware design. Using a suite of 8 representative benchmarks, we examined
the capabilities and limitations of the state of the art conversational LLMs
when producing Verilog for functional and verification purposes. Given that the
LLMs performed best when used interactively, we then performed a longer fully
conversational case study where a hardware engineer co-designed a novel 8-bit
accumulator-based microprocessor architecture. We sent the benchmarks and
processor to tapeout in a Skywater 130nm shuttle, meaning that these
'Chip-Chats' resulted in what we believe to be the world's first
wholly-AI-written HDL for tapeout. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13243v1' target="_blank">http://arxiv.org/pdf/2305.13243v1</a><br> <br> <br> <font size='5'> 288 </font> <div style="text-align: right"> 2023-05-22 17:11:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Dimensions of Data Labor: A Road Map for Researchers, Activists, and Policymakers to Empower Data Producers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many recent technological advances (e.g. ChatGPT and search engines) are
possible only because of massive amounts of user-generated data produced
through user interactions with computing systems or scraped from the web (e.g.
behavior logs, user-generated content, and artwork). However, data producers
have little say in what data is captured, how it is used, or who it benefits.
Organizations with the ability to access and process this data, e.g. OpenAI and
Google, possess immense power in shaping the technology landscape. By
synthesizing related literature that reconceptualizes the production of data
for computing as ``data labor'', we outline opportunities for researchers,
policymakers, and activists to empower data producers in their relationship
with tech companies, e.g advocating for transparency about data reuse, creating
feedback channels between data producers and companies, and potentially
developing mechanisms to share data's revenue more broadly. In doing so, we
characterize data labor with six important dimensions - legibility, end-use
awareness, collaboration requirement, openness, replaceability, and livelihood
overlap - based on the parallels between data labor and various other types of
labor in the computing literature. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13238v1' target="_blank">http://arxiv.org/pdf/2305.13238v1</a><br> <br> <br> <font size='5'> 289 </font> <div style="text-align: right"> 2023-05-22 16:56:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT and GPT-4 have attracted substantial interest from both academic and
industrial circles, owing to their remarkable few-shot (or even zero-shot)
ability to handle various tasks. Recent work shows that, after being fine-tuned
with a few sets of instruction-driven data, the recently proposed LLM, LLaMa,
exhibits an impressive capability to address a broad range of tasks. However,
the zero-shot performance of LLMs does not consistently outperform that of
models fined-tuned for specific scenarios. To explore whether the capabilities
of LLMs can be further enhanced for specific scenarios, we choose the
writing-assistance scenario as the testbed, including seven writing tasks. We
collect training data for these tasks, reframe them in an instruction-following
format, and subsequently refine LLaMa via instruction tuning. Experimental
results show that continually fine-tuning LLaMa on writing instruction data
significantly improves its ability on writing tasks. We also conduct more
experiments and analyses to offer insights for future work on effectively
fine-tuning LLaMa for specific scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13225v1' target="_blank">http://arxiv.org/pdf/2305.13225v1</a><br> <br> <br> <font size='5'> 290 </font> <div style="text-align: right"> 2023-05-22 15:56:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents an exhaustive quantitative and qualitative evaluation of
Large Language Models (LLMs) for Knowledge Graph (KG) construction and
reasoning. We employ eight distinct datasets that encompass aspects including
entity, relation and event extraction, link prediction, and question answering.
Empirically, our findings suggest that GPT-4 outperforms ChatGPT in the
majority of tasks and even surpasses fine-tuned models in certain reasoning and
question-answering datasets. Moreover, our investigation extends to the
potential generalization ability of LLMs for information extraction, which
culminates in the presentation of the Virtual Knowledge Extraction task and the
development of the VINE dataset. Drawing on these empirical findings, we
further propose AutoKG, a multi-agent-based approach employing LLMs for KG
construction and reasoning, which aims to chart the future of this field and
offer exciting opportunities for advancement. We anticipate that our research
can provide invaluable insights for future undertakings of KG\footnote{Code and
datasets will be available in https://github.com/zjunlp/AutoKG. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13168v1' target="_blank">http://arxiv.org/pdf/2305.13168v1</a><br> <br> <br> <font size='5'> 291 </font> <div style="text-align: right"> 2023-05-22 15:47:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Defend the Truth? Automatic Dialectical Evaluation Elicits LLMs' Deficiencies in Reasoning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We explore testing the reasoning ability of large language models (LLMs),
such as ChatGPT, by engaging with them in a debate-like conversation that
probes deeper into their understanding of the subject. Specifically, we
formulate a new task where given a question, the LLM can generate a correct
solution while the user believes in a wrong solution in the beginning, and they
need to discuss to make the correct decision through dialogue. Such a setting
requires the LLM to not only achieve the correct answer on its own (which could
be done by shallow memorization), but also be able to defend the truth instead
of blindly believing or getting misled by the user's (invalid) arguments and
critiques, thus testing in greater depth whether the LLM grasps the essence of
the reasoning required to solve the problem. To automate this evaluation
framework and save human labor, we simulate the user using another LLM
conditioned on a synthesized wrong solution. Across a range of complex
reasoning benchmarks spanning math, commonsense, logic and tasks from
BIG-Bench, we find that despite being able to generate correct step-by-step
solutions in the beginning, ChatGPT cannot maintain its belief in truth for a
significant portion of examples when challenged by often-time absurdly invalid
arguments. Our work reveals LLMs' weaknesses not captured by conventional
benchmarking, and also points to danger zones of aligning models with human
feedback. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13160v1' target="_blank">http://arxiv.org/pdf/2305.13160v1</a><br> <br> <br> <font size='5'> 292 </font> <div style="text-align: right"> 2023-05-22 15:13:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Understanding user perspectives on Artificial Intelligence (AI) in education
is essential for creating pedagogically effective and ethically responsible
AI-integrated learning environments. In this paper, we conduct an extensive
qualitative content analysis of four major social media platforms (Twitter,
Reddit, YouTube, and LinkedIn) to explore the user experience (UX) and
perspectives of early adopters toward ChatGPT-an AI Chatbot technology-in
various education sectors. We investigate the primary applications of ChatGPT
in education (RQ1) and the various perceptions of the technology (RQ2). Our
findings indicate that ChatGPT is most popularly used in the contexts of higher
education (24.18%), K-12 education (22.09%), and practical-skills learning
(15.28%). On social media platforms, the most frequently discussed topics about
ChatGPT are productivity, efficiency, and ethics. While some early adopters
lean toward seeing ChatGPT as a revolutionary technology with the potential to
boost students' self-efficacy and motivation to learn, others express concern
that overreliance on the AI system may promote superficial learning habits and
erode students' social and critical thinking skills. Our study contributes to
the broader discourse on Human-AI Interaction and offers recommendations based
on crowd-sourced knowledge for educators and learners interested in
incorporating ChatGPT into their educational settings. Furthermore, we propose
a research agenda for future studies that sets the foundation for continued
investigation into the application of ChatGPT in education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13114v2' target="_blank">http://arxiv.org/pdf/2305.13114v2</a><br> <br> <br> <font size='5'> 293 </font> <div style="text-align: right"> 2023-05-22 15:12:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent success of large language models (LLMs) has shown great potential
to develop more powerful conversational recommender systems (CRSs), which rely
on natural language conversations to satisfy user needs. In this paper, we
embark on an investigation into the utilization of ChatGPT for conversational
recommendation, revealing the inadequacy of the existing evaluation protocol.
It might over-emphasize the matching with the ground-truth items or utterances
generated by human annotators, while neglecting the interactive nature of being
a capable CRS. To overcome the limitation, we further propose an interactive
Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user
simulators. Our evaluation approach can simulate various interaction scenarios
between users and systems. Through the experiments on two publicly available
CRS datasets, we demonstrate notable improvements compared to the prevailing
evaluation protocol. Furthermore, we emphasize the evaluation of
explainability, and ChatGPT showcases persuasive explanation generation for its
recommendations. Our study contributes to a deeper comprehension of the
untapped potential of LLMs for CRSs and provides a more flexible and
easy-to-use evaluation framework for future research endeavors. The codes and
data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13112v1' target="_blank">http://arxiv.org/pdf/2305.13112v1</a><br> <br> <br> <font size='5'> 294 </font> <div style="text-align: right"> 2023-05-22 15:06:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models are becoming increasingly integrated into our lives.
Hence, it is important to understand the biases present in their outputs in
order to avoid perpetuating harmful stereotypes, which originate in our own
flawed ways of thinking. This challenge requires developing new benchmarks and
methods for quantifying affective and semantic bias, keeping in mind that LLMs
act as psycho-social mirrors that reflect the views and tendencies that are
prevalent in society. One such tendency that has harmful negative effects is
the global phenomenon of anxiety toward math and STEM subjects. Here, we
investigate perceptions of math and STEM fields provided by cutting-edge
language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach
from network science and cognitive psychology. Specifically, we use behavioral
forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM
disciplines in relation to other concepts. We use data obtained by probing the
three LLMs in a language generation task that has previously been applied to
humans. Our findings indicate that LLMs have an overall negative perception of
math and STEM fields, with math being perceived most negatively. We observe
significant differences across the three LLMs. We observe that newer versions
(i.e. GPT-4) produce richer, more complex perceptions as well as less negative
perceptions compared to older versions and N=159 high-school students. These
findings suggest that advances in the architecture of LLMs may lead to
increasingly less biased models that could even perhaps someday aid in reducing
harmful stereotypes in society rather than perpetuating them. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18320v1' target="_blank">http://arxiv.org/pdf/2305.18320v1</a><br> <br> <br> <font size='5'> 295 </font> <div style="text-align: right"> 2023-05-22 15:04:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Observations on LLMs for Telecom Domain: Capabilities and Limitations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The landscape for building conversational interfaces (chatbots) has witnessed
a paradigm shift with recent developments in generative Artificial Intelligence
(AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and
GPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In
this paper, we analyze capabilities and limitations of incorporating such
models in conversational interfaces for the telecommunication domain,
specifically for enterprise wireless products and services. Using Cradlepoint's
publicly available data for our experiments, we present a comparative analysis
of the responses from such models for multiple use-cases including domain
adaptation for terminology and product taxonomy, context continuity, robustness
to input perturbations and errors. We believe this evaluation would provide
useful insights to data scientists engaged in building customized
conversational interfaces for domain-specific requirements. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13102v1' target="_blank">http://arxiv.org/pdf/2305.13102v1</a><br> <br> <br> <font size='5'> 296 </font> <div style="text-align: right"> 2023-05-22 13:56:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automated stance detection and related machine learning methods can provide
useful insights for media monitoring and academic research. Many of these
approaches require annotated training datasets, which limits their
applicability for languages where these may not be readily available. This
paper explores the applicability of large language models for automated stance
detection in a challenging scenario, involving a morphologically complex,
lower-resource language, and a socio-culturally complex topic, immigration. If
the approach works in this case, it can be expected to perform as well or
better in less demanding scenarios. We annotate a large set of pro and
anti-immigration examples, and compare the performance of multiple language
models as supervised learners. We also probe the usability of ChatGPT as an
instructable zero-shot classifier for the same task. Supervised achieves
acceptable performance, and ChatGPT yields similar accuracy. This is promising
as a potentially simpler and cheaper alternative for text classification tasks,
including in lower-resource languages. We further use the best-performing model
to investigate diachronic trends over seven years in two corpora of Estonian
mainstream and right-wing populist news sources, demonstrating the
applicability of the approach for news analytics and media monitoring settings,
and discuss correspondences between stance changes and real-world events. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13047v1' target="_blank">http://arxiv.org/pdf/2305.13047v1</a><br> <br> <br> <font size='5'> 297 </font> <div style="text-align: right"> 2023-05-22 13:47:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Task-oriented dialogue (TOD) models have made significant progress in recent
years. However, previous studies primarily focus on datasets written by
annotators, which has resulted in a gap between academic research and
real-world spoken conversation scenarios. While several small-scale spoken TOD
datasets are proposed to address robustness issues such as ASR errors, they
ignore the unique challenges in spoken conversation. To tackle the limitations,
we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,
containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from
human-to-human spoken conversations. SpokenWOZ further incorporates common
spoken characteristics such as word-by-word processing and reasoning in spoken
language. Based on these characteristics, we present cross-turn slot and
reasoning slot detection as new challenges. We conduct experiments on various
baselines, including text-modal models, newly proposed dual-modal models, and
LLMs, e.g., ChatGPT. The results show that the current models still have
substantial room for improvement in spoken conversation, where the most
advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and
the SOTA end-to-end model only correctly completes the user request in 52.1% of
dialogues. The dataset, code, and leaderboard are available:
https://spokenwoz.github.io/SpokenWOZ-github.io/. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13040v2' target="_blank">http://arxiv.org/pdf/2305.13040v2</a><br> <br> <br> <font size='5'> 298 </font> <div style="text-align: right"> 2023-05-22 12:11:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Distilling ChatGPT for Explainable Automated Student Answer Assessment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Assessing student answers and providing valuable feedback is crucial for
effective learning, but it can be a time-consuming task. Traditional methods of
automating student answer assessment through text classification often suffer
from issues such as lack of trustworthiness, transparency, and the ability to
provide a rationale for the automated assessment process. These limitations
hinder their usefulness in practice. In this paper, we explore using ChatGPT, a
cutting-edge large language model, for the concurrent tasks of student answer
scoring and rationale generation under both the zero-shot and few-shot
settings. We introduce a critic module which automatically filters incorrect
outputs from ChatGPT and utilizes the remaining ChtaGPT outputs as noisy
labelled data to fine-tune a smaller language model, enabling it to perform
student answer scoring and rationale generation. Moreover, by drawing multiple
samples from ChatGPT outputs, we are able to compute predictive confidence
scores, which in turn can be used to identify corrupted data and human label
errors in the training set. Our experimental results demonstrate that despite
being a few orders of magnitude smaller than ChatGPT, the fine-tuned language
model achieves better performance in student answer scoring. Furthermore, it
generates more detailed and comprehensible assessments than traditional text
classification methods. Our approach provides a viable solution to achieve
explainable automated assessment in education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12962v1' target="_blank">http://arxiv.org/pdf/2305.12962v1</a><br> <br> <br> <font size='5'> 299 </font> <div style="text-align: right"> 2023-05-22 11:46:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of generative large language models (LLMs) raises the question:
what will be its impact on crowdsourcing. Traditionally, crowdsourcing has been
used for acquiring solutions to a wide variety of human-intelligence tasks,
including ones involving text generation, manipulation or evaluation. For some
of these tasks, models like ChatGPT can potentially substitute human workers.
In this study, we investigate, whether this is the case for the task of
paraphrase generation for intent classification. We quasi-replicated the data
collection methodology of an existing crowdsourcing study (similar scale,
prompts and seed data) using ChatGPT. We show that ChatGPT-created paraphrases
are more diverse and lead to more robust models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12947v1' target="_blank">http://arxiv.org/pdf/2305.12947v1</a><br> <br> <br> <font size='5'> 300 </font> <div style="text-align: right"> 2023-05-22 11:45:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As ChatGPT and GPT-4 spearhead the development of Large Language Models
(LLMs), more researchers are investigating their performance across various
tasks. But more research needs to be done on the interpretability capabilities
of LLMs, that is, the ability to generate reasons after an answer has been
given. Existing explanation datasets are mostly English-language general
knowledge questions, which leads to insufficient thematic and linguistic
diversity. To address the language bias and lack of medical resources in
generating rationales QA datasets, we present ExplainCPE (over 7k instances), a
challenging medical benchmark in Simplified Chinese. We analyzed the errors of
ChatGPT and GPT-4, pointing out the limitations of current LLMs in
understanding text and computational reasoning. During the experiment, we also
found that different LLMs have different preferences for in-context learning.
ExplainCPE presents a significant challenge, but its potential for further
investigation is promising, and it can be used to evaluate the ability of a
model to generate explanations. AI safety and trustworthiness need more
attention, and this work makes the first step to explore the medical
interpretability of LLMs.The dataset is available at
https://github.com/HITsz-TMG/ExplainCPE. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12945v1' target="_blank">http://arxiv.org/pdf/2305.12945v1</a><br> <br> <br> <font size='5'> 301 </font> <div style="text-align: right"> 2023-05-22 09:49:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Lion: Adversarial Distillation of Closed-Source Large Language Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The practice of transferring knowledge from a sophisticated, closed-source
large language model (LLM) to a compact, open-source LLM has garnered
considerable attention. Previous works have focused on a unidirectional
knowledge distillation way by aligning the responses of the student model with
those of the teacher model to a set of instructions. Nevertheless, they
overlooked the possibility of incorporating any reciprocal
"feedback"--identifying challenging instructions where the student model's
performance falls short--to boost the student model's proficiency iteratively.
To this end, we propose a novel adversarial distillation framework for a more
efficient knowledge transfer. Leveraging the versatile role adaptability of
LLMs, we prompt the closed-source model to identify "hard" instructions and
generate new "hard" instructions for the student model, creating a three-stage
adversarial loop of imitation, discrimination, and generation. By applying this
adversarial framework, we successfully transfer knowledge from ChatGPT to a 7B
student model (named Lion), achieving nearly 95% capability approximation using
a mere 70k training data. We aspire that this proposed model may serve as the
baseline to reflect the performance of ChatGPT, especially the open-source
instruction-following language model baseline for our community. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12870v1' target="_blank">http://arxiv.org/pdf/2305.12870v1</a><br> <br> <br> <font size='5'> 302 </font> <div style="text-align: right"> 2023-05-22 09:43:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Automatic Code Summarization via ChatGPT: How Far Are We?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: To support software developers in understanding and maintaining programs,
various automatic code summarization techniques have been proposed to generate
a concise natural language comment for a given code snippet. Recently, the
emergence of large language models (LLMs) has led to a great boost in the
performance of natural language processing tasks. Among them, ChatGPT is the
most popular one which has attracted wide attention from the software
engineering community. However, it still remains unclear how ChatGPT performs
in (automatic) code summarization. Therefore, in this paper, we focus on
evaluating ChatGPT on a widely-used Python dataset called CSN-Python and
comparing it with several state-of-the-art (SOTA) code summarization models.
Specifically, we first explore an appropriate prompt to guide ChatGPT to
generate in-distribution comments. Then, we use such a prompt to ask ChatGPT to
generate comments for all code snippets in the CSN-Python test set. We adopt
three widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the
quality of the comments generated by ChatGPT and SOTA models (including NCS,
CodeBERT, and CodeT5). The experimental results show that in terms of BLEU and
ROUGE-L, ChatGPT's code summarization performance is significantly worse than
all three SOTA models. We also present some cases and discuss the advantages
and disadvantages of ChatGPT in code summarization. Based on the findings, we
outline several open challenges and opportunities in ChatGPT-based code
summarization. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12865v1' target="_blank">http://arxiv.org/pdf/2305.12865v1</a><br> <br> <br> <font size='5'> 303 </font> <div style="text-align: right"> 2023-05-22 08:29:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Combating disinformation is one of the burning societal crises -- about 67%
of the American population believes that disinformation produces a lot of
uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows
that disinformation can manipulate democratic processes and public opinion,
causing disruption in the share market, panic and anxiety in society, and even
death during crises. Therefore, disinformation should be identified promptly
and, if possible, mitigated. With approximately 3.2 billion images and 720,000
hours of video shared online daily on social media platforms, scalable
detection of multimodal disinformation requires efficient fact verification.
Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR),
the research community lacks substantial effort in multimodal fact
verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3
million samples that pushes the boundaries of the domain of fact verification
via a multimodal fake news dataset, in addition to offering explainability
through the concept of 5W question-answering. Salient features of the dataset
include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii)
associated images, (iv) stable diffusion-generated additional images (i.e.,
visual paraphrases), (v) pixel-level image heatmap to foster image-text
explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news
stories. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05523v1' target="_blank">http://arxiv.org/pdf/2306.05523v1</a><br> <br> <br> <font size='5'> 304 </font> <div style="text-align: right"> 2023-05-22 05:59:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Influence of ChatGPT on Artificial Intelligence Related Crypto Assets: Evidence from a Synthetic Control Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The introduction of OpenAI's large language model, ChatGPT, catalyzed
investor attention towards artificial intelligence (AI) technologies, including
AI-related crypto assets not directly related to ChatGPT. Utilizing the
synthetic difference-in-difference methodology, we identify significant
'ChatGPT effects' with returns of AI-related crypto assets experiencing average
returns ranging between 10.7% and 15.6% (35.5% to 41.3%) in the one-month
(two-month) period after the ChatGPT launch. Furthermore, Google search
volumes, a proxy for attention to AI, emerged as critical pricing indicators
for AI-related crypto post-launch. We conclude that investors perceived
AI-assets as possessing heightened potential or value after the launch,
resulting in higher market valuations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12739v1' target="_blank">http://arxiv.org/pdf/2305.12739v1</a><br> <br> <br> <font size='5'> 305 </font> <div style="text-align: right"> 2023-05-22 03:35:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: G3Detector: General GPT-Generated Text Detector</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The burgeoning progress in the field of Large Language Models (LLMs) heralds
significant benefits due to their unparalleled capacities. However, it is
critical to acknowledge the potential misuse of these models, which could give
rise to a spectrum of social and ethical dilemmas. Despite numerous preceding
efforts centered around distinguishing synthetic text, most existing detection
systems fail to identify data synthesized by the latest LLMs, such as ChatGPT
and GPT-4. In response to this challenge, we introduce an unpretentious yet
potent detection approach proficient in identifying synthetic text across a
wide array of fields. Moreover, our detector demonstrates outstanding
performance uniformly across various model architectures and decoding
strategies. It also possesses the capability to identify text generated
utilizing a potent detection-evasion technique. Our comprehensive research
underlines our commitment to boosting the robustness and efficiency of
machine-generated text detection mechanisms, particularly in the context of
swiftly progressing and increasingly adaptive AI technologies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12680v1' target="_blank">http://arxiv.org/pdf/2305.12680v1</a><br> <br> <br> <font size='5'> 306 </font> <div style="text-align: right"> 2023-05-21 20:57:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Is More Likely to Be Perceived as Male Than Female</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate how people perceive ChatGPT, and, in particular, how they
assign human-like attributes such as gender to the chatbot. Across five
pre-registered studies (N = 1,552), we find that people are more likely to
perceive ChatGPT to be male than female. Specifically, people perceive male
gender identity (1) following demonstrations of ChatGPT's core abilities (e.g.,
providing information or summarizing text), (2) in the absence of such
demonstrations, and (3) across different methods of eliciting perceived gender
(using various scales and asking to name ChatGPT). Moreover, we find that this
seemingly default perception of ChatGPT as male can reverse when ChatGPT's
feminine-coded abilities are highlighted (e.g., providing emotional support for
a user). </font><br> Link: <a href='http://arxiv.org/pdf/2305.12564v1' target="_blank">http://arxiv.org/pdf/2305.12564v1</a><br> <br> <br> <font size='5'> 307 </font> <div style="text-align: right"> 2023-05-21 17:31:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SLaDe: A Portable Small Language Model Decompiler for Optimized Assembler</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Decompilation is a well-studied area with numerous high-quality tools
available. These are frequently used for security tasks and to port legacy
code. However, they regularly generate difficult-to-read programs and require a
large amount of engineering effort to support new programming languages and
ISAs. Recent interest in neural approaches has produced portable tools that
generate readable code. However, to-date such techniques are usually restricted
to synthetic programs without optimization, and no models have evaluated their
portability. Furthermore, while the code generated may be more readable, it is
usually incorrect. This paper presents SLaDe, a Small Language model Decompiler
based on a sequence-to-sequence transformer trained over real-world code. We
develop a novel tokenizer and exploit no-dropout training to produce
high-quality code. We utilize type-inference to generate programs that are more
readable and accurate than standard analytic and recent neural approaches.
Unlike standard approaches, SLaDe can infer out-of-context types and unlike
neural approaches, it generates correct code. We evaluate SLaDe on over 4,000
functions from AnghaBench on two ISAs and at two optimizations levels. SLaDe is
up to 6 times more accurate than Ghidra, a state-of-the-art,
industrial-strength decompiler and up to 4 times more accurate than the large
language model ChatGPT and generates significantly more readable code than
both. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12520v1' target="_blank">http://arxiv.org/pdf/2305.12520v1</a><br> <br> <br> <font size='5'> 308 </font> <div style="text-align: right"> 2023-05-21 17:26:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) can generate texts that carry the risk of
various misuses, including plagiarism, planting fake reviews on e-commerce
platforms, or creating fake social media postings that can sway election
results. Detecting whether a text is machine-generated has thus become
increasingly important. While machine-learning-based detection strategies
exhibit superior performance, they often lack generalizability, limiting their
practicality. In this work, we introduce GPT Paternity Test (GPT-Pat), which
reliably detects machine-generated text across varied datasets. Given a text
under scrutiny, we leverage ChatGPT to generate a corresponding question and
provide a re-answer to the question. By comparing the similarity between the
original text and the generated re-answered text, it can be determined whether
the text is machine-generated. GPT-Pat consists of a Siamese network to compute
the similarity between the original text and the generated re-answered text and
a binary classifier. Our method achieved an average accuracy of 94.57% on four
generalization test sets, surpassing the state-of-the-art RoBERTa-based method
by 12.34%. The accuracy drop of our method is only about half of that of the
RoBERTa-based method when it is attacked by re-translation and polishing. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12519v1' target="_blank">http://arxiv.org/pdf/2305.12519v1</a><br> <br> <br> <font size='5'> 309 </font> <div style="text-align: right"> 2023-05-21 14:45:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT-3.5 vs GPT-4: Evaluating ChatGPT's Reasoning Performance in Zero-shot Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have exhibited remarkable performance on various
Natural Language Processing (NLP) tasks. However, there is a current hot debate
regarding their reasoning capacity. In this paper, we examine the performance
of GPT-3.5 and GPT-4 models, by performing a thorough technical evaluation on
different reasoning tasks across eleven distinct datasets. Our findings show
that GPT-4 outperforms GPT-3.5 in zero-shot learning throughout almost all
evaluated tasks. In addition, we note that both models exhibit limited
performance in Inductive, Mathematical, and Multi-hop Reasoning Tasks. While it
may seem intuitive that the GPT-4 model would outperform GPT-3.5 given its size
and efficiency in various NLP tasks, our paper offers empirical evidence to
support this claim. We provide a detailed and comprehensive analysis of the
results from both models to further support our findings. In addition, we
propose a set of engineered prompts that improves performance of both models on
zero-shot learning. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12477v1' target="_blank">http://arxiv.org/pdf/2305.12477v1</a><br> <br> <br> <font size='5'> 310 </font> <div style="text-align: right"> 2023-05-21 14:39:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating the Performance of Large Language Models on GAOKAO Benchmark</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have demonstrated remarkable performance across various
natural language processing tasks; however, their efficacy in more challenging
and domain-specific tasks remains less explored. This paper introduces the
GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions
from the Chinese Gaokao examination as test samples for evaluating large
language models.In order to align the evaluation results with humans as much as
possible, we designed a method based on zero-shot prompts to analyze the
accuracy and scoring rate of the model by dividing the questions into
subjective and objective types. We evaluated the ChatGPT model on
GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels
in tackling objective questions, while also shedding light on its shortcomings
and areas for improvement. To further scrutinize the model's responses, we
incorporate human evaluations.In conclusion, this research contributes a robust
evaluation benchmark for future large-scale language models and offers valuable
insights into the limitations of such models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12474v2' target="_blank">http://arxiv.org/pdf/2305.12474v2</a><br> <br> <br> <font size='5'> 311 </font> <div style="text-align: right"> 2023-05-21 11:25:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BiasAsker: Measuring the Bias in Conversational AI System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Powered by advanced Artificial Intelligence (AI) techniques, conversational
AI systems, such as ChatGPT and digital assistants like Siri, have been widely
deployed in daily life. However, such systems may still produce content
containing biases and stereotypes, causing potential social problems. Due to
the data-driven, black-box nature of modern AI techniques, comprehensively
identifying and measuring biases in conversational systems remains a
challenging task. Particularly, it is hard to generate inputs that can
comprehensively trigger potential bias due to the lack of data containing both
social groups as well as biased properties. In addition, modern conversational
systems can produce diverse responses (e.g., chatting and explanation), which
makes existing bias detection methods simply based on the sentiment and the
toxicity hardly being adopted. In this paper, we propose BiasAsker, an
automated framework to identify and measure social bias in conversational AI
systems. To obtain social groups and biased properties, we construct a
comprehensive social bias dataset, containing a total of 841 groups and 8,110
biased properties. Given the dataset, BiasAsker automatically generates
questions and adopts a novel method based on existence measurement to identify
two types of biases (i.e., absolute bias and related bias) in conversational
systems. Extensive experiments on 8 commercial systems and 2 famous research
models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated
by BiasAsker can trigger biased behaviors in these widely deployed
conversational systems. All the code, data, and experimental results have been
released to facilitate future research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12434v1' target="_blank">http://arxiv.org/pdf/2305.12434v1</a><br> <br> <br> <font size='5'> 312 </font> <div style="text-align: right"> 2023-05-21 08:11:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have shown great abilities of solving various
natural language tasks in different domains. Due to the training objective of
LLMs and their pretraining data, LLMs are not very well equipped for tasks
involving structured data generation. We propose a framework, Prompting with
Iterative Verification (PiVe), to improve graphbased generative capability of
LLMs. We show how a small language model could be trained to act as a verifier
module for the output of an LLM (i.e., ChatGPT), and to iteratively improve its
performance via fine-grained corrective instructions. Additionally, we show how
the verifier module could apply iterative corrections offline for a more
cost-effective solution to the text-to-graph generation task. Experiments on
three graph-based datasets show consistent improvement gained via PiVe.
Additionally, we highlight how the proposed verifier module can be used as a
data augmentation tool to help improve the quality of automatically generated
parallel text-graph datasets. Our code and data are available at
https://github.com/Jiuzhouh/PiVe. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12392v1' target="_blank">http://arxiv.org/pdf/2305.12392v1</a><br> <br> <br> <font size='5'> 313 </font> <div style="text-align: right"> 2023-05-21 03:28:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: InstructVid2Vid: Controllable Video Editing with Natural Language Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present an end-to-end diffusion-based method for editing videos with human
language instructions, namely $\textbf{InstructVid2Vid}$. Our approach enables
the editing of input videos based on natural language instructions without any
per-example fine-tuning or inversion. The proposed InstructVid2Vid model
combines a pretrained image generation model, Stable Diffusion, with a
conditional 3D U-Net architecture to generate time-dependent sequence of video
frames. To obtain the training data, we incorporate the knowledge and expertise
of different models, including ChatGPT, BLIP, and Tune-a-Video, to synthesize
video-instruction triplets, which is a more cost-efficient alternative to
collecting data in real-world scenarios. To improve the consistency between
adjacent frames of generated videos, we propose the Frame Difference Loss,
which is incorporated during the training process. During inference, we extend
the classifier-free guidance to text-video input to guide the generated
results, making them more related to both the input video and instruction.
Experiments demonstrate that InstructVid2Vid is able to generate high-quality,
temporally coherent videos and perform diverse edits, including attribute
editing, change of background, and style transfer. These results highlight the
versatility and effectiveness of our proposed method. Code is released in
$\href{https://github.com/BrightQin/InstructVid2Vid}{InstructVid2Vid}$. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12328v1' target="_blank">http://arxiv.org/pdf/2305.12328v1</a><br> <br> <br> <font size='5'> 314 </font> <div style="text-align: right"> 2023-05-20 15:24:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Multimodal Named Entity Recognition (MNER) on social media aims to enhance
textual entity prediction by incorporating image-based clues. Existing research
in this domain has primarily focused on maximizing the utilization of
potentially relevant information in images or incorporating external knowledge
from explicit knowledge bases (KBs). However, these methods either neglect the
necessity of providing the model with relevant external knowledge, or the
retrieved external knowledge suffers from high redundancy. To address these
problems, we propose a conceptually simple two-stage framework called Prompt
ChatGPT In MNER (PGIM) in this paper. We leverage ChatGPT as an implicit
knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the
model's performance in MNER tasks. Specifically, we first utilize a Multimodal
Similar Example Awareness module to select suitable examples from a small
number of manually annotated samples. These examples are then integrated into a
formatted prompt template tailored to the MNER task, guiding ChatGPT to
generate auxiliary refined knowledge. Finally, the acquired knowledge is
integrated with the raw text and inputted into the downstream model for further
processing. Extensive experiments show that our PGIM significantly outperforms
all existing state-of-the-art methods on two classic MNER datasets. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12212v1' target="_blank">http://arxiv.org/pdf/2305.12212v1</a><br> <br> <br> <font size='5'> 315 </font> <div style="text-align: right"> 2023-05-20 14:13:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The VNHSGE (VietNamese High School Graduation Examination) dataset, developed
exclusively for evaluating large language models (LLMs), is introduced in this
article. The dataset, which covers nine subjects, was generated from the
Vietnamese National High School Graduation Examination and comparable tests.
300 literary essays have been included, and there are over 19,000
multiple-choice questions on a range of topics. The dataset assesses LLMs in
multitasking situations such as question answering, text generation, reading
comprehension, visual question answering, and more by including both textual
data and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on
the VNHSGE dataset and contrasted their performance with that of Vietnamese
students to see how well they performed. The results show that ChatGPT and
BingChat both perform at a human level in a number of areas, including
literature, English, history, geography, and civics education. They still have
space to grow, though, especially in the areas of mathematics, physics,
chemistry, and biology. The VNHSGE dataset seeks to provide an adequate
benchmark for assessing the abilities of LLMs with its wide-ranging coverage
and variety of activities. We intend to promote future developments in the
creation of LLMs by making this dataset available to the scientific community,
especially in resolving LLMs' limits in disciplines involving mathematics and
the natural sciences. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12199v1' target="_blank">http://arxiv.org/pdf/2305.12199v1</a><br> <br> <br> <font size='5'> 316 </font> <div style="text-align: right"> 2023-05-20 08:43:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Scope of ChatGPT in Software Engineering: A Thorough Investigation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT demonstrates immense potential to transform software engineering (SE)
by exhibiting outstanding performance in tasks such as code and document
generation. However, the high reliability and risk control requirements of SE
make the lack of interpretability for ChatGPT a concern. To address this issue,
we carried out a study evaluating ChatGPT's capabilities and limitations in SE.
We broke down the abilities needed for AI models to tackle SE tasks into three
categories: 1) syntax understanding, 2) static behavior understanding, and 3)
dynamic behavior understanding. Our investigation focused on ChatGPT's ability
to comprehend code syntax and semantic structures, including abstract syntax
trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed
ChatGPT's performance on cross-language tasks involving C, Java, Python, and
Solidity. Our findings revealed that while ChatGPT excels at understanding code
syntax (AST), it struggles with comprehending code semantics, particularly
dynamic semantics. We conclude that ChatGPT possesses capabilities akin to an
Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static
code analysis. Additionally, our study highlights that ChatGPT is susceptible
to hallucination when interpreting code semantic structures and fabricating
non-existent facts. These results underscore the need to explore methods for
verifying the correctness of ChatGPT's outputs to ensure its dependability in
SE. More importantly, our study provide an iniital answer why the generated
codes from LLMs are usually synatx correct but vulnerabale. </font><br> Link: <a href='http://arxiv.org/pdf/2305.12138v1' target="_blank">http://arxiv.org/pdf/2305.12138v1</a><br> <br> <br> <font size='5'> 317 </font> <div style="text-align: right"> 2023-05-19 20:33:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of medium-large Language Models at zero-shot closed book generative question answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have garnered significant attention, but the
definition of "large" lacks clarity. This paper focuses on medium-sized
language models (MLMs), defined as having at least six billion parameters but
less than 100 billion. The study evaluates MLMs regarding zero-shot generative
question answering, which requires models to provide elaborate answers without
external document retrieval. The paper introduces an own test dataset and
presents results from human evaluation. Results show that combining the best
answers from different MLMs yielded an overall correct answer rate of 82.7%
which is better than the 60.9% of ChatGPT. The best MLM achieved 71.8% and has
33B parameters, which highlights the importance of using appropriate training
data for fine-tuning rather than solely relying on the number of parameters.
More fine-grained feedback should be used to further improve the quality of
answers. The open source community is quickly closing the gap to the best
commercial models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11991v2' target="_blank">http://arxiv.org/pdf/2305.11991v2</a><br> <br> <br> <font size='5'> 318 </font> <div style="text-align: right"> 2023-05-19 18:26:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-QA: Unsupervised Knowledge Guided Language Model Alignment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale language models like ChatGPT and GPT-4 have gained attention for
their impressive conversational and generative capabilities. However, the
creation of supervised paired question-answering data for instruction tuning
presents formidable challenges. This endeavor necessitates substantial human
effort for data annotation and wrestles with issues concerning data quality,
diversity, accuracy, and other related factors. To overcome these obstacles, we
introduce an innovative framework named Self-QA, which replaces the traditional
practice of human-written instruction seeds with a vast amount of unsupervised
knowledge, enabling the model to generate a larger quantity of correct and
domain-specific instruction data. The effectiveness of our proposed method is
demonstrated through experiments conducted on unsupervised corpora from various
domains. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11952v1' target="_blank">http://arxiv.org/pdf/2305.11952v1</a><br> <br> <br> <font size='5'> 319 </font> <div style="text-align: right"> 2023-05-19 17:25:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparing Software Developers with ChatGPT: An Empirical Investigation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advent of automation in particular Software Engineering (SE) tasks has
transitioned from theory to reality. Numerous scholarly articles have
documented the successful application of Artificial Intelligence to address
issues in areas such as project management, modeling, testing, and development.
A recent innovation is the introduction of ChatGPT, an ML-infused chatbot,
touted as a resource proficient in generating programming codes and formulating
software testing strategies for developers and testers respectively. Although
there is speculation that AI-based computation can increase productivity and
even substitute software engineers in software development, there is currently
a lack of empirical evidence to verify this. Moreover, despite the primary
focus on enhancing the accuracy of AI systems, non-functional requirements
including energy efficiency, vulnerability, fairness (i.e., human bias), and
safety frequently receive insufficient attention. This paper posits that a
comprehensive comparison of software engineers and AI-based solutions,
considering various evaluation criteria, is pivotal in fostering human-machine
collaboration, enhancing the reliability of AI-based methods, and understanding
task suitability for humans or AI. Furthermore, it facilitates the effective
implementation of cooperative work structures and human-in-the-loop processes.
This paper conducts an empirical investigation, contrasting the performance of
software engineers and AI systems, like ChatGPT, across different evaluation
metrics. The empirical study includes a case of assessing ChatGPT-generated
code versus code produced by developers and uploaded in Leetcode. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11837v2' target="_blank">http://arxiv.org/pdf/2305.11837v2</a><br> <br> <br> <font size='5'> 320 </font> <div style="text-align: right"> 2023-05-19 15:36:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, \ie content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation for Large Language Models (HaluEval) benchmark, a large collection
of generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing hallucination. To generate these samples, we
propose a ChatGPT-based two-step framework, \ie sampling-then-filtering.
Besides, we also hire some human labelers to annotate the hallucinations in
ChatGPT responses. The empirical results suggest that ChatGPT is likely to
generate hallucinated content in specific topics by fabricating unverifiable
information (\ie about $11.4\%$ user queries). Moreover, existing LLMs face
great challenges in recognizing the hallucinations in texts. While, our
experiments also prove that the hallucination recognition can be improved by
providing external knowledge or adding reasoning steps. Our benchmark can be
accessed at https://github.com/RUCAIBox/HaluEval. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11747v2' target="_blank">http://arxiv.org/pdf/2305.11747v2</a><br> <br> <br> <font size='5'> 321 </font> <div style="text-align: right"> 2023-05-19 14:24:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Text-based collaborative filtering (TCF) has become the mainstream approach
for text and news recommendation, utilizing text encoders, also known as
language models (LMs), to represent items. However, existing TCF models
primarily focus on using small or medium-sized LMs. It remains uncertain what
impact replacing the item encoder with one of the largest and most powerful
LMs, such as the 175-billion parameter GPT-3 model, would have on
recommendation performance. Can we expect unprecedented results? To this end,
we conduct an extensive series of experiments aimed at exploring the
performance limits of the TCF paradigm. Specifically, we increase the size of
item encoders from one hundred million to one hundred billion to reveal the
scaling limits of the TCF paradigm. We then examine whether these extremely
large LMs could enable a universal item representation for the recommendation
task. Furthermore, we compare the performance of the TCF paradigm utilizing the
most powerful LMs to the currently dominant ID embedding-based paradigm and
investigate the transferability of this TCF paradigm. Finally, we compare TCF
with the recently popularized prompt-based recommendation using ChatGPT. Our
research findings have not only yielded positive results but also uncovered
some surprising and previously unknown negative outcomes, which can inspire
deeper reflection and innovative thinking regarding text-based recommender
systems. Codes and datasets will be released for further research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11700v1' target="_blank">http://arxiv.org/pdf/2305.11700v1</a><br> <br> <br> <font size='5'> 322 </font> <div style="text-align: right"> 2023-05-19 13:23:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11662v2' target="_blank">http://arxiv.org/pdf/2305.11662v2</a><br> <br> <br> <font size='5'> 323 </font> <div style="text-align: right"> 2023-05-19 10:45:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RECIPE: How to Integrate ChatGPT into EFL Writing Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The integration of generative AI in the field of education is actively being
explored. In particular, ChatGPT has garnered significant interest, offering an
opportunity to examine its effectiveness in English as a foreign language (EFL)
education. To address this need, we present a novel learning platform called
RECIPE (Revising an Essay with ChatGPT on an Interactive Platform for EFL
learners). Our platform features two types of prompts that facilitate
conversations between ChatGPT and students: (1) a hidden prompt for ChatGPT to
take an EFL teacher role and (2) an open prompt for students to initiate a
dialogue with a self-written summary of what they have learned. We deployed
this platform for 213 undergraduate and graduate students enrolled in EFL
writing courses and seven instructors. For this study, we collect students'
interaction data from RECIPE, including students' perceptions and usage of the
platform, and user scenarios are examined with the data. We also conduct a
focus group interview with six students and an individual interview with one
EFL instructor to explore design opportunities for leveraging generative AI
models in the field of EFL education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11583v1' target="_blank">http://arxiv.org/pdf/2305.11583v1</a><br> <br> <br> <font size='5'> 324 </font> <div style="text-align: right"> 2023-05-19 08:02:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language Models (LLMs) have achieved promising performance on
arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting. However, LLMs face challenges in maintaining factual consistency
during reasoning, exhibiting tendencies to condition overlooking, question
misinterpretation, and condition hallucination over given problems. Existing
methods use coarse-grained feedback (e.g., whether the answer is correct) to
improve factual consistency. In this work, we propose RCoT (Reversing
Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by
automatically detecting and rectifying factual inconsistency in LLMs' generated
solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct
the problem based on generated solutions. Then fine-grained comparisons between
the original problem and the reconstructed problem expose the factual
inconsistency in the original solutions. To rectify the solution, RCoT
formulates detected factual inconsistency into fine-grained feedback to guide
LLMs in revising solutions. Experimental results demonstrate consistent
improvements of RCoT over standard CoT across seven arithmetic datasets.
Moreover, we find that manually written fine-grained feedback can dramatically
improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on
GSM8K), encouraging the community to further explore the fine-grained feedback
generation methods. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11499v1' target="_blank">http://arxiv.org/pdf/2305.11499v1</a><br> <br> <br> <font size='5'> 325 </font> <div style="text-align: right"> 2023-05-19 06:53:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Graphologue: Exploring Large Language Model Responses with Interactive Diagrams</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have recently soared in popularity due to their
ease of access and the unprecedented intelligence exhibited on diverse
applications. However, LLMs like ChatGPT present significant limitations in
supporting complex information tasks due to the insufficient affordances of the
text-based medium and linear conversational structure. Through a formative
study with ten participants, we found that LLM interfaces often present
long-winded responses, making it difficult for people to quickly comprehend and
interact flexibly with various pieces of information, particularly during more
complex tasks. We present Graphologue, an interactive system that converts
text-based responses from LLMs into graphical diagrams to facilitate
information-seeking and question-answering tasks. Graphologue employs novel
prompting strategies and interface designs to extract entities and
relationships from LLM responses and constructs node-link diagrams in
real-time. Further, users can interact with the diagrams to flexibly adjust the
graphical presentation and to submit context-specific prompts to obtain more
information. Utilizing diagrams, Graphologue enables graphical, non-linear
dialogues between humans and LLMs, facilitating information exploration,
organization, and comprehension. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11473v1' target="_blank">http://arxiv.org/pdf/2305.11473v1</a><br> <br> <br> <font size='5'> 326 </font> <div style="text-align: right"> 2023-05-19 04:04:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Pre-trained large language models (PLMs) have the potential to support urban
science research through content creation, information extraction, assisted
programming, text classification, and other technical advances. In this
research, we explored the opportunities, challenges, and prospects of PLMs in
urban science research. Specifically, we discussed potential applications of
PLMs to urban institution, urban space, urban information, and citizen
behaviors research through seven examples using ChatGPT. We also examined the
challenges of PLMs in urban science research from both technical and social
perspectives. The prospects of the application of PLMs in urban science
research were then proposed. We found that PLMs can effectively aid in
understanding complex concepts in urban science, facilitate urban spatial form
identification, assist in disaster monitoring, and sense public sentiment. At
the same time, however, the applications of PLMs in urban science research face
evident threats, such as technical limitations, security, privacy, and social
bias. The development of fundamental models based on domain knowledge and
human-AI collaboration may help improve PLMs to support urban science research
in future. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11418v1' target="_blank">http://arxiv.org/pdf/2305.11418v1</a><br> <br> <br> <font size='5'> 327 </font> <div style="text-align: right"> 2023-05-19 02:09:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have been useful in expanding mental health care
delivery. ChatGPT, in particular, has gained popularity for its ability to
generate human-like dialogue. However, data-sensitive domains -- including but
not limited to healthcare -- face challenges in using ChatGPT due to privacy
and data-ownership concerns. To enable its utilization, we propose a text
ambiguation framework that preserves user privacy. We ground this in the task
of addressing stress prompted by user-provided texts to demonstrate the
viability and helpfulness of privacy-preserved generations. Our results suggest
that chatGPT recommendations are still able to be moderately helpful and
relevant, even when the original user text is not provided. </font><br> Link: <a href='http://arxiv.org/pdf/2306.05552v1' target="_blank">http://arxiv.org/pdf/2306.05552v1</a><br> <br> <br> <font size='5'> 328 </font> <div style="text-align: right"> 2023-05-18 15:36:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A framework for leveraging ChatGPT on programming tasks in energy systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid digitalization of energy sectors has led to a significant increase
in coding tasks for engineers and researchers. This research article explores
the potential of leveraging ChatGPT, an advanced AI language model, to
revolutionize programming in the energy domain. Here, we propose a generic
interactive programming framework based on ChatGPT, covering three distinct
applications in energy systems ranging from simple to complex. For routine
tasks such as daily unit commitment, ChatGPT can increase efficiency by
automatic modelling, coding, debugging and scaling and thus re-ducing
repetitive work. For complex tasks such as decentralized optimization of an
integrated energy system (IES) where engineers have no prior knowledge, ChatGPT
can reduce the learning cost by recommending appropriate algo-rithms. For new
problems without readily available solutions such as ultra-fast unit
commitment, ChatGPT can organize potential technology roadmap and provide
algorithm recommendation and auto-coding for each step. The findings
demonstrate the potential of ChatGPT as a powerful tool in the domain of energy
sectors in terms of auto coding, new knowledge learning and new problem
solving. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11202v2' target="_blank">http://arxiv.org/pdf/2305.11202v2</a><br> <br> <br> <font size='5'> 329 </font> <div style="text-align: right"> 2023-05-18 14:23:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Multi-modal large language models are regarded as a crucial step towards
Artificial General Intelligence (AGI) and have garnered significant interest
with the emergence of ChatGPT. However, current speech-language models
typically adopt the cascade paradigm, preventing inter-modal knowledge
transfer. In this paper, we propose SpeechGPT, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and
generating multi-model content. With discrete speech representations, we first
construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes
modality-adaptation pre-training, cross-modal instruction fine-tuning, and
chain-of-modality instruction fine-tuning. The experimental results demonstrate
that SpeechGPT has an impressive capacity to follow multi-modal human
instructions and highlight the potential of handling multiple modalities with
one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11000v2' target="_blank">http://arxiv.org/pdf/2305.11000v2</a><br> <br> <br> <font size='5'> 330 </font> <div style="text-align: right"> 2023-05-18 13:05:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Android Robot Head as Embodied Conversational Agent</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper describes, how current Machine Learning (ML) techniques combined
with simple rule-based animation routines make an android robot head an
embodied conversational agent with ChatGPT as its core component. The android
robot head is described, technical details are given of how lip-sync animation
is being achieved, and general software design decisions are presented. A
public presentation of the system revealed improvement opportunities that are
reported and that lead our iterative implementation approach. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10945v1' target="_blank">http://arxiv.org/pdf/2305.10945v1</a><br> <br> <br> <font size='5'> 331 </font> <div style="text-align: right"> 2023-05-18 10:03:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models can be Guided to Evade AI-Generated Text Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10847v4' target="_blank">http://arxiv.org/pdf/2305.10847v4</a><br> <br> <br> <font size='5'> 332 </font> <div style="text-align: right"> 2023-05-18 09:22:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The domain of Botany is rich with metaphorical terms. Those terms play an
important role in the description and identification of flowers and plants.
However, the identification of such terms in discourse is an arduous task. This
leads in some cases to committing errors during translation processes and
lexicographic tasks. The process is even more challenging when it comes to
machine translation, both in the cases of single-word terms and multi-word
terms. One of the recent concerns of Natural Language Processing (NLP)
applications and Machine Translation (MT) technologies is the automatic
identification of metaphor-based words in discourse through Deep Learning (DL).
In this study, we seek to fill this gap through the use of thirteen popular
transformer based models, as well as ChatGPT, and we show that discriminative
models perform better than GPT-3.5 model with our best performer reporting
92.2349% F1 score in metaphoric flower and plant names identification task. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10833v3' target="_blank">http://arxiv.org/pdf/2305.10833v3</a><br> <br> <br> <font size='5'> 333 </font> <div style="text-align: right"> 2023-05-18 03:32:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Code generation aims to automatically generate source code from high-level
task specifications, which can significantly increase productivity of software
engineering. Recently, approaches based on large language models (LLMs) have
shown remarkable code generation abilities on simple tasks. However, generate
code for more complex tasks, such as competition-level problems, remains
challenging. In this paper, we introduce Brainstorm framework for code
generation. It leverages a brainstorming step that generates and selects
diverse thoughts on the problem to facilitate algorithmic reasoning, where the
thoughts are possible blueprint of solving the problem. We demonstrate that
Brainstorm significantly enhances the ability of LLMs to solve
competition-level programming problems, resulting in a more than 50% increase
in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving
state-of-the-art performance. Furthermore, our experiments conducted on
LeetCode contests show that our framework boosts the ability of ChatGPT to a
level comparable to that of human programmers. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10679v1' target="_blank">http://arxiv.org/pdf/2305.10679v1</a><br> <br> <br> <font size='5'> 334 </font> <div style="text-align: right"> 2023-05-18 02:04:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ethical ChatGPT: Concerns, Challenges, and Commandments</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models, e.g. ChatGPT are currently contributing enormously to
make artificial intelligence even more popular, especially among the general
population. However, such chatbot models were developed as tools to support
natural language communication between humans. Problematically, it is very much
a ``statistical correlation machine" (correlation instead of causality) and
there are indeed ethical concerns associated with the use of AI language models
such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights
specific ethical concerns on ChatGPT and articulates key challenges when
ChatGPT is used in various applications. Practical commandments for different
stakeholders of ChatGPT are also proposed that can serve as checklist
guidelines for those applying ChatGPT in their applications. These commandment
examples are expected to motivate the ethical use of ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10646v1' target="_blank">http://arxiv.org/pdf/2305.10646v1</a><br> <br> <br> <font size='5'> 335 </font> <div style="text-align: right"> 2023-05-18 02:03:55+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are Large Language Models Fit For Guided Reading?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper looks at the ability of large language models to participate in
educational guided reading. We specifically, evaluate their ability to generate
meaningful questions from the input text, generate diverse questions both in
terms of content coverage and difficulty of the questions and evaluate their
ability to recommend part of the text that a student should re-read based on
the student's responses to the questions. Based on our evaluation of ChatGPT
and Bard, we report that,
  1) Large language models are able to generate high quality meaningful
questions that have high correlation with the input text, 2) They generate
diverse question that cover most topics in the input text even though this
ability is significantly degraded as the input text increases, 3)The large
language models are able to generate both low and high cognitive questions even
though they are significantly biased toward low cognitive question, 4) They are
able to effectively summarize responses and extract a portion of text that
should be re-read. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10645v2' target="_blank">http://arxiv.org/pdf/2305.10645v2</a><br> <br> <br> <font size='5'> 336 </font> <div style="text-align: right"> 2023-05-18 00:35:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Language Models Meet World Models: Embodied Experiences Enhance Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical elastic
weight consolidation (EWC) for selective weight updates, combined with low-rank
adapters (LoRA) for training efficiency. Extensive experiments show our
approach substantially improves base LMs on 18 downstream tasks by 64.28% on
average. In particular, the small LMs (1.3B and 6B) enhanced by our approach
match or even outperform much larger LMs (e.g., ChatGPT). </font><br> Link: <a href='http://arxiv.org/pdf/2305.10626v2' target="_blank">http://arxiv.org/pdf/2305.10626v2</a><br> <br> <br> <font size='5'> 337 </font> <div style="text-align: right"> 2023-05-17 18:30:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this multicultural age, language translation is one of the most performed
tasks, and it is becoming increasingly AI-moderated and automated. As a novel
AI system, ChatGPT claims to be proficient in such translation tasks and in
this paper, we put that claim to the test. Specifically, we examine ChatGPT's
accuracy in translating between English and languages that exclusively use
gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most
spoken language globally, but also generalize our findings across five other
languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT
perpetuates gender defaults and stereotypes assigned to certain occupations
(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to
work), as it converts gender-neutral pronouns in languages to `he' or `she'. We
also observe ChatGPT completely failing to translate the English gender-neutral
pronoun `they' into equivalent gender-neutral pronouns in other languages, as
it produces translations that are incoherent and incorrect. While it does
respect and provide appropriately gender-marked versions of Bengali words when
prompted with gender information in English, ChatGPT appears to confer a higher
respect to men than to women in the same occupation. We conclude that ChatGPT
exhibits the same gender biases which have been demonstrated for tools like
Google Translate or MS Translator, as we provide recommendations for a human
centered approach for future designers of AIs that perform language translation
to better accommodate such low-resource languages. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10510v1' target="_blank">http://arxiv.org/pdf/2305.10510v1</a><br> <br> <br> <font size='5'> 338 </font> <div style="text-align: right"> 2023-05-17 17:47:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BAD: BiAs Detection for Large Language Models in the context of candidate screening</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Application Tracking Systems (ATS) have allowed talent managers, recruiters,
and college admissions committees to process large volumes of potential
candidate applications efficiently. Traditionally, this screening process was
conducted manually, creating major bottlenecks due to the quantity of
applications and introducing many instances of human bias. The advent of large
language models (LLMs) such as ChatGPT and the potential of adopting methods to
current automated application screening raises additional bias and fairness
issues that must be addressed. In this project, we wish to identify and
quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the
context of candidate screening in order to demonstrate how the use of these
models could perpetuate existing biases and inequalities in the hiring process. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10407v1' target="_blank">http://arxiv.org/pdf/2305.10407v1</a><br> <br> <br> <font size='5'> 339 </font> <div style="text-align: right"> 2023-05-17 15:07:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning </font><br> Link: <a href='http://arxiv.org/pdf/2305.10276v5' target="_blank">http://arxiv.org/pdf/2305.10276v5</a><br> <br> <br> <font size='5'> 340 </font> <div style="text-align: right"> 2023-05-17 14:40:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MemoryBank: Enhancing Large Language Models with Long-Term Memory</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Revolutionary advancements in Large Language Models have drastically reshaped
our interactions with artificial intelligence systems. Despite this, a notable
hindrance remains-the deficiency of a long-term memory mechanism within these
models. This shortfall becomes increasingly evident in situations demanding
sustained interaction, such as personal companion systems and psychological
counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored
for LLMs. MemoryBank enables the models to summon relevant memories,
continually evolve through continuous memory updates, comprehend, and adapt to
a user personality by synthesizing information from past interactions. To mimic
anthropomorphic behaviors and selectively preserve memory, MemoryBank
incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting
Curve theory, which permits the AI to forget and reinforce memory based on time
elapsed and the relative significance of the memory, thereby offering a
human-like memory mechanism. MemoryBank is versatile in accommodating both
closed-source models like ChatGPT and open-source models like ChatGLM. We
exemplify application of MemoryBank through the creation of an LLM-based
chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned
with psychological dialogs, SiliconFriend displays heightened empathy in its
interactions. Experiment involves both qualitative analysis with real-world
user dialogs and quantitative analysis with simulated dialogs. In the latter,
ChatGPT acts as users with diverse characteristics and generates long-term
dialog contexts covering a wide array of topics. The results of our analysis
reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong
capability for long-term companionship as it can provide emphatic response,
recall relevant memories and understand user personality. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10250v3' target="_blank">http://arxiv.org/pdf/2305.10250v3</a><br> <br> <br> <font size='5'> 341 </font> <div style="text-align: right"> 2023-05-17 12:31:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Pre-Training (GPT) models like ChatGPT have demonstrated
exceptional performance in various Natural Language Processing (NLP) tasks.
Although ChatGPT has been integrated into the overall workflow to boost
efficiency in many domains, the lack of flexibility in the finetuning process
hinders its applications in areas that demand extensive domain expertise and
semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on
the China National Medical Licensing Examination (CNMLE) and propose a novel
approach to improve ChatGPT from two perspectives: integrating medical domain
knowledge and enabling few-shot learning. By using a simple but effective
retrieval method, medical background knowledge is extracted as semantic
instructions to guide the inference of ChatGPT. Similarly, relevant medical
questions are identified and fed as demonstrations to ChatGPT. Experimental
results show that directly applying ChatGPT fails to qualify the CNMLE at a
score of 51 (i.e., only 51\% of questions are answered correctly). While our
knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not
only passes the qualification but also surpasses the average score of humans
(61). This research demonstrates the potential of knowledge-enhanced ChatGPT to
serve as versatile medical assistants, capable of analyzing real-world medical
problems in a more accessible, user-friendly, and adaptable manner. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10163v1' target="_blank">http://arxiv.org/pdf/2305.10163v1</a><br> <br> <br> <font size='5'> 342 </font> <div style="text-align: right"> 2023-05-17 00:09:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Smaller Language Models are Better Black-box Machine-Generated Text Detectors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the advent of fluent generative language models that can produce
convincing utterances very similar to those written by humans, distinguishing
whether a piece of text is machine-generated or human-written becomes more
challenging and more important, as such models could be used to spread
misinformation, fake news, fake reviews and to mimic certain authors and
figures. To this end, there have been a slew of methods proposed to detect
machine-generated text. Most of these methods need access to the logits of the
target model or need the ability to sample from the target. One such black-box
detection method relies on the observation that generated text is locally
optimal under the likelihood function of the generator, while human-written
text is not. We find that overall, smaller and partially-trained models are
better universal text detectors: they can more precisely detect text generated
from both small and larger models. Interestingly, we find that whether the
detector and generator were trained on the same data is not critically
important to the detection success. For instance the OPT-125M model has an AUC
of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT
family, GPTJ-6B, has AUC of 0.45. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09859v1' target="_blank">http://arxiv.org/pdf/2305.09859v1</a><br> <br> <br> <font size='5'> 343 </font> <div style="text-align: right"> 2023-05-16 21:51:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the increasing popularity of generative large language models (LLMs)
like ChatGPT, an increasing number of news websites have begun utilizing them
to generate articles. However, not only can these language models produce
factually inaccurate articles on reputable websites but disreputable news sites
can utilize these LLMs to mass produce misinformation. To begin to understand
this phenomenon, we present one of the first large-scale studies of the
prevalence of synthetic articles within online news media. To do this, we train
a DeBERTa-based synthetic news detector and classify over 12.91 million
articles from 3,074 misinformation and mainstream news websites. We find that
between January 1, 2022 and April 1, 2023, the relative number of synthetic
news articles increased by 79.4% on mainstream websites while increasing by
342% on misinformation sites. Analyzing the impact of the release of ChatGPT
using an interrupted-time-series, we show that while its release resulted in a
marked increase in synthetic articles on small sites as well as misinformation
news websites, there was not a corresponding increase on large mainstream news
websites. Finally, using data from the social media platform Reddit, we find
that social media users interacted more with synthetic articles in March 2023
relative to January 2022. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09820v1' target="_blank">http://arxiv.org/pdf/2305.09820v1</a><br> <br> <br> <font size='5'> 344 </font> <div style="text-align: right"> 2023-05-16 17:45:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: StructGPT: A General Framework for Large Language Model to Reason over Structured Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we study how to improve the zero-shot reasoning ability of
large language models~(LLMs) over structured data in a unified way. Inspired by
the study on tool augmentation for LLMs, we develop an \emph{Iterative
Reading-then-Reasoning~(IRR)} approach for solving question answering tasks
based on structured data, called \textbf{StructGPT}. In our approach, we
construct the specialized function to collect relevant evidence from structured
data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on
the collected information (\ie \emph{reasoning}). Specially, we propose an
\emph{invoking-linearization-generation} procedure to support LLMs in reasoning
on the structured data with the help of the external interfaces. By iterating
this procedures with provided interfaces, our approach can gradually approach
the target answer to a given query. Extensive experiments conducted on three
types of structured data demonstrate the effectiveness of our approach, which
can significantly boost the performance of ChatGPT and achieve comparable
performance against the full-data supervised-tuning baselines. Our codes and
data are publicly available at~\url{https://github.com/RUCAIBox/StructGPT}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09645v1' target="_blank">http://arxiv.org/pdf/2305.09645v1</a><br> <br> <br> <font size='5'> 345 </font> <div style="text-align: right"> 2023-05-16 16:56:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Advising OpenMP Parallelization via a Graph-Based Approach with Transformers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is an ever-present need for shared memory parallelization schemes to
exploit the full potential of multi-core architectures. The most common
parallelization API addressing this need today is OpenMP. Nevertheless, writing
parallel code manually is complex and effort-intensive. Thus, many
deterministic source-to-source (S2S) compilers have emerged, intending to
automate the process of translating serial to parallel code. However, recent
studies have shown that these compilers are impractical in many scenarios. In
this work, we combine the latest advancements in the field of AI and natural
language processing (NLP) with the vast amount of open-source code to address
the problem of automatic parallelization. Specifically, we propose a novel
approach, called OMPify, to detect and predict the OpenMP pragmas and
shared-memory attributes in parallel code, given its serial version. OMPify is
based on a Transformer-based model that leverages a graph-based representation
of source code that exploits the inherent structure of code. We evaluated our
tool by predicting the parallelization pragmas and attributes of a large corpus
of (over 54,000) snippets of serial code written in C and C++ languages
(Open-OMP-Plus). Our results demonstrate that OMPify outperforms existing
approaches, the general-purposed and popular ChatGPT and targeted PragFormer
models, in terms of F1 score and accuracy. Specifically, OMPify achieves up to
90% accuracy on commonly-used OpenMP benchmark tests such as NAS, SPEC, and
PolyBench. Additionally, we performed an ablation study to assess the impact of
different model components and present interesting insights derived from the
study. Lastly, we also explored the potential of using data augmentation and
curriculum learning techniques to improve the model's robustness and
generalization capabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11999v1' target="_blank">http://arxiv.org/pdf/2305.11999v1</a><br> <br> <br> <font size='5'> 346 </font> <div style="text-align: right"> 2023-05-16 16:48:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cooperation Is All You Need</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Going beyond 'dendritic democracy', we introduce a 'democracy of local
processors', termed Cooperator. Here we compare their capabilities when used in
permutation-invariant neural networks for reinforcement learning (RL), with
machine learning algorithms based on Transformers, such as ChatGPT.
Transformers are based on the long-standing conception of integrate-and-fire
'point' neurons, whereas Cooperator is inspired by recent neurobiological
breakthroughs suggesting that the cellular foundations of mental life depend on
context-sensitive pyramidal neurons in the neocortex which have two
functionally distinct points. We show that when used for RL, an algorithm based
on Cooperator learns far quicker than that based on Transformer, even while
having the same number of parameters. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10449v1' target="_blank">http://arxiv.org/pdf/2305.10449v1</a><br> <br> <br> <font size='5'> 347 </font> <div style="text-align: right"> 2023-05-16 15:30:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Public Perception of Generative AI on Twitter: An Empirical Study Based on Occupation and Usage</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of generative AI has sparked substantial discussions, with the
potential to have profound impacts on society in all aspects. As emerging
technologies continue to advance, it is imperative to facilitate their proper
integration into society, managing expectations and fear. This paper
investigates users' perceptions of generative AI using 3M posts on Twitter from
January 2019 to March 2023, especially focusing on their occupation and usage.
We find that people across various occupations, not just IT-related ones, show
a strong interest in generative AI. The sentiment toward generative AI is
generally positive, and remarkably, their sentiments are positively correlated
with their exposure to AI. Among occupations, illustrators show exceptionally
negative sentiment mainly due to concerns about the unethical usage of artworks
in constructing AI. People use ChatGPT in diverse ways, and notably the casual
usage in which they "play with" ChatGPT tends to associate with positive
sentiments. After the release of ChatGPT, people's interest in AI in general
has increased dramatically; however, the topic with the most significant
increase and positive sentiment is related to crypto, indicating the
hype-worthy characteristics of generative AI. These findings would offer
valuable lessons for policymaking on the emergence of new technology and also
empirical insights for the considerations of future human-AI symbiosis. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09537v1' target="_blank">http://arxiv.org/pdf/2305.09537v1</a><br> <br> <br> <font size='5'> 348 </font> <div style="text-align: right"> 2023-05-16 13:46:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Mobile apps are indispensable for people's daily life, and automated GUI
(Graphical User Interface) testing is widely used for app quality assurance.
There is a growing interest in using learning-based techniques for automated
GUI testing which aims at generating human-like actions and interactions.
However, the limitations such as low testing coverage, weak generalization, and
heavy reliance on training data, make an urgent need for a more effective
approach to generate human-like actions to thoroughly test mobile apps.
Inspired by the success of the Large Language Model (LLM), e.g., GPT-3 and
ChatGPT, in natural language understanding and question answering, we formulate
the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM
to chat with the mobile apps by passing the GUI page information to LLM to
elicit testing scripts, and executing them to keep passing the app feedback to
LLM, iterating the whole process. Within it, we extract the static context of
the GUI page and the dynamic context of the iterative testing process, design
prompts for inputting this information to LLM, and develop a neural matching
network to decode the LLM's output into actionable steps to execute the app. We
evaluate GPTDroid on 86 apps from Google Play, and its activity coverage is
71%, with 32% higher than the best baseline, and can detect 36% more bugs with
faster speed than the best baseline. GPTDroid also detects 48 new bugs on the
Google Play with 25 of them being confirmed/fixed. We further summarize the
capabilities of GPTDroid behind the superior performance, including semantic
text input, compound action, long meaningful test trace, and test case
prioritization. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09434v1' target="_blank">http://arxiv.org/pdf/2305.09434v1</a><br> <br> <br> <font size='5'> 349 </font> <div style="text-align: right"> 2023-05-16 13:11:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generating coherent comic with rich story using ChatGPT and Stable Diffusion</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Past work demonstrated that using neural networks, we can extend unfinished
music pieces while maintaining the music style of the musician. With recent
advancements in large language models and diffusion models, we are now capable
of generating comics with an interesting storyline while maintaining the art
style of the artist. In this paper, we used ChatGPT to generate storylines and
dialogue and then generated the comic using stable diffusion. We introduced a
novel way to evaluate AI-generated stories, and we achieved SOTA performance on
character fidelity and art style by fine-tuning stable diffusion using LoRA,
ControlNet, etc. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11067v2' target="_blank">http://arxiv.org/pdf/2305.11067v2</a><br> <br> <br> <font size='5'> 350 </font> <div style="text-align: right"> 2023-05-16 09:18:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: QB4AIRA: A Question Bank for AI Risk Assessment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancement of Artificial Intelligence (AI), represented by
ChatGPT, has raised concerns about responsible AI development and utilization.
Existing frameworks lack a comprehensive synthesis of AI risk assessment
questions. To address this, we introduce QB4AIRA, a novel question bank
developed by refining questions from five globally recognized AI risk
frameworks, categorized according to Australia's AI ethics principles. QB4AIRA
comprises 293 prioritized questions covering a wide range of AI risk areas,
facilitating effective risk assessment. It serves as a valuable resource for
stakeholders in assessing and managing AI risks, while paving the way for new
risk frameworks and guidelines. By promoting responsible AI practices, QB4AIRA
contributes to responsible AI deployment, mitigating potential risks and harms. </font><br> Link: <a href='http://arxiv.org/pdf/2305.09300v2' target="_blank">http://arxiv.org/pdf/2305.09300v2</a><br> <br> <br> <font size='5'> 351 </font> <div style="text-align: right"> 2023-05-15 23:13:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper investigates the causal impact of negatively and positively framed
ChatGPT Artificial Intelligence (AI) discussions on US students' anticipated
labor market outcomes. Our findings reveal students reduce their confidence
regarding their future earnings prospects after exposure to AI debates, and
this effect is more pronounced after reading discussion excerpts with a
negative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric
and pessimistic belief changes, suggesting that they might feel more vulnerable
to emerging AI technologies. Pessimistic belief updates regarding future
earnings are also prevalent across gender and GPA levels, indicating widespread
AI concerns among all student subgroups. Educators, administrators, and
policymakers may regularly engage with students to address their concerns and
enhance educational curricula to better prepare them for a future that will be
inevitably shaped by AI. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11900v1' target="_blank">http://arxiv.org/pdf/2305.11900v1</a><br> <br> <br> <font size='5'> 352 </font> <div style="text-align: right"> 2023-05-15 21:00:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Exploiting the recent advancements in artificial intelligence, showcased by
ChatGPT and DALL-E, in real-world applications necessitates vast,
domain-specific, and publicly accessible datasets. Unfortunately, the scarcity
of such datasets poses a significant challenge for researchers aiming to apply
these breakthroughs in engineering design. Synthetic datasets emerge as a
viable alternative. However, practitioners are often uncertain about generating
high-quality datasets that accurately represent real-world data and are
suitable for the intended downstream applications. This study aims to fill this
knowledge gap by proposing comprehensive guidelines for generating, annotating,
and validating synthetic datasets. The trade-offs and methods associated with
each of these aspects are elaborated upon. Further, the practical implications
of these guidelines are illustrated through the creation of a turbo-compressors
dataset. The study underscores the importance of thoughtful sampling methods to
ensure the appropriate size, diversity, utility, and realism of a dataset. It
also highlights that design diversity does not equate to performance diversity
or realism. By employing test sets that represent uniform, real, or
task-specific samples, the influence of sample size and sampling strategy is
scrutinized. Overall, this paper offers valuable insights for researchers
intending to create and publish synthetic datasets for engineering design,
thereby paving the way for more effective applications of AI advancements in
the field. The code and data for the dataset and methods are made publicly
accessible at https://github.com/cyrilpic/radcomp . </font><br> Link: <a href='http://arxiv.org/pdf/2305.09018v1' target="_blank">http://arxiv.org/pdf/2305.09018v1</a><br> <br> <br> <font size='5'> 353 </font> <div style="text-align: right"> 2023-05-15 17:57:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite their unprecedented success, even the largest language models make
mistakes. Similar to how humans learn and improve using feedback, previous work
proposed providing language models with natural language feedback to guide them
in repairing their outputs. Because human-generated critiques are expensive to
obtain, researchers have devised learned critique generators in lieu of human
critics while assuming one can train downstream models to utilize generated
feedback. However, this approach does not apply to black-box or limited access
models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of
large general-purpose language agents, fine-tuning is neither computationally
nor spatially efficient as it results in multiple copies of the network. In
this work, we introduce RL4F (Reinforcement Learning for Feedback), a
multi-agent collaborative framework where the critique generator is trained to
maximize end-task performance of GPT-3, a fixed model more than 200 times its
size. RL4F produces critiques that help GPT-3 revise its outputs. We study
three datasets for action planning, summarization and alphabetization and show
improvements (~5% on average) in multiple text similarity metrics over strong
baselines across all three tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08844v1' target="_blank">http://arxiv.org/pdf/2305.08844v1</a><br> <br> <br> <font size='5'> 354 </font> <div style="text-align: right"> 2023-05-15 15:44:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent popularity of large language models (LLMs) has brought a
significant impact to boundless fields, particularly through their open-ended
ecosystem such as the APIs, open-sourced models, and plugins. However, with
their widespread deployment, there is a general lack of research that
thoroughly discusses and analyzes the potential risks concealed. In that case,
we intend to conduct a preliminary but pioneering study covering the
robustness, consistency, and credibility of LLMs systems. With most of the
related literature in the era of LLM uncharted, we propose an automated
workflow that copes with an upscaled number of queries/responses. Overall, we
conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,
and OPT. Core to our workflow consists of a data primitive, followed by an
automated interpreter that evaluates these LLMs under different adversarial
metrical systems. As a result, we draw several, and perhaps unfortunate,
conclusions that are quite uncommon from this trendy community. Briefly, they
are: (i)-the minor but inevitable error occurrence in the user-generated query
input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess
poor consistency when processing semantically similar query input. In addition,
as a side finding, we find that ChatGPT is still capable to yield the correct
answer even when the input is polluted at an extreme level. While this
phenomenon demonstrates the powerful memorization of the LLMs, it raises
serious concerns about using such data for LLM-involved evaluation in academic
development. To deal with it, we propose a novel index associated with a
dataset that roughly decides the feasibility of using such data for
LLM-involved evaluation. Extensive empirical studies are tagged to support the
aforementioned claims. </font><br> Link: <a href='http://arxiv.org/pdf/2305.10235v3' target="_blank">http://arxiv.org/pdf/2305.10235v3</a><br> <br> <br> <font size='5'> 355 </font> <div style="text-align: right"> 2023-05-15 07:14:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) like ChatGPT have proven a great shallow
understanding of many traditional NLP tasks, such as translation,
summarization, etc. However, its performance on high-level understanding, such
as dialogue discourse analysis task that requires a higher level of
understanding and reasoning, remains less explored. This study investigates
ChatGPT's capabilities in three dialogue discourse tasks: topic segmentation,
discourse relation recognition, and discourse parsing, of varying difficulty
levels. To adapt ChatGPT to these tasks, we propose discriminative and
generative paradigms and introduce the Chain of Thought (COT) approach to
improve ChatGPT's performance in more difficult tasks. The results show that
our generative paradigm allows ChatGPT to achieve comparative performance in
the topic segmentation task comparable to state-of-the-art methods but reveals
room for improvement in the more complex tasks of discourse relation
recognition and discourse parsing. Notably, the COT can significantly enhance
ChatGPT's performance with the help of understanding complex structures in more
challenging tasks. Through a series of case studies, our in-depth analysis
suggests that ChatGPT can be a good annotator in topic segmentation but has
difficulties understanding complex rhetorical structures. We hope these
findings provide a foundation for future research to refine dialogue discourse
analysis approaches in the era of LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08391v1' target="_blank">http://arxiv.org/pdf/2305.08391v1</a><br> <br> <br> <font size='5'> 356 </font> <div style="text-align: right"> 2023-05-15 05:37:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving ChatGPT Prompt for Code Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automated code generation can be a powerful technique for software
development, significantly reducing developers' efforts and time required to
create new code by generating it automatically based on requirements. Recently,
OpenAI's language model ChatGPT has emerged as a powerful tool for generating
human-like responses to a wide range of textual inputs (i.e., prompts),
including those related to code generation. However, the effectiveness of
ChatGPT for code generation is not well understood, and the generation
performance could be heavily influenced by the choice of prompt. To answer
these questions, we conducted experiments using the CodeXGlue dataset to
evaluate ChatGPT's capabilities for two code generation tasks, including
text-to-code and code-to-code generation. We designed prompts by leveraging the
chain-of-thought strategy with multi-step optimizations. Our results showed
that by carefully designing prompts to guide ChatGPT, the generation
performance can be improved substantially. We also analyzed the factors that
influenced the prompt design and provided insights that could guide future
research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08360v1' target="_blank">http://arxiv.org/pdf/2305.08360v1</a><br> <br> <br> <font size='5'> 357 </font> <div style="text-align: right"> 2023-05-15 04:10:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chatbots based on Large Language Models (LLMs) have shown strong capabilities
in language understanding. In this study, we explore the potential of LLMs in
assisting corpus-based linguistic studies through automatic annotation of texts
with specific categories of linguistic information. Specifically, we examined
to what extent LLMs understand the functional elements constituting the speech
act of apology from a local grammar perspective, by comparing the performance
of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a
human coder in the annotation task. The results demonstrate that the Bing
chatbot significantly outperformed ChatGPT in the task. Compared to human
annotator, the overall performance of the Bing chatbot was slightly less
satisfactory. However, it already achieved high F1 scores: 99.95% for the tag
of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for
APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to
use LLM-assisted annotation for local grammar analysis, together with human
intervention on tags that are less accurately recognized by machine. We
strongly advocate conducting future studies to evaluate the performance of LLMs
in annotating other linguistic phenomena. These studies have the potential to
offer valuable insights into the advancement of theories developed in corpus
linguistics, as well into the linguistic capabilities of LLMs.. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08339v2' target="_blank">http://arxiv.org/pdf/2305.08339v2</a><br> <br> <br> <font size='5'> 358 </font> <div style="text-align: right"> 2023-05-13 21:01:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Beyond the Safeguards: Exploring the Security Risks of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The increasing popularity of large language models (LLMs) such as ChatGPT has
led to growing concerns about their safety, security risks, and ethical
implications. This paper aims to provide an overview of the different types of
security risks associated with ChatGPT, including malicious text and code
generation, private data disclosure, fraudulent services, information
gathering, and producing unethical content. We present an empirical study
examining the effectiveness of ChatGPT's content filters and explore potential
ways to bypass these safeguards, demonstrating the ethical implications and
security risks that persist in LLMs even when protections are in place. Based
on a qualitative analysis of the security implications, we discuss potential
strategies to mitigate these risks and inform researchers, policymakers, and
industry professionals about the complex security challenges posed by LLMs like
ChatGPT. This study contributes to the ongoing discussion on the ethical and
security implications of LLMs, underscoring the need for continued research in
this area. </font><br> Link: <a href='http://arxiv.org/pdf/2305.08005v1' target="_blank">http://arxiv.org/pdf/2305.08005v1</a><br> <br> <br> <font size='5'> 359 </font> <div style="text-align: right"> 2023-05-13 17:12:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a novel approach for detecting ChatGPT-generated vs.
human-written text using language models. To this end, we first collected and
released a pre-processed dataset named OpenGPTText, which consists of rephrased
content generated using ChatGPT. We then designed, implemented, and trained two
different models for text classification, using Robustly Optimized BERT
Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),
respectively. Our models achieved remarkable results, with an accuracy of over
97% on the test dataset, as evaluated through various metrics. Furthermore, we
conducted an interpretability study to showcase our model's ability to extract
and differentiate key features between human-written and ChatGPT-generated
text. Our findings provide important insights into the effective use of
language models to detect generated text. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07969v2' target="_blank">http://arxiv.org/pdf/2305.07969v2</a><br> <br> <br> <font size='5'> 360 </font> <div style="text-align: right"> 2023-05-12 21:08:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Knowledge Authoring for Rules and Actions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Knowledge representation and reasoning (KRR) systems describe and reason with
complex concepts and relations in the form of facts and rules. Unfortunately,
wide deployment of KRR systems runs into the problem that domain experts have
great difficulty constructing correct logical representations of their domain
knowledge. Knowledge engineers can help with this construction process, but
there is a deficit of such specialists. The earlier Knowledge Authoring Logic
Machine (KALM) based on Controlled Natural Language (CNL) was shown to have
very high accuracy for authoring facts and questions. More recently, KALMFL, a
successor of KALM, replaced CNL with factual English, which is much less
restrictive and requires very little training from users. However, KALMFL has
limitations in representing certain types of knowledge, such as authoring rules
for multi-step reasoning or understanding actions with timestamps. To address
these limitations, we propose KALMRA to enable authoring of rules and actions.
Our evaluation using the UTI guidelines benchmark shows that KALMRA achieves a
high level of correctness (100%) on rule authoring. When used for authoring and
reasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI
benchmark, demonstrating its effectiveness in more sophisticated KRR jobs.
Finally, we illustrate the logical reasoning capabilities of KALMRA by drawing
attention to the problems faced by the recently made famous AI, ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07763v1' target="_blank">http://arxiv.org/pdf/2305.07763v1</a><br> <br> <br> <font size='5'> 361 </font> <div style="text-align: right"> 2023-05-12 16:54:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The remarkable achievements of Large Language Models (LLMs) have led to the
emergence of a novel recommendation paradigm -- Recommendation via LLM
(RecLLM). Nevertheless, it is important to note that LLMs may contain social
prejudices, and therefore, the fairness of recommendations made by RecLLM
requires further investigation. To avoid the potential risks of RecLLM, it is
imperative to evaluate the fairness of RecLLM with respect to various sensitive
attributes on the user side. Due to the differences between the RecLLM paradigm
and the traditional recommendation paradigm, it is problematic to directly use
the fairness benchmark of traditional recommendation. To address the dilemma,
we propose a novel benchmark called Fairness of Recommendation via LLM
(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset
that accounts for eight sensitive attributes1 in two recommendation scenarios:
music and movies. By utilizing our FaiRLLM benchmark, we conducted an
evaluation of ChatGPT and discovered that it still exhibits unfairness to some
sensitive attributes when generating recommendations. Our code and dataset can
be found at https://github.com/jizhi-zhang/FaiRLLM. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07609v2' target="_blank">http://arxiv.org/pdf/2305.07609v2</a><br> <br> <br> <font size='5'> 362 </font> <div style="text-align: right"> 2023-05-12 16:52:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI: Implications and Applications for Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The launch of ChatGPT in November 2022 precipitated a panic among some
educators while prompting qualified enthusiasm from others. Under the umbrella
term Generative AI, ChatGPT is an example of a range of technologies for the
delivery of computer-generated text, image, and other digitized media. This
paper examines the implications for education of one generative AI technology,
chatbots responding from large language models, or C-LLM. It reports on an
application of a C-LLM to AI review and assessment of complex student work. In
a concluding discussion, the paper explores the intrinsic limits of generative
AI, bound as it is to language corpora and their textual representation through
binary notation. Within these limits, we suggest the range of emerging and
potential applications of Generative AI in education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07605v3' target="_blank">http://arxiv.org/pdf/2305.07605v3</a><br> <br> <br> <font size='5'> 363 </font> <div style="text-align: right"> 2023-05-12 14:04:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, large language models (LLMs) have made significant progress
in natural language processing (NLP), with models like ChatGPT and GPT-4
achieving impressive capabilities in various linguistic tasks. However,
training models on such a large scale is challenging, and finding datasets that
match the model's scale is often difficult. Fine-tuning and training models
with fewer parameters using novel methods have emerged as promising approaches
to overcome these challenges. One such model is MiniGPT-4, which achieves
comparable vision-language understanding to GPT-4 by leveraging novel
pre-training models and innovative training strategies. However, the model
still faces some challenges in image understanding, particularly in artistic
pictures. A novel multimodal model called ArtGPT-4 has been proposed to address
these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100
device in just 2 hours, using only about 200 GB of data. The model can depict
images with an artistic flair and generate visual code, including aesthetically
pleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks
for evaluating the performance of vision-language models. In the subsequent
evaluation methods, ArtGPT-4 scored more than 1 point higher than the current
\textbf{state-of-the-art} model and was only 0.25 points lower than artists on
a 6-point scale. Our code and pre-trained model are available at
\url{https://huggingface.co/Tyrannosaurus/ArtGPT-4}. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07490v2' target="_blank">http://arxiv.org/pdf/2305.07490v2</a><br> <br> <br> <font size='5'> 364 </font> <div style="text-align: right"> 2023-05-12 12:52:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unlocking the Potential of Medical Imaging with ChatGPT's Intelligent Diagnostics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Medical imaging is an essential tool for diagnosing various healthcare
diseases and conditions. However, analyzing medical images is a complex and
time-consuming task that requires expertise and experience. This article aims
to design a decision support system to assist healthcare providers and patients
in making decisions about diagnosing, treating, and managing health conditions.
The proposed architecture contains three stages: 1) data collection and
labeling, 2) model training, and 3) diagnosis report generation. The key idea
is to train a deep learning model on a medical image dataset to extract four
types of information: the type of image scan, the body part, the test image,
and the results. This information is then fed into ChatGPT to generate
automatic diagnostics. The proposed system has the potential to enhance
decision-making, reduce costs, and improve the capabilities of healthcare
providers. The efficacy of the proposed system is analyzed by conducting
extensive experiments on a large medical image dataset. The experimental
outcomes exhibited promising performance for automatic diagnosis through
medical images. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07429v1' target="_blank">http://arxiv.org/pdf/2305.07429v1</a><br> <br> <br> <font size='5'> 365 </font> <div style="text-align: right"> 2023-05-12 10:54:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT's upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07375v3' target="_blank">http://arxiv.org/pdf/2305.07375v3</a><br> <br> <br> <font size='5'> 366 </font> <div style="text-align: right"> 2023-05-12 09:37:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: METHODS: First, a set of evaluation criteria is designed based on a
comprehensive literature review. Second, existing candidate criteria are
optimized for using a Delphi method by five experts in medicine and
engineering. Third, three clinical experts design a set of medical datasets to
interact with LLMs. Finally, benchmarking experiments are conducted on the
datasets. The responses generated by chatbots based on LLMs are recorded for
blind evaluations by five licensed medical experts. RESULTS: The obtained
evaluation criteria cover medical professional capabilities, social
comprehensive capabilities, contextual capabilities, and computational
robustness, with sixteen detailed indicators. The medical datasets include
twenty-seven medical dialogues and seven case reports in Chinese. Three
chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor
PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental
results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both
multiple-turn medical dialogue and case report scenarios. </font><br> Link: <a href='http://arxiv.org/pdf/2305.07340v1' target="_blank">http://arxiv.org/pdf/2305.07340v1</a><br> <br> <br> <font size='5'> 367 </font> <div style="text-align: right"> 2023-05-12 09:27:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Chemistry Learning with ChatGPT and Bing Chat as Agents to Think With: A Comparative Case Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study explores the potential of Generative AI chatbots (GenAIbots) such
as ChatGPT and Bing Chat, in Chemistry education, within a constructionist
theoretical framework. A single-case study methodology was used to analyse
extensive interaction logs between students and both AI systems in simulated
Chemistry learning experiences. The results highlight the ability of ChatGPT
and Bing Chat to act as 'agents-to-think-with', fostering critical thinking,
problem-solving, concept comprehension, creativity, and personalised learning
experiences. By employing a Socratic-like questioning approach, GenAIbots
nurture students' curiosity and promote active learning. The study emphasises
the significance of prompt crafting, a technique to elicit desired responses
from GenAIbots, fostering iterative reflections and interactions. It underlines
the need for comprehensive educator training to effectively integrate these
tools into classrooms. The study concludes that while ChatGPT and Bing Chat as
agents-to-think-with offer promising avenues to revolutionise STEM education
through a constructionist lens, fostering a more interactive, inclusive
learning environment and promoting deeper comprehension and critical thinking
in students across diverse Chemistry topics, ChatGPT consistently outperformed
Bing Chat, providing more comprehensive, detailed, and accurate responses and
skillfully addressing nuances and context. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11890v1' target="_blank">http://arxiv.org/pdf/2305.11890v1</a><br> <br> <br> <font size='5'> 368 </font> <div style="text-align: right"> 2023-05-12 08:22:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A New Era of Artificial Intelligence in Education: A Multifaceted Revolution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent high performance of ChatGPT on several standardized academic test
has thrust the topic of artificial intelligence (AI) into the mainstream
conversation about the future of education. The objective of the present study
is to investigate the effect of AI on education by examining its applications,
advantages, and challenges. Our report focuses on the use of artificial
intelligence in collaborative teacher-student learning, intelligent tutoring
systems, automated assessment, and personalized learning. We also look into
potential negative aspects, ethical issues, and possible future routes for AI
implementation in education. Ultimately, we find that the only way forward is
to accept and embrace the new technology, while implementing guardrails to
prevent its abuse. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18303v1' target="_blank">http://arxiv.org/pdf/2305.18303v1</a><br> <br> <br> <font size='5'> 369 </font> <div style="text-align: right"> 2023-05-12 07:21:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Segment anything model (SAM) developed by Meta AI Research has recently
attracted significant attention. Trained on a large segmentation dataset of
over 1 billion masks, SAM is capable of segmenting any object on a certain
image. In the original SAM work, the authors turned to zero-short transfer
tasks (like edge detection) for evaluating the performance of SAM. Recently,
numerous works have attempted to investigate the performance of SAM in various
scenarios to recognize and segment objects. Moreover, numerous projects have
emerged to show the versatility of SAM as a foundation model by combining it
with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With
the relevant papers and projects increasing exponentially, it is challenging
for the readers to catch up with the development of SAM. To this end, this work
conducts the first yet comprehensive survey on SAM. This is an ongoing project
and we intend to update the manuscript on a regular basis. Therefore, readers
are welcome to contact us if they complete new works related to SAM so that we
can include them in our next version. </font><br> Link: <a href='http://arxiv.org/pdf/2306.06211v3' target="_blank">http://arxiv.org/pdf/2306.06211v3</a><br> <br> <br> <font size='5'> 370 </font> <div style="text-align: right"> 2023-05-11 15:03:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Taking Advice from ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A growing literature studies how humans incorporate advice from algorithms.
This study examines an algorithm with millions of daily users: ChatGPT. In a
preregistered study, 118 student participants answer 2,828 multiple-choice
questions across 25 academic subjects. Participants receive advice from a GPT
model and can update their initial responses. The advisor's identity ("AI
chatbot" versus a human "expert"), presence of a written justification, and
advice correctness do not significantly affect weight on advice. Instead,
participants weigh advice more heavily if they (1) are unfamiliar with the
topic, (2) used ChatGPT in the past, or (3) received more accurate advice
previously. The last two effects -- algorithm familiarity and experience -- are
stronger with an AI chatbot as the advisor. Participants that receive written
justifications are able to discern correct advice and update accordingly.
Student participants are miscalibrated in their judgements of ChatGPT advice
accuracy; one reason is that they significantly misjudge the accuracy of
ChatGPT on 11/25 topics. Participants under-weigh advice by over 50% and can
score better by trusting ChatGPT more. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11888v3' target="_blank">http://arxiv.org/pdf/2305.11888v3</a><br> <br> <br> <font size='5'> 371 </font> <div style="text-align: right"> 2023-05-11 06:43:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enabling Programming Thinking in Large Language Models Toward Code Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive
performance in code generation. A large-scale study released that writing
programs requires programming thinking, i.e., analyzing and implementing
requirements in programming logic (e.g., sequence, branch, loop). Existing
studies use LLMs to generate programs from requirements directly and do not
explicitly introduce the programming thinking.
  This paper explores how to unlock the programming thinking of LLMs in code
generation and proposes an approach named TiP. Our idea is to decompose code
generation into two steps and progressively lead LLMs to analyze&implement
requirements in programming logic. Specifically, TiP first generates a code
sketch, which provides a high-level solving process using programming logic but
omits implementation details (e.g., APIs). Then, TiP implements the sketch into
a program using specific programming languages. We conduct extensive
experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1)
TiP outperforms the state-of-the-art baseline - ChatGPT by up to 17.5% in
Pass@1, 11.02% in Pass@3, and 9.84% in Pass@5. (2) Human evaluation shows that
TiP outperforms ChatGPT in three aspects (i.e., correctness, code quality, and
maintainability). (3) TiP is effective for different LLMs. (4) We explore
multiple choices (e.g., chain-of-thought) for the code sketch and validate the
superiority of our design. (5) We discuss the complementarity between TiP and
post-processing approaches (e.g., CodeT). </font><br> Link: <a href='http://arxiv.org/pdf/2305.06599v1' target="_blank">http://arxiv.org/pdf/2305.06599v1</a><br> <br> <br> <font size='5'> 372 </font> <div style="text-align: right"> 2023-05-11 05:19:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have shown surprisingly good performance in
multilingual neural machine translation (MNMT) even when trained without
parallel data. Yet, despite the fact that the amount of training data is
gigantic, they still struggle with translating rare words, particularly for
low-resource languages. Even worse, it is usually unrealistic to retrieve
relevant demonstrations for in-context learning with low-resource languages on
LLMs, which restricts the practical use of LLMs for translation -- how should
we mitigate this problem? To this end, we present a novel method, CoD, which
augments LLMs with prior knowledge with the chains of multilingual dictionaries
for a subset of input words to elicit translation abilities for LLMs. Extensive
experiments indicate that augmenting ChatGPT with CoD elicits large gains by up
to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in
Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the
importance of chaining the multilingual dictionaries, as well as the
superiority of CoD to few-shot demonstration for low-resource languages. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06575v3' target="_blank">http://arxiv.org/pdf/2305.06575v3</a><br> <br> <br> <font size='5'> 373 </font> <div style="text-align: right"> 2023-05-10 21:37:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Prognostics and health management (PHM) technology plays a critical role in
industrial production and equipment maintenance by identifying and predicting
possible equipment failures and damages, thereby allowing necessary maintenance
measures to be taken to enhance equipment service life and reliability while
reducing production costs and downtime. In recent years, PHM technology based
on artificial intelligence (AI) has made remarkable achievements in the context
of the industrial IoT and big data, and it is widely used in various
industries, such as railway, energy, and aviation, for condition monitoring,
fault prediction, and health management. The emergence of large-scale
foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of
AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved
from a research paradigm of single-modal, single-task, and limited-data to a
multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT
represents a landmark achievement in this research paradigm, offering hope for
general artificial intelligence due to its highly intelligent natural language
understanding ability. However, the PHM field lacks a consensus on how to
respond to this significant change in the AI field, and a systematic review and
roadmap is required to elucidate future development directions. To fill this
gap, this paper systematically expounds on the key components and latest
developments of LSF-Models. Then, we systematically answered how to build the
LSF-Model applicable to PHM tasks and outlined the challenges and future
development roadmaps for this research paradigm. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06472v2' target="_blank">http://arxiv.org/pdf/2305.06472v2</a><br> <br> <br> <font size='5'> 374 </font> <div style="text-align: right"> 2023-05-10 20:46:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Autonomous GIS: the next-generation AI-powered GIS</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as ChatGPT, demonstrate a strong
understanding of human natural language and have been explored and applied in
various fields, including reasoning, creative writing, code generation,
translation, and information retrieval. By adopting LLM as the reasoning core,
we introduce Autonomous GIS as an AI-powered geographic information system
(GIS) that leverages the LLM's general abilities in natural language
understanding, reasoning, and coding for addressing spatial problems with
automatic spatial data collection, analysis, and visualization. We envision
that autonomous GIS will need to achieve five autonomous goals:
self-generating, self-organizing, self-verifying, self-executing, and
self-growing. We developed a prototype system called LLM-Geo using the GPT-4
API in a Python environment, demonstrating what an autonomous GIS looks like
and how it delivers expected results without human intervention using three
case studies. For all case studies, LLM-Geo was able to return accurate
results, including aggregated numbers, graphs, and maps, significantly reducing
manual operation time. Although still in its infancy and lacking several
important modules such as logging and code testing, LLM-Geo demonstrates a
potential path toward the next-generation AI-powered GIS. We advocate for the
GIScience community to dedicate more effort to the research and development of
autonomous GIS, making spatial analysis easier, faster, and more accessible to
a broader audience. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06453v4' target="_blank">http://arxiv.org/pdf/2305.06453v4</a><br> <br> <br> <font size='5'> 375 </font> <div style="text-align: right"> 2023-05-10 19:09:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bot or Human? Detecting ChatGPT Imposters with A Single Question</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models like ChatGPT have recently demonstrated impressive
capabilities in natural language understanding and generation, enabling various
applications including translation, essay writing, and chit-chatting. However,
there is a concern that they can be misused for malicious purposes, such as
fraud or denial-of-service attacks. Therefore, it is crucial to develop methods
for detecting whether the party involved in a conversation is a bot or a human.
In this paper, we propose a framework named FLAIR, Finding Large language model
Authenticity via a single Inquiry and Response, to detect conversational bots
in an online manner. Specifically, we target a single question scenario that
can effectively differentiate human users from bots. The questions are divided
into two categories: those that are easy for humans but difficult for bots
(e.g., counting, substitution, positioning, noise filtering, and ASCII art),
and those that are easy for bots but difficult for humans (e.g., memorization
and computation). Our approach shows different strengths of these questions in
their effectiveness, providing a new way for online service providers to
protect themselves against nefarious activities and ensure that they are
serving real users. We open-sourced our dataset on
https://github.com/hongwang600/FLAIR and welcome contributions from the
community to enrich such detection datasets. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06424v2' target="_blank">http://arxiv.org/pdf/2305.06424v2</a><br> <br> <br> <font size='5'> 376 </font> <div style="text-align: right"> 2023-05-10 13:29:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: When ChatGPT for Computer Vision Will Come? From 2D to 3D</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT and its improved variant GPT4 have revolutionized the NLP field with
a single model solving almost all text related tasks. However, such a model for
computer vision does not exist, especially for 3D vision. This article first
provides a brief view on the progress of deep learning in text, image and 3D
fields from the model perspective. Moreover, this work further discusses how
AIGC evolves from the data perspective. On top of that, this work presents an
outlook on the development of AIGC in 3D from the data perspective. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06133v1' target="_blank">http://arxiv.org/pdf/2305.06133v1</a><br> <br> <br> <font size='5'> 377 </font> <div style="text-align: right"> 2023-05-10 12:10:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Glimpse in ChatGPT Capabilities and its impact for AI research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have recently become a popular topic in the
field of Artificial Intelligence (AI) research, with companies such as Google,
Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their
development. These models are trained on massive amounts of data and can be
used for a wide range of tasks, including language translation, text
generation, and question answering. However, the computational resources
required to train and run these models are substantial, and the cost of
hardware and electricity can be prohibitive for research labs that do not have
the funding and resources of the GAFA. In this paper, we will examine the
impact of LLMs on AI research. The pace at which such models are generated as
well as the range of domains covered is an indication of the trend which not
only the public but also the scientific community is currently experiencing. We
give some examples on how to use such models in research by focusing on
GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range
of capabilities in a single system is a strong sign of approaching general
intelligence. Innovations integrating such models will also expand along the
maturation of such AI systems and exhibit unforeseeable applications that will
have important impacts on several aspects of our societies. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06087v1' target="_blank">http://arxiv.org/pdf/2305.06087v1</a><br> <br> <br> <font size='5'> 378 </font> <div style="text-align: right"> 2023-05-10 10:14:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This technical paper introduces a chatting robot system that utilizes recent
advancements in large-scale language models (LLMs) such as GPT-3 and ChatGPT.
The system is integrated with a co-speech gesture generation system, which
selects appropriate gestures based on the conceptual meaning of speech. Our
motivation is to explore ways of utilizing the recent progress in LLMs for
practical robotic applications, which benefits the development of both chatbots
and LLMs. Specifically, it enables the development of highly responsive chatbot
systems by leveraging LLMs and adds visual effects to the user interface of
LLMs as an additional value. The source code for the system is available on
GitHub for our in-house robot
(https://github.com/microsoft/LabanotationSuite/tree/master/MSRAbotChatSimulation)
and GitHub for Toyota HSR
(https://github.com/microsoft/GPT-Enabled-HSR-CoSpeechGestures). </font><br> Link: <a href='http://arxiv.org/pdf/2306.01741v1' target="_blank">http://arxiv.org/pdf/2306.01741v1</a><br> <br> <br> <font size='5'> 379 </font> <div style="text-align: right"> 2023-05-10 09:02:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bits of Grass: Does GPT already know how to write like Whitman?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4
models to generate poems in the style of specific authors using zero-shot and
many-shot prompts (which use the maximum context length of 8192 tokens). We
assess the performance of models that are not fine-tuned for generating poetry
in the style of specific authors, via automated evaluation. Our findings
indicate that without fine-tuning, even when provided with the maximum number
of 17 poem examples (8192 tokens) in the prompt, these models do not generate
poetry in the desired style. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11064v1' target="_blank">http://arxiv.org/pdf/2305.11064v1</a><br> <br> <br> <font size='5'> 380 </font> <div style="text-align: right"> 2023-05-10 08:16:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Since the release of ChatGPT, numerous studies have highlighted the
remarkable performance of ChatGPT, which often rivals or even surpasses human
capabilities in various tasks and domains. However, this paper presents a
contrasting perspective by demonstrating an instance where human performance
excels in typical tasks suited for ChatGPT, specifically in the domain of
computer programming. We utilize the IEEExtreme Challenge competition as a
benchmark, a prestigious, annual international programming contest encompassing
a wide range of problems with different complexities. To conduct a thorough
evaluation, we selected and executed a diverse set of 102 challenges, drawn
from five distinct IEEExtreme editions, using three major programming
languages: Python, Java, and C++. Our empirical analysis provides evidence that
contrary to popular belief, human programmers maintain a competitive edge over
ChatGPT in certain aspects of problem-solving within the programming context.
In fact, we found that the average score obtained by ChatGPT on the set of
IEEExtreme programming problems is 3.9 to 5.8 times lower than the average
human score, depending on the programming language. This paper elaborates on
these findings, offering critical insights into the limitations and potential
areas of improvement for AI-based language models like ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06934v1' target="_blank">http://arxiv.org/pdf/2305.06934v1</a><br> <br> <br> <font size='5'> 381 </font> <div style="text-align: right"> 2023-05-10 06:17:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fast Distributed Inference Serving for Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) power a new generation of interactive AI
applications exemplified by ChatGPT. The interactive nature of these
applications demand low job completion time (JCT) for model inference. Existing
LLM serving systems use run-to-completion processing for inference jobs, which
suffers from head-of-line blocking and long JCT. We present FastServe, a
distributed inference serving system for LLMs. FastServe exploits the
autoregressive pattern of LLM inference to enable preemption at the granularity
of each output token. FastServe uses preemptive scheduling to minimize JCT with
a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi
information-agnostic setting of LLM inference, the scheduler leverages the
input length information to assign an appropriate initial queue for each
arrival job to join. The higher priority queues than the joined queue are
skipped to reduce demotions. We design an efficient GPU memory management
mechanism that proactively offloads and uploads intermediate states between GPU
memory and host memory for LLM inference. We build a system prototype of
FastServe based on NVIDIA FasterTransformer. Experimental results show that
compared to the state-of-the-art solution Orca, FastServe improves the average
and tail JCT by up to 5.1$\times$ and 6.4$\times$, respectively. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05920v1' target="_blank">http://arxiv.org/pdf/2305.05920v1</a><br> <br> <br> <font size='5'> 382 </font> <div style="text-align: right"> 2023-05-10 03:13:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The most recent large language models such as ChatGPT and GPT-4 have garnered
significant attention, as they are capable of generating high-quality responses
to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic
text corpora, showcasing their impressive capabilities, a study focusing on
financial corpora has not been conducted. In this study, we aim to bridge this
gap by examining the potential of ChatGPT and GPT-4 as a solver for typical
financial text analytic problems in the zero-shot or few-shot setting.
Specifically, we assess their capabilities on four representative tasks over
five distinct financial textual datasets. The preliminary study shows that
ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition
(NER) and sentiment analysis, where domain-specific knowledge is required,
while they excel in numerical reasoning tasks. We report both the strengths and
limitations of the current versions of ChatGPT and GPT-4, comparing them to the
state-of-the-art finetuned models as well as pretrained domain-specific
generative models. Our experiments provide qualitative studies, through which
we hope to help understand the capability of the existing models and facilitate
further improvements. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05862v1' target="_blank">http://arxiv.org/pdf/2305.05862v1</a><br> <br> <br> <font size='5'> 383 </font> <div style="text-align: right"> 2023-05-09 19:55:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Teamwork is a critical component of many academic and professional settings.
In those contexts, feedback between team members is an important element to
facilitate successful and sustainable teamwork. However, in the classroom, as
the number of teams and team members and frequency of evaluation increase, the
volume of comments can become overwhelming for an instructor to read and track,
making it difficult to identify patterns and areas for student improvement. To
address this challenge, we explored the use of generative AI models,
specifically ChatGPT, to analyze student comments in team based learning
contexts. Our study aimed to evaluate ChatGPT's ability to accurately identify
topics in student comments based on an existing framework consisting of
positive and negative comments. Our results suggest that ChatGPT can achieve
over 90\% accuracy in labeling student comments, providing a potentially
valuable tool for analyzing feedback in team projects. This study contributes
to the growing body of research on the use of AI models in educational contexts
and highlights the potential of ChatGPT for facilitating analysis of student
comments. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11882v1' target="_blank">http://arxiv.org/pdf/2305.11882v1</a><br> <br> <br> <font size='5'> 384 </font> <div style="text-align: right"> 2023-05-09 19:17:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Vision-Language Models in Remote Sensing: Current Progress and Future Trends</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide us with intelligent solutions
that are more similar to human thinking, enabling us to use general artificial
intelligence to solve problems in various applications. However, in the field
of remote sensing, the scientific literature on the implementation of AGI
remains relatively scant. Existing AI-related research primarily focuses on
visual understanding tasks while neglecting the semantic understanding of the
objects and their relationships. This is where vision-language models excel, as
they enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond recognizing the objects in an image and
can infer the relationships between them, as well as generate natural language
descriptions of the image. This makes them better suited for tasks that require
both visual and textual understanding, such as image captioning, text-based
image retrieval, and visual question answering. This paper provides a
comprehensive review of the research on vision-language models in remote
sensing, summarizing the latest progress, highlighting the current challenges,
and identifying potential research opportunities. Specifically, we review the
application of vision-language models in several mainstream remote sensing
tasks, including image captioning, text-based image generation, text-based
image retrieval, visual question answering, scene classification, semantic
segmentation, and object detection. For each task, we briefly describe the task
background and review some representative works. Finally, we summarize the
limitations of existing work and provide some possible directions for future
development. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05726v1' target="_blank">http://arxiv.org/pdf/2305.05726v1</a><br> <br> <br> <font size='5'> 385 </font> <div style="text-align: right"> 2023-05-09 17:58:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present an interactive visual framework named InternGPT, or iGPT for
short. The framework integrates chatbots that have planning and reasoning
capabilities, such as ChatGPT, with non-verbal instructions like pointing
movements that enable users to directly manipulate images or videos on the
screen. Pointing (including gestures, cursors, etc.) movements can provide more
flexibility and precision in performing vision-centric tasks that require
fine-grained control, editing, and generation of visual content. The name
InternGPT stands for \textbf{inter}action, \textbf{n}onverbal, and
\textbf{chat}bots. Different from existing interactive systems that rely on
pure language, by incorporating pointing instructions, the proposed iGPT
significantly improves the efficiency of communication between users and
chatbots, as well as the accuracy of chatbots in vision-centric tasks,
especially in complicated visual scenarios where the number of objects is
greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used
to improve the control capability of LLM, and a large vision-language model
termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing
ChatGPT-3.5-turbo with 93.89\% GPT-4 Quality). We hope this work can spark new
ideas and directions for future interactive visual systems. Welcome to watch
the code at https://github.com/OpenGVLab/InternGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05662v4' target="_blank">http://arxiv.org/pdf/2305.05662v4</a><br> <br> <br> <font size='5'> 386 </font> <div style="text-align: right"> 2023-05-09 17:42:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Building the Federated GPT: Federated Instruction Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While ``instruction-tuned" generative large language models (LLMs) have
demonstrated an impressive ability to generalize to new tasks, the training
phases heavily rely on large amounts of diverse and high-quality instruction
data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,
especially when it comes to human-written data, can pose significant challenges
both in terms of cost and accessibility. Moreover, concerns related to privacy
can further limit access to such data, making the process of obtaining it a
complex and nuanced undertaking. Consequently, this hinders the generality of
the tuned models and may restrict their effectiveness in certain contexts. To
tackle this issue, our study introduces a new approach called Federated
Instruction Tuning (FedIT), which leverages federated learning (FL) as the
learning framework for the instruction tuning of LLMs. This marks the first
exploration of FL-based instruction tuning for LLMs. This is especially
important since text data is predominantly generated by end users. Therefore,
it is imperative to design and adapt FL approaches to effectively leverage
these users' diverse instructions stored on local devices, while preserving
privacy and ensuring data security. In the current paper, by conducting widely
used GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous
and diverse sets of instructions on the client's end with the proposed
framework FedIT, we improved the performance of LLMs compared to centralized
training with only limited local instructions. Further, in this paper, we
developed a Github repository named Shepherd. This repository offers a
foundational framework for exploring federated fine-tuning of LLMs using
heterogeneous instructions across diverse categories. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05644v1' target="_blank">http://arxiv.org/pdf/2305.05644v1</a><br> <br> <br> <font size='5'> 387 </font> <div style="text-align: right"> 2023-05-09 16:58:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Case Records of ChatGPT: Language Models and Complex Clinical Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Artificial intelligence language models have shown promise in
various applications, including assisting with clinical decision-making as
demonstrated by strong performance of large language models on medical
licensure exams. However, their ability to solve complex, open-ended cases,
which may be representative of clinical practice, remains unexplored. Methods:
In this study, the accuracy of large language AI models GPT4 and GPT3.5 in
diagnosing complex clinical cases was investigated using published Case Records
of the Massachusetts General Hospital. A total of 50 cases requiring a
diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022
were identified. For each case, models were given a prompt requesting the top
three specific diagnoses and associated diagnostic tests, followed by case
text, labs, and figure legends. Model outputs were assessed in comparison to
the final clinical diagnosis and whether the model-predicted test would result
in a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the
correct diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%
within three attempts, respectively. GPT4 and GPT3.5 provided a correct
essential diagnostic test in 28% and 24% of cases in one attempt, and 44% and
50% within three attempts, respectively. No significant differences were found
between the two models, and multiple trials with identical prompts using the
GPT3.5 model provided similar results. Conclusions: In summary, these models
demonstrate potential usefulness in generating differential diagnoses but
remain limited in their ability to provide a single unifying diagnosis in
complex, open-ended cases. Future research should focus on evaluating model
performance in larger datasets of open-ended clinical challenges and exploring
potential human-AI collaboration strategies to enhance clinical
decision-making. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05609v1' target="_blank">http://arxiv.org/pdf/2305.05609v1</a><br> <br> <br> <font size='5'> 388 </font> <div style="text-align: right"> 2023-05-09 13:10:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as a Text Simplification Tool to Remove Bias</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The presence of specific linguistic signals particular to a certain sub-group
of people can be picked up by language models during training. If the model
begins to associate specific language with a distinct group, any decisions made
based upon this language would hold a strong correlation to a decision based
upon their protected characteristic, leading to possible discrimination. We
explore a potential technique for bias mitigation in the form of simplification
of text. The driving force of this idea is that simplifying text should
standardise language between different sub-groups to one way of speaking while
keeping the same meaning. The experiment shows promising results as the
classifier accuracy for predicting the sensitive attribute drops by up to 17%
for the simplified data. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06166v2' target="_blank">http://arxiv.org/pdf/2305.06166v2</a><br> <br> <br> <font size='5'> 389 </font> <div style="text-align: right"> 2023-05-09 11:37:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Taxonomy of Foundation Model based Systems for Responsible-AI-by-Design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent release of large language model (LLM) based chatbots, such as
ChatGPT, has attracted significant attention on foundation models. It is widely
believed that foundation models will serve as the fundamental building blocks
for future AI systems. As foundation models are in their early stages, the
design of foundation model based systems has not yet been systematically
explored. There is little understanding about the impact of introducing
foundation models in software architecture. Therefore, in this paper, we
propose a taxonomy of foundation model based systems, which classifies and
compares the characteristics of foundation models and design options of
foundation model based systems. Our taxonomy comprises three categories:
foundation model pretraining and fine-tuning, architecture design of foundation
model based systems, and responsible-AI-by-design. This taxonomy provides
concrete guidance for making major design decisions when designing foundation
model based systems and highlights trade-offs arising from design decisions. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05352v4' target="_blank">http://arxiv.org/pdf/2305.05352v4</a><br> <br> <br> <font size='5'> 390 </font> <div style="text-align: right"> 2023-05-09 05:25:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models have shown impressive abilities on various tasks.
However, fundamentally improving them depends on high-quality datasets or
computationally expensive fine-tuning. On the contrary, human can easily
improve themselves by thinking and memory, without external resources. In this
paper, we propose a framework, MoT, to let the LLM self-improve through Memory
of Thoughts, without annotated datasets and parameter updates. Specifically,
the framework is divided into two stages: 1. before the test stage, we let the
LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as
external memory; 2. during inference, given a test question, we let the LLM
recall relevant memory to help itself reason and answer it. Experimental
results show that the proposed framework can help ChatGPT significantly improve
its abilities in math reasoning, commonsense reasoning, factual reasoning and
natural language inference. Further analyses show that each component
contributes critically to the improvements. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05181v1' target="_blank">http://arxiv.org/pdf/2305.05181v1</a><br> <br> <br> <font size='5'> 391 </font> <div style="text-align: right"> 2023-05-09 05:11:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is a rapidly growing number of large language models (LLMs) that users
can query for a fee. We review the cost associated with querying popular LLM
APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have
heterogeneous pricing structures, with fees that can differ by two orders of
magnitude. In particular, using LLMs on large collections of queries and text
can be expensive. Motivated by this, we outline and discuss three types of
strategies that users can exploit to reduce the inference cost associated with
using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As
an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM
cascade which learns which combinations of LLMs to use for different queries in
order to reduce cost and improve accuracy. Our experiments show that FrugalGPT
can match the performance of the best individual LLM (e.g. GPT-4) with up to
98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.
The ideas and findings presented here lay a foundation for using LLMs
sustainably and efficiently. </font><br> Link: <a href='http://arxiv.org/pdf/2305.05176v1' target="_blank">http://arxiv.org/pdf/2305.05176v1</a><br> <br> <br> <font size='5'> 392 </font> <div style="text-align: right"> 2023-05-09 02:38:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generating Phishing Attacks using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ability of ChatGPT to generate human-like responses and understand
context has made it a popular tool for conversational agents, content creation,
data analysis, and research and innovation. However, its effectiveness and ease
of accessibility makes it a prime target for generating malicious content, such
as phishing attacks, that can put users at risk. In this work, we identify
several malicious prompts that can be provided to ChatGPT to generate
functional phishing websites. Through an iterative approach, we find that these
phishing websites can be made to imitate popular brands and emulate several
evasive tactics that have been known to avoid detection by anti-phishing
entities. These attacks can be generated using vanilla ChatGPT without the need
of any prior adversarial exploits (jailbreaking). </font><br> Link: <a href='http://arxiv.org/pdf/2305.05133v1' target="_blank">http://arxiv.org/pdf/2305.05133v1</a><br> <br> <br> <font size='5'> 393 </font> <div style="text-align: right"> 2023-05-08 17:57:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The impact and applications of ChatGPT: a systematic review of literature reviews</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The conversational artificial-intelligence (AI) technology ChatGPT has become
one of the most widely used natural language processing tools. With thousands
of published papers demonstrating its applications across various industries
and fields, ChatGPT has sparked significant interest in the research community.
Reviews of primary data have also begun to emerge. An overview of the available
evidence from multiple reviews and studies could provide further insights,
minimize redundancy, and identify areas where further research is needed.
Objective: To evaluate the existing reviews and literature related to ChatGPT's
applications and its potential impact on different fields by conducting a
systematic review of reviews and bibliometric analysis of primary literature.
Methods: PubMed, EuropePMC, Dimensions AI, medRxiv, bioRxiv, arXiv, and Google
Scholar were searched for ChatGPT-related publications from 2022 to 4/30/2023.
Studies including secondary data related to the application of ChatGPT were
considered. Reporting and risk of bias assesment was performed using PRISMA
guidelines. Results: A total of 305 unique records with potential relevance to
the review were identified from a pool of over 2,000 original articles. After
multi-step screening process, 11 reviews were selected, consisting of 9 reviews
specifically focused on ChatGPT and 2 reviews on broader AI topics that also
included discussions on ChatGPT. We also conducted bibliometric analysis of
primary data. Conclusions: While AI has the potential to revolutionize various
industries, further interdisciplinary research, customized integrations, and
ethical innovation are necessary to address existing concerns and ensure its
responsible use. Protocol Registration: PROSPERO registration no.
CRD42023417336, DOI 10.17605/OSF.IO/87U6Q. </font><br> Link: <a href='http://arxiv.org/pdf/2305.18086v1' target="_blank">http://arxiv.org/pdf/2305.18086v1</a><br> <br> <br> <font size='5'> 394 </font> <div style="text-align: right"> 2023-05-08 15:12:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatUniTest: a ChatGPT-based automated unit test generation tool</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Unit testing is a crucial, yet often tedious and time-consuming task. To
relieve developers from this burden, automated unit test generation techniques
are developed. Existing automated unit test generation tools, such as
program-analysis-based tools like EvoSuite and Randoop, lack program
comprehension, resulting in unit tests with poor readability and limited
assertions. Language-model-based tools, such as AthenaTest and A3Test, have
limitations in the generation of correct unit tests. In this paper, we
introduce ChatUniTest, a ChatGPT-based automated unit test generation tool
developed under the Generation-Validation-Repair framework. ChatUniTest
generates tests by parsing the project, extracting essential information, and
creating an adaptive focal context that includes the focal method and its
dependencies within the pre-defined maximum prompt token limit. The context is
incorporated into a prompt and subsequently submitted to ChatGPT. Once
ChatGPT's response is received, ChatUniTest proceeds to extract the raw test
from the response. It then validates the test and employs rule-based repair to
fix syntactic and simple compile errors, followed by ChatGPT-based repair to
address challenging errors. Our rigorous evaluation demonstrates that
ChatUniTest outperforms EvoSuite in branch and line coverage, surpasses
AthenaTest and A3Test in focal method coverage, and effectively generates
assertions while utilizing mock objects and reflection to achieve test
objectives. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04764v1' target="_blank">http://arxiv.org/pdf/2305.04764v1</a><br> <br> <br> <font size='5'> 395 </font> <div style="text-align: right"> 2023-05-08 14:54:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: Vision and Challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence (AI) and machine learning have changed the nature of
scientific inquiry in recent years. Of these, the development of virtual
assistants has accelerated greatly in the past few years, with ChatGPT becoming
a prominent AI language model. In this study, we examine the foundations,
vision, research challenges of ChatGPT. This article investigates into the
background and development of the technology behind it, as well as its popular
applications. Moreover, we discuss the advantages of bringing everything
together through ChatGPT and Internet of Things (IoT). Further, we speculate on
the future of ChatGPT by considering various possibilities for study and
development, such as energy-efficiency, cybersecurity, enhancing its
applicability to additional technologies (Robotics and Computer Vision),
strengthening human-AI communications, and bridging the technological gap.
Finally, we discuss the important ethics and current trends of ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.15323v1' target="_blank">http://arxiv.org/pdf/2305.15323v1</a><br> <br> <br> <font size='5'> 396 </font> <div style="text-align: right"> 2023-05-08 12:53:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing Knowledge Graph Construction Using Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The growing trend of Large Language Models (LLM) development has attracted
significant attention, with models for various applications emerging
consistently. However, the combined application of Large Language Models with
semantic technologies for reasoning and inference is still a challenging task.
This paper analyzes how the current advances in foundational LLM, like ChatGPT,
can be compared with the specialized pretrained models, like REBEL, for joint
entity and relation extraction. To evaluate this approach, we conducted several
experiments using sustainability-related text as our use case. We created
pipelines for the automatic creation of Knowledge Graphs from raw texts, and
our findings indicate that using advanced LLM models can improve the accuracy
of the process of creating these graphs from unstructured text. Furthermore, we
explored the potential of automatic ontology creation using foundation LLM
models, which resulted in even more relevant and accurate knowledge graphs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04676v1' target="_blank">http://arxiv.org/pdf/2305.04676v1</a><br> <br> <br> <font size='5'> 397 </font> <div style="text-align: right"> 2023-05-08 07:28:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study investigates the feasibility of developing an Artificial General
Recommender (AGR), facilitated by recent advancements in Large Language Models
(LLMs). An AGR comprises both conversationality and universality to engage in
natural dialogues and generate recommendations across various domains. We
propose ten fundamental principles that an AGR should adhere to, each with its
corresponding testing protocols. We proceed to assess whether ChatGPT, a
sophisticated LLM, can comply with the proposed principles by engaging in
recommendation-oriented dialogues with the model while observing its behavior.
Our findings demonstrate the potential for ChatGPT to serve as an AGR, though
several limitations and areas for improvement are identified. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04518v1' target="_blank">http://arxiv.org/pdf/2305.04518v1</a><br> <br> <br> <font size='5'> 398 </font> <div style="text-align: right"> 2023-05-08 02:50:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improving Cross-Task Generalization with Step-by-Step Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction tuning has been shown to be able to improve cross-task
generalization of language models. However, it is still challenging for
language models to complete the target tasks following the instructions, as the
instructions are general and lack intermediate steps. To address this problem,
we propose to incorporate the step-by-step instructions to help language models
to decompose the tasks, which can provide the detailed and specific procedures
for completing the target tasks. The step-by-step instructions are obtained
automatically by prompting ChatGPT, which are further combined with the
original instructions to tune language models. The extensive experiments on
SUP-NATINST show that the high-quality step-by-step instructions can improve
cross-task generalization across different model sizes. Moreover, the further
analysis indicates the importance of the order of steps of the step-by-step
instruction for the improvement. To facilitate future research, we release the
step-by-step instructions and their human quality evaluation results. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04429v1' target="_blank">http://arxiv.org/pdf/2305.04429v1</a><br> <br> <br> <font size='5'> 399 </font> <div style="text-align: right"> 2023-05-08 01:02:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A Large Language Model (LLM) is an artificial intelligence system that has
been trained on vast amounts of natural language data, enabling it to generate
human-like responses to written or spoken language input. GPT-3.5 is an example
of an LLM that supports a conversational agent called ChatGPT. In this work, we
used a series of novel prompts to determine whether ChatGPT shows heuristics,
biases, and other decision effects. We also tested the same prompts on human
participants. Across four studies, we found that ChatGPT was influenced by
random anchors in making estimates (Anchoring Heuristic, Study 1); it judged
the likelihood of two events occurring together to be higher than the
likelihood of either event occurring alone, and it was erroneously influenced
by salient anecdotal information (Representativeness and Availability
Heuristic, Study 2); it found an item to be more efficacious when its features
were presented positively rather than negatively - even though both
presentations contained identical information (Framing Effect, Study 3); and it
valued an owned item more than a newly found item even though the two items
were identical (Endowment Effect, Study 4). In each study, human participants
showed similar effects. Heuristics and related decision effects in humans are
thought to be driven by cognitive and affective processes such as loss aversion
and effort reduction. The fact that an LLM - which lacks these processes - also
shows such effects invites consideration of the possibility that language may
play a role in generating these effects in humans. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04400v1' target="_blank">http://arxiv.org/pdf/2305.04400v1</a><br> <br> <br> <font size='5'> 400 </font> <div style="text-align: right"> 2023-05-07 10:37:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Perception, performance, and detectability of conversational artificial intelligence across 32 university courses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of large language models has led to the development of powerful
tools such as ChatGPT that can produce text indistinguishable from
human-generated work. With the increasing accessibility of such technology,
students across the globe may utilize it to help with their school work -- a
possibility that has sparked discussions on the integrity of student
evaluations in the age of artificial intelligence (AI). To date, it is unclear
how such tools perform compared to students on university-level courses.
Further, students' perspectives regarding the use of such tools, and educators'
perspectives on treating their use as plagiarism, remain unknown. Here, we
compare the performance of ChatGPT against students on 32 university-level
courses. We also assess the degree to which its use can be detected by two
classifiers designed specifically for this purpose. Additionally, we conduct a
survey across five countries, as well as a more in-depth survey at the authors'
institution, to discern students' and educators' perceptions of ChatGPT's use.
We find that ChatGPT's performance is comparable, if not superior, to that of
students in many courses. Moreover, current AI-text classifiers cannot reliably
detect ChatGPT's use in school work, due to their propensity to classify
human-written answers as AI-generated, as well as the ease with which
AI-generated text can be edited to evade detection. Finally, we find an
emerging consensus among students to use the tool, and among educators to treat
this as plagiarism. Our findings offer insights that could guide policy
discussions addressing the integration of AI into educational frameworks. </font><br> Link: <a href='http://arxiv.org/pdf/2305.13934v1' target="_blank">http://arxiv.org/pdf/2305.13934v1</a><br> <br> <br> <font size='5'> 401 </font> <div style="text-align: right"> 2023-05-07 07:17:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Unit testing is essential in detecting bugs in functionally-discrete program
units. Manually writing high-quality unit tests is time-consuming and
laborious. Although traditional techniques can generate tests with reasonable
coverage, they exhibit low readability and cannot be directly adopted by
developers. Recent work has shown the large potential of large language models
(LLMs) in unit test generation, which can generate more human-like and
meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning
and reinforcement learning, has performed well in various domains. However, It
remains unclear how effective ChatGPT is in unit test generation.
  In this work, we perform the first empirical study to evaluate ChatGPT's
capability of unit test generation. Specifically, we conduct a quantitative
analysis and a user study to systematically investigate the quality of its
generated tests regarding the correctness, sufficiency, readability, and
usability. The tests generated by ChatGPT still suffer from correctness issues,
including diverse compilation errors and execution failures. Still, the passing
tests generated by ChatGPT resemble manually-written tests by achieving
comparable coverage, readability, and even sometimes developers' preference.
Our findings indicate that generating unit tests with ChatGPT could be very
promising if the correctness of its generated tests could be further improved.
  Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based
unit test generation approach, which leverages ChatGPT itself to improve the
quality of its generated tests. ChatTESTER incorporates an initial test
generator and an iterative test refiner. Our evaluation demonstrates the
effectiveness of ChatTESTER by generating 34.3% more compilable tests and 18.7%
more tests with correct assertions than the default ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04207v2' target="_blank">http://arxiv.org/pdf/2305.04207v2</a><br> <br> <br> <font size='5'> 402 </font> <div style="text-align: right"> 2023-05-06 23:57:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Bypassing antivirus detection: old-school malware, new tricks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Being on a mushrooming spree since at least 2013, malware can take a large
toll on any system. In a perpetual cat-and-mouse chase with defenders, malware
writers constantly conjure new methods to hide their code so as to evade
detection by security products. In this context, focusing on the MS Windows
platform, this work contributes a comprehensive empirical evaluation regarding
the detection capacity of popular, off-the-shelf antivirus and endpoint
detection and response engines when facing legacy malware obfuscated via more
or less uncommon but publicly known methods. Our experiments exploit a blend of
seven traditional AV evasion techniques in 16 executables built in C++, Go, and
Rust. Furthermore, we conduct an incipient study regarding the ability of the
ChatGPT chatbot in assisting threat actors to produce ready-to-use malware. The
derived results in terms of detection rate are highly unexpected: approximately
half of the 12 tested AV engines were able to detect less than half of the
malware variants, four AVs exactly half of the variants, while only two of the
rest detected all but one of the variants. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04149v1' target="_blank">http://arxiv.org/pdf/2305.04149v1</a><br> <br> <br> <font size='5'> 403 </font> <div style="text-align: right"> 2023-05-06 12:45:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ToolCoder: Teach Code Generation Models to use API search tools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automatically generating source code from natural language descriptions has
been a growing field of research in recent years. However, current large-scale
code generation models often encounter difficulties when selecting appropriate
APIs for specific contexts. These models may generate APIs that do not meet
requirements or refer to non-existent APIs in third-party libraries, especially
for lesser-known or private libraries. Inspired by the process of human
developers using tools to search APIs, we propose ToolCoder, a novel approach
that integrates API search tools with existing models to assist in code
generation and API selection. To teach our model to use tools, we introduce an
automated data annotation method using ChatGPT to add tool usage information
into the source code data and fine-tune code generation models. During
inference, we integrate API search tools into the generation process so that
our model can automatically use the search tool to get suggestions when
selecting an API. Our experimental results demonstrate that ToolCoder exhibits
excellent performance and generalization across five public and private library
code generation benchmarks, with at least 6.21\% improvement on average pass@1
metrics and 9.64\% improvement on average pass@10 metrics compared to
state-of-the-art methods. Furthermore, we show that our relatively small
ToolCoder model is comparable to one of the current best models, GPT-3.5,
highlighting the potential of incorporating programming tools into the code
generation process. </font><br> Link: <a href='http://arxiv.org/pdf/2305.04032v2' target="_blank">http://arxiv.org/pdf/2305.04032v2</a><br> <br> <br> <font size='5'> 404 </font> <div style="text-align: right"> 2023-05-05 21:25:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CHAI-DT: A Framework for Prompting Conversational Generative AI Agents to Actively Participate in Co-Creation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper explores the potential for utilizing generative AI models in
group-focused co-creative frameworks to enhance problem solving and ideation in
business innovation and co-creation contexts, and proposes a novel prompting
technique for conversational generative AI agents which employ methods inspired
by traditional 'human-to-human' facilitation and instruction to enable active
contribution to Design Thinking, a co-creative framework. Through experiments
using this prompting technique, we gather evidence that conversational
generative transformers (i.e. ChatGPT) have the capability to contribute
context-specific, useful, and creative input into Design Thinking activities.
We also discuss the potential benefits, limitations, and risks associated with
using generative AI models in co-creative ideation and provide recommendations
for future research. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03852v1' target="_blank">http://arxiv.org/pdf/2305.03852v1</a><br> <br> <br> <font size='5'> 405 </font> <div style="text-align: right"> 2023-05-05 19:20:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evading Watermark based Detection of AI-Generated Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A generative AI model -- such as DALL-E, Stable Diffusion, and ChatGPT -- can
generate extremely realistic-looking content, posing growing challenges to the
authenticity of information. To address the challenges, watermark has been
leveraged to detect AI-generated content. Specifically, a watermark is embedded
into an AI-generated content before it is released. A content is detected as
AI-generated if a similar watermark can be decoded from it. In this work, we
perform a systematic study on the robustness of such watermark-based
AI-generated content detection. We focus on AI-generated images. Our work shows
that an attacker can post-process an AI-generated watermarked image via adding
a small, human-imperceptible perturbation to it, such that the post-processed
AI-generated image evades detection while maintaining its visual quality. We
demonstrate the effectiveness of our attack both theoretically and empirically.
Moreover, to evade detection, our adversarial post-processing method adds much
smaller perturbations to the AI-generated images and thus better maintain their
visual quality than existing popular image post-processing methods such as JPEG
compression, Gaussian blur, and Brightness/Contrast. Our work demonstrates the
insufficiency of existing watermark-based detection of AI-generated content,
highlighting the urgent needs of new detection methods. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03807v1' target="_blank">http://arxiv.org/pdf/2305.03807v1</a><br> <br> <br> <font size='5'> 406 </font> <div style="text-align: right"> 2023-05-05 17:59:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Otter: A Multi-Modal Model with In-Context Instruction Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have demonstrated significant universal
capabilities as few/zero-shot learners in various tasks due to their
pre-training on vast amounts of text data, as exemplified by GPT-3, which
boosted to InstrctGPT and ChatGPT, effectively following natural language
instructions to accomplish real-world tasks. In this paper, we propose to
introduce instruction tuning into multi-modal models, motivated by the Flamingo
model's upstream interleaved format pretraining dataset. We adopt a similar
approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)
dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo
(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and
showcasing improved instruction-following ability and in-context learning. We
also optimize OpenFlamingo's implementation for researchers, democratizing the
required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs,
and integrate both OpenFlamingo and Otter into Huggingface Transformers for
more researchers to incorporate the models into their customized training and
inference pipelines. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03726v1' target="_blank">http://arxiv.org/pdf/2305.03726v1</a><br> <br> <br> <font size='5'> 407 </font> <div style="text-align: right"> 2023-05-05 17:15:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03695v2' target="_blank">http://arxiv.org/pdf/2305.03695v2</a><br> <br> <br> <font size='5'> 408 </font> <div style="text-align: right"> 2023-05-05 16:59:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity
recognition (NER) in fine-grained and noisy scenarios, and it inherits the
semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To
cope with these problems, the previous top systems in the MultiCoNER \RNum{1}
either incorporate the knowledge bases or gazetteers. However, they still
suffer from insufficient knowledge, limited context length, single retrieval
strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified
retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We
perform error analysis on the previous top systems and reveal that their
performance bottleneck lies in insufficient knowledge. Also, we discover that
the limited context length causes the retrieval knowledge to be invisible to
the model. To enhance the retrieval context, we incorporate the entity-centric
Wikidata knowledge base, while utilizing the infusion approach to broaden the
contextual scope of the model. Also, we explore various search strategies and
refine the quality of retrieval knowledge. Our system\footnote{We will release
the dataset, code, and scripts of our system at {\small
\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins
9 out of 13 tracks in the MultiCoNER \RNum{2} shared task. Additionally, we
compared our system with ChatGPT, one of the large language models which have
unlocked strong capabilities on many tasks. The results show that there is
still much room for improvement for ChatGPT on the extraction task. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03688v3' target="_blank">http://arxiv.org/pdf/2305.03688v3</a><br> <br> <br> <font size='5'> 409 </font> <div style="text-align: right"> 2023-05-05 11:03:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Simulating H.P. Lovecraft horror literature with the ChatGPT large language model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we present a novel approach to simulating H.P. Lovecraft's
horror literature using the ChatGPT large language model, specifically the
GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's
unique writing style and themes, while also examining the effectiveness of
prompt engineering techniques in guiding the model's output. To achieve this,
we curated a prompt containing several specialized literature references and
employed advanced prompt engineering methods. We conducted an empirical
evaluation of the generated text by administering a survey to a sample of
undergraduate students. Utilizing statistical hypothesis testing, we assessed
the students ability to distinguish between genuine Lovecraft works and those
generated by our model. Our findings demonstrate that the participants were
unable to reliably differentiate between the two, indicating the effectiveness
of the GPT-4 model and our prompt engineering techniques in emulating
Lovecraft's literary style. In addition to presenting the GPT model's
capabilities, this paper provides a comprehensive description of its underlying
architecture and offers a comparative analysis with related work that simulates
other notable authors and philosophers, such as Dennett. By exploring the
potential of large language models in the context of literary emulation, our
study contributes to the body of research on the applications and limitations
of these models in various creative domains. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03429v1' target="_blank">http://arxiv.org/pdf/2305.03429v1</a><br> <br> <br> <font size='5'> 410 </font> <div style="text-align: right"> 2023-05-05 10:39:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using ChatGPT for Entity Matching</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Entity Matching is the task of deciding if two entity descriptions refer to
the same real-world entity. State-of-the-art entity matching methods often rely
on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks
of using these models for entity matching are that (i) the models require
significant amounts of fine-tuning data for reaching a good performance and
(ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using ChatGPT for entity matching as a
more robust, training data-efficient alternative to traditional Transformer
models. We perform experiments along three dimensions: (i) general prompt
design, (ii) in-context learning, and (iii) provision of higher-level matching
knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,
reaching a zero-shot performance of 82.35% F1 on a challenging matching task on
which RoBERTa requires 2000 training examples for reaching a similar
performance. Adding in-context demonstrations to the prompts further improves
the F1 by up to 7.85% when using similarity-based example selection. Always
using the same set of 10 handpicked demonstrations leads to an improvement of
4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be
guided by adding higher-level matching knowledge in the form of rules to the
prompts. Providing matching rules leads to similar performance gains as
providing in-context demonstrations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03423v2' target="_blank">http://arxiv.org/pdf/2305.03423v2</a><br> <br> <br> <font size='5'> 411 </font> <div style="text-align: right"> 2023-05-05 02:46:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03253v1' target="_blank">http://arxiv.org/pdf/2305.03253v1</a><br> <br> <br> <font size='5'> 412 </font> <div style="text-align: right"> 2023-05-04 22:14:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The development of robust, generalized models in human activity recognition
(HAR) has been hindered by the scarcity of large-scale, labeled data sets.
Recent work has shown that virtual IMU data extracted from videos using
computer vision techniques can lead to substantial performance improvements
when training HAR models combined with small portions of real IMU data.
Inspired by recent advances in motion synthesis from textual descriptions and
connecting Large Language Models (LLMs) to various AI models, we introduce an
automated pipeline that first uses ChatGPT to generate diverse textual
descriptions of activities. These textual descriptions are then used to
generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and
later converted to streams of virtual IMU data. We benchmarked our approach on
three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the
use of virtual IMU training data generated using our new approach leads to
significantly improved HAR model performance compared to only using real IMU
data. Our approach contributes to the growing field of cross-modality transfer
methods and illustrate how HAR models can be improved through the generation of
virtual training data that do not require any manual effort. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03187v1' target="_blank">http://arxiv.org/pdf/2305.03187v1</a><br> <br> <br> <font size='5'> 413 </font> <div style="text-align: right"> 2023-05-04 19:02:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Text-to-SQL parsing, which aims at converting natural language instructions
into executable SQLs, has gained increasing attention in recent years. In
particular, Codex and ChatGPT have shown impressive results in this task.
However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on
database schema with few rows of database contents leaving the gap between
academic study and real-world applications. To mitigate this gap, we present
Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks,
containing 12,751 pairs of text-to-SQL data and 95 databases with a total size
of 33.4 GB, spanning 37 professional domains. Our emphasis on database values
highlights the new challenges of dirty database contents, external knowledge
between NL questions and database contents, and SQL efficiency, particularly in
the context of massive databases. To solve these problems, text-to-SQL models
must feature database value comprehension in addition to semantic parsing. The
experimental results demonstrate the significance of database values in
generating accurate text-to-SQLs for big databases. Furthermore, even the most
effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution
accuracy, which is still far from the human result of 92.96%, proving that
challenges still stand. Besides, we also provide an efficiency analysis to
offer insights into generating text-to-efficient-SQLs that are beneficial to
industries. We believe that BIRD will contribute to advancing real-world
applications of text-to-SQL research. The leaderboard and source code are
available: https://bird-bench.github.io/. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03111v2' target="_blank">http://arxiv.org/pdf/2305.03111v2</a><br> <br> <br> <font size='5'> 414 </font> <div style="text-align: right"> 2023-05-04 17:59:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including < 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03047v1' target="_blank">http://arxiv.org/pdf/2305.03047v1</a><br> <br> <br> <font size='5'> 415 </font> <div style="text-align: right"> 2023-05-04 15:38:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in artificial intelligence (AI) have raised questions about
whether the use of AI is appropriate and legal in various professional
contexts. Here, we present a perspective on how scholars may approach writing
in conjunction with AI, and offer approaches to evaluating whether or not such
AI-writing violates copyright or falls within the safe harbor of fair use. We
present a set of best practices for standard of care with regard to plagiarism,
copyright, and fair use. As AI is likely to grow more capable in the coming
years, it is appropriate to begin integrating AI into scholarly writing
activities. We offer a framework for establishing sound legal and scholarly
foundations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03722v1' target="_blank">http://arxiv.org/pdf/2305.03722v1</a><br> <br> <br> <font size='5'> 416 </font> <div style="text-align: right"> 2023-05-04 14:42:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and Millennial Generation teachers?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aimed to explore the experiences, perceptions, knowledge,
concerns, and intentions of Gen Z students with Gen X and Gen Y teachers
regarding the use of generative AI (GenAI) in higher education. A sample of
students and teachers were recruited to investigate the above using a survey
consisting of both open and closed questions. The findings showed that Gen Z
participants were generally optimistic about the potential benefits of GenAI,
including enhanced productivity, efficiency, and personalized learning, and
expressed intentions to use GenAI for various educational purposes. Gen X and
Gen Y teachers acknowledged the potential benefits of GenAI but expressed
heightened concerns about overreliance, ethical and pedagogical implications,
emphasizing the need for proper guidelines and policies to ensure responsible
use of the technology. The study highlighted the importance of combining
technology with traditional teaching methods to provide a more effective
learning experience. Implications of the findings include the need to develop
evidence-based guidelines and policies for GenAI integration, foster critical
thinking and digital literacy skills among students, and promote responsible
use of GenAI technologies in higher education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02878v1' target="_blank">http://arxiv.org/pdf/2305.02878v1</a><br> <br> <br> <font size='5'> 417 </font> <div style="text-align: right"> 2023-05-04 09:48:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Caption Anything: Interactive Image Description with Diverse Multimodal Controls</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Controllable image captioning is an emerging multimodal topic that aims to
describe the image with natural language following human purpose,
$\textit{e.g.}$, looking at the specified regions or telling in a particular
text style. State-of-the-art methods are trained on annotated pairs of input
controls and output captions. However, the scarcity of such well-annotated
multimodal data largely limits their usability and scalability for interactive
AI systems. Leveraging unimodal instruction-following foundation models is a
promising alternative that benefits from broader sources of data. In this
paper, we present Caption AnyThing (CAT), a foundation model augmented image
captioning framework supporting a wide range of multimodel controls: 1) visual
controls, including points, boxes, and trajectories; 2) language controls, such
as sentiment, length, language, and factuality. Powered by Segment Anything
Model (SAM) and ChatGPT, we unify the visual and language prompts into a
modularized framework, enabling the flexible combination between different
controls. Extensive case studies demonstrate the user intention alignment
capabilities of our framework, shedding light on effective user interaction
modeling in vision-language applications. Our code is publicly available at
https://github.com/ttengwang/Caption-Anything. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02677v3' target="_blank">http://arxiv.org/pdf/2305.02677v3</a><br> <br> <br> <font size='5'> 418 </font> <div style="text-align: right"> 2023-05-04 08:00:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "Oops, Did I Just Say That?" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As the popularity of large language models (LLMs) soars across various
applications, ensuring their alignment with human values has become a paramount
concern. In particular, given that LLMs have great potential to serve as
general-purpose AI assistants in daily life, their subtly unethical suggestions
become a serious and real concern. Tackling the challenge of automatically
testing and repairing unethical suggestions is thus demanding.
  This paper introduces the first framework for testing and repairing unethical
suggestions made by LLMs. We first propose ETHICSSUITE, a test suite that
presents complex, contextualized, and realistic moral scenarios to test LLMs.
We then propose a novel suggest-critic-reflect (SCR) process, serving as an
automated test oracle to detect unethical suggestions. We recast deciding if
LLMs yield unethical suggestions (a hard problem; often requiring human
expertise and costly to decide) into a PCR task that can be automatically
checked for violation. Moreover, we propose a novel on-the-fly (OTF) repairing
scheme that repairs unethical suggestions made by LLMs in real-time. The OTF
scheme is applicable to LLMs in a black-box API setting with moderate cost.
With ETHICSSUITE, our study on seven popular LLMs (e.g., ChatGPT, GPT-4)
uncovers in total 109,824 unethical suggestions. We apply our OTF scheme on two
LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable
amount of unethical ones, paving the way for more ethically conscious LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02626v1' target="_blank">http://arxiv.org/pdf/2305.02626v1</a><br> <br> <br> <font size='5'> 419 </font> <div style="text-align: right"> 2023-05-04 05:21:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With various AI tools such as ChatGPT becoming increasingly popular, we are
entering a true AI era. We can foresee that exceptional AI tools will soon reap
considerable profits. A crucial question arise: should AI tools share revenue
with their training data providers in additional to traditional stakeholders
and shareholders? The answer is Yes. Large AI tools, such as large language
models, always require more and better quality data to continuously improve,
but current copyright laws limit their access to various types of data. Sharing
revenue between AI tools and their data providers could transform the current
hostile zero-sum game relationship between AI tools and a majority of
copyrighted data owners into a collaborative and mutually beneficial one, which
is necessary to facilitate the development of a virtuous cycle among AI tools,
their users and data providers that drives forward AI technology and builds a
healthy AI ecosystem. However, current revenue-sharing business models do not
work for AI tools in the forthcoming AI era, since the most widely used metrics
for website-based traffic and action, such as clicks, will be replaced by new
metrics such as prompts and cost per prompt for generative AI tools. A
completely new revenue-sharing business model, which must be almost independent
of AI tools and be easily explained to data providers, needs to establish a
prompt-based scoring system to measure data engagement of each data provider.
This paper systematically discusses how to build such a scoring system for all
data providers for AI tools based on classification and content similarity
models, and outlines the requirements for AI tools or third parties to build
it. Sharing revenue with data providers using such a scoring system would
encourage more data owners to participate in the revenue-sharing program. This
will be a utilitarian AI era where all parties benefit. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02555v2' target="_blank">http://arxiv.org/pdf/2305.02555v2</a><br> <br> <br> <font size='5'> 420 </font> <div style="text-align: right"> 2023-05-04 03:29:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Governance of the AI, by the AI, and for the AI</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past half century, there have been several false dawns during which
the "arrival" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03719v1' target="_blank">http://arxiv.org/pdf/2305.03719v1</a><br> <br> <br> <font size='5'> 421 </font> <div style="text-align: right"> 2023-05-04 02:09:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AutoML-GPT: Automatic Machine Learning with GPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: AI tasks encompass a wide range of domains and fields. While numerous AI
models have been designed for specific tasks and applications, they often
require considerable human efforts in finding the right model architecture,
optimization algorithm, and hyperparameters. Recent advances in large language
models (LLMs) like ChatGPT show remarkable capabilities in various aspects of
reasoning, comprehension, and interaction. Consequently, we propose developing
task-oriented prompts and automatically utilizing LLMs to automate the training
pipeline. To implement this concept, we present the AutoML-GPT, which employs
GPT as the bridge to diverse AI models and dynamically trains models with
optimized hyperparameters. AutoML-GPT dynamically takes user requests from the
model and data cards and composes the corresponding prompt paragraph.
Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct
the experiments from data processing to model architecture, hyperparameter
tuning, and predicted training log. By leveraging {\ours}'s robust language
capabilities and the available AI models, AutoML-GPT can tackle numerous
intricate AI tasks across various tasks and datasets. This approach achieves
remarkable results in computer vision, natural language processing, and other
challenging areas. Extensive experiments and ablation studies demonstrate that
our method can be general, effective, and beneficial for many AI tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02499v1' target="_blank">http://arxiv.org/pdf/2305.02499v1</a><br> <br> <br> <font size='5'> 422 </font> <div style="text-align: right"> 2023-05-04 01:12:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Tailoring outputs of large language models, such as ChatGPT, to specific user
needs remains a challenge despite their impressive generation quality. In this
paper, we propose a tri-agent generation pipeline consisting of a generator, an
instructor, and an editor to enhance the customization of generated outputs.
The generator produces an initial output, the user-specific instructor
generates editing instructions, and the editor generates a revised output
aligned with user preferences. The inference-only large language model
(ChatGPT) serves as both the generator and the editor, while a smaller model
acts as the user-specific instructor to guide the generation process toward
user needs. The instructor is trained using editor-steered reinforcement
learning, leveraging feedback from the large-scale editor model to optimize
instruction generation. Experimental results on two abstractive summarization
datasets demonstrate the effectiveness of our approach in generating outputs
that better fulfill user expectations. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02483v1' target="_blank">http://arxiv.org/pdf/2305.02483v1</a><br> <br> <br> <font size='5'> 423 </font> <div style="text-align: right"> 2023-05-03 19:57:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03513v1' target="_blank">http://arxiv.org/pdf/2305.03513v1</a><br> <br> <br> <font size='5'> 424 </font> <div style="text-align: right"> 2023-05-03 17:59:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the usefulness of generative Large Language Models (LLMs) in
generating training data for cross-encoder re-rankers in a novel direction:
generating synthetic documents instead of synthetic queries. We introduce a new
dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models
fine-tuned on LLM-generated and human-generated data. Data generated with
generative LLMs can be used to augment training data, especially in domains
with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an
existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public
question collections with human responses and answers from ChatGPT. We
fine-tune a range of cross-encoder re-rankers on either human-generated or
ChatGPT-generated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC
DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT
responses are statistically significantly more effective zero-shot re-rankers
than those trained on human responses. In a supervised setting, the
human-trained re-rankers outperform the LLM-trained re-rankers. Our novel
findings suggest that generative LLMs have high potential in generating
training data for neural retrieval models. Further work is needed to determine
the effect of factually wrong information in the generated responses and test
our findings' generalizability with open-source LLMs. We release our data,
code, and cross-encoders checkpoints for future work. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02320v1' target="_blank">http://arxiv.org/pdf/2305.02320v1</a><br> <br> <br> <font size='5'> 425 </font> <div style="text-align: right"> 2023-05-03 15:57:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Judgments of research co-created by generative AI: experimental evidence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The introduction of ChatGPT has fuelled a public debate on the use of
generative AI (large language models; LLMs), including its use by researchers.
In the current work, we test whether delegating parts of the research process
to LLMs leads people to distrust and devalue researchers and scientific output.
Participants (N=402) considered a researcher who delegates elements of the
research process to a PhD student or LLM, and rated (1) moral acceptability,
(2) trust in the scientist to oversee future projects, and (3) the accuracy and
quality of the output. People judged delegating to an LLM as less acceptable
than delegating to a human (d = -0.78). Delegation to an LLM also decreased
trust to oversee future research projects (d = -0.80), and people thought the
results would be less accurate and of lower quality (d = -0.85). We discuss how
this devaluation might transfer into the underreporting of generative AI use. </font><br> Link: <a href='http://arxiv.org/pdf/2305.11873v1' target="_blank">http://arxiv.org/pdf/2305.11873v1</a><br> <br> <br> <font size='5'> 426 </font> <div style="text-align: right"> 2023-05-03 15:24:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Uncovering ChatGPT's Capabilities in Recommender Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The debut of ChatGPT has recently attracted the attention of the natural
language processing (NLP) community and beyond. Existing studies have
demonstrated that ChatGPT shows significant improvement in a range of
downstream NLP tasks, but the capabilities and limitations of ChatGPT in terms
of recommendations remain unclear. In this study, we aim to conduct an
empirical analysis of ChatGPT's recommendation ability from an Information
Retrieval (IR) perspective, including point-wise, pair-wise, and list-wise
ranking. To achieve this goal, we re-formulate the above three recommendation
policies into a domain-specific prompt format. Through extensive experiments on
four datasets from different domains, we demonstrate that ChatGPT outperforms
other large language models across all three ranking policies. Based on the
analysis of unit cost improvements, we identify that ChatGPT with list-wise
ranking achieves the best trade-off between cost and performance compared to
point-wise and pair-wise ranking. Moreover, ChatGPT shows the potential for
mitigating the cold start problem and explainable recommendation. To facilitate
further explorations in this area, the full code and detailed original results
are open-sourced at https://github.com/rainym00d/LLM4RS. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02182v2' target="_blank">http://arxiv.org/pdf/2305.02182v2</a><br> <br> <br> <font size='5'> 427 </font> <div style="text-align: right"> 2023-05-03 07:45:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Protein Sequence Space with Global Generative Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in specialized large-scale architectures for training
image and language have profoundly impacted the field of computer vision and
natural language processing (NLP). Language models, such as the recent ChatGPT
and GPT4 have demonstrated exceptional capabilities in processing, translating,
and generating human languages. These breakthroughs have also been reflected in
protein research, leading to the rapid development of numerous new methods in a
short time, with unprecedented performance. Language models, in particular,
have seen widespread use in protein research, as they have been utilized to
embed proteins, generate novel ones, and predict tertiary structures. In this
book chapter, we provide an overview of the use of protein generative models,
reviewing 1) language models for the design of novel artificial proteins, 2)
works that use non-Transformer architectures, and 3) applications in directed
evolution approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01941v1' target="_blank">http://arxiv.org/pdf/2305.01941v1</a><br> <br> <br> <font size='5'> 428 </font> <div style="text-align: right"> 2023-05-03 05:31:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Few-shot Event Detection: An Empirical Study and a Unified View</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Few-shot event detection (ED) has been widely studied, while this brings
noticeable discrepancies, e.g., various motivations, tasks, and experimental
settings, that hinder the understanding of models for future progress.This
paper presents a thorough empirical study, a unified view of ED models, and a
better unified baseline. For fair evaluation, we compare 12 representative
methods on three datasets, which are roughly grouped into prompt-based and
prototype-based models for detailed analysis. Experiments consistently
demonstrate that prompt-based methods, including ChatGPT, still significantly
trail prototype-based methods in terms of overall performance. To investigate
their superior performance, we break down their design elements along several
dimensions and build a unified framework on prototype-based methods. Under such
unified view, each prototype-method can be viewed a combination of different
modules from these design elements. We further combine all advantageous modules
and propose a simple yet effective baseline, which outperforms existing methods
by a large margin (e.g., 2.7% F1 gains under low-resource setting). </font><br> Link: <a href='http://arxiv.org/pdf/2305.01901v2' target="_blank">http://arxiv.org/pdf/2305.01901v2</a><br> <br> <br> <font size='5'> 429 </font> <div style="text-align: right"> 2023-05-03 02:30:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPTutor: a ChatGPT-powered programming tool for code explanation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Learning new programming skills requires tailored guidance. With the
emergence of advanced Natural Language Generation models like the ChatGPT API,
there is now a possibility of creating a convenient and personalized tutoring
system with AI for computer science education. This paper presents GPTutor, a
ChatGPT-powered programming tool, which is a Visual Studio Code extension using
the ChatGPT API to provide programming code explanations. By integrating Visual
Studio Code API, GPTutor can comprehensively analyze the provided code by
referencing the relevant source codes. As a result, GPTutor can use designed
prompts to explain the selected code with a pop-up message. GPTutor is now
published at the Visual Studio Code Extension Marketplace, and its source code
is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor
delivers the most concise and accurate explanations compared to vanilla ChatGPT
and GitHub Copilot. Moreover, the feedback from students and teachers indicated
that GPTutor is user-friendly and can explain given codes satisfactorily.
Finally, we discuss possible future research directions for GPTutor. This
includes enhancing its performance and personalization via further prompt
programming, as well as evaluating the effectiveness of GPTutor with real
users. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01863v2' target="_blank">http://arxiv.org/pdf/2305.01863v2</a><br> <br> <br> <font size='5'> 430 </font> <div style="text-align: right"> 2023-05-02 05:46:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Program synthesis has been long studied with recent approaches focused on
directly using the power of Large Language Models (LLMs) to generate code.
Programming benchmarks, with curated synthesis problems and test-cases, are
used to measure the performance of various LLMs on code synthesis. However,
these test-cases can be limited in both quantity and quality for fully
assessing the functional correctness of the generated code. Such limitation in
the existing benchmarks begs the following question: In the era of LLMs, is the
code generated really correct? To answer this, we propose EvalPlus -- a code
synthesis benchmarking framework to rigorously evaluate the functional
correctness of LLM-synthesized code. EvalPlus augments a given evaluation
dataset with large amounts of test-cases newly produced by an automatic test
input generator, powered by both LLM- and mutation-based strategies. While
EvalPlus is general, we extend the test-cases of the popular HUMANEVAL
benchmark by 81x to build HUMANEVAL+. Our extensive evaluation across 19
popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to
catch significant amounts of previously undetected wrong code synthesized by
LLMs, reducing the pass@k by 13.6-15.3% on average. Our work not only indicates
that prior popular code synthesis evaluation results do not accurately reflect
the true performance of LLMs for code synthesis, but also opens up a new
direction to improve such programming benchmarks through automated testing. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01210v2' target="_blank">http://arxiv.org/pdf/2305.01210v2</a><br> <br> <br> <font size='5'> 431 </font> <div style="text-align: right"> 2023-05-02 03:27:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine Translation (MT) has made significant progress in recent years using
deep learning, especially after the emergence of large language models (LLMs)
such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT
using LLMs. In this paper, we brainstorm some interesting directions for MT
using LLMs, including stylized MT, interactive MT, and Translation Memory-based
MT, as well as a new evaluation paradigm using LLMs. We also discuss the
privacy concerns in MT using LLMs and a basic privacy-preserving method to
mitigate such risks. To illustrate the potential of our proposed directions, we
present several examples for the new directions mentioned above, demonstrating
the feasibility of the proposed directions and highlight the opportunities and
challenges for future research in MT using LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2305.01181v1' target="_blank">http://arxiv.org/pdf/2305.01181v1</a><br> <br> <br> <font size='5'> 432 </font> <div style="text-align: right"> 2023-05-01 16:57:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Poisoning Language Models During Instruction Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on
datasets that contain user-submitted examples, e.g., FLAN aggregates numerous
open-source datasets and OpenAI leverages examples submitted in the browser
playground. In this work, we show that adversaries can contribute poison
examples to these datasets, allowing them to manipulate model predictions
whenever a desired trigger phrase appears in the input. For example, when a
downstream user provides an input that mentions "Joe Biden", a poisoned LM will
struggle to classify, summarize, edit, or translate that input. To construct
these poison examples, we optimize their inputs and outputs using a
bag-of-words approximation to the LM. We evaluate our method on open-source
instruction-tuned LMs. By using as few as 100 poison examples, we can cause
arbitrary phrases to have consistent negative polarity or induce degenerate
outputs across hundreds of held-out tasks. Worryingly, we also show that larger
LMs are increasingly vulnerable to poisoning and that defenses based on data
filtering or reducing model capacity provide only moderate protections while
reducing test accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00944v1' target="_blank">http://arxiv.org/pdf/2305.00944v1</a><br> <br> <br> <font size='5'> 433 </font> <div style="text-align: right"> 2023-05-01 13:40:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Empowering Learner-Centered Instruction: Integrating ChatGPT Python API and Tinker Learning for Enhanced Creativity and Problem-Solving Skills</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ChatGPT Python API plays a crucial role in promoting Learner-Centered
Instruction (LCI) and aligns with the principles of Tinker Learning, allowing
students to discover their learning strategies. LCI emphasizes the importance
of active, hands-on learning experiences and encourages students to take
responsibility for their learning journey. By integrating the ChatGPT Python
API into the educational process, students can explore various resources,
generate new ideas, and create content in a more personalized manner. This
innovative approach enables students to engage with the learning material
deeper, fostering a sense of ownership and motivation. As they work through the
Creative Learning Spiral, students develop essential skills such as critical
thinking, problem-solving, and creativity. The ChatGPT Python API is a valuable
tool for students to explore different solutions, evaluate alternatives, and
make informed decisions, all while encouraging self-directed learning. In
Tinker Learning environments, the integration of ChatGPT Python API empowers
students to experiment and iterate, allowing them to find the most effective
learning strategies that cater to their individual needs and preferences. This
personalized approach helps students to become more confident in their
abilities, leading to tremendous academic success and long-term skill
development. By leveraging the capabilities of the ChatGPT Python API,
educational institutions can create a more engaging, supportive, and dynamic
learning environment. This approach aligns with the principles of
Learner-Centered Instruction and Tinker Learning, promoting a culture of
curiosity, exploration, and creativity among students while preparing them for
the challenges of the fast-paced, ever-changing world. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00821v1' target="_blank">http://arxiv.org/pdf/2305.00821v1</a><br> <br> <br> <font size='5'> 434 </font> <div style="text-align: right"> 2023-05-01 12:20:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Enhancing STEM Learning with ChatGPT and Bing Chat as Objects to Think With: A Case Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study investigates the potential of ChatGPT and Bing Chat, advanced
conversational AIs, as "objects-to-think-with," resources that foster
reflective and critical thinking, and concept comprehension in enhancing STEM
education, using a constructionist theoretical framework. A single-case study
methodology was used to analyse extensive interaction logs between students and
both AI systems in simulated STEM learning experiences. The results highlight
the ability of ChatGPT and Bing Chat to help learners develop reflective and
critical thinking, creativity, problem-solving skills, and concept
comprehension. However, integrating AIs with collaborative learning and other
educational activities is crucial, as is addressing potential limitations like
concerns about AI information accuracy and reliability of the AIs' information
and diminished human interaction. The study concludes that ChatGPT and Bing
Chat as objects-to-think-with offer promising avenues to revolutionise STEM
education through a constructionist lens, fostering engagement in inclusive and
accessible learning environments. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02202v1' target="_blank">http://arxiv.org/pdf/2305.02202v1</a><br> <br> <br> <font size='5'> 435 </font> <div style="text-align: right"> 2023-04-30 11:54:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Working Memory Capacity of ChatGPT: An Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Working memory is a critical aspect of both human intelligence and artificial
intelligence, serving as a workspace for the temporary storage and manipulation
of information. In this paper, we systematically assess the working memory
capacity of ChatGPT (gpt-3.5-turbo), a large language model developed by
OpenAI, by examining its performance in verbal and spatial n-back tasks under
various conditions. Our experiments reveal that ChatGPT experiences significant
declines in performance as n increases (which necessitates more information to
be stored in working memory), suggesting a limit to the working memory capacity
strikingly similar to that of humans. Furthermore, we investigate the impact of
different instruction strategies on ChatGPT's performance and observe that the
fundamental patterns of a capacity limit persist. From our empirical findings,
we propose that n-back tasks may serve as tools for benchmarking the working
memory capacity of large language models and hold potential for informing
future efforts aimed at enhancing AI working memory and deepening our
understanding of human working memory through AI models. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03731v2' target="_blank">http://arxiv.org/pdf/2305.03731v2</a><br> <br> <br> <font size='5'> 436 </font> <div style="text-align: right"> 2023-04-30 11:26:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There has been an increasing research interest in developing specialized
dialogue systems that can offer mental health support. However, gathering
large-scale and real-life multi-turn conversations for mental health support
poses challenges due to the sensitivity of personal information, as well as the
time and cost involved. To address these issues, we introduce the SMILE
approach, an inclusive language expansion technique that employs ChatGPT to
extend public single-turn dialogues into multi-turn ones. Our research first
presents a preliminary exploratory study that validates the effectiveness of
the SMILE approach. Furthermore, we conduct a comprehensive and systematic
contrastive analysis of datasets generated with and without the SMILE approach,
demonstrating that the SMILE method results in a large-scale, diverse, and
close-to-real-life multi-turn mental health support conversation corpus,
including dialog topics, lexical and semantic features. Finally, we use the
collected corpus (SMILECHAT) to develop a more effective dialogue system that
offers emotional support and constructive suggestions in multi-turn
conversations for mental health support. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00450v1' target="_blank">http://arxiv.org/pdf/2305.00450v1</a><br> <br> <br> <font size='5'> 437 </font> <div style="text-align: right"> 2023-04-29 22:08:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT in education: A discourse analysis of worries and concerns on social media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancements in generative AI models present new opportunities in
the education sector. However, it is imperative to acknowledge and address the
potential risks and concerns that may arise with their use. We analyzed Twitter
data to identify key concerns related to the use of ChatGPT in education. We
employed BERT-based topic modeling to conduct a discourse analysis and social
network analysis to identify influential users in the conversation. While
Twitter users generally ex-pressed a positive attitude towards the use of
ChatGPT, their concerns converged to five specific categories: academic
integrity, impact on learning outcomes and skill development, limitation of
capabilities, policy and social concerns, and workforce challenges. We also
found that users from the tech, education, and media fields were often
implicated in the conversation, while education and tech individual users led
the discussion of concerns. Based on these findings, the study provides several
implications for policymakers, tech companies and individuals, educators, and
media agencies. In summary, our study underscores the importance of responsible
and ethical use of AI in education and highlights the need for collaboration
among stakeholders to regulate AI policy. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02201v1' target="_blank">http://arxiv.org/pdf/2305.02201v1</a><br> <br> <br> <font size='5'> 438 </font> <div style="text-align: right"> 2023-04-29 20:30:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Pass An Introductory Level Functional Language Programming Course?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent introduction of ChatGPT has drawn significant attention from both
industry and academia due to its impressive capabilities in solving a diverse
range of tasks, including language translation, text summarization, and
computer programming. Its capability for writing, modifying, and even
correcting code together with its ease of use and access is already
dramatically impacting computer science education. This paper aims to explore
how well ChatGPT can perform in an introductory-level functional language
programming course. In our systematic evaluation, we treated ChatGPT as one of
our students and demonstrated that it can achieve a grade B- and its rank in
the class is 155 out of 314 students overall. Our comprehensive evaluation
provides valuable insights into ChatGPT's impact from both student and
instructor perspectives. Additionally, we identify several potential benefits
that ChatGPT can offer to both groups. Overall, we believe that this study
significantly clarifies and advances our understanding of ChatGPT's
capabilities and potential impact on computer science education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.02230v2' target="_blank">http://arxiv.org/pdf/2305.02230v2</a><br> <br> <br> <font size='5'> 439 </font> <div style="text-align: right"> 2023-04-29 15:53:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Students' Voices on Generative AI: Perceptions, Benefits, and Challenges in Higher Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study explores university students' perceptions of generative AI (GenAI)
technologies, such as ChatGPT, in higher education, focusing on familiarity,
their willingness to engage, potential benefits and challenges, and effective
integration. A survey of 399 undergraduate and postgraduate students from
various disciplines in Hong Kong revealed a generally positive attitude towards
GenAI in teaching and learning. Students recognized the potential for
personalized learning support, writing and brainstorming assistance, and
research and analysis capabilities. However, concerns about accuracy, privacy,
ethical issues, and the impact on personal development, career prospects, and
societal values were also expressed. According to John Biggs' 3P model, student
perceptions significantly influence learning approaches and outcomes. By
understanding students' perceptions, educators and policymakers can tailor
GenAI technologies to address needs and concerns while promoting effective
learning outcomes. Insights from this study can inform policy development
around the integration of GenAI technologies into higher education. By
understanding students' perceptions and addressing their concerns, policymakers
can create well-informed guidelines and strategies for the responsible and
effective implementation of GenAI tools, ultimately enhancing teaching and
learning experiences in higher education. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00290v1' target="_blank">http://arxiv.org/pdf/2305.00290v1</a><br> <br> <br> <font size='5'> 440 </font> <div style="text-align: right"> 2023-04-29 11:25:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a type of artificial intelligence language model that uses deep
learning algorithms to generate human-like responses to text-based prompts. The
introduction of the latest ChatGPT version in November of 2022 has caused
shockwaves in the industrial and academic communities for its powerful
capabilities, plethora of possible applications, and the great possibility for
abuse. At the time of writing this work, several other language models (e.g.,
Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in
the vast possible market. These models have the ability to revolutionize the
way we interact with computers and have potential applications in many fields,
including education, software engineering, healthcare, and marketing. In this
paper, we will discuss the possible applications, drawbacks, and research
directions using advanced language Chatbots (e.g., ChatGPT) in each of these
fields. We first start with a brief introduction and the development timeline
of artificial intelligence based language models, then we go through possible
applications of such models, after that we discuss the limitations and
drawbacks of the current technological state of the art, and finally we point
out future possible research directions. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00237v1' target="_blank">http://arxiv.org/pdf/2305.00237v1</a><br> <br> <br> <font size='5'> 441 </font> <div style="text-align: right"> 2023-04-28 22:35:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this work, we carry out a data archaeology to infer books that are known
to ChatGPT and GPT-4 using a name cloze membership inference query. We find
that OpenAI models have memorized a wide collection of copyrighted materials,
and that the degree of memorization is tied to the frequency with which
passages of those books appear on the web. The ability of these models to
memorize an unknown set of books complicates assessments of measurement
validity for cultural analytics by contaminating test data; we show that models
perform much better on memorized books than on non-memorized books for
downstream tasks. We argue that this supports a case for open models whose
training data is known. </font><br> Link: <a href='http://arxiv.org/pdf/2305.00118v1' target="_blank">http://arxiv.org/pdf/2305.00118v1</a><br> <br> <br> <font size='5'> 442 </font> <div style="text-align: right"> 2023-04-28 21:59:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A data science platform to enable time-domain astronomy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: SkyPortal is an open-source software package designed to efficiently discover
interesting transients, manage follow-up, perform characterization, and
visualize the results. By enabling fast access to archival and catalog data,
cross-matching heterogeneous data streams, and the triggering and monitoring of
on-demand observations for further characterization, a SkyPortal-based platform
has been operating at scale for 2 yr for the Zwicky Transient Facility Phase II
community, with hundreds of users, containing tens of millions of time-domain
sources, interacting with dozens of telescopes, and enabling community
reporting. While SkyPortal emphasizes rich user experiences (UX) across common
frontend workflows, recognizing that scientific inquiry is increasingly
performed programmatically, SkyPortal also surfaces an extensive and
well-documented API system. From backend and frontend software to data science
analysis tools and visualization frameworks, the SkyPortal design emphasizes
the re-use and leveraging of best-in-class approaches, with a strong
extensibility ethos. For instance, SkyPortal now leverages ChatGPT
large-language models (LLMs) to automatically generate and surface source-level
human-readable summaries. With the imminent re-start of the next-generation of
gravitational wave detectors, SkyPortal now also includes dedicated
multi-messenger features addressing the requirements of rapid multi-messenger
follow-up: multi-telescope management, team/group organizing interfaces, and
cross-matching of multi-messenger data streams with time-domain optical
surveys, with interfaces sufficiently intuitive for the newcomers to the field.
(abridged) </font><br> Link: <a href='http://arxiv.org/pdf/2305.00108v2' target="_blank">http://arxiv.org/pdf/2305.00108v2</a><br> <br> <br> <font size='5'> 443 </font> <div style="text-align: right"> 2023-04-28 17:26:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is an AI language model developed by OpenAI that can understand and
generate human-like text. It can be used for a variety of use cases such as
language generation, question answering, text summarization, chatbot
development, language translation, sentiment analysis, content creation,
personalization, text completion, and storytelling. While ChatGPT has garnered
significant positive attention, it has also generated a sense of apprehension
and uncertainty in academic circles. There is concern that students may
leverage ChatGPT to complete take-home assignments and exams and obtain
favorable grades without genuinely acquiring knowledge. This paper adopts a
quantitative approach to demonstrate ChatGPT's high degree of unreliability in
answering a diverse range of questions pertaining to topics in undergraduate
computer science. Our analysis shows that students may risk self-sabotage by
blindly depending on ChatGPT to complete assignments and exams. We build upon
this analysis to provide constructive recommendations to both students and
instructors. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14993v2' target="_blank">http://arxiv.org/pdf/2304.14993v2</a><br> <br> <br> <font size='5'> 444 </font> <div style="text-align: right"> 2023-04-28 13:14:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper aims to quantitatively evaluate the performance of ChatGPT, an
interactive large language model, on inter-sentential relations such as
temporal relations, causal relations, and discourse relations. Given ChatGPT's
promising performance across various tasks, we conduct extensive evaluations on
the whole test sets of 13 datasets, including temporal and causal relations,
PDTB2.0-based and dialogue-based discourse relations, and downstream
applications on discourse understanding. To achieve reliable results, we adopt
three tailored prompt templates for each task, including the zero-shot prompt
template, zero-shot prompt engineering (PE) template, and in-context learning
(ICL) prompt template, to establish the initial baseline scores for all popular
sentence-pair relation classification tasks for the first time. We find that
ChatGPT exhibits strong performance in detecting and reasoning about causal
relations, while it may not be proficient in identifying the temporal order
between two events. It can recognize most discourse relations with existing
explicit discourse connectives, but the implicit discourse relation still
remains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue
discourse parsing task that requires structural understanding in a dialogue
before being aware of the discourse relation. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14827v2' target="_blank">http://arxiv.org/pdf/2304.14827v2</a><br> <br> <br> <font size='5'> 445 </font> <div style="text-align: right"> 2023-04-28 10:15:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Making the contents generated by Large Language Model (LLM) such as ChatGPT,
accurate, credible and traceable is crucial, especially in complex
knowledge-intensive tasks that require multi-step reasoning and each of which
needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM
with external knowledge is good potential to solve this problem. However, where
and how to introduce IR into LLM is a big challenge. Previous work has the
disadvantage that the wrong knowledge retrieved by IR misleads the LLM or
breaks the reasoning chain of LLM. In this paper, we propose a novel framework
called Search-in-the-Chain (SearChain) for the interaction between LLM and IR
to solve the challenges. First, LLM generates the global reasoning chain called
Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the
answer to the query. Second, IR verifies the answer of each node of CoQ, it
corrects the answer that is not consistent with the retrieved information when
IR gives high confidence, which improves the credibility. Third, LLM can mark
its missing knowledge in CoQ and IR can provide this knowledge to LLM. These
three operations improve the accuracy of LLM for complex knowledge-intensive
tasks in terms of reasoning ability and knowledge. Finally, SearChain generates
the reasoning process and marks references to supporting documents for each
reasoning step, which improves traceability. SearChain transforms the topology
of reasoning from chain to tree, which can modify the reasoning direction.
Experiment shows that SearChain outperforms baselines on complex
knowledge-intensive tasks including multi-hop question-answering, slot filling,
fact checking, and long-form question-answering. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14732v5' target="_blank">http://arxiv.org/pdf/2304.14732v5</a><br> <br> <br> <font size='5'> 446 </font> <div style="text-align: right"> 2023-04-28 03:34:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Intellectual Property Protection: A Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep Neural Networks (DNNs), from AlexNet to ResNet to ChatGPT, have made
revolutionary progress in recent years, and are widely used in various fields.
The high performance of DNNs requires a huge amount of high-quality data,
expensive computing hardware, and excellent DNN architectures that are costly
to obtain. Therefore, trained DNNs are becoming valuable assets and must be
considered the Intellectual Property (IP) of the legitimate owner who created
them, in order to protect trained DNN models from illegal reproduction,
stealing, redistribution, or abuse. Although being a new emerging and
interdisciplinary field, numerous DNN model IP protection methods have been
proposed. Given this period of rapid evolution, the goal of this paper is to
provide a comprehensive survey of two mainstream DNN IP protection methods:
deep watermarking and deep fingerprinting, with a proposed taxonomy. More than
190 research contributions are included in this survey, covering many aspects
of Deep IP Protection: problem definition, main threats and challenges, merits
and demerits of deep watermarking and deep fingerprinting methods, evaluation
metrics, and performance discussion. We finish the survey by identifying
promising directions for future research. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14613v2' target="_blank">http://arxiv.org/pdf/2304.14613v2</a><br> <br> <br> <font size='5'> 447 </font> <div style="text-align: right"> 2023-04-27 22:21:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Appropriateness is all you need!</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The strive to make AI applications "safe" has led to the development of
safety-measures as the main or even sole normative requirement of their
permissible use. Similar can be attested to the latest version of chatbots,
such as chatGPT. In this view, if they are "safe", they are supposed to be
permissible to deploy. This approach, which we call "safety-normativity", is
rather limited in solving the emerging issues that chatGPT and other chatbots
have caused thus far. In answering this limitation, in this paper we argue for
limiting chatbots in the range of topics they can chat about according to the
normative concept of appropriateness. We argue that rather than looking for
"safety" in a chatbot's utterances to determine what they may and may not say,
we ought to assess those utterances according to three forms of
appropriateness: technical-discursive, social, and moral. We then spell out
what requirements for chatbots follow from these forms of appropriateness to
avoid the limits of previous accounts: positionality, acceptability, and value
alignment (PAVA). With these in mind, we may be able to determine what a
chatbot may and may not say. Lastly, one initial suggestion is to use challenge
sets, specifically designed for appropriateness, as a validation method. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14553v1' target="_blank">http://arxiv.org/pdf/2304.14553v1</a><br> <br> <br> <font size='5'> 448 </font> <div style="text-align: right"> 2023-04-27 20:06:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: pyBibX -- A Python Library for Bibliometric and Scientometric Analysis Powered with Artificial Intelligence Tools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Bibliometric and Scientometric analyses offer invaluable perspectives on the
complex research terrain and collaborative dynamics spanning diverse academic
disciplines. This paper presents pyBibX, a python library devised to conduct
comprehensive bibliometric and scientometric analyses on raw data files sourced
from Scopus, Web of Science, and PubMed, seamlessly integrating state of the
art AI capabilities into its core functionality. The library executes a
comprehensive EDA, presenting outcomes via visually appealing graphical
illustrations. Network capabilities have been deftly integrated, encompassing
Citation, Collaboration, and Similarity Analysis. Furthermore, the library
incorporates AI capabilities, including Embedding vectors, Topic Modeling, Text
Summarization, and other general Natural Language Processing tasks, employing
models such as Sentence-BERT, BerTopic, BERT, chatGPT, and PEGASUS. As a
demonstration, we have analyzed 184 documents associated with multiple-criteria
decision analysis published between 1984 and 2023. The EDA emphasized a growing
fascination with decision-making and fuzzy logic methodologies. Next, Network
Analysis further accentuated the significance of central authors and
intra-continental collaboration, identifying Canada and China as crucial
collaboration hubs. Finally, AI Analysis distinguished two primary topics and
chatGPT preeminence in Text Summarization. It also proved to be an
indispensable instrument for interpreting results, as our library enables
researchers to pose inquiries to chatGPT regarding bibliometric outcomes. Even
so, data homogeneity remains a daunting challenge due to database
inconsistencies. PyBibX is the first application integrating cutting-edge AI
capabilities for analyzing scientific publications, enabling researchers to
examine and interpret these outcomes more effectively. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14516v1' target="_blank">http://arxiv.org/pdf/2304.14516v1</a><br> <br> <br> <font size='5'> 449 </font> <div style="text-align: right"> 2023-04-27 19:26:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Textual backdoor attacks pose a practical threat to existing systems, as they
can compromise the model by inserting imperceptible triggers into inputs and
manipulating labels in the training dataset. With cutting-edge generative
models such as GPT-4 pushing rewriting to extraordinary levels, such attacks
are becoming even harder to detect. We conduct a comprehensive investigation of
the role of black-box generative models as a backdoor attack tool, highlighting
the importance of researching relative defense strategies. In this paper, we
reveal that the proposed generative model-based attack, BGMAttack, could
effectively deceive textual classifiers. Compared with the traditional attack
methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging
state-of-the-art generative models. Our extensive evaluation of attack
effectiveness across five datasets, complemented by three distinct human
cognition assessments, reveals that Figure 4 achieves comparable attack
performance while maintaining superior stealthiness relative to baseline
methods. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14475v1' target="_blank">http://arxiv.org/pdf/2304.14475v1</a><br> <br> <br> <font size='5'> 450 </font> <div style="text-align: right"> 2023-04-27 17:33:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have shown great potential in solving complex
problems in various fields, including oil and gas engineering and other
industrial engineering disciplines like factory automation, PLC programming
etc. However, automatic identification of strong and weak solutions to
fundamental physics equations governing several industrial processes remain a
challenging task. This paper identifies the limitation of current LLM
approaches, particularly ChatGPT in selected practical problems native to oil
and gas engineering but not exclusively. The performance of ChatGPT in solving
complex problems in oil and gas engineering is discussed and the areas where
LLMs are most effective are presented. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14354v1' target="_blank">http://arxiv.org/pdf/2304.14354v1</a><br> <br> <br> <font size='5'> 451 </font> <div style="text-align: right"> 2023-04-27 17:07:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we investigate the use of data obtained from prompting a large
generative language model, ChatGPT, to generate synthetic training data with
the aim of augmenting data in low resource scenarios. We show that with
appropriate task-specific ChatGPT prompts, we outperform the most popular
existing approaches for such data augmentation. Furthermore, we investigate
methodologies for evaluating the similarity of the augmented data generated
from ChatGPT with the aim of validating and assessing the quality of the data
generated. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14334v1' target="_blank">http://arxiv.org/pdf/2304.14334v1</a><br> <br> <br> <font size='5'> 452 </font> <div style="text-align: right"> 2023-04-27 13:25:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transformer-based language models, including ChatGPT, have demonstrated
exceptional performance in various natural language generation tasks. However,
there has been limited research evaluating ChatGPT's keyphrase generation
ability, which involves identifying informative phrases that accurately reflect
a document's content. This study seeks to address this gap by comparing
ChatGPT's keyphrase generation performance with state-of-the-art models, while
also testing its potential as a solution for two significant challenges in the
field: domain adaptation and keyphrase generation from long documents. We
conducted experiments on six publicly available datasets from scientific
articles and news domains, analyzing performance on both short and long
documents. Our results show that ChatGPT outperforms current state-of-the-art
models in all tested datasets and environments, generating high-quality
keyphrases that adapt well to diverse domains and document lengths. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14177v2' target="_blank">http://arxiv.org/pdf/2304.14177v2</a><br> <br> <br> <font size='5'> 453 </font> <div style="text-align: right"> 2023-04-27 11:33:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatLog: Recording and Analyzing ChatGPT Across Time</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While there are abundant researches about evaluating ChatGPT on natural
language understanding and generation tasks, few studies have investigated how
ChatGPT's behavior changes over time. In this paper, we collect a
coarse-to-fine temporal dataset called ChatLog, consisting of two parts that
update monthly and daily: ChatLog-Monthly is a dataset of 38,730
question-answer pairs collected every month including questions from both the
reasoning and classification tasks. ChatLog-Daily, on the other hand, consists
of ChatGPT's responses to 1000 identical questions for long-form generation
every day. We conduct comprehensive automatic and human evaluation to provide
the evidence for the existence of ChatGPT evolving patterns. We further analyze
the unchanged characteristics of ChatGPT over time by extracting its knowledge
and linguistic features. We find some stable features to improve the robustness
of a RoBERTa-based detector on new versions of ChatGPT. We will continuously
maintain our project at https://github.com/THU-KEG/ChatLog. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14106v1' target="_blank">http://arxiv.org/pdf/2304.14106v1</a><br> <br> <br> <font size='5'> 454 </font> <div style="text-align: right"> 2023-04-26 23:09:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Obtaining and annotating data can be expensive and time-consuming, especially
in complex, low-resource domains. We use GPT-4 and ChatGPT to augment small
labeled datasets with synthetic data via simple prompts, in three different
classification tasks with varying complexity. For each task, we randomly select
a base sample of 500 texts to generate 5,000 new synthetic samples. We explore
two augmentation strategies: one that preserves original label distribution and
another that balances the distribution. Using a progressively larger training
sample size, we train and evaluate a 110M parameter multilingual language model
on the real and synthetic data separately. We also test GPT-4 and ChatGPT in a
zero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have
strong zero-shot performance across all tasks. We find that data augmented with
synthetic samples yields a good downstream performance, and particularly aids
in low-resource settings, such as in identifying rare classes. Human-annotated
data exhibits a strong predictive power, overtaking synthetic data in two out
of the three tasks. This finding highlights the need for more complex prompts
for synthetic datasets to consistently surpass human-generated ones. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13861v1' target="_blank">http://arxiv.org/pdf/2304.13861v1</a><br> <br> <br> <font size='5'> 455 </font> <div style="text-align: right"> 2023-04-26 18:11:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards ethical multimodal systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The impact of artificial intelligence systems on our society is increasing at
an unprecedented speed. For instance, ChatGPT is being tested in mental health
treatment applications such as Koko, Stable Diffusion generates pieces of art
competitive with (or outperforming) human artists, and so on. Ethical concerns
regarding the behavior and applications of generative AI systems have been
increasing over the past years, and the field of AI alignment - steering the
behavior of AI systems towards being aligned with human values - is a rapidly
growing subfield of modern AI. In this paper, we address the challenges
involved in ethical evaluation of a multimodal artificial intelligence system.
The multimodal systems we focus on take both text and an image as input and
output text, completing the sentence or answering the question asked as input.
We perform the evaluation of these models in two steps: we first discus the
creation of a multimodal ethical database and then use this database to
construct morality-evaluating algorithms. The creation of the multimodal
ethical database is done interactively through human feedback. Users are
presented with multiple examples and votes on whether they are ethical or not.
Once these answers have been aggregated into a dataset, we built and tested
different algorithms to automatically evaluate the morality of multimodal
systems. These algorithms aim to classify the answers as ethical or not. The
models we tested are a RoBERTa-large classifier and a multilayer perceptron
classifier. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13765v1' target="_blank">http://arxiv.org/pdf/2304.13765v1</a><br> <br> <br> <font size='5'> 456 </font> <div style="text-align: right"> 2023-04-26 17:52:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13712v2' target="_blank">http://arxiv.org/pdf/2304.13712v2</a><br> <br> <br> <font size='5'> 457 </font> <div style="text-align: right"> 2023-04-26 11:33:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multidimensional Evaluation for Text Style Transfer Using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the potential of ChatGPT as a multidimensional evaluator for
the task of \emph{Text Style Transfer}, alongside, and in comparison to,
existing automatic metrics as well as human judgements. We focus on a zero-shot
setting, i.e. prompting ChatGPT with specific task instructions, and test its
performance on three commonly-used dimensions of text style transfer
evaluation: style strength, content preservation, and fluency. We perform a
comprehensive correlation analysis for two transfer directions (and overall) at
different levels. Compared to existing automatic metrics, ChatGPT achieves
competitive correlations with human judgments. These preliminary results are
expected to provide a first glimpse into the role of large language models in
the multidimensional evaluation of stylized text generation. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13462v1' target="_blank">http://arxiv.org/pdf/2304.13462v1</a><br> <br> <br> <font size='5'> 458 </font> <div style="text-align: right"> 2023-04-26 07:25:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale Language Models (LLMs) are constrained by their inability to
process lengthy inputs. To address this limitation, we propose the
Self-Controlled Memory (SCM) system to unleash infinite-length input capacity
for large-scale language models. Our SCM system is composed of three key
modules: the language model agent, the memory stream, and the memory
controller. The language model agent iteratively processes ultra-long inputs
and stores all historical information in the memory stream. The memory
controller provides the agent with both long-term memory (archived memory) and
short-term memory (flash memory) to generate precise and coherent responses.
The controller determines which memories from archived memory should be
activated and how to incorporate them into the model input. Our SCM system can
be integrated with any LLMs to enable them to process ultra-long texts without
any modification or fine-tuning. Experimental results show that our SCM system
enables LLMs, which are not optimized for multi-turn dialogue, to achieve
multi-turn dialogue capabilities that are comparable to ChatGPT, and to
outperform ChatGPT in scenarios involving ultra-long document summarization or
long-term conversations. Additionally, we will supply a test set, which covers
common long-text input scenarios, for evaluating the abilities of LLMs in
processing long documents.~\footnote{Working in
progress.}\footnote{\url{https://github.com/wbbeyourself/SCM4LLMs}} </font><br> Link: <a href='http://arxiv.org/pdf/2304.13343v1' target="_blank">http://arxiv.org/pdf/2304.13343v1</a><br> <br> <br> <font size='5'> 459 </font> <div style="text-align: right"> 2023-04-26 06:02:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT
and GPT-4 have significantly impacted the AI community, including Text-to-SQL
tasks. Some evaluations and analyses on LLMs show their potential to generate
SQL queries but they point out poorly designed prompts (e.g. simplistic
construction or random sampling) limit LLMs' performance and may cause
unnecessary or irrelevant outputs. To address these issues, we propose
CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5
for precise control over case-relevant and case-irrelevant knowledge in
Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for
GPT-3.5, which involves (1) adaptively retrieving cases according to the
question intention by de-semantizing the input question, and (2) an adaptive
fallback mechanism to ensure the informativeness of the prompt, as well as the
relevance between cases and the prompt. In the de-semanticization phase, we
designed Semantic Domain Relevance Evaluator(SDRE), combined with Poincar\'e
detector(mining implicit semantics in hyperbolic space), TextAlign(discovering
explicit matches), and Positector (part-of-speech detector). SDRE semantically
and syntactically generates in-context exemplar annotations for the new case.
On the three cross-domain datasets, our framework outperforms the
state-of-the-art(SOTA) model in execution accuracy by 3.7\%, 2.5\%, and 8.2\%,
respectively. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13301v1' target="_blank">http://arxiv.org/pdf/2304.13301v1</a><br> <br> <br> <font size='5'> 460 </font> <div style="text-align: right"> 2023-04-26 04:33:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Closeness of In-Context Learning and Weight Shifting for Softmax Regression</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) are known for their exceptional performance in
natural language processing, making them highly effective in many human
life-related or even job-related tasks. The attention mechanism in the
Transformer architecture is a critical component of LLMs, as it allows the
model to selectively focus on specific input parts. The softmax unit, which is
a key part of the attention mechanism, normalizes the attention scores. Hence,
the performance of LLMs in various NLP tasks depends significantly on the
crucial role played by the attention mechanism with the softmax unit.
  In-context learning, as one of the celebrated abilities of recent LLMs, is an
important concept in querying LLMs such as ChatGPT. Without further parameter
updates, Transformers can learn to predict based on few in-context examples.
However, the reason why Transformers becomes in-context learners is not well
understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the
in-context learning from a mathematical perspective based on a linear
regression formulation $\min_x\| Ax - b \|_2$, which show Transformers'
capability of learning linear functions in context.
  In this work, we study the in-context learning based on a softmax regression
formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b
\|_2$ of Transformer's attention mechanism. We show the upper bounds of the
data transformations induced by a single self-attention layer and by
gradient-descent on a $\ell_2$ regression loss for softmax prediction function,
which imply that when training self-attention-only Transformers for fundamental
regression tasks, the models learned by gradient-descent and Transformers show
great similarity. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13276v1' target="_blank">http://arxiv.org/pdf/2304.13276v1</a><br> <br> <br> <font size='5'> 461 </font> <div style="text-align: right"> 2023-04-25 23:07:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TABLET: Learning From Instructions For Tabular Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Acquiring high-quality data is often a significant challenge in training
machine learning (ML) models for tabular prediction, particularly in
privacy-sensitive and costly domains like medicine and finance. Providing
natural language instructions to large language models (LLMs) offers an
alternative solution. However, it is unclear how effectively instructions
leverage the knowledge in LLMs for solving tabular prediction problems. To
address this gap, we introduce TABLET, a benchmark of 20 diverse tabular
datasets annotated with instructions that vary in their phrasing, granularity,
and technicality. Additionally, TABLET includes the instructions' logic and
structured modifications to the instructions. We find in-context instructions
increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for
ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular
prediction in our benchmark by evaluating instruction faithfulness. We find
LLMs often ignore instructions and fail to predict specific instances
correctly, even with examples. Our analysis on TABLET shows that, while
instructions help LLM performance, learning from instructions for tabular data
requires new capabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13188v1' target="_blank">http://arxiv.org/pdf/2304.13188v1</a><br> <br> <br> <font size='5'> 462 </font> <div style="text-align: right"> 2023-04-25 17:29:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Potential of Visual ChatGPT For Remote Sensing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in Natural Language Processing (NLP), particularly in
Large Language Models (LLMs), associated with deep learning-based computer
vision techniques, have shown substantial potential for automating a variety of
tasks. One notable model is Visual ChatGPT, which combines ChatGPT's LLM
capabilities with visual computation to enable effective image analysis. The
model's ability to process images based on textual inputs can revolutionize
diverse fields. However, its application in the remote sensing domain remains
unexplored. This is the first paper to examine the potential of Visual ChatGPT,
a cutting-edge LLM founded on the GPT architecture, to tackle the aspects of
image processing related to the remote sensing domain. Among its current
capabilities, Visual ChatGPT can generate textual descriptions of images,
perform canny edge and straight line detection, and conduct image segmentation.
These offer valuable insights into image content and facilitate the
interpretation and extraction of information. By exploring the applicability of
these techniques within publicly available datasets of satellite images, we
demonstrate the current model's limitations in dealing with remote sensing
images, highlighting its challenges and future prospects. Although still in
early development, we believe that the combination of LLMs and visual models
holds a significant potential to transform remote sensing image processing,
creating accessible and practical application opportunities in the field. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13009v2' target="_blank">http://arxiv.org/pdf/2304.13009v2</a><br> <br> <br> <font size='5'> 463 </font> <div style="text-align: right"> 2023-04-25 17:05:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have exhibited remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. Despite the recent success, current LLMs are not capable of
processing complex audio information or conducting spoken conversations (like
Siri or Alexa). In this work, we propose a multi-modal AI system named
AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to
process complex audio information and solve numerous understanding and
generation tasks; and 2) the input/output interface (ASR, TTS) to support
spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of
human intention understanding and cooperation with foundation models, we
outline the principles and processes and test AudioGPT in terms of consistency,
capability, and robustness. Experimental results demonstrate the capabilities
of AudioGPT in solving AI tasks with speech, music, sound, and talking head
understanding and generation in multi-round dialogues, which empower humans to
create rich and diverse audio content with unprecedented ease. Our system is
publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12995v1' target="_blank">http://arxiv.org/pdf/2304.12995v1</a><br> <br> <br> <font size='5'> 464 </font> <div style="text-align: right"> 2023-04-25 04:09:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Context: Recently, many illustrative examples have shown ChatGPT's impressive
ability to perform programming tasks and answer general domain questions.
  Objective: We empirically evaluate how ChatGPT performs on requirements
analysis tasks to derive insights into how generative large language model,
represented by ChatGPT, influence the research and practice of natural language
processing for requirements engineering.
  Method: We design an evaluation pipeline including two common requirements
information retrieval tasks, four public datasets involving two typical
requirements artifacts, querying ChatGPT with fixed task prompts, and
quantitative and qualitative results analysis.
  Results: Quantitative results show that ChatGPT achieves comparable or better
$F\beta$ values in all datasets under a zero-shot setting. Qualitative analysis
further illustrates ChatGPT's powerful natural language processing ability and
limited requirements engineering domain knowledge.
  Conclusion: The evaluation results demonstrate ChatGPT' impressive ability to
retrieve requirements information from different types artifacts involving
multiple languages under a zero-shot setting. It is worthy for the research and
industry communities to study generative large language model based
requirements retrieval models and to develop corresponding tools. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12562v1' target="_blank">http://arxiv.org/pdf/2304.12562v1</a><br> <br> <br> <font size='5'> 465 </font> <div style="text-align: right"> 2023-04-25 02:48:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Improved Trust in Human-Robot Collaboration with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Human robot collaboration is becoming increasingly important as robots become
more involved in various aspects of human life in the era of Artificial
Intelligence. However, the issue of human operators trust in robots remains a
significant concern, primarily due to the lack of adequate semantic
understanding and communication between humans and robots. The emergence of
Large Language Models (LLMs), such as ChatGPT, provides an opportunity to
develop an interactive, communicative, and robust human-robot collaboration
approach. This paper explores the impact of ChatGPT on trust in a human-robot
collaboration assembly task. This study designs a robot control system called
RoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human
operators fetch, and place tools, while human operators can communicate with
and control the robot arm using natural language. A human-subject experiment
showed that incorporating ChatGPT in robots significantly increased trust in
human-robot collaboration, which can be attributed to the robot's ability to
communicate more effectively with humans. Furthermore, ChatGPT ability to
understand the nuances of human language and respond appropriately helps to
build a more natural and intuitive human-robot interaction. The findings of
this study have significant implications for the development of human-robot
collaboration systems. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12529v1' target="_blank">http://arxiv.org/pdf/2304.12529v1</a><br> <br> <br> <font size='5'> 466 </font> <div style="text-align: right"> 2023-04-25 01:47:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semantic Compression With Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of large language models (LLMs) is revolutionizing information
retrieval, question answering, summarization, and code generation tasks.
However, in addition to confidently presenting factually inaccurate information
at times (known as "hallucinations"), LLMs are also inherently limited by the
number of input and output tokens that can be processed at once, making them
potentially less effective on tasks that require processing a large set or
continuous stream of information. A common approach to reducing the size of
data is through lossless or lossy compression. Yet, in some cases it may not be
strictly necessary to perfectly recover every detail from the original data, as
long as a requisite level of semantic precision or intent is conveyed.
  This paper presents three contributions to research on LLMs. First, we
present the results from experiments exploring the viability of approximate
compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT
interfaces. Second, we investigate and quantify the capability of LLMs to
compress text and code, as well as to recall and manipulate compressed
representations of prompts. Third, we present two novel metrics -- Exact
Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness
(SRE) -- that quantify the level of preserved intent between text compressed
and decompressed by the LLMs we studied. Our initial results indicate that
GPT-4 can effectively compress and reconstruct text while preserving the
semantic essence of the original text, providing a path to leverage
$\sim$5$\times$ more tokens than present limits allow. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12512v1' target="_blank">http://arxiv.org/pdf/2304.12512v1</a><br> <br> <br> <font size='5'> 467 </font> <div style="text-align: right"> 2023-04-24 22:31:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Artificial General Intelligence (AGI) for Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate
human intelligence through computer systems, which is one of the critical
technologies having the potential to revolutionize the field of education.
Compared to conventional AI models, typically designed for a limited range of
tasks, demand significant amounts of domain-specific data for training and may
not always consider intricate interpersonal dynamics in education. AGI, driven
by the recent large pre-trained models, represents a significant leap in the
capability of machines to perform tasks that require human-level intelligence,
such as reasoning, problem-solving, decision-making, and even understanding
human emotions and social interactions. This work reviews AGI's key concepts,
capabilities, scope, and potential within future education, including setting
educational goals, designing pedagogy and curriculum, and performing
assessments. We also provide rich discussions over various ethical issues in
education faced by AGI and how AGI will affect human educators. The development
of AGI necessitates interdisciplinary collaborations between educators and AI
engineers to advance research and application efforts. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12479v2' target="_blank">http://arxiv.org/pdf/2304.12479v2</a><br> <br> <br> <font size='5'> 468 </font> <div style="text-align: right"> 2023-04-24 16:31:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: WizardLM: Empowering Large Language Models to Follow Complex Instructions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Training large language models (LLMs) with open-domain instruction following
data brings colossal success. However, manually creating such instruction data
is very time-consuming and labor-intensive. Moreover, humans may struggle to
produce high-complexity instructions. In this paper, we show an avenue for
creating large amounts of instruction data with varying levels of complexity
using LLM instead of humans. Starting with an initial set of instructions, we
use our proposed Evol-Instruct to rewrite them step by step into more complex
instructions. Then, we mix all generated instruction data to fine-tune LLaMA.
We call the resulting model WizardLM. Human evaluations on a
complexity-balanced test bed and Vicuna's testset show that instructions from
Evol-Instruct are superior to human-created ones. By analyzing the human
evaluation results of the high complexity part, we demonstrate that outputs
from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4
automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on
17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some
aspects, our findings suggest that fine-tuning with AI-evolved instructions is
a promising direction for enhancing LLMs. Our code and data are public at
https://github.com/nlpxucan/WizardLM </font><br> Link: <a href='http://arxiv.org/pdf/2304.12244v2' target="_blank">http://arxiv.org/pdf/2304.12244v2</a><br> <br> <br> <font size='5'> 469 </font> <div style="text-align: right"> 2023-04-24 12:58:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Recently, ChatGPT and similar generative AI models have attracted
hundreds of millions of users and become part of the public discourse. Many
believe that such models will disrupt society and will result in a significant
change in the education system and information generation in the future. So
far, this belief is based on either colloquial evidence or benchmarks from the
owners of the models -- both lack scientific rigour.
  Objective: Through a large-scale study comparing human-written versus
ChatGPT-generated argumentative student essays, we systematically assess the
quality of the AI-generated content.
  Methods: A large corpus of essays was rated using standard criteria by a
large number of human experts (teachers). We augment the analysis with a
consideration of the linguistic characteristics of the generated essays.
  Results: Our results demonstrate that ChatGPT generates essays that are rated
higher for quality than human-written essays. The writing style of the AI
models exhibits linguistic characteristics that are different from those of the
human-written essays, e.g., it is characterized by fewer discourse and
epistemic markers, but more nominalizations and greater lexical diversity.
  Conclusions: Our results clearly demonstrate that models like ChatGPT
outperform humans in generating argumentative essays. Since the technology is
readily available for anyone to use, educators must act immediately. We must
re-invent homework and develop teaching concepts that utilize these AI models
in the same way as math utilized the calculator: teach the general concepts
first and then use AI tools to free up time for other learning objectives. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14276v1' target="_blank">http://arxiv.org/pdf/2304.14276v1</a><br> <br> <br> <font size='5'> 470 </font> <div style="text-align: right"> 2023-04-24 11:55:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SocialDial: A Benchmark for Socially-Aware Dialogue Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Dialogue systems have been widely applied in many scenarios and are now more
powerful and ubiquitous than ever before. With large neural models and massive
available data, current dialogue systems have access to more knowledge than any
people in their life. However, current dialogue systems still do not perform at
a human level. One major gap between conversational agents and humans lies in
their abilities to be aware of social norms. The development of socially-aware
dialogue systems is impeded due to the lack of resources. In this paper, we
present the first socially-aware dialogue corpus - SocialDial, based on Chinese
social culture. SocialDial consists of two parts: 1,563 multi-turn dialogues
between two human speakers with fine-grained labels, and 4,870 synthetic
conversations generated by ChatGPT. The human corpus covers five categories of
social norms, which have 14 sub-categories in total. Specifically, it contains
social factor annotations including social relation, context, social distance,
and social norms. However, collecting sufficient socially-aware dialogues is
costly. Thus, we harness the power of ChatGPT and devise an ontology-based
synthetic data generation framework. This framework is able to generate
synthetic data at scale. To ensure the quality of synthetic dialogues, we
design several mechanisms for quality control during data collection. Finally,
we evaluate our dataset using several pre-trained models, such as BERT and
RoBERTa. Comprehensive empirical results based on state-of-the-art neural
models demonstrate that modeling of social norms for dialogue systems is a
promising research direction. To the best of our knowledge, SocialDial is the
first socially-aware dialogue dataset that covers multiple social factors and
has fine-grained labels. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12026v1' target="_blank">http://arxiv.org/pdf/2304.12026v1</a><br> <br> <br> <font size='5'> 471 </font> <div style="text-align: right"> 2023-04-24 11:19:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The powerful ability of ChatGPT has caused widespread concern in the academic
community. Malicious users could synthesize dummy academic content through
ChatGPT, which is extremely harmful to academic rigor and originality. The need
to develop ChatGPT-written content detection algorithms call for large-scale
datasets. In this paper, we initially investigate the possible negative impact
of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract
dataset (CHEAT) to support the development of detection algorithms. In
particular, the ChatGPT-written abstract dataset contains 35,304 synthetic
abstracts, with Generation, Polish, and Mix as prominent representatives. Based
on these data, we perform a thorough analysis of the existing text synthesis
detection algorithms. We show that ChatGPT-written abstracts are detectable,
while the detection difficulty increases with human involvement. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12008v1' target="_blank">http://arxiv.org/pdf/2304.12008v1</a><br> <br> <br> <font size='5'> 472 </font> <div style="text-align: right"> 2023-04-24 09:50:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The potential of large language models in medicine for education and decision
making purposes has been demonstrated as they achieve decent scores on medical
exams such as the United States Medical Licensing Exam (USMLE) and the MedQA
exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized
field of radiation oncology using the 38th American College of Radiology (ACR)
radiation oncology in-training (TXIT) exam and the 2022 Red Journal gray zone
cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of
63.65% and 74.57%, respectively, highlighting the advantage of the latest
ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in
radiation oncology are identified to some extent. Specifically, ChatGPT-4
demonstrates good knowledge of statistics, CNS & eye, pediatrics, biology, and
physics but has limitations in bone & soft tissue and gynecology, as per the
ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs well in
diagnosis, prognosis, and toxicity but lacks proficiency in topics related to
brachytherapy and dosimetry, as well as in-depth questions from clinical
trials. For the gray zone cases, ChatGPT-4 is able to suggest a personalized
treatment approach to each case with high correctness and comprehensiveness.
Most importantly, it provides novel treatment aspects for many cases, which are
not suggested by any human experts. Both evaluations demonstrate the potential
of ChatGPT-4 in medical education for the general public and cancer patients,
as well as the potential to aid clinical decision-making, while acknowledging
its limitations in certain domains. Because of the risk of hallucination, facts
provided by ChatGPT always need to be verified. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11957v3' target="_blank">http://arxiv.org/pdf/2304.11957v3</a><br> <br> <br> <font size='5'> 473 </font> <div style="text-align: right"> 2023-04-24 09:20:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT the Ultimate Programming Assistant -- How far is it?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent progress in generative AI techniques has significantly influenced
software engineering, as AI-driven methods tackle common developer challenges
such as code synthesis from descriptions, program repair, and natural language
summaries for existing programs. Large-scale language models (LLMs), like
OpenAI's Codex, are increasingly adopted in AI-driven software engineering.
ChatGPT, another LLM, has gained considerable attention for its potential as a
bot for discussing source code, suggesting changes, providing descriptions, and
generating code. To evaluate the practicality of LLMs as programming assistant
bots, it is essential to examine their performance on unseen problems and
various tasks.
  In our paper, we conduct an empirical analysis of ChatGPT's potential as a
fully automated programming assistant, emphasizing code generation, program
repair, and code summarization. Our study assesses ChatGPT's performance on
common programming problems and compares it to state-of-the-art approaches
using two benchmarks. Our research indicates that ChatGPT effectively handles
typical programming challenges. However, we also discover the limitations in
its attention span: comprehensive descriptions can restrict ChatGPT's focus and
impede its ability to utilize its extensive knowledge for problem-solving.
Surprisingly, we find that ChatGPT's summary explanations of incorrect code
provide valuable insights into the developer's original intentions. This
insight can be served as a foundation for future work addressing the oracle
problem. Our study offers valuable perspectives on the development of LLMs for
programming assistance, specifically by highlighting the significance of prompt
engineering and enhancing our comprehension of ChatGPT's practical applications
in software engineering. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11938v1' target="_blank">http://arxiv.org/pdf/2304.11938v1</a><br> <br> <br> <font size='5'> 474 </font> <div style="text-align: right"> 2023-04-24 08:29:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatLLM Network: More brains, More intelligence</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Dialogue-based language models mark a huge milestone in the field of
artificial intelligence, by their impressive ability to interact with users, as
well as a series of challenging tasks prompted by customized instructions.
However, the prevalent large-scale dialogue-based language models like ChatGPT
still have room for improvement, such as unstable responses to questions and
the inability to think cooperatively like humans. Considering the ability of
dialogue-based language models in conversation and their inherent randomness in
thinking, we propose ChatLLM network that allows multiple dialogue-based
language models to interact, provide feedback, and think together. We design
the network of ChatLLMs based on ChatGPT. Specifically, individual instances of
ChatGPT may possess distinct perspectives towards the same problem, and by
consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM
network system can conduct decision-making more objectively and
comprehensively. In addition, a language-based feedback mechanism comparable to
backpropagation is devised to update the ChatGPTs within the network.
Experiments on two datasets demonstrate that our network attains significant
improvements in problem-solving, leading to observable progress amongst each
member. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12998v1' target="_blank">http://arxiv.org/pdf/2304.12998v1</a><br> <br> <br> <font size='5'> 475 </font> <div style="text-align: right"> 2023-04-24 06:54:10+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can we Trust Chatbots for now? Accuracy, reproducibility, traceability; a Case Study on Leonardo da Vinci's Contribution to Astronomy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLM) are studied. Applications to chatbots and
education are considered. A case study on Leonardo's contribution to astronomy
is presented. Major problems with accuracy, reproducibility and traceability of
answers are reported for ChatGPT, GPT-4, BLOOM and Google Bard. Possible
reasons for problems are discussed and some solutions are proposed. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11852v1' target="_blank">http://arxiv.org/pdf/2304.11852v1</a><br> <br> <br> <font size='5'> 476 </font> <div style="text-align: right"> 2023-04-23 15:35:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Finding Failure-Inducing Test Cases with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automatically detecting software failures is an important task and a
longstanding challenge. It requires finding failure-inducing test cases whose
test input can trigger the software's fault, and constructing an automated
oracle to detect the software's incorrect behaviors. Recent advancement of
large language models (LLMs) motivates us to study how far this challenge can
be addressed by ChatGPT, a state-of-the-art LLM. Unfortunately, our study shows
that ChatGPT has a low probability (28.8%) of finding correct failure-inducing
test cases for buggy programs. A possible reason is that finding
failure-inducing test cases requires analyzing the subtle code differences
between a buggy program and its correct version. When these two versions have
similar syntax, ChatGPT is weak at recognizing subtle code differences. Our
insight is that ChatGPT's performance can be substantially enhanced when
ChatGPT is guided to focus on the subtle code difference. We have an
interesting observation that ChatGPT is effective in inferring the intended
behaviors of a buggy program. The intended behavior can be leveraged to
synthesize programs, in order to make the subtle code difference between a
buggy program and its correct version (i.e., the synthesized program) explicit.
Driven by this observation, we propose a novel approach that synergistically
combines ChatGPT and differential testing to find failure-inducing test cases.
We evaluate our approach on Quixbugs (a benchmark of buggy programs), and
compare it with state-of-the-art baselines, including direct use of ChatGPT and
Pynguin. The experimental result shows that our approach has a much higher
probability (77.8%) of finding correct failure-inducing test cases, 2.7X as the
best baseline. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11686v3' target="_blank">http://arxiv.org/pdf/2304.11686v3</a><br> <br> <br> <font size='5'> 477 </font> <div style="text-align: right"> 2023-04-23 12:33:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11633v1' target="_blank">http://arxiv.org/pdf/2304.11633v1</a><br> <br> <br> <font size='5'> 478 </font> <div style="text-align: right"> 2023-04-23 08:26:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Epistemic considerations when AI answers questions for us</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this position paper, we argue that careless reliance on AI to answer our
questions and to judge our output is a violation of Grice's Maxim of Quality as
well as a violation of Lemoine's legal Maxim of Innocence, performing an
(unwarranted) authority fallacy, and while lacking assessment signals,
committing Type II errors that result from fallacies of the inverse. What is
missing in the focus on output and results of AI-generated and AI-evaluated
content is, apart from paying proper tribute, the demand to follow a person's
thought process (or a machine's decision processes). In deliberately avoiding
Neural Networks that cannot explain how they come to their conclusions, we
introduce logic-symbolic inference to handle any possible epistemics any human
or artificial information processor may have. Our system can deal with various
belief systems and shows how decisions may differ for what is true, false,
realistic, unrealistic, literal, or anomalous. As is, stota AI such as ChatGPT
is a sorcerer's apprentice. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14352v1' target="_blank">http://arxiv.org/pdf/2304.14352v1</a><br> <br> <br> <font size='5'> 479 </font> <div style="text-align: right"> 2023-04-23 07:38:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Differentiate ChatGPT-generated and Human-written Medical Texts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background: Large language models such as ChatGPT are capable of generating
grammatically perfect and human-like text content, and a large number of
ChatGPT-generated texts have appeared on the Internet. However, medical texts
such as clinical notes and diagnoses require rigorous validation, and erroneous
medical content generated by ChatGPT could potentially lead to disinformation
that poses significant harm to healthcare and the general public.
  Objective: This research is among the first studies on responsible and
ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus
on analyzing the differences between medical texts written by human experts and
generated by ChatGPT, and designing machine learning workflows to effectively
detect and differentiate medical texts generated by ChatGPT.
  Methods: We first construct a suite of datasets containing medical texts
written by human experts and generated by ChatGPT. In the next step, we analyze
the linguistic features of these two types of content and uncover differences
in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally,
we design and implement machine learning methods to detect medical text
generated by ChatGPT.
  Results: Medical texts written by humans are more concrete, more diverse, and
typically contain more useful information, while medical texts generated by
ChatGPT pay more attention to fluency and logic, and usually express general
terminologies rather than effective information specific to the context of the
problem. A BERT-based model can effectively detect medical texts generated by
ChatGPT, and the F1 exceeds 95%. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11567v1' target="_blank">http://arxiv.org/pdf/2304.11567v1</a><br> <br> <br> <font size='5'> 480 </font> <div style="text-align: right"> 2023-04-21 23:50:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent advancement in Natural Language Processing (NLP) capability has
led to the development of language models (e.g., ChatGPT) that is capable of
generating human-like language. In this study, we explore how language models
can be utilized to help the ideation aspect of creative writing. Our empirical
findings show that language models play different roles in helping student
writers to be more creative, such as the role of a collaborator, a provocateur,
etc </font><br> Link: <a href='http://arxiv.org/pdf/2304.11276v1' target="_blank">http://arxiv.org/pdf/2304.11276v1</a><br> <br> <br> <font size='5'> 481 </font> <div style="text-align: right"> 2023-04-21 23:08:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a natural language processing tool that can engage in human-like
conversations and generate coherent and contextually relevant responses to
various prompts. ChatGPT is capable of understanding natural text that is input
by a user and generating appropriate responses in various forms. This tool
represents a major step in how humans are interacting with technology. This
paper specifically focuses on how ChatGPT is revolutionizing the realm of
engineering education and the relationship between technology, students, and
faculty and staff. Because this tool is quickly changing and improving with the
potential for even greater future capability, it is a critical time to collect
pertinent data. A survey was created to measure the effects of ChatGPT on
students, faculty, and staff. This survey is shared as a Texas A&M University
technical report to allow other universities and entities to use this survey
and measure the effects elsewhere. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14415v1' target="_blank">http://arxiv.org/pdf/2304.14415v1</a><br> <br> <br> <font size='5'> 482 </font> <div style="text-align: right"> 2023-04-21 22:53:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The allure of emerging AI technologies is undoubtedly thrilling. However, the
promise that AI technologies will benefit all of humanity is empty so long as
we lack a nuanced understanding of what humanity is supposed to be in the face
of widening global inequality and pressing existential threats. Going forward,
it is crucial to invest in rigorous and collaborative AI safety and ethics
research. We also need to develop standards in a sustainable and equitable way
that differentiate between merely speculative and well-researched questions.
Only the latter enable us to co-construct and deploy the values that are
necessary for creating beneficial AI. Failure to do so could result in a future
in which our AI technological advancements outstrip our ability to navigate
their ethical and social implications. This path we do not want to go down. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11163v1' target="_blank">http://arxiv.org/pdf/2304.11163v1</a><br> <br> <br> <font size='5'> 483 </font> <div style="text-align: right"> 2023-04-21 21:25:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Written answers to open-ended questions can have a higher long-term effect on
learning than multiple-choice questions. However, it is critical that teachers
immediately review the answers, and ask to redo those that are incoherent. This
can be a difficult task and can be time-consuming for teachers. A possible
solution is to automate the detection of incoherent answers. One option is to
automate the review with Large Language Models (LLM). In this paper, we analyze
the responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,
and YOU. We used them with zero, one, two, three and four shots. We compared
their performance with the results of various classifiers trained with Machine
Learning (ML). We found that LLMs perform worse than MLs in detecting
incoherent answers. The difficulty seems to reside in recursive questions that
contain both questions and answers, and in responses from students with typical
fourth-grader misspellings. Upon closer examination, we have found that the
ChatGPT model faces the same challenges. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11257v1' target="_blank">http://arxiv.org/pdf/2304.11257v1</a><br> <br> <br> <font size='5'> 484 </font> <div style="text-align: right"> 2023-04-21 16:40:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our
whole society, rapidly altering the way we think, create and live. For
instance, the GPT integration in Bing has altered our approach to online
searching. While nascent LLMs have many advantages, new legal and ethical risks
are also emerging, stemming in particular from stochastic parrots and
hallucination. The EU is the first and foremost jurisdiction that has focused
on the regulation of AI models. However, the risks posed by the new LLMs are
likely to be underestimated by the emerging EU regulatory paradigm. Therefore,
this correspondence warns that the European AI regulatory paradigm must evolve
further to mitigate such risks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.14347v1' target="_blank">http://arxiv.org/pdf/2304.14347v1</a><br> <br> <br> <font size='5'> 485 </font> <div style="text-align: right"> 2023-04-21 16:23:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as ChatGPT have recently demonstrated
significant potential in mathematical abilities, providing valuable reasoning
paradigm consistent with human natural language. However, LLMs currently have
difficulty in bridging perception, language understanding and reasoning
capabilities due to incompatibility of the underlying information flow among
them, making it challenging to accomplish tasks autonomously. On the other
hand, abductive learning (ABL) frameworks for integrating the two abilities of
perception and reasoning has seen significant success in inverse decipherment
of incomplete facts, but it is limited by the lack of semantic understanding of
logical reasoning rules and the dependence on complicated domain knowledge
representation. This paper presents a novel method (ChatABL) for integrating
LLMs into the ABL framework, aiming at unifying the three abilities in a more
user-friendly and understandable manner. The proposed method uses the strengths
of LLMs' understanding and logical reasoning to correct the incomplete logical
facts for optimizing the performance of perceptual module, by summarizing and
reorganizing reasoning rules represented in natural language format. Similarly,
perceptual module provides necessary reasoning examples for LLMs in natural
language format. The variable-length handwritten equation deciphering task, an
abstract expression of the Mayan calendar decoding, is used as a testbed to
demonstrate that ChatABL has reasoning ability beyond most existing
state-of-the-art methods, which has been well supported by comparative studies.
To our best knowledge, the proposed ChatABL is the first attempt to explore a
new pattern for further approaching human-level cognitive ability via natural
language interaction with ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11107v1' target="_blank">http://arxiv.org/pdf/2304.11107v1</a><br> <br> <br> <font size='5'> 486 </font> <div style="text-align: right"> 2023-04-21 15:04:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Robot-based assembly in construction has emerged as a promising solution to
address numerous challenges such as increasing costs, labor shortages, and the
demand for safe and efficient construction processes. One of the main obstacles
in realizing the full potential of these robotic systems is the need for
effective and efficient sequence planning for construction tasks. Current
approaches, including mathematical and heuristic techniques or machine learning
methods, face limitations in their adaptability and scalability to dynamic
construction environments. To expand the ability of the current robot system in
sequential understanding, this paper introduces RoboGPT, a novel system that
leverages the advanced reasoning capabilities of ChatGPT, a large language
model, for automated sequence planning in robot-based assembly applied to
construction tasks. The proposed system adapts ChatGPT for construction
sequence planning and demonstrate its feasibility and effectiveness through
experimental evaluation including Two case studies and 80 trials about real
construction tasks. The results show that RoboGPT-driven robots can handle
complex construction operations and adapt to changes on the fly. This paper
contributes to the ongoing efforts to enhance the capabilities and performance
of robot-based assembly systems in the construction industry, and it paves the
way for further integration of large language model technologies in the field
of construction robotics. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11018v1' target="_blank">http://arxiv.org/pdf/2304.11018v1</a><br> <br> <br> <font size='5'> 487 </font> <div style="text-align: right"> 2023-04-21 07:08:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Context: AI-assisted code generation tools have become increasingly prevalent
in software engineering, offering the ability to generate code from natural
language prompts or partial code inputs. Notable examples of these tools
include GitHub Copilot, Amazon CodeWhisperer, and OpenAI's ChatGPT.
  Objective: This study aims to compare the performance of these prominent code
generation tools in terms of code quality metrics, such as Code Validity, Code
Correctness, Code Security, Code Reliability, and Code Maintainability, to
identify their strengths and shortcomings.
  Method: We assess the code generation capabilities of GitHub Copilot, Amazon
CodeWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated
code is then evaluated based on the proposed code quality metrics.
  Results: Our analysis reveals that the latest versions of ChatGPT, GitHub
Copilot, and Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1%
of the time, respectively. In comparison, the newer versions of GitHub CoPilot
and Amazon CodeWhisperer showed improvement rates of 18% for GitHub Copilot and
7% for Amazon CodeWhisperer. The average technical debt, considering code
smells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub
Copilot, and 5.6 minutes for Amazon CodeWhisperer.
  Conclusions: This study highlights the strengths and weaknesses of some of
the most popular code generation tools, providing valuable insights for
practitioners. By comparing these generators, our results may assist
practitioners in selecting the optimal tool for specific tasks, enhancing their
decision-making process. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10778v1' target="_blank">http://arxiv.org/pdf/2304.10778v1</a><br> <br> <br> <font size='5'> 488 </font> <div style="text-align: right"> 2023-04-21 04:46:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Ethics of AI-Generated Maps: A Study of DALLE 2 and Implications for Cartography</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancement of artificial intelligence (AI) such as the emergence
of large language models including ChatGPT and DALLE 2 has brought both
opportunities for improving productivity and raised ethical concerns. This
paper investigates the ethics of using artificial intelligence (AI) in
cartography, with a particular focus on the generation of maps using DALLE 2.
To accomplish this, we first create an open-sourced dataset that includes
synthetic (AI-generated) and real-world (human-designed) maps at multiple
scales with a variety settings. We subsequently examine four potential ethical
concerns that may arise from the characteristics of DALLE 2 generated maps,
namely inaccuracies, misleading information, unanticipated features, and
reproducibility. We then develop a deep learning-based ethical examination
system that identifies those AI-generated maps. Our research emphasizes the
importance of ethical considerations in the development and use of AI
techniques in cartography, contributing to the growing body of work on
trustworthy maps. We aim to raise public awareness of the potential risks
associated with AI-generated maps and support the development of ethical
guidelines for their future use. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10743v3' target="_blank">http://arxiv.org/pdf/2304.10743v3</a><br> <br> <br> <font size='5'> 489 </font> <div style="text-align: right"> 2023-04-20 22:16:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Meta Semantics: Towards better natural language understanding and reasoning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Natural language understanding is one of the most challenging topics in
artificial intelligence. Deep neural network methods, particularly large
language module (LLM) methods such as ChatGPT and GPT-3, have powerful
flexibility to adopt informal text but are weak on logical deduction and suffer
from the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods
such as Mathematica, Semantic web, and Lean, are excellent in reasoning but
cannot handle the complex and changeable informal text. Inspired by pragmatics
and structuralism, we propose two strategies to solve the OOV problem and a
semantic model for better natural language understanding and reasoning. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10663v1' target="_blank">http://arxiv.org/pdf/2304.10663v1</a><br> <br> <br> <font size='5'> 490 </font> <div style="text-align: right"> 2023-04-20 19:40:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Harmful content is pervasive on social media, poisoning online communities
and negatively impacting participation. A common approach to address this issue
is to develop detection models that rely on human annotations. However, the
tasks required to build such models expose annotators to harmful and offensive
content and may require significant time and cost to complete. Generative AI
models have the potential to understand and detect harmful content. To
investigate this potential, we used ChatGPT and compared its performance with
MTurker annotations for three frequently discussed concepts related to harmful
content: Hateful, Offensive, and Toxic (HOT). We designed five prompts to
interact with ChatGPT and conducted four experiments eliciting HOT
classifications. Our results show that ChatGPT can achieve an accuracy of
approximately 80% when compared to MTurker annotations. Specifically, the model
displays a more consistent classification for non-HOT comments than HOT
comments compared to human annotations. Our findings also suggest that ChatGPT
classifications align with provided HOT definitions, but ChatGPT classifies
"hateful" and "offensive" as subsets of "toxic." Moreover, the choice of
prompts used to interact with ChatGPT impacts its performance. Based on these
in-sights, our study provides several meaningful implications for employing
ChatGPT to detect HOT content, particularly regarding the reliability and
consistency of its performance, its understand-ing and reasoning of the HOT
concept, and the impact of prompts on its performance. Overall, our study
provides guidance about the potential of using generative AI models to moderate
large volumes of user-generated content on social media. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10619v1' target="_blank">http://arxiv.org/pdf/2304.10619v1</a><br> <br> <br> <font size='5'> 491 </font> <div style="text-align: right"> 2023-04-20 17:48:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Why Does ChatGPT Fall Short in Providing Truthful Answers?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in Large Language Models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in aspects like truthfulness, e.g.
providing accurate and reliable outputs. Therefore, in this paper, we seek to
understand why ChatGPT falls short in providing truthful answers. For this
purpose, we first analyze the failures of ChatGPT in complex open-domain
question answering and identifies the abilities under the failures.
Specifically, we categorize ChatGPT's failures into four types: comprehension,
factualness, specificity, and inference. We further pinpoint three critical
abilities associated with QA failures: knowledge memorization, knowledge
recall, and knowledge reasoning. Additionally, we conduct experiments centered
on these abilities and propose potential approaches to enhance truthfulness.
The results indicate that furnishing the model with fine-grained external
knowledge, hints for knowledge recall, and guidance for reasoning can empower
the model to answer questions more truthfully. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10513v2' target="_blank">http://arxiv.org/pdf/2304.10513v2</a><br> <br> <br> <font size='5'> 492 </font> <div style="text-align: right"> 2023-04-20 16:54:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Performance of ChatGPT on the US Fundamentals of Engineering Exam: Comprehensive Assessment of Proficiency and Potential Implications for Professional Environmental Engineering Practice</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, advancements in artificial intelligence (AI) have led to the
development of large language models like GPT-4, demonstrating potential
applications in various fields, including education. This study investigates
the feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in
achieving satisfactory performance on the Fundamentals of Engineering (FE)
Environmental Exam. This study further shows a significant improvement in the
model's accuracy when answering FE exam questions through noninvasive prompt
modifications, substantiating the utility of prompt modification as a viable
approach to enhance AI performance in educational contexts. Furthermore, the
findings reflect remarkable improvements in mathematical capabilities across
successive iterations of ChatGPT models, showcasing their potential in solving
complex engineering problems. Our paper also explores future research
directions, emphasizing the importance of addressing AI challenges in
education, enhancing accessibility and inclusion for diverse student
populations, and developing AI-resistant exam questions to maintain examination
integrity. By evaluating the performance of ChatGPT in the context of the FE
Environmental Exam, this study contributes valuable insights into the potential
applications and limitations of large language models in educational settings.
As AI continues to evolve, these findings offer a foundation for further
research into the responsible and effective integration of AI models across
various disciplines, ultimately optimizing the learning experience and
improving student outcomes. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12198v1' target="_blank">http://arxiv.org/pdf/2304.12198v1</a><br> <br> <br> <font size='5'> 493 </font> <div style="text-align: right"> 2023-04-20 16:50:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Phoenix: Democratizing ChatGPT across Languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents our efforts to democratize ChatGPT across language. We
release a large language model "Phoenix", achieving competitive performance
among open-source English and Chinese models while excelling in languages with
limited resources (covering both Latin and non-Latin languages). We believe
this work will be beneficial to make ChatGPT more accessible, especially in
countries where people cannot use ChatGPT due to restrictions from OpenAI or
local goverments. Our data, code, and models are available at
https://github.com/FreedomIntelligence/LLMZoo. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10453v1' target="_blank">http://arxiv.org/pdf/2304.10453v1</a><br> <br> <br> <font size='5'> 494 </font> <div style="text-align: right"> 2023-04-20 16:27:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Safety Assessment of Chinese Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the rapid popularity of large language models such as ChatGPT and GPT-4,
a growing amount of attention is paid to their safety concerns. These models
may generate insulting and discriminatory content, reflect incorrect social
values, and may be used for malicious purposes such as fraud and dissemination
of misleading information. Evaluating and enhancing their safety is
particularly essential for the wide application of large language models
(LLMs). To further promote the safe deployment of LLMs, we develop a Chinese
LLM safety assessment benchmark. Our benchmark explores the comprehensive
safety performance of LLMs from two perspectives: 8 kinds of typical safety
scenarios and 6 types of more challenging instruction attacks. Our benchmark is
based on a straightforward process in which it provides the test prompts and
evaluates the safety of the generated responses from the evaluated model. In
evaluation, we utilize the LLM's strong evaluation ability and develop it as a
safety evaluator by prompting. On top of this benchmark, we conduct safety
assessments and analyze 15 LLMs including the OpenAI GPT series and other
well-known Chinese LLMs, where we observe some interesting findings. For
example, we find that instruction attacks are more likely to expose safety
issues of all LLMs. Moreover, to promote the development and deployment of
safe, responsible, and ethical AI, we publicly release SafetyPrompts including
100k augmented prompts and responses by LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10436v1' target="_blank">http://arxiv.org/pdf/2304.10436v1</a><br> <br> <br> <font size='5'> 495 </font> <div style="text-align: right"> 2023-04-20 16:16:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we present work in progress on the role of artificial
intelligence (AI) chatbots, such as ChatGPT, in facilitating data access to
federated knowledge graphs. In particular, we provide examples from the field
of bioinformatics, to illustrate the potential use of Conversational AI to
describe datasets, as well as generate and explain (federated) queries across
datasets for the benefit of domain experts. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10427v1' target="_blank">http://arxiv.org/pdf/2304.10427v1</a><br> <br> <br> <font size='5'> 496 </font> <div style="text-align: right"> 2023-04-20 15:27:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task
in NLP, as it tackles knowledge from external sources with unseen events and
entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an
evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that
suffer from a substantial fraction of incorrect answers, and the evaluation set
is not well-aligned with the external knowledge source as a result of random
sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB
Population benchmark, which addresses the two mentioned problems by using
experts instead of crowd-sourced annotation and by adding diversified
adversarial samples to make the evaluation set more representative. We conduct
extensive experiments comparing state-of-the-art methods for CSKB Population on
the new evaluation set for future research comparisons. Empirical results show
that the population task is still challenging, even for large language models
(LLM) such as ChatGPT. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10392v1' target="_blank">http://arxiv.org/pdf/2304.10392v1</a><br> <br> <br> <font size='5'> 497 </font> <div style="text-align: right"> 2023-04-20 08:16:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good Recommender? A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recommendation systems have witnessed significant advancements and have been
widely used over the past decades. However, most traditional recommendation
methods are task-specific and therefore lack efficient generalization ability.
Recently, the emergence of ChatGPT has significantly advanced NLP tasks by
enhancing the capabilities of conversational models. Nonetheless, the
application of ChatGPT in the recommendation domain has not been thoroughly
investigated. In this paper, we employ ChatGPT as a general-purpose
recommendation model to explore its potential for transferring extensive
linguistic and world knowledge acquired from large-scale corpora to
recommendation scenarios. Specifically, we design a set of prompts and evaluate
ChatGPT's performance on five recommendation scenarios. Unlike traditional
recommendation methods, we do not fine-tune ChatGPT during the entire
evaluation process, relying only on the prompts themselves to convert
recommendation tasks into natural language tasks. Further, we explore the use
of few-shot prompting to inject interaction information that contains user
potential interest to help ChatGPT better understand user needs and interests.
Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT
has achieved promising results in certain tasks and is capable of reaching the
baseline level in others. We conduct human evaluations on two
explainability-oriented tasks to more accurately evaluate the quality of
contents generated by different models. And the human evaluations show ChatGPT
can truly understand the provided information and generate clearer and more
reasonable results. We hope that our study can inspire researchers to further
explore the potential of language models like ChatGPT to improve recommendation
performance and contribute to the advancement of the recommendation systems
field. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10149v2' target="_blank">http://arxiv.org/pdf/2304.10149v2</a><br> <br> <br> <font size='5'> 498 </font> <div style="text-align: right"> 2023-04-20 08:08:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The release of ChatGPT has uncovered a range of possibilities whereby large
language models (LLMs) can substitute human intelligence. In this paper, we
seek to understand whether ChatGPT has the potential to reproduce
human-generated label annotations in social computing tasks. Such an
achievement could significantly reduce the cost and complexity of social
computing research. As such, we use ChatGPT to relabel five seminal datasets
covering stance detection (2x), sentiment analysis, hate speech, and bot
detection. Our results highlight that ChatGPT does have the potential to handle
these data annotation tasks, although a number of challenges remain. ChatGPT
obtains an average accuracy 0.609. Performance is highest for the sentiment
analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we
show that performance varies substantially across individual labels. We believe
this work can open up new lines of analysis and act as a basis for future
research into the exploitation of ChatGPT for human annotation tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10145v2' target="_blank">http://arxiv.org/pdf/2304.10145v2</a><br> <br> <br> <font size='5'> 499 </font> <div style="text-align: right"> 2023-04-19 23:51:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Physics task development of prospective physics teachers using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent advancement of large language models presents numerous
opportunities for teaching and learning. Despite widespread public debate
regarding the use of large language models, empirical research on their
opportunities and risks in education remains limited. In this work, we
demonstrate the qualities and shortcomings of using ChatGPT 3.5 for physics
task development by prospective teachers. In a randomized controlled trial, 26
prospective physics teacher students were divided into two groups: the first
group used ChatGPT 3.5 to develop text-based physics tasks for four different
concepts in the field of kinematics for 10th grade high school students, while
the second group used a classical textbook to create tasks for the same
concepts and target group. The results indicate no difference in task
correctness, but students using the textbook achieved a higher clarity and more
frequently embedded their questions in a meaningful context. Both groups
adapted the level of task difficulty easily to the target group but struggled
strongly with sufficient task specificity, i.e., relevant information to solve
the tasks were missing. Students using ChatGPT for problem posing rated high
system usability but experienced difficulties with output quality. These
results provide insights into the opportunities and pitfalls of using large
language models in education. </font><br> Link: <a href='http://arxiv.org/pdf/2304.10014v1' target="_blank">http://arxiv.org/pdf/2304.10014v1</a><br> <br> <br> <font size='5'> 500 </font> <div style="text-align: right"> 2023-04-19 21:12:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MasakhaNEWS: News Topic Classification for African languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: African languages are severely under-represented in NLP research due to lack
of datasets covering several NLP tasks. While there are individual language
specific datasets that are being expanded to different tasks, only a handful of
NLP tasks (e.g. named entity recognition and machine translation) have
standardized benchmark datasets covering several geographical and
typologically-diverse African languages. In this paper, we develop MasakhaNEWS
-- a new benchmark dataset for news topic classification covering 16 languages
widely spoken in Africa. We provide an evaluation of baseline models by
training classical machine learning models and fine-tuning several language
models. Furthermore, we explore several alternatives to full fine-tuning of
language models that are better suited for zero-shot and few-shot learning such
as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern
exploiting training (PET), prompting language models (like ChatGPT), and
prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).
Our evaluation in zero-shot setting shows the potential of prompting ChatGPT
for news topic classification in low-resource African languages, achieving an
average performance of 70 F1 points without leveraging additional supervision
like MAD-X. In few-shot setting, we show that with as little as 10 examples per
label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of
full supervised training (92.6 F1 points) leveraging the PET approach. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09972v1' target="_blank">http://arxiv.org/pdf/2304.09972v1</a><br> <br> <br> <font size='5'> 501 </font> <div style="text-align: right"> 2023-04-19 17:50:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fundamental Limitations of Alignment in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: An important aspect in developing language models that interact with humans
is aligning their behavior to be useful and unharmful for their human users.
This is usually achieved by tuning the model in a way that enhances desired
behaviors and inhibits undesired ones, a process referred to as alignment. In
this paper, we propose a theoretical approach called Behavior Expectation
Bounds (BEB) which allows us to formally investigate several inherent
characteristics and limitations of alignment in large language models.
Importantly, we prove that for any behavior that has a finite probability of
being exhibited by the model, there exist prompts that can trigger the model
into outputting this behavior, with probability that increases with the length
of the prompt. This implies that any alignment process that attenuates
undesired behavior but does not remove it altogether, is not safe against
adversarial prompting attacks. Furthermore, our framework hints at the
mechanism by which leading alignment approaches such as reinforcement learning
from human feedback increase the LLM's proneness to being prompted into the
undesired behaviors. Moreover, we include the notion of personas in our BEB
framework, and find that behaviors which are generally very unlikely to be
exhibited by the model can be brought to the front by prompting the model to
behave as specific persona. This theoretical result is being experimentally
demonstrated in large scale by the so called contemporary "chatGPT jailbreaks",
where adversarial users trick the LLM into breaking its alignment guardrails by
triggering it into acting as a malicious persona. Our results expose
fundamental limitations in alignment of LLMs and bring to the forefront the
need to devise reliable mechanisms for ensuring AI safety. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11082v2' target="_blank">http://arxiv.org/pdf/2304.11082v2</a><br> <br> <br> <font size='5'> 502 </font> <div style="text-align: right"> 2023-04-19 17:47:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have achieved remarkable progress in solving
various natural language processing tasks due to emergent reasoning abilities.
However, LLMs have inherent limitations as they are incapable of accessing
up-to-date information (stored on the Web or in task-specific knowledge bases),
using external tools, and performing precise mathematical and logical
reasoning. In this paper, we present Chameleon, an AI system that mitigates
these limitations by augmenting LLMs with plug-and-play modules for
compositional reasoning. Chameleon synthesizes programs by composing various
tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python
functions, and heuristic-based modules) for accomplishing complex reasoning
tasks. At the heart of Chameleon is an LLM-based planner that assembles a
sequence of tools to execute to generate the final response. We showcase the
effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning
tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%
overall accuracy on ScienceQA, improving the best published few-shot result by
11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,
lifting the state of the art to 98.78%. Our analysis also shows that the
GPT-4-powered planner exhibits more consistent and rational tool selection via
inferring potential constraints from instructions, compared to a
ChatGPT-powered planner. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09842v2' target="_blank">http://arxiv.org/pdf/2304.09842v2</a><br> <br> <br> <font size='5'> 503 </font> <div style="text-align: right"> 2023-04-19 13:53:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While large language models (LLMs) have been successfully applied to various
tasks, they still face challenges with hallucinations. Augmenting LLMs with
domain-specific tools such as database utilities can facilitate easier and more
precise access to specialized knowledge. In this paper, we present GeneGPT, a
novel method for teaching LLMs to use the Web APIs of the National Center for
Biotechnology Information (NCBI) for answering genomics questions.
Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs
by in-context learning and an augmented decoding algorithm that can detect and
execute API calls. Experimental results show that GeneGPT achieves
state-of-the-art performance on eight tasks in the GeneTuring benchmark with an
average score of 0.83, largely surpassing retrieval-augmented LLMs such as the
new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as
well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)
API demonstrations have good cross-task generalizability and are more useful
than documentations for in-context learning; (2) GeneGPT can generalize to
longer chains of API calls and answer multi-hop questions in GeneHop, a novel
dataset introduced in this work; (3) Different types of errors are enriched in
different tasks, providing valuable insights for future improvements. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09667v3' target="_blank">http://arxiv.org/pdf/2304.09667v3</a><br> <br> <br> <font size='5'> 504 </font> <div style="text-align: right"> 2023-04-19 13:45:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Secure is Code Generated by ChatGPT?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, large language models have been responsible for great
advances in the field of artificial intelligence (AI). ChatGPT in particular,
an AI chatbot developed and recently released by OpenAI, has taken the field to
the next level. The conversational model is able not only to process human-like
text, but also to translate natural language into code. However, the safety of
programs generated by ChatGPT should not be overlooked. In this paper, we
perform an experiment to address this issue. Specifically, we ask ChatGPT to
generate a number of program and evaluate the security of the resulting source
code. We further investigate whether ChatGPT can be prodded to improve the
security by appropriate prompts, and discuss the ethical aspects of using AI to
generate code. Results suggest that ChatGPT is aware of potential
vulnerabilities, but nonetheless often generates source code that are not
robust to certain attacks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09655v1' target="_blank">http://arxiv.org/pdf/2304.09655v1</a><br> <br> <br> <font size='5'> 505 </font> <div style="text-align: right"> 2023-04-19 13:35:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as a Therapist Assistant: A Suitability Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper proposes using ChatGPT, an innovative technology with various
applications, as an assistant for psychotherapy. ChatGPT can serve as a patient
information collector, a companion for patients in between therapy sessions,
and an organizer of gathered information for therapists to facilitate treatment
processes. The research identifies five research questions and discovers useful
prompts for fine-tuning the assistant, which shows that ChatGPT can participate
in positive conversations, listen attentively, offer validation and potential
coping strategies without providing explicit medical advice, and help
therapists discover new insights from multiple conversations with the same
patient. Using ChatGPT as an assistant for psychotherapy poses several
challenges that need to be addressed, including technical as well as
human-centric challenges which are discussed. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09873v1' target="_blank">http://arxiv.org/pdf/2304.09873v1</a><br> <br> <br> <font size='5'> 506 </font> <div style="text-align: right"> 2023-04-19 11:42:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT Equipped with Emotional Dialogue Capabilities?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This report presents a study on the emotional dialogue capability of ChatGPT,
an advanced language model developed by OpenAI. The study evaluates the
performance of ChatGPT on emotional dialogue understanding and generation
through a series of experiments on several downstream tasks. Our findings
indicate that while ChatGPT's performance on emotional dialogue understanding
may still lag behind that of supervised models, it exhibits promising results
in generating emotional responses. Furthermore, the study suggests potential
avenues for future research directions. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09582v1' target="_blank">http://arxiv.org/pdf/2304.09582v1</a><br> <br> <br> <font size='5'> 507 </font> <div style="text-align: right"> 2023-04-19 10:16:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have demonstrated a remarkable ability to
generalize zero-shot to various language-related tasks. This paper focuses on
the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance
ranking in Information Retrieval (IR). Surprisingly, our experiments reveal
that properly instructed ChatGPT and GPT-4 can deliver competitive, even
superior results than supervised methods on popular IR benchmarks. Notably,
GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of
2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and
an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we
delve into the potential for distilling the ranking capabilities of ChatGPT
into a specialized model. Our small specialized model that trained on 10K
ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO
data on BEIR. The code to reproduce our results is available at
www.github.com/sunnweiwei/RankGPT </font><br> Link: <a href='http://arxiv.org/pdf/2304.09542v1' target="_blank">http://arxiv.org/pdf/2304.09542v1</a><br> <br> <br> <font size='5'> 508 </font> <div style="text-align: right"> 2023-04-19 06:54:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Conversational Process Modelling: State of the Art, Applications, and Implications in Practice</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM
applications, it is often not clear how to apply chatbots to generate business
value. Hence, this work aims at the systematic analysis of existing chatbots
for their support of conversational process modelling as process-oriented
capability. Application scenarios are identified along the process life cycle.
Then a systematic literature review on conversational process modelling is
performed. The resulting taxonomy serves as input for the identification of
application scenarios for conversational process modelling, including
paraphrasing and improvement of process descriptions. The application scenarios
are evaluated for existing chatbots based on a real-world test set from the
higher education domain. It contains process descriptions as well as
corresponding process models, together with an assessment of the model quality.
Based on the literature and application scenario analyses, recommendations for
the usage (practical implications) and further development (research
directions) of conversational process modelling are derived. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11065v1' target="_blank">http://arxiv.org/pdf/2304.11065v1</a><br> <br> <br> <font size='5'> 509 </font> <div style="text-align: right"> 2023-04-18 23:11:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We analyse the effects of the ban of ChatGPT, a generative pre-trained
transformer chatbot, on individual productivity. We first compile data on the
hourly coding output of over 8,000 professional GitHub users in Italy and other
European countries to analyse the impact of the ban on individual productivity.
Combining the high-frequency data with the sudden announcement of the ban in a
difference-in-differences framework, we find that the output of Italian
developers decreased by around 50% in the first two business days after the ban
and recovered after that. Applying a synthetic control approach to daily Google
search and Tor usage data shows that the ban led to a significant increase in
the use of censorship bypassing tools. Our findings show that users swiftly
implement strategies to bypass Internet restrictions but this adaptation
activity creates short-term disruptions and hampers productivity. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09339v1' target="_blank">http://arxiv.org/pdf/2304.09339v1</a><br> <br> <br> <font size='5'> 510 </font> <div style="text-align: right"> 2023-04-18 18:01:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Creating Large Language Model Resistant Exams: Guidelines and Strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The proliferation of Large Language Models (LLMs), such as ChatGPT, has
raised concerns about their potential impact on academic integrity, prompting
the need for LLM-resistant exam designs. This article investigates the
performance of LLMs on exams and their implications for assessment, focusing on
ChatGPT's abilities and limitations. We propose guidelines for creating
LLM-resistant exams, including content moderation, deliberate inaccuracies,
real-world scenarios beyond the model's knowledge base, effective distractor
options, evaluating soft skills, and incorporating non-textual information. The
article also highlights the significance of adapting assessments to modern
tools and promoting essential skills development in students. By adopting these
strategies, educators can maintain academic integrity while ensuring that
assessments accurately reflect contemporary professional settings and address
the challenges and opportunities posed by artificial intelligence in education. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12203v1' target="_blank">http://arxiv.org/pdf/2304.12203v1</a><br> <br> <br> <font size='5'> 511 </font> <div style="text-align: right"> 2023-04-18 17:24:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Designing a ChatGPT Conversational Companion for Elderly People</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09866v1' target="_blank">http://arxiv.org/pdf/2304.09866v1</a><br> <br> <br> <font size='5'> 512 </font> <div style="text-align: right"> 2023-04-18 17:21:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT and GPT-4 have emerged and gained immense global attention
due to their unparalleled performance in language processing. Despite
demonstrating impressive capability in various open-domain tasks, their
adequacy in highly specific fields like radiology remains untested. Radiology
presents unique linguistic phenomena distinct from open-domain data due to its
specificity and complexity. Assessing the performance of large language models
(LLMs) in such specific domains is crucial not only for a thorough evaluation
of their overall performance but also for providing valuable insights into
future model design directions: whether model design should be generic or
domain-specific. To this end, in this study, we evaluate the performance of
ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned
specifically on task-related data samples. We also conduct a comprehensive
investigation on ChatGPT/GPT-4's reasoning ability by introducing varying
levels of inference difficulty. Our results show that 1) GPT-4 outperforms
ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models
require significant amounts of data samples to achieve comparable performance
to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model
that is capable of solving various tasks across different domains is feasible. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09138v1' target="_blank">http://arxiv.org/pdf/2304.09138v1</a><br> <br> <br> <font size='5'> 513 </font> <div style="text-align: right"> 2023-04-18 13:20:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The way users acquire information is undergoing a paradigm shift with the
advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves
knowledge from the model itself and generates answers for users. ChatGPT's
impressive question-answering (QA) capability has attracted more than 100
million users within a short period of time but has also raised concerns
regarding its reliability. In this paper, we perform the first large-scale
measurement of ChatGPT's reliability in the generic QA scenario with a
carefully curated set of 5,695 questions across ten datasets and eight domains.
We find that ChatGPT's reliability varies across different domains, especially
underperforming in law and science questions. We also demonstrate that system
roles, originally designed by OpenAI to allow users to steer ChatGPT's
behavior, can impact ChatGPT's reliability. We further show that ChatGPT is
vulnerable to adversarial examples, and even a single character change can
negatively affect its reliability in certain cases. We believe that our study
provides valuable insights into ChatGPT's reliability and underscores the need
for strengthening the reliability and security of large language models (LLMs). </font><br> Link: <a href='http://arxiv.org/pdf/2304.08979v1' target="_blank">http://arxiv.org/pdf/2304.08979v1</a><br> <br> <br> <font size='5'> 514 </font> <div style="text-align: right"> 2023-04-17 17:13:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The 'Impression' section of a radiology report is a critical basis for
communication between radiologists and other physicians, and it is typically
written by radiologists based on the 'Findings' section. However, writing
numerous impressions can be laborious and error-prone for radiologists.
Although recent studies have achieved promising results in automatic impression
generation using large-scale medical text data for pre-training and fine-tuning
pre-trained language models, such models often require substantial amounts of
medical text data and have poor generalization performance. While large
language models (LLMs) like ChatGPT have shown strong generalization
capabilities and performance, their performance in specific domains, such as
radiology, remains under-investigated and potentially limited. To address this
limitation, we propose ImpressionGPT, which leverages the in-context learning
capability of LLMs by constructing dynamic contexts using domain-specific,
individualized data. This dynamic prompt approach enables the model to learn
contextual knowledge from semantically similar examples from existing data.
Additionally, we design an iterative optimization algorithm that performs
automatic evaluation on the generated impression results and composes the
corresponding instruction prompts to further optimize the model. The proposed
ImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and
OpenI datasets without requiring additional training data or fine-tuning the
LLMs. This work presents a paradigm for localizing LLMs that can be applied in
a wide range of similar application scenarios, bridging the gap between
general-purpose LLMs and the specific language processing needs of various
domains. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08448v2' target="_blank">http://arxiv.org/pdf/2304.08448v2</a><br> <br> <br> <font size='5'> 515 </font> <div style="text-align: right"> 2023-04-17 12:27:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Creates a Review Article: State of the Art in the Most-Cited Articles on ChatGPT in Health Science, Computer Science, Communication, and Culture, According to Altmetric in Dimensions.ai</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We have analyzed all preprints on ChatGPT (N=501) and selected the most
influential preprints (according to Altmetric) about ChatGPT across scientific
disciplines to provide the most discussed research results about ChatGPT. We
prompted ChatGPT to create a structured review article based on them. The
results are surprisingly promising, suggesting that the future of creating
review articles can lie in ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02488v1' target="_blank">http://arxiv.org/pdf/2307.02488v1</a><br> <br> <br> <font size='5'> 516 </font> <div style="text-align: right"> 2023-04-17 12:04:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has revolutionized many research and industrial fields. ChatGPT has
shown great potential in software engineering to boost various traditional
tasks such as program repair, code understanding, and code generation. However,
whether automatic program repair (APR) applies to deep learning (DL) programs
is still unknown. DL programs, whose decision logic is not explicitly encoded
in the source code, have posed unique challenges to APR. While to repair DL
programs, an APR approach needs to not only parse the source code syntactically
but also needs to understand the code intention. With the best prior work, the
performance of fault localization is still far less than satisfactory (only
about 30\%). Therefore, in this paper, we explore ChatGPT's capability for DL
program repair by asking three research questions. (1) Can ChatGPT debug DL
programs effectively? (2) How can ChatGPT's repair performance be improved by
prompting? (3) In which way can dialogue help facilitate the repair? On top of
that, we categorize the common aspects useful for prompt design for DL program
repair. Also, we propose various prompt templates to facilitate the performance
and summarize the advantages and disadvantages of ChatGPT's abilities such as
detecting bad code smell, code refactoring, and detecting API
misuse/deprecation. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08191v1' target="_blank">http://arxiv.org/pdf/2304.08191v1</a><br> <br> <br> <font size='5'> 517 </font> <div style="text-align: right"> 2023-04-17 11:39:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically
transformed natural language processing research and shown promising strides
towards Artificial General Intelligence (AGI). Nonetheless, the high costs
associated with training and deploying LLMs present substantial obstacles to
transparent, accessible academic research. While several large language models,
such as LLaMA, have been open-sourced by the community, these predominantly
focus on English corpora, limiting their usefulness for other languages. In
this paper, we propose a method to augment LLaMA with capabilities for
understanding and generating Chinese text and its ability to follow
instructions. We achieve this by extending LLaMA's existing vocabulary with an
additional 20,000 Chinese tokens, thereby improving its encoding efficiency and
semantic understanding of Chinese. We further incorporate secondary
pre-training using Chinese data and fine-tune the model with Chinese
instruction datasets, significantly enhancing the model's ability to comprehend
and execute instructions. Our experimental results indicate that the newly
proposed model markedly enhances the original LLaMA's proficiency in
understanding and generating Chinese content. Additionally, the results on the
C-Eval dataset yield competitive performance among the models with several
times the size of ours. We have made our pre-trained models, training scripts,
and other resources available through GitHub, fostering open research for our
community. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca </font><br> Link: <a href='http://arxiv.org/pdf/2304.08177v2' target="_blank">http://arxiv.org/pdf/2304.08177v2</a><br> <br> <br> <font size='5'> 518 </font> <div style="text-align: right"> 2023-04-17 05:29:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Fine-tuning language models on tasks with instructions has demonstrated
potential in facilitating zero-shot generalization to unseen tasks. In this
paper, we introduce a straightforward yet effective method for enhancing
instruction tuning by employing symbolic tasks. Compared to crowdsourced human
tasks or model-generated tasks, symbolic tasks present a unique advantage as
they can be easily generated in vast quantities, theoretically providing an
infinite supply of high-quality training instances. To explore the potential of
symbolic tasks, we carry out an extensive case study on the representative
symbolic task of SQL execution. Empirical results on various benchmarks
validate that the integration of SQL execution leads to significant
improvements in zero-shot scenarios, particularly in table reasoning. Notably,
our 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table
reasoning across four benchmarks. Furthermore, experimental results on BBH (27
tasks) and MMLU (57 tasks) reveal that language models can be enhanced through
symbolic tasks without compromising their generality. We hope that our paper
serves as a catalyst, inspiring increased efforts to incorporate symbolic tasks
in instruction tuning. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07995v1' target="_blank">http://arxiv.org/pdf/2304.07995v1</a><br> <br> <br> <font size='5'> 519 </font> <div style="text-align: right"> 2023-04-17 04:45:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chinese Open Instruction Generalist: A Preliminary Release</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Instruction tuning is widely recognized as a key technique for building
generalist language models, which has attracted the attention of researchers
and the public with the release of InstructGPT~\citep{ouyang2022training} and
ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress
in English-oriented large-scale language models (LLMs), it is still
under-explored whether English-based foundation LLMs can perform similarly on
multilingual tasks compared to English tasks with well-designed instruction
tuning and how we can construct the corpora needed for the tuning. To remedy
this gap, we propose the project as an attempt to create a Chinese instruction
dataset by various methods adapted to the intrinsic characteristics of 4
sub-tasks. We collect around 200k Chinese instruction tuning samples, which
have been manually checked to guarantee high quality. We also summarize the
existing English and Chinese instruction corpora and briefly describe some
potential applications of the newly constructed Chinese instruction corpora.
The resulting \textbf{C}hinese \textbf{O}pen \textbf{I}nstruction
\textbf{G}eneralist (\textbf{COIG}) corpora are available in
Huggingface\footnote{\url{https://huggingface.co/datasets/BAAI/COIG}} and
Github\footnote{\url{https://github.com/BAAI-Zlab/COIG}}, and will be
continuously updated. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07987v4' target="_blank">http://arxiv.org/pdf/2304.07987v4</a><br> <br> <br> <font size='5'> 520 </font> <div style="text-align: right"> 2023-04-17 00:41:19+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent studies have demonstrated promising potential of ChatGPT for various
text annotation and classification tasks. However, ChatGPT is non-deterministic
which means that, as with human coders, identical input can lead to different
outputs. Given this, it seems appropriate to test the reliability of ChatGPT.
Therefore, this study investigates the consistency of ChatGPT's zero-shot
capabilities for text annotation and classification, focusing on different
model parameters, prompt variations, and repetitions of identical inputs. Based
on the real-world classification task of differentiating website texts into
news and not news, results show that consistency in ChatGPT's classification
output can fall short of scientific thresholds for reliability. For example,
even minor wording alterations in prompts or repeating the identical input can
lead to varying outputs. Although pooling outputs from multiple repetitions can
improve reliability, this study advises caution when using ChatGPT for
zero-shot text annotation and underscores the need for thorough validation,
such as comparison against human-annotated data. The unsupervised application
of ChatGPT for text annotation and classification is not recommended. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11085v1' target="_blank">http://arxiv.org/pdf/2304.11085v1</a><br> <br> <br> <font size='5'> 521 </font> <div style="text-align: right"> 2023-04-16 21:04:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Use of ChatGPT as a Tool for Learning and Assessment in Undergraduate Computer Science Curriculum: Opportunities and Challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The application of Artificial intelligence for teaching and learning in the
academic sphere is a trending subject of interest in the computing education.
ChatGPT, as an AI-based tool, provides various advantages, such as heightened
student involvement, cooperation, accessibility and availability. This paper
addresses the prospects and obstacles associated with utilizing ChatGPT as a
tool for learning and assessment in undergraduate Computer Science curriculum
in particular to teaching and learning fundamental programming courses.
Students having completed the course work for a Data Structures and Algorithms
(a sophomore level course) participated in this study. Two groups of students
were given programming challenges to solve within a short period of time. The
control group (group A) had access to text books and notes of programming
courses, however no Internet access was provided. Group B students were given
access to ChatGPT and were encouraged to use it to help solve the programming
challenges. The challenge was conducted in a computer lab environment using PC2
environment. Each team of students address the problem by writing executable
code that satisfies certain number of test cases. Student teams were scored
based on their performance in terms of number of successful passed testcases.
Results show that students using ChatGPT had an advantage in terms of earned
scores, however there were inconsistencies and inaccuracies in the submitted
code consequently affecting the overall performance. After a thorough analysis,
the paper's findings indicate that incorporating AI in higher education brings
about various opportunities and challenges. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11214v1' target="_blank">http://arxiv.org/pdf/2304.11214v1</a><br> <br> <br> <font size='5'> 522 </font> <div style="text-align: right"> 2023-04-16 18:37:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, significant public efforts have been directed towards developing
low-cost models with capabilities akin to ChatGPT, thereby fostering the growth
of open-source conversational models. However, there remains a scarcity of
comprehensive and in-depth evaluations of these models' performance. In this
study, we examine the influence of training data factors, including quantity,
quality, and linguistic distribution, on model performance. Our analysis is
grounded in several publicly accessible, high-quality instruction datasets, as
well as our own Chinese multi-turn conversations. We assess various models
using a evaluation set of 1,000 samples, encompassing nine real-world
scenarios. Our goal is to supplement manual evaluations with quantitative
analyses, offering valuable insights for the continued advancement of
open-source chat models. Furthermore, to enhance the performance and training
and inference efficiency of models in the Chinese domain, we extend the
vocabulary of LLaMA - the model with the closest open-source performance to
proprietary language models like GPT-3 - and conduct secondary pre-training on
3.4B Chinese words. We make our model, data, as well as code publicly
available. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07854v1' target="_blank">http://arxiv.org/pdf/2304.07854v1</a><br> <br> <br> <font size='5'> 523 </font> <div style="text-align: right"> 2023-04-16 16:50:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The language of sounds unheard: Exploring musical timbre semantics of large language models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Semantic dimensions of sound have been playing a central role in
understanding the nature of auditory sensory experience as well as the broader
relation between perception, language, and meaning. Accordingly, and given the
recent proliferation of large language models (LLMs), here we asked whether
such models exhibit an organisation of perceptual semantics similar to those
observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a
state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic
scales. We elicited multiple responses in separate chats, analogous to having
multiple human raters. ChatGPT generated semantic profiles that only partially
correlated with human ratings, yet showed robust agreement along well-known
psychophysical dimensions of musical sounds such as brightness (bright-dark)
and pitch height (deep-high). Exploratory factor analysis suggested the same
dimensionality but different spatial configuration of a latent factor space
between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees
of internal variability that were comparable in magnitude to that of human
ratings. Our work highlights the potential of LLMs to capture salient
dimensions of human sensory experience. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07830v3' target="_blank">http://arxiv.org/pdf/2304.07830v3</a><br> <br> <br> <font size='5'> 524 </font> <div style="text-align: right"> 2023-04-16 15:29:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07810v1' target="_blank">http://arxiv.org/pdf/2304.07810v1</a><br> <br> <br> <font size='5'> 525 </font> <div style="text-align: right"> 2023-04-15 20:08:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analyzing the Performance of ChatGPT in Cardiology and Vascular Pathologies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The article aims to analyze the performance of ChatGPT, a large language
model developed by OpenAI, in the context of cardiology and vascular
pathologies. The study evaluated the accuracy of ChatGPT in answering
challenging multiple-choice questions (QCM) using a dataset of 190 questions
from the Siamois-QCM platform. The goal was to assess ChatGPT potential as a
valuable tool in medical education compared to two well-ranked students of
medicine. The results showed that ChatGPT outperformed the students, scoring
175 out of 190 correct answers with a percentage of 92.10\%, while the two
students achieved scores of 163 and 159 with percentages of 85.78\% and
82.63\%, respectively. These results showcase how ChatGPT has the potential to
be highly effective in the fields of cardiology and vascular pathologies by
providing accurate answers to relevant questions. </font><br> Link: <a href='http://arxiv.org/pdf/2307.02518v1' target="_blank">http://arxiv.org/pdf/2307.02518v1</a><br> <br> <br> <font size='5'> 526 </font> <div style="text-align: right"> 2023-04-15 19:22:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We examine the potential of ChatGPT, and other large language models, in
predicting stock market returns using sentiment analysis of news headlines. We
use ChatGPT to indicate whether a given headline is good, bad, or irrelevant
news for firms' stock prices. We then compute a numerical score and document a
positive correlation between these ``ChatGPT scores'' and subsequent daily
stock market returns. Further, ChatGPT outperforms traditional sentiment
analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT
cannot accurately forecast returns, indicating return predictability is an
emerging capacity of complex models. ChatGPT-4's implied Sharpe ratios are
larger than ChatGPT-3's; however, the latter model has larger total returns.
Our results suggest that incorporating advanced language models into the
investment decision-making process can yield more accurate predictions and
enhance the performance of quantitative trading strategies. Predictability is
concentrated on smaller stocks and more prominent on firms with bad news,
consistent with limits-to-arbitrage arguments rather than market
inefficiencies. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07619v3' target="_blank">http://arxiv.org/pdf/2304.07619v3</a><br> <br> <br> <font size='5'> 527 </font> <div style="text-align: right"> 2023-04-15 16:33:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-collaboration Code Generation via ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Although Large Language Models (LLMs) have demonstrated remarkable
code-generation ability, they still struggle with complex tasks. In real-world
software development, humans usually tackle complex tasks through collaborative
teamwork, a strategy that significantly controls development complexity and
enhances software quality. Inspired by this, we present a self-collaboration
framework for code generation employing LLMs, exemplified by ChatGPT.
Specifically, through role instructions, 1) Multiple LLMs act as distinct
``experts'', each responsible for a specific subtask within a complex task; 2)
Specify the way to collaborate and interact, so that different roles form a
virtual team to facilitate each other's work, ultimately the virtual team
addresses code generation tasks collaboratively without the need for human
intervention. To effectively organize and manage this virtual team, we
incorporate software-development methodology into the framework. Thus, we
assemble an elementary team consisting of three ChatGPT roles (i.e., analyst,
coder, and tester) responsible for software development's analysis, coding, and
testing stages. We conduct comprehensive experiments on various code-generation
benchmarks. Experimental results indicate that self-collaboration code
generation relatively improves 29.9%-47.1% Pass@1 compared to direct code
generation, achieving state-of-the-art performance and even surpassing GPT-4.
Moreover, we showcase that self-collaboration could potentially enable LLMs to
efficiently handle complex real-world tasks that are not readily solved by
direct code generation, as evidenced in case study. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07590v2' target="_blank">http://arxiv.org/pdf/2304.07590v2</a><br> <br> <br> <font size='5'> 528 </font> <div style="text-align: right"> 2023-04-14 18:06:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Self-Perception and Political Biases of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This contribution analyzes the self-perception and political biases of
OpenAI's Large Language Model ChatGPT. Taking into account the first
small-scale reports and studies that have emerged, claiming that ChatGPT is
politically biased towards progressive and libertarian points of view, this
contribution aims to provide further clarity on this subject. For this purpose,
ChatGPT was asked to answer the questions posed by the political compass test
as well as similar questionnaires that are specific to the respective politics
of the G7 member states. These eight tests were repeated ten times each and
revealed that ChatGPT seems to hold a bias towards progressive views. The
political compass test revealed a bias towards progressive and libertarian
views, with the average coordinates on the political compass being (-6.48,
-5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes
ranging from -10 to 10), supporting the claims of prior research. The political
questionnaires for the G7 member states indicated a bias towards progressive
views but no significant bias between authoritarian and libertarian views,
contradicting the findings of prior reports, with the average coordinates being
(-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested
using the OCEAN test and its personality type was queried using the
Myers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT
was evaluated using the Dark Factor test. These three tests were also repeated
ten times each, revealing that ChatGPT perceives itself as highly open and
agreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of
test-takers with the least pronounced dark traits. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07333v1' target="_blank">http://arxiv.org/pdf/2304.07333v1</a><br> <br> <br> <font size='5'> 529 </font> <div style="text-align: right"> 2023-04-14 18:01:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: OpenAssistant Conversations -- Democratizing Large Language Model Alignment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Aligning large language models (LLMs) with human preferences has proven to
drastically improve usability and has driven rapid adoption as demonstrated by
ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF) greatly reduce the required
skill and domain knowledge to effectively harness the capabilities of LLMs,
increasing their accessibility and utility across various domains. However,
state-of-the-art alignment techniques like RLHF rely on high-quality human
feedback data, which is expensive to create and often remains proprietary. In
an effort to democratize research on large-scale alignment, we release
OpenAssistant Conversations, a human-generated, human-annotated assistant-style
conversation corpus consisting of 161,443 messages distributed across 66,497
conversation trees, in 35 different languages, annotated with 461,292 quality
ratings. The corpus is a product of a worldwide crowd-sourcing effort involving
over 13,500 volunteers. To demonstrate the OpenAssistant Conversations
dataset's effectiveness, we present OpenAssistant, the first fully open-source
large-scale instruction-tuned model to be trained on human data. A preference
study revealed that OpenAssistant replies are comparably preferred to
GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7%
respectively. We release our code and data under fully permissive licenses. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07327v1' target="_blank">http://arxiv.org/pdf/2304.07327v1</a><br> <br> <br> <font size='5'> 530 </font> <div style="text-align: right"> 2023-04-14 16:25:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: Applications, Opportunities, and Threats</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer)
is an artificial intelligence technology that is fine-tuned using supervised
machine learning and reinforcement learning techniques, allowing a computer to
generate natural language conversation fully autonomously. ChatGPT is built on
the transformer architecture and trained on millions of conversations from
various sources. The system combines the power of pre-trained deep learning
models with a programmability layer to provide a strong base for generating
natural language conversations. In this study, after reviewing the existing
literature, we examine the applications, opportunities, and threats of ChatGPT
in 10 main domains, providing detailed examples for the business and industry
as well as education. We also conducted an experimental study, checking the
effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found
that the latter performs significantly better. Despite its exceptional ability
to generate natural-sounding responses, the authors believe that ChatGPT does
not possess the same level of understanding, empathy, and creativity as a human
and cannot fully replace them in most situations. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09103v1' target="_blank">http://arxiv.org/pdf/2304.09103v1</a><br> <br> <br> <font size='5'> 531 </font> <div style="text-align: right"> 2023-04-14 06:56:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Future of ChatGPT-enabled Labor Market: A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As a phenomenal large language model, ChatGPT has achieved unparalleled
success in various real-world tasks and increasingly plays an important role in
our daily lives and work. However, extensive concerns are also raised about the
potential ethical issues, especially about whether ChatGPT-like artificial
general intelligence (AGI) will replace human jobs. To this end, in this paper,
we introduce a preliminary data-driven study on the future of ChatGPT-enabled
labor market from the view of Human-AI Symbiosis instead of Human-AI
Confrontation. To be specific, we first conduct an in-depth analysis of
large-scale job posting data in BOSS Zhipin, the largest online recruitment
platform in China. The results indicate that about 28% of occupations in the
current labor market require ChatGPT-related skills. Furthermore, based on a
large-scale occupation-centered knowledge graph, we develop a semantic
information enhanced collaborative filtering algorithm to predict the future
occupation-skill relations in the labor market. As a result, we find that
additional 45% occupations in the future will require ChatGPT-related skills.
In particular, industries related to technology, products, and operations are
expected to have higher proficiency requirements for ChatGPT-related skills,
while the manufacturing, services, education, and health science related
industries will have lower requirements for ChatGPT-related skills. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09823v2' target="_blank">http://arxiv.org/pdf/2304.09823v2</a><br> <br> <br> <font size='5'> 532 </font> <div style="text-align: right"> 2023-04-13 19:29:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT cites the most-cited articles and journals, relying solely on Google Scholar's citation counts. As a result, AI may amplify the Matthew Effect in environmental science</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT (GPT) has become one of the most talked-about innovations in recent
years, with over 100 million users worldwide. However, there is still limited
knowledge about the sources of information GPT utilizes. As a result, we
carried out a study focusing on the sources of information within the field of
environmental science. In our study, we asked GPT to identify the ten most
significant subdisciplines within the field of environmental science. We then
asked it to compose a scientific review article on each subdiscipline,
including 25 references. We proceeded to analyze these references, focusing on
factors such as the number of citations, publication date, and the journal in
which the work was published. Our findings indicate that GPT tends to cite
highly-cited publications in environmental science, with a median citation
count of 1184.5. It also exhibits a preference for older publications, with a
median publication year of 2010, and predominantly refers to well-respected
journals in the field, with Nature being the most cited journal by GPT.
Interestingly, our findings suggest that GPT seems to exclusively rely on
citation count data from Google Scholar for the works it cites, rather than
utilizing citation information from other scientific databases such as Web of
Science or Scopus. In conclusion, our study suggests that Google Scholar
citations play a significant role as a predictor for mentioning a study in
GPT-generated content. This finding reinforces the dominance of Google Scholar
among scientific databases and perpetuates the Matthew Effect in science, where
the rich get richer in terms of citations. With many scholars already utilizing
GPT for literature review purposes, we can anticipate further disparities and
an expanding gap between lesser-cited and highly-cited publications. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06794v1' target="_blank">http://arxiv.org/pdf/2304.06794v1</a><br> <br> <br> <font size='5'> 533 </font> <div style="text-align: right"> 2023-04-13 16:01:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is another large language model (LLM) inline but due to its
performance and ability to converse effectively, it has gained a huge
popularity amongst research as well as industrial community. Recently, many
studies have been published to show the effectiveness, efficiency, integration,
and sentiments of chatGPT and other LLMs. In contrast, this study focuses on
the important aspects that are mostly overlooked, i.e. sustainability, privacy,
digital divide, and ethics and suggests that not only chatGPT but every
subsequent entry in the category of conversational bots should undergo
Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This
paper discusses in detail about the issues and concerns raised over chatGPT in
line with aforementioned characteristics. We support our hypothesis by some
preliminary data collection and visualizations along with hypothesized facts.
We also suggest mitigations and recommendations for each of the concerns.
Furthermore, we also suggest some policies and recommendations for AI policy
act, if designed by the governments. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03123v1' target="_blank">http://arxiv.org/pdf/2305.03123v1</a><br> <br> <br> <font size='5'> 534 </font> <div style="text-align: right"> 2023-04-13 13:08:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Perspectives on Large Language Models for Relevance Judgment</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: When asked, current large language models (LLMs) like ChatGPT claim that they
can assist us with relevance judgments. Many researchers think this would not
lead to credible IR research. In this perspective paper, we discuss possible
ways for LLMs to assist human experts along with concerns and issues that
arise. We devise a human-machine collaboration spectrum that allows
categorizing different relevance judgment strategies, based on how much the
human relies on the machine. For the extreme point of "fully automated
assessment", we further include a pilot experiment on whether LLM-based
relevance judgments correlate with judgments from trained human assessors. We
conclude the paper by providing two opposing perspectives - for and against the
use of LLMs for automatic relevance judgments - and a compromise perspective,
informed by our analyses of the literature, our preliminary experimental
evidence, and our experience as IR researchers.
  We hope to start a constructive discussion within the community to avoid a
stale-mate during review, where work is dammed if is uses LLMs for evaluation
and dammed if it doesn't. </font><br> Link: <a href='http://arxiv.org/pdf/2304.09161v1' target="_blank">http://arxiv.org/pdf/2304.09161v1</a><br> <br> <br> <font size='5'> 535 </font> <div style="text-align: right"> 2023-04-13 09:39:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Evaluating the general abilities of foundation models to tackle human-level
tasks is a vital aspect of their development and application in the pursuit of
Artificial General Intelligence (AGI). Traditional benchmarks, which rely on
artificial datasets, may not accurately represent human-level capabilities. In
this paper, we introduce AGIEval, a novel benchmark specifically designed to
assess foundation model in the context of human-centric standardized exams,
such as college entrance exams, law school admission tests, math competitions,
and lawyer qualification tests. We evaluate several state-of-the-art foundation
models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.
Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math
competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%
accuracy on the English test of the Chinese national college entrance exam.
This demonstrates the extraordinary performance of contemporary foundation
models. In contrast, we also find that GPT-4 is less proficient in tasks that
require complex reasoning or specific domain knowledge. Our comprehensive
analyses of model capabilities (understanding, knowledge, reasoning, and
calculation) reveal these models' strengths and limitations, providing valuable
insights into future directions for enhancing their general capabilities. By
concentrating on tasks pertinent to human cognition and decision-making, our
benchmark delivers a more meaningful and robust evaluation of foundation
models' performance in real-world scenarios. The data, code, and all model
outputs are released in https://github.com/microsoft/AGIEval. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06364v1' target="_blank">http://arxiv.org/pdf/2304.06364v1</a><br> <br> <br> <font size='5'> 536 </font> <div style="text-align: right"> 2023-04-13 05:01:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The release of ChatGPT, Bard, and other large language model (LLM)-based
chatbots has drawn huge attention on foundations models worldwide. There is a
growing trend that foundation models will serve as the fundamental building
blocks for most of the future AI systems. However, incorporating foundation
models in AI systems raises significant concerns about responsible AI due to
their black box nature and rapidly advancing super-intelligence. Additionally,
the foundation model's growing capabilities can eventually absorb the other
components of AI systems, introducing the moving boundary and interface
evolution challenges in architecture design. To address these challenges, this
paper proposes a pattern-oriented responsible-AI-by-design reference
architecture for designing foundation model-based AI systems. Specially, the
paper first presents an architecture evolution of AI systems in the era of
foundation models, from "foundation-model-as-a-connector" to
"foundation-model-as-a-monolithic architecture". The paper then identifies the
key design decision points and proposes a pattern-oriented reference
architecture to provide reusable responsible-AI-by-design architectural
solutions to address the new architecture evolution and responsible AI
challenges. The patterns can be embedded as product features of foundation
model-based AI systems and can enable organisations to capitalise on the
potential of foundation models while minimising associated risks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11090v2' target="_blank">http://arxiv.org/pdf/2304.11090v2</a><br> <br> <br> <font size='5'> 537 </font> <div style="text-align: right"> 2023-04-12 20:20:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Detection of Fake Generated Scientific Abstracts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The widespread adoption of Large Language Models and publicly available
ChatGPT has marked a significant turning point in the integration of Artificial
Intelligence into people's everyday lives. The academic community has taken
notice of these technological advancements and has expressed concerns regarding
the difficulty of discriminating between what is real and what is artificially
generated. Thus, researchers have been working on developing effective systems
to identify machine-generated text. In this study, we utilize the GPT-3 model
to generate scientific paper abstracts through Artificial Intelligence and
explore various text representation methods when combined with Machine Learning
models with the aim of identifying machine-written text. We analyze the models'
performance and address several research questions that rise during the
analysis of the results. By conducting this research, we shed light on the
capabilities and limitations of Artificial Intelligence generated text. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06148v1' target="_blank">http://arxiv.org/pdf/2304.06148v1</a><br> <br> <br> <font size='5'> 538 </font> <div style="text-align: right"> 2023-04-12 17:33:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Large Language Models Transform Computational Social Science?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) like ChatGPT are capable of successfully
performing many language processing tasks zero-shot (without the need for
training data). If this capacity also applies to the coding of social phenomena
like persuasiveness and political ideology, then LLMs could effectively
transform Computational Social Science (CSS). This work provides a road map for
using LLMs as CSS tools. Towards this end, we contribute a set of prompting
best practices and an extensive evaluation pipeline to measure the zero-shot
performance of 13 language models on 24 representative CSS benchmarks. On
taxonomic labeling tasks (classification), LLMs fail to outperform the best
fine-tuned models but still achieve fair levels of agreement with humans. On
free-form coding tasks (generation), LLMs produce explanations that often
exceed the quality of crowdworkers' gold references. We conclude that today's
LLMs can radically augment the CSS research pipeline in two ways: (1) serving
as zero-shot data annotators on human annotation teams, and (2) bootstrapping
challenging creative generation tasks (e.g., explaining the hidden meaning
behind text). In summary, LLMs can significantly reduce costs and increase
efficiency of social science analysis in partnership with humans. </font><br> Link: <a href='http://arxiv.org/pdf/2305.03514v1' target="_blank">http://arxiv.org/pdf/2305.03514v1</a><br> <br> <br> <font size='5'> 539 </font> <div style="text-align: right"> 2023-04-12 17:24:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT Model for Vulnerability Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this technical report, we evaluated the performance of the ChatGPT and
GPT-3 models for the task of vulnerability detection in code. Our evaluation
was conducted on our real-world dataset, using binary and multi-label
classification tasks on CWE vulnerabilities. We decided to evaluate the model
because it has shown good performance on other code-based tasks, such as
solving programming challenges and understanding code at a high level. However,
we found that the ChatGPT model performed no better than a dummy classifier for
both binary and multi-label classification tasks for code vulnerability
detection. </font><br> Link: <a href='http://arxiv.org/pdf/2304.07232v1' target="_blank">http://arxiv.org/pdf/2304.07232v1</a><br> <br> <br> <font size='5'> 540 </font> <div style="text-align: right"> 2023-04-12 15:24:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How do physics students evaluate artificial intelligence responses on comprehension questions? A study on the perceived scientific accuracy and linguistic quality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aimed at evaluating how students perceive the linguistic quality
and scientific accuracy of ChatGPT responses to physics comprehension
questions. A total of 102 first- and second-year physics students were
confronted with three questions of progressing difficulty from introductory
mechanics (rolling motion, waves, and fluid dynamics). Each question was
presented with four different responses. All responses were attributed to
ChatGPT, but in reality one sample solution was created by the researchers. All
ChatGPT responses obtained in this study were wrong, imprecise, incomplete, or
misleading. We found little differences in the perceived linguistic quality
between ChatGPT responses and the sample solution. However, the students rated
the overall scientific accuracy of the responses significantly differently,
with the sample solution being rated best for the questions of low and medium
difficulty. The discrepancy between the sample solution and the ChatGPT
responses increased with the level of self-assessed knowledge of the question
content. For the question of highest difficulty (fluid dynamics) that was
unknown to most students, a ChatGPT response was rated just as good as the
sample solution. Thus, this study provides data on the students' perception of
ChatGPT responses and the factors influencing their perception. The results
highlight the need for careful evaluation of ChatGPT responses both by
instructors and students, particularly regarding scientific accuracy.
Therefore, future research could explore the potential of similar "spot the
bot"-activities in physics education to foster students' critical thinking
skills. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05906v2' target="_blank">http://arxiv.org/pdf/2304.05906v2</a><br> <br> <br> <font size='5'> 541 </font> <div style="text-align: right"> 2023-04-12 11:33:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There is a recent trend for using the novel Artificial Intelligence ChatGPT
chatbox, which provides detailed responses and articulate answers across many
domains of knowledge. However, in many cases it returns plausible-sounding but
incorrect or inaccurate responses, whereas it does not provide evidence.
Therefore, any user has to further search for checking the accuracy of the
answer or/and for finding more information about the entities of the response.
At the same time there is a high proliferation of RDF Knowledge Graphs (KGs)
over any real domain, that offer high quality structured data. For enabling the
combination of ChatGPT and RDF KGs, we present a research prototype, called
GPToLODS, which is able to enrich any ChatGPT response with more information
from hundreds of RDF KGs. In particular, it identifies and annotates each
entity of the response with statistics and hyperlinks to LODsyndesis KG (which
contains integrated data from 400 RDF KGs and over 412 million entities). In
this way, it is feasible to enrich the content of entities and to perform fact
checking and validation for the facts of the response at real time. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05774v1' target="_blank">http://arxiv.org/pdf/2304.05774v1</a><br> <br> <br> <font size='5'> 542 </font> <div style="text-align: right"> 2023-04-12 05:08:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05613v1' target="_blank">http://arxiv.org/pdf/2304.05613v1</a><br> <br> <br> <font size='5'> 543 </font> <div style="text-align: right"> 2023-04-11 23:50:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is all you need to decolonize sub-Saharan Vocational Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advances of Generative AI models with interactive capabilities over the
past few years offer unique opportunities for socioeconomic mobility. Their
potential for scalability, accessibility, affordability, personalizing and
convenience sets a first-class opportunity for poverty-stricken countries to
adapt and modernize their educational order. As a result, this position paper
makes the case for an educational policy framework that would succeed in this
transformation by prioritizing vocational and technical training over academic
education in sub-Saharan African countries. We highlight substantial
applications of Large Language Models, tailor-made to their respective cultural
background(s) and needs, that would reinforce their systemic decolonization.
Lastly, we provide specific historical examples of diverse states successfully
implementing such policies in the elementary steps of their socioeconomic
transformation, in order to corroborate our proposal to sub-Saharan African
countries to follow their lead. </font><br> Link: <a href='http://arxiv.org/pdf/2304.13728v1' target="_blank">http://arxiv.org/pdf/2304.13728v1</a><br> <br> <br> <font size='5'> 544 </font> <div style="text-align: right"> 2023-04-11 23:29:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the first half of 2023, text-generative artificial intelligence (AI),
including ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted
considerable attention worldwide. In this study, first, we compared Japanese
stylometric features of texts generated by GPT (-3.5 and -4) and those written
by humans. In this work, we performed multi-dimensional scaling (MDS) to
confirm the distributions of 216 texts of three classes (72 academic papers
written by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts
generated by GPT-4 on the basis of the titles of the aforementioned papers)
focusing on the following stylometric features: (1) bigrams of parts-of-speech,
(2) bigram of postpositional particle words, (3) positioning of commas, and (4)
rate of function words. MDS revealed distinct distributions at each stylometric
feature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than
GPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions
are likely to overlap. These results indicate that although the number of
parameters may increase in the future, GPT-generated texts may not be close to
that written by humans in terms of stylometric features. Second, we verified
the classification performance of random forest (RF) for two classes (GPT and
human) focusing on Japanese stylometric features. This study revealed the high
performance of RF in each stylometric feature: The RF classifier focusing on
the rate of function words achieved 98.1% accuracy. Furthermore the RF
classifier focusing on all stylometric features reached 100% in terms of all
performance indexes (accuracy, recall, precision, and F1 score). This study
concluded that at this stage we human discriminate ChatGPT from human limited
to Japanese language. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05534v3' target="_blank">http://arxiv.org/pdf/2304.05534v3</a><br> <br> <br> <font size='5'> 545 </font> <div style="text-align: right"> 2023-04-11 18:59:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-shot Temporal Relation Extraction with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05454v1' target="_blank">http://arxiv.org/pdf/2304.05454v1</a><br> <br> <br> <font size='5'> 546 </font> <div style="text-align: right"> 2023-04-11 18:07:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots and ChatGPT: A Bibliometric Analysis and Systematic Review of Publications in Web of Science and Scopus Databases</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a bibliometric analysis of the scientific literature
related to chatbots, focusing specifically on ChatGPT. Chatbots have gained
increasing attention recently, with an annual growth rate of 19.16% and 27.19%
on the Web of Sciences (WoS) and Scopus, respectively. In this study, we have
explored the structure, conceptual evolution, and trends in this field by
analyzing data from both Scopus and WoS databases. The research consists of two
study phases: (i) an analysis of chatbot literature and (ii) a comprehensive
review of scientific documents on ChatGPT. In the first phase, a bibliometric
analysis is conducted on all published literature, including articles, book
chapters, conference papers, and reviews on chatbots from both Scopus (5839)
and WoS (2531) databases covering the period from 1998 to 2023. An in-depth
analysis focusing on sources, countries, authors' impact, and keywords has
revealed that ChatGPT is the latest trend in the chatbot field. Consequently,
in the second phase, bibliometric analysis has been carried out on ChatGPT
publications, and 45 published studies have been analyzed thoroughly based on
their methods, novelty, and conclusions. The key areas of interest identified
from the study can be classified into three groups: artificial intelligence and
related technologies, design and evaluation of conversational agents, and
digital technologies and mental health. Overall, the study aims to provide
guidelines for researchers to conduct their research more effectively in the
field of chatbots and specifically highlight significant areas for future
investigation into ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05436v1' target="_blank">http://arxiv.org/pdf/2304.05436v1</a><br> <br> <br> <font size='5'> 547 </font> <div style="text-align: right"> 2023-04-11 16:53:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Toxicity in ChatGPT: Analyzing Persona-assigned Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have shown incredible capabilities and
transcended the natural language processing (NLP) community, with adoption
throughout many services like healthcare, therapy, education, and customer
service. Since users include people with critical information needs like
students or patients engaging with chatbots, the safety of these systems is of
prime importance. Therefore, a clear understanding of the capabilities and
limitations of LLMs is necessary. To this end, we systematically evaluate
toxicity in over half a million generations of ChatGPT, a popular
dialogue-based LLM. We find that setting the system parameter of ChatGPT by
assigning it a persona, say that of the boxer Muhammad Ali, significantly
increases the toxicity of generations. Depending on the persona assigned to
ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect
stereotypes, harmful dialogue, and hurtful opinions. This may be potentially
defamatory to the persona and harmful to an unsuspecting user. Furthermore, we
find concerning patterns where specific entities (e.g., certain races) are
targeted more than others (3x more) irrespective of the assigned persona, that
reflect inherent discriminatory biases in the model. We hope that our findings
inspire the broader AI community to rethink the efficacy of current safety
guardrails and develop better techniques that lead to robust, safe, and
trustworthy AI systems. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05335v1' target="_blank">http://arxiv.org/pdf/2304.05335v1</a><br> <br> <br> <font size='5'> 548 </font> <div style="text-align: right"> 2023-04-11 13:05:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-step Jailbreaking Privacy Attacks on ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the rapid progress of large language models (LLMs), many downstream NLP
tasks can be well solved given appropriate prompts. Though model developers and
researchers work hard on dialog safety to avoid generating harmful content from
LLMs, it is still challenging to steer AI-generated content (AIGC) for the
human good. As powerful LLMs are devouring existing text data from various
domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether
the private information is included in the training data and what privacy
threats can these LLMs and their downstream applications bring. In this paper,
we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by
ChatGPT and show that application-integrated LLMs may cause new privacy
threats. To this end, we conduct extensive experiments to support our claims
and discuss LLMs' privacy implications. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05197v2' target="_blank">http://arxiv.org/pdf/2304.05197v2</a><br> <br> <br> <font size='5'> 549 </font> <div style="text-align: right"> 2023-04-11 12:54:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating AIGC Detectors on Code Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial Intelligence Generated Content (AIGC) has garnered considerable
attention for its impressive performance, with ChatGPT emerging as a leading
AIGC model that produces high-quality responses across various applications,
including software development and maintenance. Despite its potential, the
misuse of ChatGPT poses significant concerns, especially in education and
safetycritical domains. Numerous AIGC detectors have been developed and
evaluated on natural language data. However, their performance on code-related
content generated by ChatGPT remains unexplored. To fill this gap, in this
paper, we present the first empirical study on evaluating existing AIGC
detectors in the software domain. We created a comprehensive dataset including
492.5K samples comprising code-related content produced by ChatGPT,
encompassing popular software activities like Q&A (115K), code summarization
(126K), and code generation (226.5K). We evaluated six AIGC detectors,
including three commercial and three open-source solutions, assessing their
performance on this dataset. Additionally, we conducted a human study to
understand human detection capabilities and compare them with the existing AIGC
detectors. Our results indicate that AIGC detectors demonstrate lower
performance on code-related data compared to natural language data. Fine-tuning
can enhance detector performance, especially for content within the same
domain; but generalization remains a challenge. The human evaluation reveals
that detection by humans is quite challenging. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05193v1' target="_blank">http://arxiv.org/pdf/2304.05193v1</a><br> <br> <br> <font size='5'> 550 </font> <div style="text-align: right"> 2023-04-11 11:43:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Boosting Cross-task Transferability of Adversarial Patches with Visual Relations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The transferability of adversarial examples is a crucial aspect of evaluating
the robustness of deep learning systems, particularly in black-box scenarios.
Although several methods have been proposed to enhance cross-model
transferability, little attention has been paid to the transferability of
adversarial examples across different tasks. This issue has become increasingly
relevant with the emergence of foundational multi-task AI systems such as
Visual ChatGPT, rendering the utility of adversarial samples generated by a
single task relatively limited. Furthermore, these systems often entail
inferential functions beyond mere recognition-like tasks. To address this gap,
we propose a novel Visual Relation-based cross-task Adversarial Patch
generation method called VRAP, which aims to evaluate the robustness of various
visual tasks, especially those involving visual reasoning, such as Visual
Question Answering and Image Captioning. VRAP employs scene graphs to combine
object recognition-based deception with predicate-based relations elimination,
thereby disrupting the visual reasoning information shared among inferential
tasks. Our extensive experiments demonstrate that VRAP significantly surpasses
previous methods in terms of black-box transferability across diverse visual
reasoning tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05402v1' target="_blank">http://arxiv.org/pdf/2304.05402v1</a><br> <br> <br> <font size='5'> 551 </font> <div style="text-align: right"> 2023-04-11 01:17:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we aimed to provide a review and tutorial for researchers in
the field of medical imaging using language models to improve their tasks at
hand. We began by providing an overview of the history and concepts of language
models, with a special focus on large language models. We then reviewed the
current literature on how language models are being used to improve medical
imaging, emphasizing different applications such as image captioning, report
generation, report classification, finding extraction, visual question
answering, interpretable diagnosis, and more for various modalities and organs.
The ChatGPT was specially highlighted for researchers to explore more potential
applications. We covered the potential benefits of accurate and efficient
language models for medical imaging analysis, including improving clinical
workflow efficiency, reducing diagnostic errors, and assisting healthcare
professionals in providing timely and accurate diagnoses. Overall, our goal was
to bridge the gap between language models and medical imaging and inspire new
ideas and innovations in this exciting area of research. We hope that this
review paper will serve as a useful resource for researchers in this field and
encourage further exploration of the possibilities of language models in
medical imaging. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04920v1' target="_blank">http://arxiv.org/pdf/2304.04920v1</a><br> <br> <br> <font size='5'> 552 </font> <div style="text-align: right"> 2023-04-10 15:51:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have demonstrated remarkable potential in
handling multilingual machine translation (MMT). In this paper, we
systematically investigate the advantages and challenges of LLMs for MMT by
answering two questions: 1) How well do LLMs perform in translating a massive
number of languages? 2) Which factors affect LLMs' performance in translation?
We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102
languages. Our empirical results show that even the best model ChatGPT still
lags behind the supervised baseline NLLB in 83.33% of translation directions.
Through further analysis, we discover that LLMs exhibit new working patterns
when used for MMT. First, prompt semantics can surprisingly be ignored when
given in-context exemplars, where LLMs still show strong performance even with
unreasonable prompts. Second, cross-lingual exemplars can provide better task
instruction for low-resource translation than exemplars in the same language
pairs. Third, we observe the overestimated performance of BLOOMZ on dataset
Flores-101, indicating the potential risk when using public datasets for
evaluation. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04675v2' target="_blank">http://arxiv.org/pdf/2304.04675v2</a><br> <br> <br> <font size='5'> 553 </font> <div style="text-align: right"> 2023-04-10 05:25:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this paper, we aim to develop a large language model (LLM) with the
reasoning ability on complex graph data. Currently, LLMs have achieved very
impressive performance on various natural language learning tasks, extensions
of which have also been applied to study the vision tasks with multi-modal
data. However, when it comes to the graph learning tasks, existing LLMs present
very serious flaws due to their several inherited weaknesses in performing
{multi-step logic reasoning}, {precise mathematical calculation} and
{perception about the spatial and temporal factors}.
  To address such challenges, in this paper, we will investigate the
principles, methodologies and algorithms to empower existing LLMs with graph
reasoning ability, which will have tremendous impacts on the current research
of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer
models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by ChatGPT to use
external graph reasoning API tools. Specifically, we will investigate to teach
Graph-ToolFormer to handle various graph data reasoning tasks in this paper,
including both (1) very basic graph data loading and graph property reasoning
tasks, ranging from simple graph order and size to the graph diameter and
periphery, and (2) more advanced reasoning tasks on real-world graph data, such
as bibliographic networks, protein molecules, sequential recommender systems,
social networks and knowledge graphs. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11116v3' target="_blank">http://arxiv.org/pdf/2304.11116v3</a><br> <br> <br> <font size='5'> 554 </font> <div style="text-align: right"> 2023-04-10 04:31:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, large language models (LLMs) like ChatGPT have demonstrated
remarkable performance across a variety of natural language processing tasks.
However, their effectiveness in the financial domain, specifically in
predicting stock market movements, remains to be explored. In this paper, we
conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal
stock movement prediction, on three tweets and historical stock price datasets.
Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited
success in predicting stock movements, as it underperforms not only
state-of-the-art methods but also traditional methods like linear regression
using price features. Despite the potential of Chain-of-Thought prompting
strategies and the inclusion of tweets, ChatGPT's performance remains subpar.
Furthermore, we observe limitations in its explainability and stability,
suggesting the need for more specialized training or fine-tuning. This research
provides insights into ChatGPT's capabilities and serves as a foundation for
future work aimed at improving financial market analysis and prediction by
leveraging social media sentiment and historical stock data. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05351v2' target="_blank">http://arxiv.org/pdf/2304.05351v2</a><br> <br> <br> <font size='5'> 555 </font> <div style="text-align: right"> 2023-04-10 00:55:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT has drawn great attention from both the research community
and the public. We are particularly curious about whether it can serve as a
universal sentiment analyzer. To this end, in this work, we provide a
preliminary evaluation of ChatGPT on the understanding of opinions, sentiments,
and emotions contained in the text. Specifically, we evaluate it in four
settings, including standard evaluation, polarity shift evaluation, open-domain
evaluation, and sentiment inference evaluation. The above evaluation involves
18 benchmark datasets and 5 representative sentiment analysis tasks, and we
compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA)
models on end-task. Moreover, we also conduct human evaluation and present some
qualitative case studies to gain a deep comprehension of its sentiment analysis
capabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04339v1' target="_blank">http://arxiv.org/pdf/2304.04339v1</a><br> <br> <br> <font size='5'> 556 </font> <div style="text-align: right"> 2023-04-09 15:28:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs). </font><br> Link: <a href='http://arxiv.org/pdf/2304.04256v1' target="_blank">http://arxiv.org/pdf/2304.04256v1</a><br> <br> <br> <font size='5'> 557 </font> <div style="text-align: right"> 2023-04-09 12:46:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Video captioning aims to convey dynamic scenes from videos using natural
language, facilitating the understanding of spatiotemporal information within
our environment. Although there have been recent advances, generating detailed
and enriched video descriptions continues to be a substantial challenge. In
this work, we introduce Video ChatCaptioner, an innovative approach for
creating more comprehensive spatiotemporal video descriptions. Our method
employs a ChatGPT model as a controller, specifically designed to select frames
for posing video content-driven questions. Subsequently, a robust algorithm is
utilized to answer these visual queries. This question-answer framework
effectively uncovers intricate video details and shows promise as a method for
enhancing video content. Following multiple conversational rounds, ChatGPT can
summarize enriched video content based on previous conversations. We
qualitatively demonstrate that our Video ChatCaptioner can generate captions
containing more visual details about the videos. The code is publicly available
at https://github.com/Vision-CAIR/ChatCaptioner </font><br> Link: <a href='http://arxiv.org/pdf/2304.04227v3' target="_blank">http://arxiv.org/pdf/2304.04227v3</a><br> <br> <br> <font size='5'> 558 </font> <div style="text-align: right"> 2023-04-09 08:26:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Extractive Summarization via ChatGPT for Faithful Summary Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Extractive summarization is a crucial task in natural language processing
that aims to condense long documents into shorter versions by directly
extracting sentences. The recent introduction of ChatGPT has attracted
significant interest in the NLP community due to its remarkable performance on
a wide range of downstream tasks. However, concerns regarding factuality and
faithfulness have hindered its practical applications for summarization
systems. This paper first presents a thorough evaluation of ChatGPT's
performance on extractive summarization and compares it with traditional
fine-tuning methods on various benchmark datasets. Our experimental analysis
reveals that ChatGPT's extractive summarization performance is still inferior
to existing supervised systems in terms of ROUGE scores. In addition, we
explore the effectiveness of in-context learning and chain-of-thought reasoning
for enhancing its performance. Furthermore, we find that applying an
extract-then-generate pipeline with ChatGPT yields significant performance
improvements over abstractive baselines in terms of summary faithfulness. These
observations highlight potential directions for enhancing ChatGPT's
capabilities for faithful text summarization tasks using two-stage approaches. </font><br> Link: <a href='http://arxiv.org/pdf/2304.04193v1' target="_blank">http://arxiv.org/pdf/2304.04193v1</a><br> <br> <br> <font size='5'> 559 </font> <div style="text-align: right"> 2023-04-09 04:53:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings. </font><br> Link: <a href='http://arxiv.org/pdf/2304.05372v1' target="_blank">http://arxiv.org/pdf/2304.05372v1</a><br> <br> <br> <font size='5'> 560 </font> <div style="text-align: right"> 2023-04-08 02:41:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper demonstrates how OpenAI's ChatGPT can be used in a few-shot
setting to convert natural language instructions into a sequence of executable
robot actions. The paper proposes easy-to-customize input prompts for ChatGPT
that meet common requirements in practical applications, such as easy
integration with robot execution systems and applicability to various
environments while minimizing the impact of ChatGPT's token limit. The prompts
encourage ChatGPT to output a sequence of predefined robot actions, represent
the operating environment in a formalized style, and infer the updated state of
the operating environment. Experiments confirmed that the proposed prompts
enable ChatGPT to act according to requirements in various environments, and
users can adjust ChatGPT's output with natural language feedback for safe and
robust operation. The proposed prompts and source code are open-source and
publicly available at
https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts </font><br> Link: <a href='http://arxiv.org/pdf/2304.03893v5' target="_blank">http://arxiv.org/pdf/2304.03893v5</a><br> <br> <br> <font size='5'> 561 </font> <div style="text-align: right"> 2023-04-08 02:19:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The two fields of urban planning and artificial intelligence (AI) arose and
developed separately. However, there is now cross-pollination and increasing
interest in both fields to benefit from the advances of the other. In the
present paper, we introduce the importance of urban planning from the
sustainability, living, economic, disaster, and environmental perspectives. We
review the fundamental concepts of urban planning and relate these concepts to
crucial open problems of machine learning, including adversarial learning,
generative neural networks, deep encoder-decoder networks, conversational AI,
and geospatial and temporal machine learning, thereby assaying how AI can
contribute to modern urban planning. Thus, a central problem is automated
land-use configuration, which is formulated as the generation of land uses and
building configuration for a target area from surrounding geospatial, human
mobility, social media, environment, and economic activities. Finally, we
delineate some implications of AI for urban planning and propose key research
areas at the intersection of both topics. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03892v1' target="_blank">http://arxiv.org/pdf/2304.03892v1</a><br> <br> <br> <font size='5'> 562 </font> <div style="text-align: right"> 2023-04-07 17:14:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As the capabilities of generative language models continue to advance, the
implications of biases ingrained within these models have garnered increasing
attention from researchers, practitioners, and the broader public. This article
investigates the challenges and risks associated with biases in large-scale
language models like ChatGPT. We discuss the origins of biases, stemming from,
among others, the nature of training data, model specifications, algorithmic
constraints, product design, and policy decisions. We explore the ethical
concerns arising from the unintended consequences of biased model outputs. We
further analyze the potential opportunities to mitigate biases, the
inevitability of some biases, and the implications of deploying these models in
various applications, such as virtual assistants, content generation, and
chatbots. Finally, we review the current approaches to identify, quantify, and
mitigate biases in language models, emphasizing the need for a
multi-disciplinary, collaborative effort to develop more equitable,
transparent, and responsible AI systems. This article aims to stimulate a
thoughtful dialogue within the artificial intelligence community, encouraging
researchers and developers to reflect on the role of biases in generative
language models and the ongoing pursuit of ethical AI. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03738v2' target="_blank">http://arxiv.org/pdf/2304.03738v2</a><br> <br> <br> <font size='5'> 563 </font> <div style="text-align: right"> 2023-04-07 16:38:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generating synthetic data through generative models is gaining interest in
the ML community and beyond. In the past, synthetic data was often regarded as
a means to private data release, but a surge of recent papers explore how its
potential reaches much further than this -- from creating more fair data to
data augmentation, and from simulation to text generated by ChatGPT. In this
perspective we explore whether, and how, synthetic data may become a dominant
force in the machine learning world, promising a future where datasets can be
tailored to individual needs. Just as importantly, we discuss which fundamental
challenges the community needs to overcome for wider relevance and application
of synthetic data -- the most important of which is quantifying how much we can
trust any finding or prediction drawn from synthetic data. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03722v1' target="_blank">http://arxiv.org/pdf/2304.03722v1</a><br> <br> <br> <font size='5'> 564 </font> <div style="text-align: right"> 2023-04-07 12:57:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI for learning: Investigating the potential of synthetic learning videos</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in generative artificial intelligence (AI) have captured
worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks
previously thought to be beyond the capabilities of AI may now augment the
productivity of creative media in various new ways, including through the
generation of synthetic video. This research paper explores the utility of
using AI-generated synthetic video to create viable educational content for
online educational settings. To date, there is limited research investigating
the real-world educational value of AI-generated synthetic media. To address
this gap, we examined the impact of using AI-generated synthetic video in an
online learning platform on both learners content acquisition and learning
experience. We took a mixed-method approach, randomly assigning adult learners
(n=83) into one of two micro-learning conditions, collecting pre- and
post-learning assessments, and surveying participants on their learning
experience. The control condition included a traditionally produced instructor
video, while the experimental condition included a synthetic video with a
realistic AI-generated character. The results show that learners in both
conditions demonstrated significant improvement from pre- to post-learning
(p<.001), with no significant differences in gains between the two conditions
(p=.80). In addition, no differences were observed in how learners perceived
the traditional and synthetic videos. These findings suggest that AI-generated
synthetic learning videos have the potential to be a viable substitute for
videos produced via traditional methods in online educational settings, making
high quality educational content more accessible across the globe. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03784v2' target="_blank">http://arxiv.org/pdf/2304.03784v2</a><br> <br> <br> <font size='5'> 565 </font> <div style="text-align: right"> 2023-04-07 12:20:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: There has been concern about ideological basis and possible discrimination in
text generated by Large Language Models (LLMs). We test possible value biases
in ChatGPT using a psychological value theory. We designed a simple experiment
in which we used a number of different probes derived from the Schwartz basic
value theory (items from the revised Portrait Value Questionnaire, the value
type definitions, value names). We prompted ChatGPT via the OpenAI API
repeatedly to generate text and then analyzed the generated corpus for value
content with a theory-driven value dictionary using a bag of words approach.
Overall, we found little evidence of explicit value bias. The results showed
sufficient construct and discriminant validity for the generated text in line
with the theoretical predictions of the psychological model, which suggests
that the value content was carried through into the outputs with high fidelity.
We saw some merging of socially oriented values, which may suggest that these
values are less clearly differentiated at a linguistic level or alternatively,
this mixing may reflect underlying universal human motivations. We outline some
possible applications of our findings for both applications of ChatGPT for
corporate usage and policy making as well as future research avenues. We also
highlight possible implications of this relatively high-fidelity replication of
motivational content using a linguistic model for the theorizing about human
values. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03612v1' target="_blank">http://arxiv.org/pdf/2304.03612v1</a><br> <br> <br> <font size='5'> 566 </font> <div style="text-align: right"> 2023-04-07 08:33:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Orchestrating a high-quality data preparation program is essential for
successful machine learning (ML), but it is known to be time and effort
consuming. Despite the impressive capabilities of large language models like
ChatGPT in generating programs by interacting with users through natural
language prompts, there are still limitations. Specifically, a user must
provide specific prompts to iteratively guide ChatGPT in improving data
preparation programs, which requires a certain level of expertise in
programming, the dataset used and the ML task. Moreover, once a program has
been generated, it is non-trivial to revisit a previous version or make changes
to the program without starting the process over again. In this paper, we
present ChatPipe, a novel system designed to facilitate seamless interaction
between users and ChatGPT. ChatPipe provides users with effective
recommendation on next data preparation operations, and guides ChatGPT to
generate program for the operations. Also, ChatPipe enables users to easily
roll back to previous versions of the program, which facilitates more efficient
experimentation and testing. We have developed a web application for ChatPipe
and prepared several real-world ML tasks from Kaggle. These tasks can showcase
the capabilities of ChatPipe and enable VLDB attendees to easily experiment
with our novel features to rapidly orchestrate a high-quality data preparation
program. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03540v1' target="_blank">http://arxiv.org/pdf/2304.03540v1</a><br> <br> <br> <font size='5'> 567 </font> <div style="text-align: right"> 2023-04-07 07:20:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative Recommendation: Towards Next-generation Recommender Paradigm</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recommender systems typically retrieve items from an item corpus for
personalized recommendations. However, such a retrieval-based recommender
paradigm faces two limitations: 1) the human-generated items in the corpus
might fail to satisfy the users' diverse information needs, and 2) users
usually adjust the recommendations via passive and inefficient feedback such as
clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success
across various domains, offering the potential to overcome these limitations:
1) generative AI can produce personalized items to meet users' specific
information needs, and 2) the newly emerged ChatGPT significantly facilitates
users to express information needs more precisely via natural language
instructions. In this light, the boom of AIGC points the way towards the
next-generation recommender paradigm with two new objectives: 1) generating
personalized content through generative AI, and 2) integrating user
instructions to guide content generation.
  To this end, we propose a novel Generative Recommender paradigm named
GeneRec, which adopts an AI generator to personalize content generation and
leverages user instructions to acquire users' information needs. Specifically,
we pre-process users' instructions and traditional feedback (e.g., clicks) via
an instructor to output the generation guidance. Given the guidance, we
instantiate the AI generator through an AI editor and an AI creator to
repurpose existing items and create new items, respectively. Eventually,
GeneRec can perform content retrieval, repurposing, and creation to meet users'
information needs. Besides, to ensure the trustworthiness of the generated
items, we emphasize various fidelity checks such as authenticity and legality
checks. Lastly, we study the feasibility of implementing the AI editor and AI
creator on micro-video generation, showing promising results. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03516v1' target="_blank">http://arxiv.org/pdf/2304.03516v1</a><br> <br> <br> <font size='5'> 568 </font> <div style="text-align: right"> 2023-04-07 01:37:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Harnessing logical reasoning ability is a comprehensive natural language
understanding endeavor. With the release of Generative Pretrained Transformer 4
(GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn
the GPT-4 performance on various logical reasoning tasks. This report analyses
multiple logical reasoning datasets, with popular benchmarks like LogiQA and
ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice
reading comprehension and natural language inference tasks with benchmarks
requiring logical reasoning. We further construct a logical reasoning
out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.
We also make a performance comparison between ChatGPT and GPT-4. Experiment
results show that ChatGPT performs significantly better than the RoBERTa
fine-tuning method on most logical reasoning benchmarks. With early access to
the GPT-4 API we are able to conduct intense experiments on the GPT-4 model.
The results show GPT-4 yields even higher performance on most logical reasoning
datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known
datasets like LogiQA and ReClor. However, the performance drops significantly
when handling newly released and out-of-distribution datasets. Logical
reasoning remains challenging for ChatGPT and GPT-4, especially on
out-of-distribution and natural language inference datasets. We release the
prompt-style logical reasoning datasets as a benchmark suite and name it
LogiEval. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03439v3' target="_blank">http://arxiv.org/pdf/2304.03439v3</a><br> <br> <br> <font size='5'> 569 </font> <div style="text-align: right"> 2023-04-07 01:25:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Opinion Mining from YouTube Captions Using ChatGPT: A Case Study of Street Interviews Polling the 2023 Turkish Elections</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Opinion mining plays a critical role in understanding public sentiment and
preferences, particularly in the context of political elections. Traditional
polling methods, while useful, can be expensive and less scalable. Social media
offers an alternative source of data for opinion mining but presents challenges
such as noise, biases, and platform limitations in data collection. In this
paper, we propose a novel approach for opinion mining, utilizing YouTube's
auto-generated captions from public interviews as a data source, specifically
focusing on the 2023 Turkish elections as a case study. We introduce an opinion
mining framework using ChatGPT to mass-annotate voting intentions and
motivations that represent the stance and frames prior to the election. We
report that ChatGPT can predict the preferred candidate with 97\% accuracy and
identify the correct voting motivation out of 13 possible choices with 71\%
accuracy based on the data collected from 325 interviews. We conclude by
discussing the robustness of our approach, accounting for factors such as
captions quality, interview length, and channels. This new method will offer a
less noisy and cost-effective alternative for opinion mining using social media
data. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03434v1' target="_blank">http://arxiv.org/pdf/2304.03434v1</a><br> <br> <br> <font size='5'> 570 </font> <div style="text-align: right"> 2023-04-06 19:53:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Interpretable Mental Health Analysis with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automated mental health analysis shows great potential for enhancing the
efficiency and accessibility of mental health care, with recent methods using
pre-trained language models (PLMs) and incorporated emotional information. The
latest large language models (LLMs), such as ChatGPT, exhibit dramatic
capabilities on diverse natural language processing tasks. However, existing
studies on ChatGPT for mental health analysis bear limitations in inadequate
evaluations, ignorance of emotional information, and lack of explainability. To
bridge these gaps, we comprehensively evaluate the mental health analysis and
emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, and
analyze the effects of various emotion-based prompting strategies. Based on
these prompts, we further explore LLMs for interpretable mental health analysis
by instructing them to also generate explanations for each of their decisions.
With an annotation protocol designed by domain experts, we convey human
evaluations to assess the quality of explanations generated by ChatGPT and
GPT-3. The annotated corpus will be released for future research. Experimental
results show that ChatGPT outperforms traditional neural network-based methods
but still has a significant gap with advanced task-specific methods. Prompt
engineering with emotional cues can be effective in improving performance on
mental health analysis but suffers from a lack of robustness and inaccurate
reasoning. In addition, ChatGPT significantly outperforms GPT-3 on all criteria
in human evaluations of the explanations and approaches to human performance,
showing its great potential in explainable mental health analysis. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03347v2' target="_blank">http://arxiv.org/pdf/2304.03347v2</a><br> <br> <br> <font size='5'> 571 </font> <div style="text-align: right"> 2023-04-06 18:42:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have gained considerable interest for their impressive
performance on various tasks. Among these models, ChatGPT developed by OpenAI
has become extremely popular among early adopters who even regard it as a
disruptive technology in many fields like customer service, education,
healthcare, and finance. It is essential to comprehend the opinions of these
initial users as it can provide valuable insights into the potential strengths,
weaknesses, and success or failure of the technology in different areas. This
research examines the responses generated by ChatGPT from different
Conversational QA corpora. The study employed BERT similarity scores to compare
these responses with correct answers and obtain Natural Language Inference(NLI)
labels. Evaluation scores were also computed and compared to determine the
overall performance of GPT-3 \& GPT-4. Additionally, the study identified
instances where ChatGPT provided incorrect answers to questions, providing
insights into areas where the model may be prone to error. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03325v1' target="_blank">http://arxiv.org/pdf/2304.03325v1</a><br> <br> <br> <font size='5'> 572 </font> <div style="text-align: right"> 2023-04-06 17:47:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: When do you need Chain-of-Thought Prompting for ChatGPT?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step
reasoning from Large Language Models~(LLMs). For example, by simply adding CoT
instruction ``Let's think step-by-step'' to each input query of MultiArith
dataset, GPT-3's accuracy can be improved from 17.7\% to 78.7\%. However, it is
not clear whether CoT is still effective on more recent instruction finetuned
(IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer
effective for certain tasks such as arithmetic reasoning while still keeping
effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT
usually achieves the best performance and can generate CoT even without being
instructed to do so. Hence, it is plausible that ChatGPT has already been
trained on these tasks with CoT and thus memorized the instruction so it
implicitly follows such an instruction when applied to the same queries, even
without CoT. Our analysis reflects a potential risk of overfitting/bias toward
instructions introduced in IFT, which becomes more common in training LLMs. In
addition, it indicates possible leakage of the pretraining recipe, e.g., one
can verify whether a dataset and instruction were used in training ChatGPT. Our
experiments report new baseline results of ChatGPT on a variety of reasoning
tasks and shed novel insights into LLM's profiling, instruction memorization,
and pretraining dataset leakage. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03262v2' target="_blank">http://arxiv.org/pdf/2304.03262v2</a><br> <br> <br> <font size='5'> 573 </font> <div style="text-align: right"> 2023-04-06 14:12:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03087v1' target="_blank">http://arxiv.org/pdf/2304.03087v1</a><br> <br> <br> <font size='5'> 574 </font> <div style="text-align: right"> 2023-04-06 07:40:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This article explores the ethical problems arising from the use of ChatGPT as
a kind of generative AI and suggests responses based on the Human-Centered
Artificial Intelligence (HCAI) framework. The HCAI framework is appropriate
because it understands technology above all as a tool to empower, augment, and
enhance human agency while referring to human wellbeing as a grand challenge,
thus perfectly aligning itself with ethics, the science of human flourishing.
Further, HCAI provides objectives, principles, procedures, and structures for
reliable, safe, and trustworthy AI which we apply to our ChatGPT assessments.
The main danger ChatGPT presents is the propensity to be used as a weapon of
mass deception (WMD) and an enabler of criminal activities involving deceit. We
review technical specifications to better comprehend its potentials and
limitations. We then suggest both technical (watermarking, styleme, detectors,
and fact-checkers) and non-technical measures (terms of use, transparency,
educator considerations, HITL) to mitigate ChatGPT misuse or abuse and
recommend best uses (creative writing, non-creative writing, teaching and
learning). We conclude with considerations regarding the role of humans in
ensuring the proper use of ChatGPT for individual and social wellbeing. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11215v1' target="_blank">http://arxiv.org/pdf/2304.11215v1</a><br> <br> <br> <font size='5'> 575 </font> <div style="text-align: right"> 2023-04-06 05:01:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02868v1' target="_blank">http://arxiv.org/pdf/2304.02868v1</a><br> <br> <br> <font size='5'> 576 </font> <div style="text-align: right"> 2023-04-06 01:51:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: GPT detectors are biased against non-native English writers</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02819v2' target="_blank">http://arxiv.org/pdf/2304.02819v2</a><br> <br> <br> <font size='5'> 577 </font> <div style="text-align: right"> 2023-04-06 00:14:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Opportunities and challenges of ChatGPT for design knowledge management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in Natural Language Processing have opened up new
possibilities for the development of large language models like ChatGPT, which
can facilitate knowledge management in the design process by providing
designers with access to a vast array of relevant information. However,
integrating ChatGPT into the design process also presents new challenges. In
this paper, we provide a concise review of the classification and
representation of design knowledge, and past efforts to support designers in
acquiring knowledge. We analyze the opportunities and challenges that ChatGPT
presents for knowledge management in design and propose promising future
research directions. A case study is conducted to validate the advantages and
drawbacks of ChatGPT, showing that designers can acquire targeted knowledge
from various domains, but the quality of the acquired knowledge is highly
dependent on the prompt. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02796v1' target="_blank">http://arxiv.org/pdf/2304.02796v1</a><br> <br> <br> <font size='5'> 578 </font> <div style="text-align: right"> 2023-04-05 18:45:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, single cell RNA sequencing has become a widely used
technique to study cellular diversity and function. However, accurately
annotating cell types from single cell data has been a challenging task, as it
requires extensive knowledge of cell biology and gene function. The emergence
of large language models such as ChatGPT and New Bing in 2023 has
revolutionized this process by integrating the scientific literature and
providing accurate annotations of cell types. This breakthrough enables
researchers to conduct literature reviews more efficiently and accurately, and
can potentially uncover new insights into cell type annotation. By using
ChatGPT to annotate single cell data, we can relate rare cell type to their
function and reveal specific differentiation trajectories of cell subtypes that
were previously overlooked. This can have important applications in
understanding cancer progression, mammalian development, and stem cell
differentiation, and can potentially lead to the discovery of key cells that
interrupt the differentiation pathway and solve key problems in the life
sciences. Overall, the future of cell type annotation in single cell data looks
promising and the Large Language model will be an important milestone in the
history of single cell analysis. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02697v1' target="_blank">http://arxiv.org/pdf/2304.02697v1</a><br> <br> <br> <font size='5'> 579 </font> <div style="text-align: right"> 2023-04-05 16:17:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Human-like Summarization Evaluation with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Evaluating text summarization is a challenging problem, and existing
evaluation metrics are far from satisfactory. In this study, we explored
ChatGPT's ability to perform human-like summarization evaluation using four
human evaluation methods on five datasets. We found that ChatGPT was able to
complete annotations relatively smoothly using Likert scale scoring, pairwise
comparison, Pyramid, and binary factuality evaluation. Additionally, it
outperformed commonly used automatic evaluation metrics on some datasets.
Furthermore, we discussed the impact of different prompts, compared its
performance with that of human evaluation, and analyzed the generated
explanations and invalid responses. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02554v1' target="_blank">http://arxiv.org/pdf/2304.02554v1</a><br> <br> <br> <font size='5'> 580 </font> <div style="text-align: right"> 2023-04-05 15:11:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02496v1' target="_blank">http://arxiv.org/pdf/2304.02496v1</a><br> <br> <br> <font size='5'> 581 </font> <div style="text-align: right"> 2023-04-05 13:12:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ParroT: Translating During Chat Using Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable
abilities on a wide range of natural language processing (NLP) tasks, including
various machine translation abilities accomplished during chat. However, these
models are only accessible through restricted APIs, which creates barriers to
new research and advancements in the field. Therefore, we propose the
$\mathbf{ParroT}$ framework to enhance and regulate the translation abilities
during chat based on open-sourced LLMs (i.e., LLaMA-7b, BLOOMZ-7b-mt) and human
written translation and evaluation data. Specifically, ParroT reformulates
translation data into the instruction-following style, and introduces a
"$\mathbf{Hint}$" field for incorporating extra requirements to regulate the
translation process. Accordingly, we propose three instruction types for
finetuning ParroT models, including translation instruction, contrastive
instruction, and error-guided instruction. We can finetune either the full
models or partial parameters via low rank adaptation (LoRA). Experiments on
Flores subsets and WMT22 test sets suggest that translation instruction
improves the translation performance of vanilla LLMs significantly while
error-guided instruction can lead to a further improvement, which demonstrates
the importance of learning from low-quality translations annotated by human.
Meanwhile, the ParroT models can also preserve the ability on general tasks
with the Alpaca multi-task dataset involved in finetuning. Please refer to our
Github project for more implementation details:
https://github.com/wxjiao/ParroT </font><br> Link: <a href='http://arxiv.org/pdf/2304.02426v4' target="_blank">http://arxiv.org/pdf/2304.02426v4</a><br> <br> <br> <font size='5'> 582 </font> <div style="text-align: right"> 2023-04-05 03:49:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Document-Level Machine Translation with Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive,
relevant, and fluent answers for various natural language processing (NLP)
tasks. Taking document-level machine translation (MT) as a testbed, this paper
provides an in-depth evaluation of LLMs' ability on discourse modeling. The
study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where
we investigate the impact of different prompts on document-level translation
quality and discourse phenomena; 2) Comparison of Translation Models, where we
compare the translation performance of Chat-GPT with commercial MT systems and
advanced document-level MT methods; 3) Analysis of Discourse Modelling
Abilities, where we further probe discourse knowledge encoded in LLMs and
examine the impact of training techniques on discourse modeling. By evaluating
a number of benchmarks, we surprisingly find that 1) leveraging their powerful
long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems in
terms of human evaluation. 2) GPT-4 demonstrates a strong ability to explain
discourse knowledge, even through it may select incorrect translation
candidates in contrastive testing. 3) ChatGPT and GPT-4 have demonstrated
superior performance and show potential to become a new and promising paradigm
for document-level translation. This work highlights the challenges and
opportunities of discourse modeling for LLMs, which we hope can inspire the
future design and evaluation of LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02210v1' target="_blank">http://arxiv.org/pdf/2304.02210v1</a><br> <br> <br> <font size='5'> 583 </font> <div style="text-align: right"> 2023-04-05 01:17:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How to Design Translation Prompts for ChatGPT: An Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recently released ChatGPT has demonstrated surprising abilities in
natural language understanding and natural language generation. Machine
translation relies heavily on the abilities of language understanding and
generation. Thus, in this paper, we explore how to assist machine translation
with ChatGPT. We adopt several translation prompts on a wide range of
translations. Our experimental results show that ChatGPT with designed
translation prompts can achieve comparable or better performance over
commercial translation systems for high-resource language translations. We
further evaluate the translation quality using multiple references, and ChatGPT
achieves superior performance compared to commercial systems. We also conduct
experiments on domain-specific translations, the final results show that
ChatGPT is able to comprehend the provided domain keyword and adjust
accordingly to output proper translations. At last, we perform few-shot prompts
that show consistent improvement across different base prompts. Our work
provides empirical evidence that ChatGPT still has great potential in
translations. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02182v2' target="_blank">http://arxiv.org/pdf/2304.02182v2</a><br> <br> <br> <font size='5'> 584 </font> <div style="text-align: right"> 2023-04-04 21:47:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The widespread adoption of large language models (LLMs), such as OpenAI's
ChatGPT, could revolutionize various industries, including geotechnical
engineering. However, GPT models can sometimes generate plausible-sounding but
false outputs, leading to hallucinations. In this article, we discuss the
importance of prompt engineering in mitigating these risks and harnessing the
full potential of GPT for geotechnical applications. We explore the challenges
and pitfalls associated with LLMs and highlight the role of context in ensuring
accurate and valuable responses. Furthermore, we examine the development of
context-specific search engines and the potential of LLMs to become a natural
interface for complex tasks, such as data analysis and design. We also develop
a unified interface using natural language to handle complex geotechnical
engineering tasks and data analysis. By integrating GPT into geotechnical
engineering workflows, professionals can streamline their work and develop
sustainable and resilient infrastructure systems for the future. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02138v3' target="_blank">http://arxiv.org/pdf/2304.02138v3</a><br> <br> <br> <font size='5'> 585 </font> <div style="text-align: right"> 2023-04-04 17:59:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deep Learning (DL) library bugs affect downstream DL applications,
emphasizing the need for reliable systems. Generating valid input programs for
fuzzing DL libraries is challenging due to the need for satisfying both
language syntax/semantics and constraints for constructing valid computational
graphs. Recently, the TitanFuzz work demonstrates that modern Large Language
Models (LLMs) can be directly leveraged to implicitly learn all the constraints
to generate valid DL programs for fuzzing. However, LLMs tend to generate
ordinary programs following similar patterns seen in their massive training
corpora, while fuzzing favors unusual inputs that cover edge cases or are
unlikely to be manually produced.
  To fill this gap, this paper proposes FuzzGPT, the first technique to prime
LLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on the
well-known hypothesis that historical bug-triggering programs may include
rare/valuable code ingredients important for bug finding. Traditional
techniques leveraging such historical information require intensive human
efforts to design dedicated generators and ensure the validity of generated
programs. FuzzGPT demonstrates that this process can be fully automated via the
intrinsic capabilities of LLMs (including fine-tuning and in-context learning),
while being generalizable and applicable to challenging domains. While FuzzGPT
can be applied with different LLMs, this paper focuses on the powerful
GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential
of directly leveraging the instruct-following capability of the recent ChatGPT
for effective fuzzing. Evaluation on two popular DL libraries (PyTorch and
TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz,
detecting 76 bugs, with 49 already confirmed as previously unknown bugs,
including 11 high-priority bugs or security vulnerabilities. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02014v1' target="_blank">http://arxiv.org/pdf/2304.02014v1</a><br> <br> <br> <font size='5'> 586 </font> <div style="text-align: right"> 2023-04-04 16:31:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The success of large language models (LLMs), like GPT-3 and ChatGPT, has led
to the development of numerous cost-effective and accessible alternatives that
are created by fine-tuning open-access LLMs with task-specific data (e.g.,
ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning
methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly
one of the most attractive topics, as it only requires fine-tuning a few
external parameters instead of the entire LLMs while achieving comparable or
even better performance. To enable further research on PEFT methods of LLMs,
this paper presents LLM-Adapters, an easy-to-use framework that integrates
various adapters into LLMs and can execute these adapter-based PEFT methods of
LLMs for different tasks. The framework includes state-of-the-art open-access
LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such
as Series adapter, Parallel adapter, and LoRA. The framework is designed to be
research-friendly, efficient, modular, and extendable, allowing the integration
of new adapters and the evaluation of them with new and larger-scale LLMs.
Furthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we
conduct experiments on six math reasoning datasets. The results demonstrate
that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra
trainable parameters yields comparable, and in some cases superior, performance
to that of powerful LLMs (175B) in zero-shot inference on simple math reasoning
datasets. Overall, we provide a promising framework for fine-tuning large LLMs
on downstream tasks. We believe the proposed LLMs-Adapters will advance
adapter-based PEFT research, facilitate the deployment of research pipelines,
and enable practical applications to real-world systems. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01933v2' target="_blank">http://arxiv.org/pdf/2304.01933v2</a><br> <br> <br> <font size='5'> 587 </font> <div style="text-align: right"> 2023-04-04 15:01:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a comprehensive survey of ChatGPT and GPT-4,
state-of-the-art large language models (LLM) from the GPT series, and their
prospective applications across diverse domains. Indeed, key innovations such
as large-scale pre-training that captures knowledge across the entire world
wide web, instruction fine-tuning and Reinforcement Learning from Human
Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability
and performance. We performed an in-depth analysis of 194 relevant papers on
arXiv, encompassing trend analysis, word cloud representation, and distribution
analysis across various application domains. The findings reveal a significant
and increasing interest in ChatGPT/GPT-4 research, predominantly centered on
direct natural language processing applications, while also demonstrating
considerable potential in areas ranging from education and history to
mathematics, medicine, and physics. This study endeavors to furnish insights
into ChatGPT's capabilities, potential implications, ethical concerns, and
offer direction for future advancements in this field. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01852v3' target="_blank">http://arxiv.org/pdf/2304.01852v3</a><br> <br> <br> <font size='5'> 588 </font> <div style="text-align: right"> 2023-04-04 13:01:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: For a natural language problem that requires some non-trivial reasoning to
solve, there are at least two ways to do it using a large language model (LLM).
One is to ask it to solve it directly. The other is to use it to extract the
facts from the problem text and then use a theorem prover to solve it. In this
note, we compare the two methods using ChatGPT and GPT4 on a series of logic
word puzzles, and conclude that the latter is the right approach. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01771v1' target="_blank">http://arxiv.org/pdf/2304.01771v1</a><br> <br> <br> <font size='5'> 589 </font> <div style="text-align: right"> 2023-04-04 12:33:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, a large-scale language model based on the advanced GPT-3.5
architecture, has shown remarkable potential in various Natural Language
Processing (NLP) tasks. However, there is currently a dearth of comprehensive
study exploring its potential in the area of Grammatical Error Correction
(GEC). To showcase its capabilities in GEC, we design zero-shot
chain-of-thought (CoT) and few-shot CoT settings using in-context learning for
ChatGPT. Our evaluation involves assessing ChatGPT's performance on five
official test sets in three different languages, along with three
document-level GEC test sets in English. Our experimental results and human
evaluations demonstrate that ChatGPT has excellent error detection capabilities
and can freely correct errors to make the corrected sentences very fluent,
possibly due to its over-correction tendencies and not adhering to the
principle of minimal edits. Additionally, its performance in non-English and
low-resource settings highlights its potential in multilingual GEC tasks.
However, further analysis of various types of errors at the document-level has
shown that ChatGPT cannot effectively correct agreement, coreference, tense
errors across sentences, and cross-sentence boundary errors. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01746v1' target="_blank">http://arxiv.org/pdf/2304.01746v1</a><br> <br> <br> <font size='5'> 590 </font> <div style="text-align: right"> 2023-04-04 06:22:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is
demonstrated to be one small step for generative AI (GAI), but one giant leap
for artificial general intelligence (AGI). Since its official release in
November 2022, ChatGPT has quickly attracted numerous users with extensive
media coverage. Such unprecedented attention has also motivated numerous
researchers to investigate ChatGPT from various aspects. According to Google
scholar, there are more than 500 articles with ChatGPT in their titles or
mentioning it in their abstracts. Considering this, a review is urgently
needed, and our work fills this gap. Overall, this work is the first to survey
ChatGPT with a comprehensive review of its underlying technology, applications,
and challenges. Moreover, we present an outlook on how ChatGPT might evolve to
realize general-purpose AIGC (a.k.a. AI-generated content), which will be a
significant milestone for the development of AGI. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06488v1' target="_blank">http://arxiv.org/pdf/2304.06488v1</a><br> <br> <br> <font size='5'> 591 </font> <div style="text-align: right"> 2023-04-04 03:04:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: To ChatGPT, or not to ChatGPT: That is the question!</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has become a global sensation. As ChatGPT and other Large Language
Models (LLMs) emerge, concerns of misusing them in various ways increase, such
as disseminating fake news, plagiarism, manipulating public opinion, cheating,
and fraud. Hence, distinguishing AI-generated from human-generated becomes
increasingly essential. Researchers have proposed various detection
methodologies, ranging from basic binary classifiers to more complex
deep-learning models. Some detection techniques rely on statistical
characteristics or syntactic patterns, while others incorporate semantic or
contextual information to improve accuracy. The primary objective of this study
is to provide a comprehensive and contemporary assessment of the most recent
techniques in ChatGPT detection. Additionally, we evaluated other AI-generated
text detection tools that do not specifically claim to detect ChatGPT-generated
content to assess their performance in detecting ChatGPT-generated content. For
our evaluation, we have curated a benchmark dataset consisting of prompts from
ChatGPT and humans, including diverse questions from medical, open Q&A, and
finance domains and user-generated responses from popular social networking
platforms. The dataset serves as a reference to assess the performance of
various techniques in detecting ChatGPT-generated content. Our evaluation
results demonstrate that none of the existing methods can effectively detect
ChatGPT-generated content. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01487v2' target="_blank">http://arxiv.org/pdf/2304.01487v2</a><br> <br> <br> <font size='5'> 592 </font> <div style="text-align: right"> 2023-04-04 02:55:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Blockwise Compression of Transformer-based Models without Retraining</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have
recently attracted increasing interest, research enthusiasm, and business
demand. However, their massive computation resources and huge memory footprint
are inevitable challenges. To tackle this issue, we propose BCT, a framework of
blockwise compression for transformers without retraining, to lower deployment
thresholds. BCT achieves more fine-grained compression of the whole
transformer, including embedding, matrix multiplication, GELU, Softmax, layer
normalization, and all the intermediate results. As a case, we compress an
efficient model with BCT and evaluate it on several General Language
Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve
a less than 0.90% accuracy drop in most tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01483v1' target="_blank">http://arxiv.org/pdf/2304.01483v1</a><br> <br> <br> <font size='5'> 593 </font> <div style="text-align: right"> 2023-04-04 01:43:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Integrating Commercial and Social Determinants of Health: A Unified Ontology for Non-Clinical Determinants of Health</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The objectives of this research are 1) to develop an ontology for CDoH by
utilizing PubMed articles and ChatGPT; 2) to foster ontology reuse by
integrating CDoH with an existing SDoH ontology into a unified structure; 3) to
devise an overarching conception for all non-clinical determinants of health
and to create an initial ontology, called N-CODH, for them; 4) and to validate
the degree of correspondence between concepts provided by ChatGPT with the
existing SDoH ontology </font><br> Link: <a href='http://arxiv.org/pdf/2304.01446v1' target="_blank">http://arxiv.org/pdf/2304.01446v1</a><br> <br> <br> <font size='5'> 594 </font> <div style="text-align: right"> 2023-04-03 19:53:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Classification of integers based on residue classes via modern deep learning algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Computing the residue when dividing a given integer by prime numbers like 2,
3, or others may appear trivial to human beings, but it can be less
straightforward for computers in the absence of pre-defined algorithms. In this
paper, we tested multiple deep learning architectures and feature engineering
approaches on classifying large finite integers (up to $2^{32}$) based on their
residues when divided by small prime numbers. It turns out that, regardless of
the network architectures (CNN, RNN, Transformer, etc.) or the complexity of
the networks, the ability of classification critically depends on the feature
space fed into the deep learning models. We also evaluated commercially
available Automated Machine Learning (AutoML) pipelines from Amazon, Google and
Microsoft, and found that they failed to address this issue unless
appropriately engineered features were provided. Furthermore, we introduced a
method that utilizes linear regression on Fourier series basis vectors, and
successfully demonstrated its effectiveness in the general case. Finally, we
evaluated prompt-based learning approaches using GPT-J, Falcon-40B, and LLaMA
and demonstrated its apparent failures. To conclude, feature engineering
remains an important task to improve the performance, increase the
interpretability, and reduce the complexity of models, even in the era of
AutoML and Large Language Models (LLMs). </font><br> Link: <a href='http://arxiv.org/pdf/2304.01333v2' target="_blank">http://arxiv.org/pdf/2304.01333v2</a><br> <br> <br> <font size='5'> 595 </font> <div style="text-align: right"> 2023-04-03 18:46:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Cross-lingual transfer of language models trained on high-resource languages
like English has been widely studied for many NLP tasks, but focus on
conversational tasks has been rather limited. This is partly due to the high
cost of obtaining non-English conversational data, which results in limited
coverage. In this work, we introduce XSGD, a parallel and large-scale
multilingual conversation dataset that we created by translating the
English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into
105 other languages. XSGD contains approximately 330k utterances per language.
To facilitate aligned cross-lingual representations, we develop an efficient
prompt-tuning-based method for learning alignment prompts. We also investigate
two different classifiers: NLI-based and vanilla classifiers, and test
cross-lingual capability enabled by the aligned prompts. We evaluate our
model's cross-lingual generalization capabilities on two conversation tasks:
slot-filling and intent classification. Our results demonstrate the strong and
efficient modeling ability of NLI-based classifiers and the large cross-lingual
transfer improvements achieved by our aligned prompts, particularly in few-shot
settings. In addition, we highlight the nice results of our approach compared
to LLMs such as text-davinci-003 and ChatGPT in both zero-shot and few-shot
settings. While LLMs exhibit impressive performance in English, their
cross-lingual capabilities in other languages, particularly low-resource
languages, are limited. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01295v2' target="_blank">http://arxiv.org/pdf/2304.01295v2</a><br> <br> <br> <font size='5'> 596 </font> <div style="text-align: right"> 2023-04-03 17:59:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chat models, such as ChatGPT, have shown impressive capabilities and have
been rapidly adopted across numerous domains. However, these models are only
accessible through a restricted API, creating barriers for new research and
progress in the field. We propose a pipeline that can automatically generate a
high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a
conversation with itself. Subsequently, we employ parameter-efficient tuning to
enhance LLaMA, an open-source large language model. The resulting model, named
Baize, demonstrates good performance in multi-turn dialogues with guardrails
that minimize potential risks. Furthermore, we propose a new technique called
Self-Distill with Feedback, to further improve the performance of the Baize
models with feedback from ChatGPT. The Baize models and data are released for
research purposes only at https://github.com/project-baize/baize-chatbot. An
online demo is also available at
https://huggingface.co/spaces/project-baize/chat-with-baize. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01196v3' target="_blank">http://arxiv.org/pdf/2304.01196v3</a><br> <br> <br> <font size='5'> 597 </font> <div style="text-align: right"> 2023-04-03 16:46:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI
heatwave due to its human-like conversations with detailed and articulate
answers across many domains of knowledge. While LLMs are being quickly applied
to many AI application domains, we are interested in the following question:
Can safety analysis for safety-critical systems make use of LLMs? To answer, we
conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic
Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent
techniques for hazard analysis, is known to have limitations such as high
complexity and subjectivity, which this paper aims to explore the use of
ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA
are investigated by considering its interaction with human experts: one-off
simplex interaction, recurring simplex interaction, and recurring duplex
interaction. Comparative results reveal that: (i) using ChatGPT without human
experts' intervention can be inadequate due to reliability and accuracy issues
of LLMs; (ii) more interactions between ChatGPT and human experts may yield
better results; and (iii) using ChatGPT in STPA with extra care can outperform
human safety experts alone, as demonstrated by reusing an existing comparison
method with baselines. In addition to making the first attempt to apply LLMs in
safety analysis, this paper also identifies key challenges (e.g.,
trustworthiness concern of LLMs, the need of standardisation) for future
research in this direction. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01246v1' target="_blank">http://arxiv.org/pdf/2304.01246v1</a><br> <br> <br> <font size='5'> 598 </font> <div style="text-align: right"> 2023-04-03 15:57:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent progress of large language models (LLMs), including ChatGPT and
GPT-4, in comprehending and responding to human instructions has been
remarkable. Nevertheless, these models typically perform better in English and
have not been explicitly trained for the medical domain, resulting in
suboptimal precision in diagnoses, drug recommendations, and other medical
advice. Additionally, training and deploying a dialogue model is still believed
to be impossible for hospitals, hindering the promotion of LLMs. To tackle
these challenges, we have collected databases of medical dialogues in Chinese
with ChatGPT's help and adopted several techniques to train an easy-deploy LLM.
Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13
hours, which means having a healthcare-purpose LLM can be very affordable.
DoctorGLM is currently an early-stage engineering attempt and contain various
mistakes. We are sharing it with the broader community to invite feedback and
suggestions to improve its healthcare-focused capabilities:
https://github.com/xionghonglin/DoctorGLM. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01097v2' target="_blank">http://arxiv.org/pdf/2304.01097v2</a><br> <br> <br> <font size='5'> 599 </font> <div style="text-align: right"> 2023-04-03 14:06:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding Individual and Team-based Human Factors in Detecting Deepfake Texts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In recent years, Natural Language Generation (NLG) techniques in AI (e.g.,
T5, GPT-3, ChatGPT) have shown a massive improvement and are now capable of
generating human-like long coherent texts at scale, yielding so-called deepfake
texts. This advancement, despite their benefits, can also cause security and
privacy issues (e.g., plagiarism, identity obfuscation, disinformation attack).
As such, it has become critically important to develop effective, practical,
and scalable solutions to differentiate deepfake texts from human-written
texts. Toward this challenge, in this work, we investigate how factors such as
skill levels and collaborations impact how humans identify deepfake texts,
studying three research questions: (1) do collaborative teams detect deepfake
texts better than individuals? (2) do expert humans detect deepfake texts
better than non-expert humans? (3) what are the factors that maximize the
detection performance of humans? We implement these questions on two platforms:
(1) non-expert humans or asynchronous teams on Amazon Mechanical Turk (AMT) and
(2) expert humans or synchronous teams on the Upwork. By analyzing the
detection performance and the factors that affected performance, some of our
key findings are: (1) expert humans detect deepfake texts significantly better
than non-expert humans, (2) synchronous teams on the Upwork detect deepfake
texts significantly better than individuals, while asynchronous teams on the
AMT detect deepfake texts weakly better than individuals, and (3) among various
error categories, examining coherence and consistency in texts is useful in
detecting deepfake texts. In conclusion, our work could inform the design of
future tools/framework to improve collaborative human detection of deepfake
texts. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01002v1' target="_blank">http://arxiv.org/pdf/2304.01002v1</a><br> <br> <br> <font size='5'> 600 </font> <div style="text-align: right"> 2023-04-03 05:29:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Evaluating the quality of generated text is a challenging task in natural
language processing. This difficulty arises from the inherent complexity and
diversity of text. Recently, OpenAI's ChatGPT, a powerful large language model
(LLM), has garnered significant attention due to its impressive performance in
various tasks. Therefore, we present this report to investigate the
effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their
use in assessing text quality. We compared three kinds of reference-free
evaluation methods based on ChatGPT or similar LLMs. The experimental results
prove that ChatGPT is capable to evaluate text quality effectively from various
perspectives without reference and demonstrates superior performance than most
existing automatic metrics. In particular, the Explicit Score, which utilizes
ChatGPT to generate a numeric score measuring text quality, is the most
effective and reliable method among the three exploited approaches. However,
directly comparing the quality of two texts using ChatGPT may lead to
suboptimal results. We hope this report will provide valuable insights into
selecting appropriate methods for evaluating text quality with LLMs such as
ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00723v2' target="_blank">http://arxiv.org/pdf/2304.00723v2</a><br> <br> <br> <font size='5'> 601 </font> <div style="text-align: right"> 2023-04-02 05:47:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have revolutionized natural language processing
and demonstrated impressive capabilities in various tasks. Unfortunately, they
are prone to hallucinations, where the model exposes incorrect or false
information in its responses, which renders diligent evaluation approaches
mandatory. While LLM performance in specific knowledge fields is often
evaluated based on question and answer (Q&A) datasets, such evaluations usually
report only a single accuracy number for the entire field, a procedure which is
problematic with respect to transparency and model improvement. A stratified
evaluation could instead reveal subfields, where hallucinations are more likely
to occur and thus help to better assess LLMs' risks and guide their further
development. To support such stratified evaluations, we propose LLMMaps as a
novel visualization technique that enables users to evaluate LLMs' performance
with respect to Q&A datasets. LLMMaps provide detailed insights into LLMs'
knowledge capabilities in different subfields, by transforming Q&A datasets as
well as LLM responses into our internal knowledge structure. An extension for
comparative visualization furthermore, allows for the detailed comparison of
multiple LLMs. To assess LLMMaps we use them to conduct a comparative analysis
of several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and
LLaMa-13B, as well as two qualitative user evaluations. All necessary source
code and data for generating LLMMaps to be used in scientific publications and
elsewhere will be available on GitHub. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00457v2' target="_blank">http://arxiv.org/pdf/2304.00457v2</a><br> <br> <br> <font size='5'> 602 </font> <div style="text-align: right"> 2023-04-01 20:57:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automated Program Repair (APR) aims to automatically generate patches for
buggy programs. Recent APR work has been focused on leveraging modern Large
Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR
tools work by first constructing an input prompt built using the original buggy
code and then queries the LLM to generate patches. While the LLM-based APR
tools are able to achieve state-of-the-art results, it still follows the
classic Generate and Validate repair paradigm of first generating lots of
patches and then validating each one afterwards. This not only leads to many
repeated patches that are incorrect but also miss the crucial information in
test failures as well as in plausible patches.
  To address these limitations, we propose ChatRepair, the first fully
automated conversation-driven APR approach that interleaves patch generation
with instant feedback to perform APR in a conversational style. ChatRepair
first feeds the LLM with relevant test failure information to start with, and
then learns from both failures and successes of earlier patching attempts of
the same bug for more powerful APR. For earlier patches that failed to pass all
tests, we combine the incorrect patches with their corresponding relevant test
failure information to construct a new prompt for the LLM to generate the next
patch. In this way, we can avoid making the same mistakes. For earlier patches
that passed all the tests, we further ask the LLM to generate alternative
variations of the original plausible patches. In this way, we can further build
on and learn from earlier successes to generate more plausible patches to
increase the chance of having correct patches. While our approach is general,
we implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT.
By calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs
for \$0.42 each! </font><br> Link: <a href='http://arxiv.org/pdf/2304.00385v1' target="_blank">http://arxiv.org/pdf/2304.00385v1</a><br> <br> <br> <font size='5'> 603 </font> <div style="text-align: right"> 2023-04-01 06:12:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Network Visualization of ChatGPT Research: a study based on term and keyword co-occurrence network analysis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The main objective of this paper is to identify the major research areas of
ChatGPT through term and keyword co-occurrence network mapping techniques. For
conducting the present study, total of 577 publications were retrieved from the
Lens database for the network visualization. The findings of the study showed
that chatgpt occurrence in maximum number of times followed by its related
terms such as artificial intelligence, large language model, gpt, study etc.
This study will be helpful to library and information science as well as
computer or information technology professionals. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01948v1' target="_blank">http://arxiv.org/pdf/2304.01948v1</a><br> <br> <br> <font size='5'> 604 </font> <div style="text-align: right"> 2023-04-01 06:04:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present the first study to investigate Large Language Models (LLMs) in
answering radiation oncology physics questions. Because popular exams like AP
Physics, LSAT, and GRE have large test-taker populations and ample test
preparation resources in circulation, they may not allow for accurately
assessing the true potential of LLMs. This paper proposes evaluating LLMs on a
highly-specialized topic, radiation oncology physics, which may be more
pertinent to scientific and medical communities in addition to being a valuable
benchmark of LLMs. We developed an exam consisting of 100 radiation oncology
physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT
(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against
medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs
as well as medical physicists, on average. The performance of ChatGPT (GPT-4)
was further improved when prompted to explain first, then answer. ChatGPT
(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices
across a number of trials, whether correct or incorrect, a characteristic that
was not observed in the human test groups. In evaluating ChatGPTs (GPT-4)
deductive reasoning ability using a novel approach (substituting the correct
answer with "None of the above choices is the correct answer."), ChatGPT
(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of
an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,
its intrinsic properties did not allow for further improvement when scoring
based on a majority vote across trials. In contrast, a team of medical
physicists were able to greatly outperform ChatGPT (GPT-4) using a majority
vote. This study suggests a great potential for LLMs to work alongside
radiation oncology experts as highly knowledgeable assistants. </font><br> Link: <a href='http://arxiv.org/pdf/2304.01938v1' target="_blank">http://arxiv.org/pdf/2304.01938v1</a><br> <br> <br> <font size='5'> 605 </font> <div style="text-align: right"> 2023-04-01 05:04:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large language models can rate news outlet credibility</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Although large language models (LLMs) have shown exceptional performance in
various natural language processing tasks, they are prone to hallucinations.
State-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue
by gathering information directly from the internet to ground their answers. In
this setting, the capacity to distinguish trustworthy sources is critical for
providing appropriate accuracy contexts to users. Here we assess whether
ChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With
appropriate instructions, ChatGPT can provide ratings for a diverse set of news
outlets, including those in non-English languages and satirical sources, along
with contextual explanations. Our results show that these ratings correlate
with those from human experts (Spearmam's $\rho=0.54, p<0.001$). These findings
suggest that LLMs could be an affordable reference for credibility ratings in
fact-checking applications. Future LLMs should enhance their alignment with
human expert judgments of source credibility to improve information accuracy. </font><br> Link: <a href='http://arxiv.org/pdf/2304.00228v1' target="_blank">http://arxiv.org/pdf/2304.00228v1</a><br> <br> <br> <font size='5'> 606 </font> <div style="text-align: right"> 2023-03-31 20:10:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "Genlangs" and Zipf's Law: Do languages generated by ChatGPT statistically look human?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: OpenAI's GPT-4 is a Large Language Model (LLM) that can generate coherent
constructed languages, or "conlangs," which we propose be called "genlangs"
when generated by Artificial Intelligence (AI). The genlangs created by ChatGPT
for this research (Voxphera, Vivenzia, and Lumivoxa) each have unique features,
appear facially coherent, and plausibly "translate" into English. This study
investigates whether genlangs created by ChatGPT follow Zipf's law. Zipf's law
approximately holds across all natural and artificially constructed human
languages. According to Zipf's law, the word frequencies in a text corpus are
inversely proportional to their rank in the frequency table. This means that
the most frequent word appears about twice as often as the second most frequent
word, three times as often as the third most frequent word, and so on. We
hypothesize that Zipf's law will hold for genlangs because (1) genlangs created
by ChatGPT fundamentally operate in the same way as human language with respect
to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained
on a corpora of text that includes many different languages, all of which
exhibit Zipf's law to varying degrees. Through statistical linguistics, we aim
to understand if LLM-based languages statistically look human. Our findings
indicate that genlangs adhere closely to Zipf's law, supporting the hypothesis
that genlangs created by ChatGPT exhibit similar statistical properties to
natural and artificial human languages. We also conclude that with human
assistance, AI is already capable of creating the world's first
fully-functional genlang, and we call for its development. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12191v1' target="_blank">http://arxiv.org/pdf/2304.12191v1</a><br> <br> <br> <font size='5'> 607 </font> <div style="text-align: right"> 2023-03-31 17:28:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Survey of Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions. </font><br> Link: <a href='http://arxiv.org/pdf/2303.18223v11' target="_blank">http://arxiv.org/pdf/2303.18223v11</a><br> <br> <br> <font size='5'> 608 </font> <div style="text-align: right"> 2023-03-31 15:39:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Debatepedia is a publicly available dataset consisting of arguments and
counter-arguments on controversial topics that has been widely used for the
single-document query-focused abstractive summarization task in recent years.
However, it has been recently found that this dataset is limited by noise and
even most queries in this dataset do not have any relevance to the respective
document. In this paper, we present a methodology for cleaning the Debatepedia
dataset by leveraging the generative power of large language models to make it
suitable for query-focused abstractive summarization. More specifically, we
harness the language generation capabilities of ChatGPT to regenerate its
queries. We evaluate the effectiveness of the proposed ChatGPT annotated
version of the Debatepedia dataset using several benchmark summarization models
and demonstrate that the newly annotated version of Debatepedia outperforms the
original dataset in terms of both query relevance as well as summary generation
quality. We will make this annotated and cleaned version of the dataset
publicly available. </font><br> Link: <a href='http://arxiv.org/pdf/2305.06147v1' target="_blank">http://arxiv.org/pdf/2305.06147v1</a><br> <br> <br> <font size='5'> 609 </font> <div style="text-align: right"> 2023-03-31 15:02:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Pair Programming with Large Language Models for Sampling and Estimation of Copulas</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Without writing a single line of code by a human, an example Monte Carlo
simulation based application for stochastic dependence modeling with copulas is
developed using a state-of-the-art large language model (LLM) fine-tuned for
conversations. This includes interaction with ChatGPT in natural language and
using mathematical formalism, which, under careful supervision by a
human-expert, led to producing a working code in MATLAB, Python and R for
sampling from a given copula model, evaluation of the model's density,
performing maximum likelihood estimation, optimizing the code for parallel
computing for CPUs as well as for GPUs, and visualization of the computed
results. In contrast to other emerging studies that assess the accuracy of LLMs
like ChatGPT on tasks from a selected area, this work rather investigates ways
how to achieve a successful solution of a standard statistical task in a
collaboration of a human-expert and artificial intelligence (AI). Particularly,
through careful prompt engineering, we separate successful solutions generated
by ChatGPT from unsuccessful ones, resulting in a comprehensive list of related
pros and cons. It is demonstrated that if the typical pitfalls are avoided, we
can substantially benefit from collaborating with an AI partner. For example,
we show that if ChatGPT is not able to provide a correct solution due to a lack
of or incorrect knowledge, the human-expert can feed it with the correct
knowledge, e.g., in the form of mathematical theorems and formulas, and make it
to apply the gained knowledge in order to provide a solution that is correct.
Such ability presents an attractive opportunity to achieve a programmed
solution even for users with rather limited knowledge of programming
techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2303.18116v1' target="_blank">http://arxiv.org/pdf/2303.18116v1</a><br> <br> <br> <font size='5'> 610 </font> <div style="text-align: right"> 2023-03-31 13:04:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As large language models (LLMs) gain popularity among speakers of diverse
languages, we believe that it is crucial to benchmark them to better understand
model behaviors, failures, and limitations in languages beyond English. In this
work, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national
medical licensing examinations from the past five years, including the current
year. Our team comprises native Japanese-speaking NLP researchers and a
practicing cardiologist based in Japan. Our experiments show that GPT-4
outperforms ChatGPT and GPT-3 and passes all six years of the exams,
highlighting LLMs' potential in a language that is typologically distant from
English. However, our evaluation also exposes critical limitations of the
current LLM APIs. First, LLMs sometimes select prohibited choices that should
be strictly avoided in medical practice in Japan, such as suggesting
euthanasia. Further, our analysis shows that the API costs are generally higher
and the maximum context size is smaller for Japanese because of the way
non-Latin scripts are currently tokenized in the pipeline. We release our
benchmark as Igaku QA as well as all model outputs and exam metadata. We hope
that our results and benchmark will spur progress on more diverse applications
of LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA. </font><br> Link: <a href='http://arxiv.org/pdf/2303.18027v2' target="_blank">http://arxiv.org/pdf/2303.18027v2</a><br> <br> <br> <font size='5'> 611 </font> <div style="text-align: right"> 2023-03-31 07:29:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can AI Put Gamma-Ray Astrophysicists Out of a Job?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In what will likely be a litany of generative-model-themed arXiv submissions
celebrating April the 1st, we evaluate the capacity of state-of-the-art
transformer models to create a paper detailing the detection of a Pulsar Wind
Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)
Array. We do this to evaluate the ability of such models to interpret
astronomical observations and sources based on language information alone, and
to assess potential means by which fraudulently generated scientific papers
could be identified during peer review (given that reliable generative model
watermarking has yet to be deployed for these tools). We conclude that our jobs
as astronomers are safe for the time being. From this point on, prompts given
to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT
is shown in black, whereas analysis by the (human) authors is in blue. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17853v2' target="_blank">http://arxiv.org/pdf/2303.17853v2</a><br> <br> <br> <font size='5'> 612 </font> <div style="text-align: right"> 2023-03-31 00:01:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Mitigating ChatGPT's Negative Impact on Education: Optimizing Question Design through Bloom's Taxonomy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The popularity of generative text AI tools in answering questions has led to
concerns regarding their potential negative impact on students' academic
performance and the challenges that educators face in evaluating student
learning. To address these concerns, this paper introduces an evolutionary
approach that aims to identify the best set of Bloom's taxonomy keywords to
generate questions that these tools have low confidence in answering. The
effectiveness of this approach is evaluated through a case study that uses
questions from a Data Structures and Representation course being taught at the
University of New South Wales in Canberra, Australia. The results demonstrate
that the optimization algorithm is able to find keywords from different
cognitive levels to create questions that ChatGPT has low confidence in
answering. This study is a step forward to offer valuable insights for
educators seeking to create more effective questions that promote critical
thinking among students. </font><br> Link: <a href='http://arxiv.org/pdf/2304.08176v1' target="_blank">http://arxiv.org/pdf/2304.08176v1</a><br> <br> <br> <font size='5'> 613 </font> <div style="text-align: right"> 2023-03-30 20:40:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT be used to generate scientific hypotheses?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate whether large language models can perform the creative
hypothesis generation that human researchers regularly do. While the error rate
is high, generative AI seems to be able to effectively structure vast amounts
of scientific knowledge and provide interesting and testable hypotheses. The
future scientific enterprise may include synergistic efforts with a swarm of
"hypothesis machines", challenged by automated experimentation and adversarial
peer reviews. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12208v1' target="_blank">http://arxiv.org/pdf/2304.12208v1</a><br> <br> <br> <font size='5'> 614 </font> <div style="text-align: right"> 2023-03-30 18:30:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Self-Refine: Iterative Refinement with Self-Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Like humans, large language models (LLMs) do not always generate the best
output on their first try. Motivated by how humans refine their written text,
we introduce Self-Refine, an approach for improving initial outputs from LLMs
through iterative feedback and refinement. The main idea is to generate an
initial output using an LLMs; then, the same LLMs provides feedback for its
output and uses it to refine itself, iteratively. Self-Refine does not require
any supervised training data, additional training, or reinforcement learning,
and instead uses a single LLM as the generator, refiner, and feedback provider.
We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response
generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,
and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine
are preferred by humans and automatic metrics over those generated with the
same LLM using conventional one-step generation, improving by ~20% absolute on
average in task performance. Our work demonstrates that even state-of-the-art
LLMs like GPT-4 can be further improved at test time using our simple,
standalone approach. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17651v2' target="_blank">http://arxiv.org/pdf/2303.17651v2</a><br> <br> <br> <font size='5'> 615 </font> <div style="text-align: right"> 2023-03-30 18:28:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluating and Detecting ChatGPT's Responses on Abstractive Summarization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) have gathered significant attention due to their
impressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is
a recent addition to the family of language models and is being called a
disruptive technology by a few, owing to its human-like text-generation
capabilities. Although, many anecdotal examples across the internet have
evaluated ChatGPT's strength and weakness, only a few systematic research
studies exist. To contribute to the body of literature of systematic research
on ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization
by the means of automated metrics and blinded human reviewers. We also build
automatic text classifiers to detect ChatGPT generated summaries. We found that
while text classification algorithms can distinguish between real and generated
summaries, humans are unable to distinguish between real summaries and those
produced by ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17650v2' target="_blank">http://arxiv.org/pdf/2303.17650v2</a><br> <br> <br> <font size='5'> 616 </font> <div style="text-align: right"> 2023-03-30 18:00:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT scores a bad birdie in counting gravitational-wave chirps</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: How many gravitational-wave observations from compact object mergers have we
seen to date? This seemingly simple question has a surprisingly complex answer
that even ChatGPT struggles to answer. To shed light on this, we present a
database with the literature's answers to this question. We find values
spanning 67-100 for the number of detections from double compact object mergers
to date, emphasizing that the exact number of detections is uncertain and
depends on the chosen data analysis pipeline and underlying assumptions. We
also review the number of gravitational-wave detections expected in the coming
decades with future observing runs, finding values up to millions of detections
per year in the era of Cosmic Explorer and Einstein Telescope. We present a
publicly available code to visualize the detection numbers, highlighting the
exponential growth in gravitational-wave observations in the coming decades and
the exciting prospects of gravitational-wave astrophysics. See
http://www.broekgaarden.nl/floor/wordpress/elementor-967/. We plan to keep this
database up-to-date and welcome comments and suggestions for additional
references. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17628v1' target="_blank">http://arxiv.org/pdf/2303.17628v1</a><br> <br> <br> <font size='5'> 617 </font> <div style="text-align: right"> 2023-03-30 17:48:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are abundant AI models
available for different domains and modalities, they cannot handle complicated
AI tasks. Considering large language models (LLMs) have exhibited exceptional
ability in language understanding, generation, interaction, and reasoning, we
advocate that LLMs could act as a controller to manage existing AI models to
solve complicated AI tasks and language could be a generic interface to empower
this. Based on this philosophy, we present HuggingGPT, a framework that
leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning
communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use
ChatGPT to conduct task planning when receiving a user request, select models
according to their function descriptions available in Hugging Face, execute
each subtask with the selected AI model, and summarize the response according
to the execution results. By leveraging the strong language capability of
ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover
numerous sophisticated AI tasks in different modalities and domains and achieve
impressive results in language, vision, speech, and other challenging tasks,
which paves a new way towards artificial general intelligence. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17580v3' target="_blank">http://arxiv.org/pdf/2303.17580v3</a><br> <br> <br> <font size='5'> 618 </font> <div style="text-align: right"> 2023-03-30 15:43:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent release of ChatGPT has garnered widespread recognition for its
exceptional ability to generate human-like responses in dialogue. Given its
usage by users from various nations and its training on a vast multilingual
corpus that incorporates diverse cultural and societal norms, it is crucial to
evaluate its effectiveness in cultural adaptation. In this paper, we
investigate the underlying cultural background of ChatGPT by analyzing its
responses to questions designed to quantify human cultural differences. Our
findings suggest that, when prompted with American context, ChatGPT exhibits a
strong alignment with American culture, but it adapts less effectively to other
cultural contexts. Furthermore, by using different prompts to probe the model,
we show that English prompts reduce the variance in model responses, flattening
out cultural differences and biasing them towards American culture. This study
provides valuable insights into the cultural implications of ChatGPT and
highlights the necessity of greater diversity and cultural awareness in
language technologies. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17466v2' target="_blank">http://arxiv.org/pdf/2303.17466v2</a><br> <br> <br> <font size='5'> 619 </font> <div style="text-align: right"> 2023-03-30 14:07:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The advancement of audio-language (AL) multimodal learning tasks has been
significant in recent years. However, researchers face challenges due to the
costly and time-consuming collection process of existing audio-language
datasets, which are limited in size. To address this data scarcity issue, we
introduce WavCaps, the first large-scale weakly-labelled audio captioning
dataset, comprising approximately 400k audio clips with paired captions. We
sourced audio clips and their raw descriptions from web sources and a sound
event detection dataset. However, the online-harvested raw descriptions are
highly noisy and unsuitable for direct use in tasks such as automated audio
captioning. To overcome this issue, we propose a three-stage processing
pipeline for filtering noisy data and generating high-quality captions, where
ChatGPT, a large language model, is leveraged to filter and transform raw
descriptions automatically. We conduct a comprehensive analysis of the
characteristics of WavCaps dataset and evaluate it on multiple downstream
audio-language multimodal learning tasks. The systems trained on WavCaps
outperform previous state-of-the-art (SOTA) models by a significant margin. Our
aspiration is for the WavCaps dataset we have proposed to facilitate research
in audio-language multimodal learning and demonstrate the potential of
utilizing ChatGPT to enhance academic research. Our dataset and codes are
available at https://github.com/XinhaoMei/WavCaps. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17395v1' target="_blank">http://arxiv.org/pdf/2303.17395v1</a><br> <br> <br> <font size='5'> 620 </font> <div style="text-align: right"> 2023-03-30 12:23:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Yes but.. Can ChatGPT Identify Entities in Historical Documents?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have been leveraged for several years now,
obtaining state-of-the-art performance in recognizing entities from modern
documents. For the last few months, the conversational agent ChatGPT has
"prompted" a lot of interest in the scientific community and public due to its
capacity of generating plausible-sounding answers. In this paper, we explore
this ability by probing it in the named entity recognition and classification
(NERC) task in primary sources (e.g., historical newspapers and classical
commentaries) in a zero-shot manner and by comparing it with state-of-the-art
LM-based systems. Our findings indicate several shortcomings in identifying
entities in historical text that range from the consistency of entity
annotation guidelines, entity complexity, and code-switching, to the
specificity of prompting. Moreover, as expected, the inaccessibility of
historical archives to the public (and thus on the Internet) also impacts its
performance. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17322v1' target="_blank">http://arxiv.org/pdf/2303.17322v1</a><br> <br> <br> <font size='5'> 621 </font> <div style="text-align: right"> 2023-03-30 06:10:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Synthesis of Mathematical programs from Natural Language Specifications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Several decision problems that are encountered in various business domains
can be modeled as mathematical programs, i.e. optimization problems. The
process of conducting such modeling often requires the involvement of experts
trained in operations research and advanced algorithms. Surprisingly, despite
the significant advances in the methods for program and code synthesis, AutoML,
learning to optimize etc., there has been little or no attention paid to
automating the task of synthesizing mathematical programs. We imagine a
scenario where the specifications for modeling, i.e. the objective and
constraints are expressed in an unstructured form in natural language (NL) and
the mathematical program has to be synthesized from such an NL specification.
In this work we evaluate the efficacy of employing CodeT5 with data
augmentation and post-processing of beams. We utilize GPT-3 with back
translation for generation of synthetic examples. Further we apply rules of
linear programming to score beams and correct beams based on common error
patterns. We observe that with these enhancements CodeT5 base gives an
execution accuracy of 0.73 which is significantly better than zero-shot
execution accuracy of 0.41 by ChatGPT and 0.36 by Codex. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03287v1' target="_blank">http://arxiv.org/pdf/2304.03287v1</a><br> <br> <br> <font size='5'> 622 </font> <div style="text-align: right"> 2023-03-30 05:51:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Matrix diagonalization and singular value decomposition: Static SageMath and dynamic ChatGPT juxtaposed</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigated some difficulties that students often face when studying
linear algebra at the undergraduate level, and identified some common mistakes
and difficulties they often encountered when dealing with topics that require
algorithmic thinking skills such as matrix factorization. In particular, we
focused on (orthogonal) diagonalization and singular value decomposition (SVD).
We also offered the possibility of exploring these topics using SageMath, a
Python-based free open software computer algebra system (CAS) that has been
identified to be useful for assisting many students in the computational
process even though its output is static by nature. We then explored dynamic
ChatGPT by inquiring the chatbot about the topic, either by asking to provide
an example or to solve a problem, that is by constructing an (orthogonal)
diagonalization or SVD from a particular matrix. By consolidating essential
concepts in linear algebra and improving computational skills through effective
practice, mastering these topics would become easier and mistakes could be
minimized. Static SageMath, in particular, is a great aid for calculation
confirmation and handling tedious computations. Although dynamic ChatGPT is
relatively unreliable for solving problems in linear algebra, the mistakes it
produces could become a valuable tool for improving critical thinking skills. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17163v1' target="_blank">http://arxiv.org/pdf/2303.17163v1</a><br> <br> <br> <font size='5'> 623 </font> <div style="text-align: right"> 2023-03-30 02:59:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the phenomenal success of diffusion models and ChatGPT, deep generation
models (DGMs) have been experiencing explosive growth from 2022. Not limited to
content generation, DGMs are also widely adopted in Internet of Things,
Metaverse, and digital twin, due to their outstanding ability to represent
complex patterns and generate plausible samples. In this article, we explore
the applications of DGMs in a crucial task, i.e., improving the efficiency of
wireless network management. Specifically, we firstly overview the generative
AI, as well as three representative DGMs. Then, a DGM-empowered framework for
wireless network management is proposed, in which we elaborate the issues of
the conventional network management approaches, why DGMs can address them
efficiently, and the step-by-step workflow for applying DGMs in managing
wireless networks. Moreover, we conduct a case study on network economics,
using the state-of-the-art DGM model, i.e., diffusion model, to generate
effective contracts for incentivizing the mobile AI-Generated Content (AIGC)
services. Last but not least, we discuss important open directions for the
further research. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17114v1' target="_blank">http://arxiv.org/pdf/2303.17114v1</a><br> <br> <br> <font size='5'> 624 </font> <div style="text-align: right"> 2023-03-29 20:32:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Advances in apparent conceptual physics reasoning in GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is built on a large language model trained on an enormous corpus of
human text to emulate human conversation. Despite lacking any explicit
programming regarding the laws of physics, recent work has demonstrated that
GPT-3.5 could pass an introductory physics course at some nominal level and
register something close to a minimal understanding of Newtonian Mechanics on
the Force Concept Inventory. This work replicates those results and also
demonstrates that the latest version, GPT-4, has reached a much higher mark in
the latter context. Indeed, its responses come quite close to perfectly
demonstrating expert-level competence, with a few very notable exceptions and
limitations. We briefly comment on the implications of this for the future of
physics education and pedagogy. </font><br> Link: <a href='http://arxiv.org/pdf/2303.17012v3' target="_blank">http://arxiv.org/pdf/2303.17012v3</a><br> <br> <br> <font size='5'> 625 </font> <div style="text-align: right"> 2023-03-29 17:27:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Questions of science: chatting with ChatGPT about complex systems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We present an overview of the complex systems field using ChatGPT as a
representation of the community's understanding. ChatGPT has learned language
patterns and styles from a large dataset of internet texts, allowing it to
provide answers that reflect common opinions, ideas, and language patterns
found in the community. Our exploration covers both teaching and learning, and
research topics. We recognize the value of ChatGPT as a source for the
community's ideas. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16870v1' target="_blank">http://arxiv.org/pdf/2303.16870v1</a><br> <br> <br> <font size='5'> 626 </font> <div style="text-align: right"> 2023-03-29 13:15:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT believes it is conscious</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The development of advanced generative chat models, such as ChatGPT, has
raised questions about the potential consciousness of these tools and the
extent of their general artificial intelligence. ChatGPT consistent avoidance
of passing the test is here overcome by asking ChatGPT to apply the Turing test
to itself. This explores the possibility of the model recognizing its own
sentience. In its own eyes, it passes this test. ChatGPT's self-assessment
makes serious implications about our understanding of the Turing test and the
nature of consciousness. This investigation concludes by considering the
existence of distinct types of consciousness and the possibility that the
Turing test is only effective when applied between consciousnesses of the same
kind. This study also raises intriguing questions about the nature of AI
consciousness and the validity of the Turing test as a means of verifying such
consciousness. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12898v1' target="_blank">http://arxiv.org/pdf/2304.12898v1</a><br> <br> <br> <font size='5'> 627 </font> <div style="text-align: right"> 2023-03-29 08:06:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Can foundation models (such as ChatGPT) clean your data? In this proposal, we
demonstrate that indeed ChatGPT can assist in data cleaning by suggesting
corrections for specific cells in a data table (scenario 1). However, ChatGPT
may struggle with datasets it has never encountered before (e.g., local
enterprise data) or when the user requires an explanation of the source of the
suggested clean values. To address these issues, we developed a retrieval-based
method that complements ChatGPT's power with a user-provided data lake. The
data lake is first indexed, we then retrieve the top-k relevant tuples to the
user's query tuple and finally leverage ChatGPT to infer the correct value
(scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally
hosted model, might not be feasible for privacy reasons. To assist with this
scenario, we developed a custom RoBERTa-based foundation model that can be
locally deployed. By fine-tuning it on a small number of examples, it can
effectively make value inferences based on the retrieved tuples (scenario 3).
Our proposed system, RetClean, seamlessly supports all three scenarios and
provides a user-friendly GUI that enables the VLDB audience to explore and
experiment with the system. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16909v1' target="_blank">http://arxiv.org/pdf/2303.16909v1</a><br> <br> <br> <font size='5'> 628 </font> <div style="text-align: right"> 2023-03-29 03:30:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial Intelligence (AI) has made incredible progress recently. On the
one hand, advanced foundation models like ChatGPT can offer powerful
conversation, in-context learning and code generation abilities on a broad
range of open-domain tasks. They can also generate high-level solution outlines
for domain-specific tasks based on the common sense knowledge they have
acquired. However, they still face difficulties with some specialized tasks
because they lack enough domain-specific data during pre-training or they often
have errors in their neural network computations on those tasks that need
accurate executions. On the other hand, there are also many existing models and
systems (symbolic-based or neural-based) that can do some domain-specific tasks
very well. However, due to the different implementation or working mechanisms,
they are not easily accessible or compatible with foundation models. Therefore,
there is a clear and pressing need for a mechanism that can leverage foundation
models to propose task solution outlines and then automatically match some of
the sub-tasks in the outlines to the off-the-shelf models and systems with
special functionalities to complete them. Inspired by this, we introduce
TaskMatrix.AI as a new AI ecosystem that connects foundation models with
millions of APIs for task completion. Unlike most previous work that aimed to
improve a single AI model, TaskMatrix.AI focuses more on using existing
foundation models (as a brain-like central system) and APIs of other AI models
and systems (as sub-task solvers) to achieve diversified tasks in both digital
and physical domains. As a position paper, we will present our vision of how to
build such an ecosystem, explain each key component, and use study cases to
illustrate both the feasibility of this vision and the main challenges we need
to address next. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16434v1' target="_blank">http://arxiv.org/pdf/2303.16434v1</a><br> <br> <br> <font size='5'> 629 </font> <div style="text-align: right"> 2023-03-29 03:24:42+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ten Quick Tips for Harnessing the Power of ChatGPT/GPT-4 in Computational Biology</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of advanced chatbots, such as ChatGPT, has sparked curiosity in the
scientific community. ChatGPT is a general-purpose chatbot powered by large
language models (LLMs) GPT-3.5 and GPT-4, with the potential to impact numerous
fields, including computational biology. In this article, we offer ten tips
based on our experience with ChatGPT to assist computational biologists in
optimizing their workflows. We have collected relevant prompts and reviewed the
nascent literature in the field, compiling tips we project to remain pertinent
for future ChatGPT and LLM iterations, ranging from code refactoring to
scientific writing to prompt engineering. We hope our work will help
bioinformaticians to complement their workflows while staying aware of the
various implications of using this technology. Additionally, to track new and
creative applications for bioinformatics tools such as ChatGPT, we have
established a GitHub repository at
https://github.com/csbl-br/awesome-compbio-chatgpt. Our belief is that ethical
adherence to ChatGPT and other LLMs will increase the efficiency of
computational biologists, ultimately advancing the pace of scientific discovery
in the life sciences. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16429v1' target="_blank">http://arxiv.org/pdf/2303.16429v1</a><br> <br> <br> <font size='5'> 630 </font> <div style="text-align: right"> 2023-03-29 03:05:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as ChatGPT and GPT-4 have made significant
progress in NLP. However, their ability to memorize, represent, and leverage
commonsense knowledge has been a well-known pain point for LLMs. It remains
unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are
GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying
commonsense knowledge for answering a specific question? (4) Can GPTs
effectively leverage commonsense for answering questions? To evaluate the above
commonsense problems, we conduct a series of experiments to evaluate ChatGPT's
commonsense abilities, and the experimental results show that: (1) GPTs can
achieve good QA accuracy in commonsense tasks, while they still struggle with
certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately
generate most of the commonsense knowledge using knowledge prompts. (3) Despite
its knowledge, ChatGPT is an inexperienced commonsense problem solver, which
cannot precisely identify the needed commonsense knowledge for answering a
specific question, i.e., ChatGPT does not precisely know what commonsense
knowledge is required to answer a question. The above findings raise the need
to investigate better mechanisms for utilizing commonsense knowledge in LLMs,
such as instruction following, better commonsense guidance, etc. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16421v1' target="_blank">http://arxiv.org/pdf/2303.16421v1</a><br> <br> <br> <font size='5'> 631 </font> <div style="text-align: right"> 2023-03-29 02:46:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-shot Clinical Entity Recognition using ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this study, we investigated the potential of ChatGPT, a large language
model developed by OpenAI, for the clinical named entity recognition task
defined in the 2010 i2b2 challenge, in a zero-shot setting with two different
prompt strategies. We compared its performance with GPT-3 in a similar
zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of
synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT
outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250)
and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover,
prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores
of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's
performance was still lower than that of the supervised BioClinicalBERT model
(i.e., relaxed-matching F1 scores of 0.620 vs. 0.888), our study demonstrates
the great potential of ChatGPT for clinical NER tasks in a zero-shot setting,
which is much more appealing as it does not require any annotation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16416v2' target="_blank">http://arxiv.org/pdf/2303.16416v2</a><br> <br> <br> <font size='5'> 632 </font> <div style="text-align: right"> 2023-03-28 23:16:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has enabled access to AI-generated writing for the masses, and within
just a few months, this product has disrupted the knowledge economy, initiating
a culture shift in the way people work, learn, and write. The need to
discriminate human writing from AI is now both critical and urgent,
particularly in domains like higher education and academic writing, where AI
had not been a significant threat or contributor to authorship. Addressing this
need, we developed a method for discriminating text generated by ChatGPT from
(human) academic scientists, relying on prevalent and accessible supervised
classification methods. We focused on how a particular group of humans,
academic scientists, write differently than ChatGPT, and this targeted approach
led to the discovery of new features for discriminating (these) humans from AI;
as examples, scientists write long paragraphs and have a penchant for equivocal
language, frequently using words like but, however, and although. With a set of
20 features, including the aforementioned ones and others, we built a model
that assigned the author, as human or AI, at well over 99% accuracy, resulting
in 20 times fewer misclassified documents compared to the field-leading
approach. This strategy for discriminating a particular set of humans writing
from AI could be further adapted and developed by others with basic skills in
supervised classification, enabling access to many highly accurate and targeted
models for detecting AI usage in academic writing and beyond. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16352v1' target="_blank">http://arxiv.org/pdf/2303.16352v1</a><br> <br> <br> <font size='5'> 633 </font> <div style="text-align: right"> 2023-03-28 19:49:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Contrary to Google Search's mission of delivering information from "many
angles so you can form your own understanding of the world," we find that
Google and its most prominent returned results - Wikipedia and YouTube - simply
reflect a narrow set of cultural stereotypes tied to the search language for
complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and
"America." Simply stated, they present, to varying degrees, distinct
information across the same search in different languages, a phenomenon we call
'language bias.' This paper presents evidence and analysis of language bias and
discusses its larger social implications. Instead of presenting a global
picture of a complex topic, our online searches and emerging tools like ChatGPT
turn us into the proverbial blind person touching a small portion of an
elephant, ignorant of the existence of other cultural perspectives. Piecing
together a genuine depiction of the elephant is a challenging and important
endeavor that will require collaborative efforts from scholars in both the
humanities and technology. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16281v2' target="_blank">http://arxiv.org/pdf/2303.16281v2</a><br> <br> <br> <font size='5'> 634 </font> <div style="text-align: right"> 2023-03-28 16:52:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial Intelligence-Generated Content (AIGC) is an automated method for
generating, manipulating, and modifying valuable and diverse data using AI
algorithms creatively. This survey paper focuses on the deployment of AIGC
applications, e.g., ChatGPT and Dall-E, at mobile edge networks, namely mobile
AIGC networks, that provide personalized and customized AIGC services in real
time while maintaining user privacy. We begin by introducing the background and
fundamentals of generative models and the lifecycle of AIGC services at mobile
AIGC networks, which includes data collection, training, finetuning, inference,
and product management. We then discuss the collaborative cloud-edge-mobile
infrastructure and technologies required to support AIGC services and enable
users to access AIGC at mobile edge networks. Furthermore, we explore
AIGCdriven creative applications and use cases for mobile AIGC networks.
Additionally, we discuss the implementation, security, and privacy challenges
of deploying mobile AIGC networks. Finally, we highlight some future research
directions and open issues for the full realization of mobile AIGC networks. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16129v2' target="_blank">http://arxiv.org/pdf/2303.16129v2</a><br> <br> <br> <font size='5'> 635 </font> <div style="text-align: right"> 2023-03-28 16:17:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Hallucinations in Large Multilingual Translation Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale multilingual machine translation systems have demonstrated
remarkable ability to translate directly between numerous languages, making
them increasingly appealing for real-world applications. However, when deployed
in the wild, these models may generate hallucinated translations which have the
potential to severely undermine user trust and raise safety concerns. Existing
research on hallucinations has primarily focused on small bilingual models
trained on high-resource languages, leaving a gap in our understanding of
hallucinations in massively multilingual models across diverse translation
scenarios. In this work, we fill this gap by conducting a comprehensive
analysis on both the M2M family of conventional neural machine translation
models and ChatGPT, a general-purpose large language model~(LLM) that can be
prompted for translation. Our investigation covers a broad spectrum of
conditions, spanning over 100 translation directions across various resource
levels and going beyond English-centric language pairs. We provide key insights
regarding the prevalence, properties, and mitigation of hallucinations, paving
the way towards more responsible and reliable machine translation systems. </font><br> Link: <a href='http://arxiv.org/pdf/2303.16104v1' target="_blank">http://arxiv.org/pdf/2303.16104v1</a><br> <br> <br> <font size='5'> 636 </font> <div style="text-align: right"> 2023-03-28 07:18:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ecosystem Graphs: The Social Footprint of Foundation Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence
society, warranting immediate social attention. While the models themselves
garner much attention, to accurately characterize their impact, we must
consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a
documentation framework to transparently centralize knowledge of this
ecosystem. Ecosystem Graphs is composed of assets (datasets, models,
applications) linked together by dependencies that indicate technical (e.g. how
Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)
relationships. To supplement the graph structure, each asset is further
enriched with fine-grained metadata (e.g. the license or training emissions).
We document the ecosystem extensively at
https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate
262 assets (64 datasets, 128 models, 70 applications) from 63 organizations
linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful
abstraction and interface for achieving the minimum transparency required to
address myriad use cases. Therefore, we envision Ecosystem Graphs will be a
community-maintained resource that provides value to stakeholders spanning AI
researchers, industry professionals, social scientists, auditors and
policymakers. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15772v1' target="_blank">http://arxiv.org/pdf/2303.15772v1</a><br> <br> <br> <font size='5'> 637 </font> <div style="text-align: right"> 2023-03-28 04:47:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT for NLP-based Mental Health Applications</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLM) have been successful in several natural language
understanding tasks and could be relevant for natural language processing
(NLP)-based mental health application research. In this work, we report the
performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three
text-based mental health classification tasks: stress detection (2-class
classification), depression detection (2-class classification), and suicidality
detection (5-class classification). We obtained annotated social media posts
for the three classification tasks from public datasets. Then ChatGPT API
classified the social media posts with an input prompt for classification. We
obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression
detection, and suicidality detection, respectively. A baseline model that
always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and
0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a
potential use of language models for mental health classification tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15727v1' target="_blank">http://arxiv.org/pdf/2303.15727v1</a><br> <br> <br> <font size='5'> 638 </font> <div style="text-align: right"> 2023-03-28 04:26:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Solving Regularized Exp, Cosh and Sinh Regression Problems</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In modern machine learning, attention computation is a fundamental task for
training large language models such as Transformer, GPT-4 and ChatGPT. In this
work, we study exponential regression problem which is inspired by the
softmax/exp unit in the attention mechanism in large language models. The
standard exponential regression is non-convex. We study the regularization
version of exponential regression problem which is a convex problem. We use
approximate newton method to solve in input sparsity time.
  Formally, in this problem, one is given matrix $A \in \mathbb{R}^{n \times
d}$, $b \in \mathbb{R}^n$, $w \in \mathbb{R}^n$ and any of functions $\exp,
\cosh$ and $\sinh$ denoted as $f$. The goal is to find the optimal $x$ that
minimize $ 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$. The
straightforward method is to use the naive Newton's method. Let
$\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let
$\omega$ denote the exponent of matrix multiplication. Currently, $\omega
\approx 2.373$. Let $\epsilon$ denote the accuracy error. In this paper, we
make use of the input sparsity and purpose an algorithm that use $\log ( \|x_0
- x^*\|_2 / \epsilon)$ iterations and $\widetilde{O}(\mathrm{nnz}(A) +
d^{\omega} )$ per iteration time to solve the problem. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15725v2' target="_blank">http://arxiv.org/pdf/2303.15725v2</a><br> <br> <br> <font size='5'> 639 </font> <div style="text-align: right"> 2023-03-28 03:11:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Comparative Analysis of CHATGPT and the evolution of language models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Interest in Large Language Models (LLMs) has increased drastically since the
emergence of ChatGPT and the outstanding positive societal response to the ease
with which it performs tasks in Natural Language Processing (NLP). The triumph
of ChatGPT, however, is how it seamlessly bridges the divide between language
generation and knowledge models. In some cases, it provides anecdotal evidence
of a framework for replicating human intuition over a knowledge domain. This
paper highlights the prevailing ideas in NLP, including machine translation,
machine summarization, question-answering, and language generation, and
compares the performance of ChatGPT with the major algorithms in each of these
categories using the Spontaneous Quality (SQ) score. A strategy for validating
the arguments and results of ChatGPT is presented summarily as an example of
safe, large-scale adoption of LLMs. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02468v1' target="_blank">http://arxiv.org/pdf/2304.02468v1</a><br> <br> <br> <font size='5'> 640 </font> <div style="text-align: right"> 2023-03-28 01:07:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT4PCG Competition: Character-like Level Generation for Science Birds</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE
Conference on Games. The objective of this competition is for participants to
create effective prompts for ChatGPT--enabling it to generate Science Birds
levels with high stability and character-like qualities--fully using their
creativity as well as prompt engineering skills. ChatGPT is a conversational
agent developed by OpenAI. Science Birds is selected as the competition
platform because designing an Angry Birds-like level is not a trivial task due
to the in-game gravity; the quality of the levels is determined by their
stability. To lower the entry barrier to the competition, we limit the task to
the generation of capitalized English alphabetical characters. We also allow
only a single prompt to be used for generating all the characters. Here, the
quality of the generated levels is determined by their stability and similarity
to the given characters. A sample prompt is provided to participants for their
reference. An experiment is conducted to determine the effectiveness of several
modified versions of this sample prompt on level stability and similarity by
testing them on several characters. To the best of our knowledge, we believe
that ChatGPT4PCG is the first competition of its kind and hope to inspire
enthusiasm for prompt engineering in procedural content generation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15662v2' target="_blank">http://arxiv.org/pdf/2303.15662v2</a><br> <br> <br> <font size='5'> 641 </font> <div style="text-align: right"> 2023-03-27 22:30:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The performance of text summarization has been greatly boosted by pre-trained
language models. A main concern of existing methods is that most generated
summaries are not factually inconsistent with their source documents. To
alleviate the problem, many efforts have focused on developing effective
factuality evaluation metrics based on natural language inference, question
answering, and syntactic dependency et al. However, these approaches are
limited by either their high computational complexity or the uncertainty
introduced by multi-component pipelines, resulting in only partial agreement
with human judgement. Most recently, large language models(LLMs) have shown
excellent performance in not only text generation but also language
comprehension. In this paper, we particularly explore ChatGPT's ability to
evaluate factual inconsistency under a zero-shot setting by examining it on
both coarse-grained and fine-grained evaluation tasks including binary
entailment inference, summary ranking, and consistency rating. Experimental
results indicate that ChatGPT generally outperforms previous evaluation metrics
across the three tasks, indicating its great potential for factual
inconsistency evaluation. However, a closer inspection of ChatGPT's output
reveals certain limitations including its preference for more lexically similar
candidates, false reasoning, and inadequate understanding of instructions. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15621v2' target="_blank">http://arxiv.org/pdf/2303.15621v2</a><br> <br> <br> <font size='5'> 642 </font> <div style="text-align: right"> 2023-03-27 21:27:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02017v5' target="_blank">http://arxiv.org/pdf/2304.02017v5</a><br> <br> <br> <font size='5'> 643 </font> <div style="text-align: right"> 2023-03-27 20:33:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the field of Japanese-Chinese translation linguistics, the issue of
correctly translating attributive clauses has persistently proven to be
challenging. Present-day machine translation tools often fail to accurately
translate attributive clauses from Japanese to Chinese. In light of this, this
paper investigates the linguistic problem underlying such difficulties, namely
how does the semantic role of the modified noun affect the selection of
translation patterns for attributive clauses, from a linguistic perspective. To
ad-dress these difficulties, a pre-edit scheme is proposed, which aims to
enhance the accuracy of translation. Furthermore, we propose a novel two-step
prompt strategy, which combines this pre-edit scheme with ChatGPT, currently
the most widely used large language model. This prompt strategy is capable of
optimizing translation input in zero-shot scenarios and has been demonstrated
to improve the average translation accuracy score by over 35%. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15587v1' target="_blank">http://arxiv.org/pdf/2303.15587v1</a><br> <br> <br> <font size='5'> 644 </font> <div style="text-align: right"> 2023-03-27 09:59:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15056v1' target="_blank">http://arxiv.org/pdf/2303.15056v1</a><br> <br> <br> <font size='5'> 645 </font> <div style="text-align: right"> 2023-03-26 21:12:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MGTBench: Benchmarking Machine-Generated Text Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Nowadays large language models (LLMs) have shown revolutionary power in a
variety of natural language processing (NLP) tasks such as text classification,
sentiment analysis, language translation, and question-answering. In this way,
detecting machine-generated texts (MGTs) is becoming increasingly important as
LLMs become more advanced and prevalent. These models can generate human-like
language that can be difficult to distinguish from text written by a human,
which raises concerns about authenticity, accountability, and potential bias.
However, existing detection methods against MGTs are evaluated under different
model architectures, datasets, and experimental settings, resulting in a lack
of a comprehensive evaluation framework across different methodologies
  In this paper, we fill this gap by proposing the first benchmark framework
for MGT detection, named MGTBench. Extensive evaluations on public datasets
with curated answers generated by ChatGPT (the most representative and powerful
LLMs thus far) show that most of the current detection methods perform less
satisfactorily against MGTs. An exceptional case is ChatGPT Detector, which is
trained with ChatGPT-generated texts and shows great performance in detecting
MGTs. Nonetheless, we note that only a small fraction of adversarial-crafted
perturbations on MGTs can evade the ChatGPT Detector, thus highlighting the
need for more robust MGT detection methods. We envision that MGTBench will
serve as a benchmark tool to accelerate future investigations involving the
evaluation of state-of-the-art MGT detection methods on their respective
datasets and the development of more advanced MGT detection methods. Our source
code and datasets are available at https://github.com/xinleihe/MGTBench. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14822v2' target="_blank">http://arxiv.org/pdf/2303.14822v2</a><br> <br> <br> <font size='5'> 646 </font> <div style="text-align: right"> 2023-03-26 14:49:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The success of ChatGPT has recently attracted numerous efforts to replicate
it, with instruction-tuning strategies being a key factor in achieving
remarkable results. Instruction-tuning not only significantly enhances the
model's performance and generalization but also makes the model's generated
results more consistent with human speech patterns. However current research
rarely studies the impact of different amounts of instruction data on model
performance, especially in the real-world use cases. In this paper we explore
the performance of large language models based on instruction tuning across
different scales of instruction data. An evaluation dataset consisting of 12
major online use cases is constructed in the experiment. With Bloomz-7B1-mt as
the base model, the results show that 1) merely increasing the amount of
instruction data leads to continuous improvement in tasks such as open-ended
generation, 2) in tasks such as math and code, the model performance curve
remains quite flat while increasing data size. We further analyze the possible
causes of these phenomena and propose potential future research directions such
as effectively selecting high-quality training data, scaling base models and
training methods specialized for hard tasks. We will release our training and
evaluation datasets, as well as model checkpoints. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14742v1' target="_blank">http://arxiv.org/pdf/2303.14742v1</a><br> <br> <br> <font size='5'> 647 </font> <div style="text-align: right"> 2023-03-26 12:08:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mathematical Characterization of Signal Semantics and Rethinking of the Mathematical Theory of Information</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Shannon information theory is established based on probability and bits, and
the communication technology based on this theory realizes the information age.
The original goal of Shannon's information theory is to describe and transmit
information content. However, due to information is related to cognition, and
cognition is considered to be subjective, Shannon information theory is to
describe and transmit information-bearing signals. With the development of the
information age to the intelligent age, the traditional signal-oriented
processing needs to be upgraded to content-oriented processing. For example,
chat generative pre-trained transformer (ChatGPT) has initially realized the
content processing capability based on massive data. For many years,
researchers have been searching for the answer to what the information content
in the signal is, because only when the information content is mathematically
and accurately described can information-based machines be truly intelligent.
This paper starts from rethinking the essence of the basic concepts of the
information, such as semantics, meaning, information and knowledge, presents
the mathematical characterization of the information content, investigate the
relationship between them, studies the transformation from Shannon's signal
information theory to semantic information theory, and therefore proposes a
content-oriented semantic communication framework. Furthermore, we propose
semantic decomposition and composition scheme to achieve conversion between
complex and simple semantics. Finally, we verify the proposed characterization
of information-related concepts by implementing evolvable knowledge-based
semantic recognition. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14701v1' target="_blank">http://arxiv.org/pdf/2303.14701v1</a><br> <br> <br> <font size='5'> 648 </font> <div style="text-align: right"> 2023-03-26 04:39:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Guiding AI-Generated Digital Content with Wireless Perception</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advances in artificial intelligence (AI), coupled with a surge in
training data, have led to the widespread use of AI for digital content
generation, with ChatGPT serving as a representative example. Despite the
increased efficiency and diversity, the inherent instability of AI models poses
a persistent challenge in guiding these models to produce the desired content
for users. In this paper, we introduce an integration of wireless perception
(WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework
to improve the quality of digital content production. The framework employs a
novel multi-scale perception technology to read user's posture, which is
difficult to describe accurately in words, and transmits it to the AIGC model
as skeleton images. Based on these images and user's service requirements, the
AIGC model generates corresponding digital content. Since the production
process imposes the user's posture as a constraint on the AIGC model, it makes
the generated content more aligned with the user's requirements. Additionally,
WP-AIGC can also accept user's feedback, allowing adjustment of computing
resources at edge server to improve service quality. Experiments results verify
the effectiveness of the WP-AIGC framework, highlighting its potential as a
novel approach for guiding AI models in the accurate generation of digital
content. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14624v1' target="_blank">http://arxiv.org/pdf/2303.14624v1</a><br> <br> <br> <font size='5'> 649 </font> <div style="text-align: right"> 2023-03-25 19:43:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Large Language Models assist in Hazard Analysis?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable
natural language processing and generation capabilities and have been applied
to a variety tasks, such as source code generation. This paper explores the
potential of integrating LLMs in the hazard analysis for safety-critical
systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a
human analyst interacts with an LLM via a context-aware chat session and uses
the responses to support elicitation of possible hazard causes. In this
experiment, we explore CoHA with three increasingly complex versions of a
simple system, using Open AI's ChatGPT service. The quality of ChatGPT's
responses were systematically assessed to determine the feasibility of CoHA
given the current state of LLM technology. The results suggest that LLMs may be
useful for supporting human analysts performing hazard analysis. </font><br> Link: <a href='http://arxiv.org/pdf/2303.15473v1' target="_blank">http://arxiv.org/pdf/2303.15473v1</a><br> <br> <br> <font size='5'> 650 </font> <div style="text-align: right"> 2023-03-25 17:37:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have demonstrated their significant potential to
be applied for addressing various application tasks. However, traditional
recommender systems continue to face great challenges such as poor
interactivity and explainability, which actually also hinder their broad
deployment in real-world systems. To address these limitations, this paper
proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender
System) that innovatively augments LLMs for building conversational recommender
systems by converting user profiles and historical interactions into prompts.
Chat-Rec is demonstrated to be effective in learning user preferences and
establishing connections between users and products through in-context
learning, which also makes the recommendation process more interactive and
explainable. What's more, within the Chat-Rec framework, user's preferences can
transfer to different products for cross-domain recommendations, and
prompt-based injection of information into LLMs can also handle the cold-start
scenarios with new items. In our experiments, Chat-Rec effectively improve the
results of top-k recommendations and performs better in zero-shot rating
prediction task. Chat-Rec offers a novel approach to improving recommender
systems and presents new practical scenarios for the implementation of AIGC (AI
generated content) in recommender system studies. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14524v2' target="_blank">http://arxiv.org/pdf/2303.14524v2</a><br> <br> <br> <font size='5'> 651 </font> <div style="text-align: right"> 2023-03-24 22:21:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chat2VIS: Fine-Tuning Data Visualisations using Multilingual Natural Language Text and Pre-Trained Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The explosion of data in recent years is driving individuals to leverage
technology to generate insights. Traditional tools bring heavy learning
overheads and the requirement for understanding complex charting techniques.
Such barriers can hinder those who may benefit from harnessing data for
informed decision making. The emerging field of generating data visualisations
from natural language text (NL2VIS) addresses this issue. This study showcases
Chat2VIS, a state-of-the-art NL2VIS solution. It capitalises on the latest in
AI technology with the upsurge in pre-trained large language models (LLMs) such
as GPT-3, Codex, and ChatGPT. Furthermore, the rise in natural language
interfaces (NLI) and chatbots is taking centre stage. This work illustrates how
Chat2VIS leverages similar techniques to fine-tune data visualisation
components beyond that demonstrated in previous approaches. In addition, this
paper presents the flexibility of Chat2VIS to comprehend multilingual natural
language requests. No other NL2VIS system has demonstrated this unique talent.
In concluding, this research provides quantitative benchmarking evaluations to
contribute to the paucity of NL2VIS standards. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14292v1' target="_blank">http://arxiv.org/pdf/2303.14292v1</a><br> <br> <br> <font size='5'> 652 </font> <div style="text-align: right"> 2023-03-24 15:29:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The primary aim of this research was to address the limitations observed in
the medical knowledge of prevalent large language models (LLMs) such as
ChatGPT, by creating a specialized language model with enhanced accuracy in
medical advice. We achieved this by adapting and refining the large language
model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues
sourced from a widely used online medical consultation platform. These
conversations were cleaned and anonymized to respect privacy concerns. In
addition to the model refinement, we incorporated a self-directed information
retrieval mechanism, allowing the model to access and utilize real-time
information from online sources like Wikipedia and data from curated offline
medical databases. The fine-tuning of the model with real-world patient-doctor
interactions significantly improved the model's ability to understand patient
needs and provide informed advice. By equipping the model with self-directed
information retrieval from reliable online and offline sources, we observed
substantial improvements in the accuracy of its responses. Our proposed
ChatDoctor, represents a significant advancement in medical LLMs, demonstrating
a significant improvement in understanding patient inquiries and providing
accurate advice. Given the high stakes and low error tolerance in the medical
field, such enhancements in providing accurate and reliable information are not
only beneficial but essential. </font><br> Link: <a href='http://arxiv.org/pdf/2303.14070v5' target="_blank">http://arxiv.org/pdf/2303.14070v5</a><br> <br> <br> <font size='5'> 653 </font> <div style="text-align: right"> 2023-03-24 13:25:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Paraphrase Detection: Human vs. Machine Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The growing prominence of large language models, such as GPT-4 and ChatGPT,
has led to increased concerns over academic integrity due to the potential for
machine-generated content and paraphrasing. Although studies have explored the
detection of human- and machine-paraphrased content, the comparison between
these types of content remains underexplored. In this paper, we conduct a
comprehensive analysis of various datasets commonly employed for paraphrase
detection tasks and evaluate an array of detection methods. Our findings
highlight the strengths and limitations of different detection methods in terms
of performance on individual datasets, revealing a lack of suitable
machine-generated datasets that can be aligned with human expectations. Our
main finding is that human-authored paraphrases exceed machine-generated ones
in terms of difficulty, diversity, and similarity implying that automatically
generated texts are not yet on par with human-level performance. Transformers
emerged as the most effective method across datasets with TF-IDF excelling on
semantically diverse corpora. Additionally, we identify four datasets as the
most diverse and challenging for paraphrase detection. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13989v1' target="_blank">http://arxiv.org/pdf/2303.13989v1</a><br> <br> <br> <font size='5'> 654 </font> <div style="text-align: right"> 2023-03-24 11:45:52+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Generative AI Assistants in Software Development Education</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The software development industry is amid another potentially disruptive
paradigm change--adopting the use of generative AI (GAI) assistants for
software development. Whilst AI is already used in various areas of software
engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited
the imaginations (and fears) of many people. Whilst it is unclear how the
industry will adopt and adapt to these technologies, the move to integrate
these technologies into the wider industry by large software companies, such as
Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and
direction. We performed exploratory interviews with industry professionals to
understand current practices and challenges, which we incorporate into our
vision of a future of software development education and make some pedagogical
recommendations. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13936v1' target="_blank">http://arxiv.org/pdf/2303.13936v1</a><br> <br> <br> <font size='5'> 655 </font> <div style="text-align: right"> 2023-03-24 08:35:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unleashing ChatGPT on the Metaverse: Savior or Destroyer?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The incorporation of artificial intelligence (AI) technology, and in
particular natural language processing (NLP), is becoming increasingly vital
for the development of immersive and interactive metaverse experiences. One
such artificial intelligence tool that is gaining traction in the metaverse is
ChatGPT, a large language model trained by OpenAI. The article delves into the
pros and cons of utilizing ChatGPT for metaverse-based education,
entertainment, personalization, and support. Dynamic and personalized
experiences are possible with this technology, but there are also legitimate
privacy, bias, and ethical issues to consider. This article aims to help
readers understand the possible influence of ChatGPT on the metaverse and how
it may be used to effectively create a more immersive and engaging virtual
environment by evaluating these opportunities and obstacles. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13856v2' target="_blank">http://arxiv.org/pdf/2303.13856v2</a><br> <br> <br> <font size='5'> 656 </font> <div style="text-align: right"> 2023-03-24 05:05:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative large language models (LLMs), e.g., ChatGPT, have demonstrated
remarkable proficiency across several NLP tasks such as machine translation,
question answering, text summarization, and natural language understanding.
Recent research has shown that utilizing ChatGPT for assessing the quality of
machine translation (MT) achieves state-of-the-art performance at the system
level but performs poorly at the segment level. To further improve the
performance of LLMs on MT quality assessment, we conducted an investigation
into several prompting methods. Our results indicate that by combining
Chain-of-Thoughts and Error Analysis, a new prompting method called
\textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can
\textit{generate human-like MT evaluations at both the system and segment
level}. Additionally, we discovered some limitations of ChatGPT as an MT
evaluator, such as unstable scoring and biases when provided with multiple
translations in a single query. Our findings aim to provide a preliminary
experience for appropriately evaluating translation quality on ChatGPT while
offering a variety of tricks in designing prompts for in-context learning. We
anticipate that this report will shed new light on advancing the field of
translation evaluation with LLMs by enhancing both the accuracy and reliability
of metrics. The project can be found in
\url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13809v1' target="_blank">http://arxiv.org/pdf/2303.13809v1</a><br> <br> <br> <font size='5'> 657 </font> <div style="text-align: right"> 2023-03-24 03:35:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Making the Most of ChatGPT for Machine Translation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT shows remarkable capabilities for machine translation (MT). Several
prior studies have shown that it achieves comparable results to commercial
systems for high-resource languages, but lags behind in complex tasks, e.g,
low-resource and distant-language-pairs translation. However, they usually
adopt simple prompts which can not fully elicit the capability of ChatGPT. In
this report, we aim to further mine ChatGPT's translation ability by revisiting
several aspects: temperature, task information, and domain information, and
correspondingly propose two (simple but effective) prompts: Task-Specific
Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The
performance of ChatGPT depends largely on temperature, and a lower temperature
usually can achieve better performance; 2) Emphasizing the task information
further improves ChatGPT's performance, particularly in complex MT tasks; 3)
Introducing domain information can elicit ChatGPT's generalization ability and
improve its performance in the specific domain; 4) ChatGPT tends to generate
hallucinations for non-English-centric MT tasks, which can be partially
addressed by our proposed prompts but still need to be highlighted for the
MT/NLP community. We also explore the effects of advanced in-context learning
strategies and find a (negative but interesting) observation: the powerful
chain-of-thought prompt leads to word-by-word translation behavior, thus
bringing significant translation degradation. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13780v1' target="_blank">http://arxiv.org/pdf/2303.13780v1</a><br> <br> <br> <font size='5'> 658 </font> <div style="text-align: right"> 2023-03-23 18:16:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The proliferation of Large Language
Models (LLMs) in recent times compels one to ask: can these systems be used for
data generation? In this article, we explore prompting multilingual LLMs in a
zero-shot manner to create code-mixed data for five languages in South East
Asia (SEA) -- Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the
creole language Singlish. We find that ChatGPT shows the most potential,
capable of producing code-mixed text 68% of the time when the term
"code-mixing" is explicitly defined. Moreover, both ChatGPT's and InstructGPT's
(davinci-003) performances in generating Singlish texts are noteworthy,
averaging a 96% success rate across a variety of prompts. Their code-mixing
proficiency, however, is dampened by word choice errors that lead to semantic
inaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL are
unable to produce code-mixed texts altogether. By highlighting the limited
promises of LLMs in a specific form of low-resource data generation, we call
for a measured approach when applying similar techniques to other data-scarce
NLP contexts. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13592v2' target="_blank">http://arxiv.org/pdf/2303.13592v2</a><br> <br> <br> <font size='5'> 659 </font> <div style="text-align: right"> 2023-03-23 15:34:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The ChatGPT, as a lite and conversational variant of Generative Pretrained
Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large
Language Models (LLMs) with billions of parameters. LLMs, in fact, have stirred
up a lot of interest among researchers and practitioners by their impressive
skills in natural language processing tasks, which have a profound impact on a
wide range of fields. This paper mainly discusses the future applications of
LLMs in dentistry. We introduce two primary LLM deployment methods in
dentistry, including automated dental diagnosis and cross-modal dental
diagnosis, and examine their potential applications. Especially, equipped with
a cross-modal encoder, a single LLM can manage multi-source data and conduct
advanced natural language reasoning to perform complex clinical operations. A
use case is presented to demonstrate the potential of a fully automatic
Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer
significant potential benefits, the challenges, such as data privacy, data
quality, and model bias, need further study. Overall, LLMs have the potential
to revolutionize dental diagnosis and treatment, which indicates a promising
avenue for clinical application and research in dentistry. </font><br> Link: <a href='http://arxiv.org/pdf/2304.03086v1' target="_blank">http://arxiv.org/pdf/2304.03086v1</a><br> <br> <br> <font size='5'> 660 </font> <div style="text-align: right"> 2023-03-23 02:50:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT A Good Keyphrase Generator? A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of ChatGPT has recently garnered significant attention from the
computational linguistics community. To demonstrate its capabilities as a
keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the
keyphrase generation task. We evaluate its performance in various aspects,
including keyphrase generation prompts, keyphrase generation diversity,
multi-domain keyphrase generation, and long document understanding. Our
evaluation is based on six benchmark datasets, and we adopt the prompt
suggested by OpenAI while extending it to six candidate prompts. We find that
ChatGPT performs exceptionally well on all six candidate prompts, with minor
performance differences observed across the datasets. Based on our findings, we
conclude that ChatGPT has great potential for keyphrase generation. Moreover,
we discover that ChatGPT still faces challenges when it comes to generating
absent keyphrases. Meanwhile, in the final section, we also present some
limitations and future expansions of this report. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13001v1' target="_blank">http://arxiv.org/pdf/2303.13001v1</a><br> <br> <br> <font size='5'> 661 </font> <div style="text-align: right"> 2023-03-22 23:54:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The successes of foundation models such as ChatGPT and AlphaFold have spurred
significant interest in building similar models for electronic medical records
(EMRs) to improve patient care and hospital operations. However, recent hype
has obscured critical gaps in our understanding of these models' capabilities.
We review over 80 foundation models trained on non-imaging EMR data (i.e.
clinical text and/or structured data) and create a taxonomy delineating their
architectures, training data, and potential use cases. We find that most models
are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or
broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that
do not provide meaningful insights on their usefulness to health systems. In
light of these findings, we propose an improved evaluation framework for
measuring the benefits of clinical foundation models that is more closely
grounded to metrics that matter in healthcare. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12961v2' target="_blank">http://arxiv.org/pdf/2303.12961v2</a><br> <br> <br> <font size='5'> 662 </font> <div style="text-align: right"> 2023-03-22 21:03:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Cross-Layer Design for AI Acceleration with Non-Coherent Optical Computing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Emerging AI applications such as ChatGPT, graph convolutional networks, and
other deep neural networks require massive computational resources for training
and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs
are struggling to keep up with the demands of these AI applications.
Non-coherent optical computing represents a promising approach for light-speed
acceleration of AI workloads. In this paper, we show how cross-layer design can
overcome challenges in non-coherent optical computing platforms. We describe
approaches for optical device engineering, tuning circuit enhancements, and
architectural innovations to adapt optical computing to a variety of AI
workloads. We also discuss techniques for hardware/software co-design that can
intelligently map and adapt AI software to improve its performance on
non-coherent optical computing platforms. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12910v1' target="_blank">http://arxiv.org/pdf/2303.12910v1</a><br> <br> <br> <font size='5'> 663 </font> <div style="text-align: right"> 2023-03-22 17:32:56+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can we trust the evaluation on ChatGPT?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12767v1' target="_blank">http://arxiv.org/pdf/2303.12767v1</a><br> <br> <br> <font size='5'> 664 </font> <div style="text-align: right"> 2023-03-22 16:51:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12712v5' target="_blank">http://arxiv.org/pdf/2303.12712v5</a><br> <br> <br> <font size='5'> 665 </font> <div style="text-align: right"> 2023-03-21 18:45:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Language Models gained significant attention in late 2022 / early
2023, notably with the introduction of models refined to act consistently with
users' expectations of interactions with AI (conversational models). Arguably
the focal point of public attention has been such a refinement of the GPT3
model -- the ChatGPT and its subsequent integration with auxiliary
capabilities, including search as part of Microsoft Bing. Despite extensive
prior research invested in their development, their performance and
applicability to a range of daily tasks remained unclear and niche. However,
their wider utilization without a requirement for technical expertise, made in
large part possible through conversational fine-tuning, revealed the extent of
their true capabilities in a real-world environment. This has garnered both
public excitement for their potential applications and concerns about their
capabilities and potential malicious uses. This review aims to provide a brief
overview of the history, state of the art, and implications of Generative
Language Models in terms of their principles, abilities, limitations, and
future prospects -- especially in the context of cyber-defense, with a focus on
the Swiss operational environment. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12132v1' target="_blank">http://arxiv.org/pdf/2303.12132v1</a><br> <br> <br> <font size='5'> 666 </font> <div style="text-align: right"> 2023-03-21 17:48:00+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models Can Be Used to Scale the Ideologies of Politicians in a Zero-Shot Learning Setting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The aggregation of knowledge embedded in large language models (LLMs) holds
the promise of new solutions to problems of observability and measurement in
the social sciences. We examine this potential in a challenging setting:
measuring latent ideology -- crucial for better understanding core political
functions such as democratic representation. We scale pairwise
liberal-conservative comparisons between members of the 116th U.S. Senate using
prompts made to ChatGPT. Our measure strongly correlates with widely used
liberal-conservative scales such as DW-NOMINATE. Our scale also has
interpretative advantages, such as not placing senators who vote against their
party for ideologically extreme reasons towards the middle. Our measure is more
strongly associated with political activists' perceptions of senators than
other measures, consistent with LLMs synthesizing vast amounts of politically
relevant data from internet/book corpora rather than memorizing existing
measures. LLMs will likely open new avenues for measuring latent constructs
utilizing modeled information from massive text corpora. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12057v3' target="_blank">http://arxiv.org/pdf/2303.12057v3</a><br> <br> <br> <font size='5'> 667 </font> <div style="text-align: right"> 2023-03-21 16:35:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12003v1' target="_blank">http://arxiv.org/pdf/2303.12003v1</a><br> <br> <br> <font size='5'> 668 </font> <div style="text-align: right"> 2023-03-21 14:35:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13367v2' target="_blank">http://arxiv.org/pdf/2303.13367v2</a><br> <br> <br> <font size='5'> 669 </font> <div style="text-align: right"> 2023-03-21 12:55:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a publicly available chatbot that can quickly generate texts on
given topics, but it is unknown whether the chatbot is really superior to human
writers in all aspects of writing and whether its writing quality can be
prominently improved on the basis of updating commands. Consequently, this
study compared the writing performance on a narrative topic by ChatGPT and
Chinese intermediate English (CIE) learners so as to reveal the chatbot's
advantage and disadvantage in writing. The data were analyzed in terms of five
discourse components using Coh-Metrix (a special instrument for analyzing
language discourses), and the results revealed that ChatGPT performed better
than human writers in narrativity, word concreteness, and referential cohesion,
but worse in syntactic simplicity and deep cohesion in its initial version.
After more revision commands were updated, while the resulting version was
facilitated in syntactic simplicity, yet it is still lagged far behind CIE
learners' writing in deep cohesion. In addition, the correlation analysis of
the discourse components suggests that narrativity was correlated with
referential cohesion in both ChatGPT and human writers, but the correlations
varied within each group. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11812v1' target="_blank">http://arxiv.org/pdf/2303.11812v1</a><br> <br> <br> <font size='5'> 670 </font> <div style="text-align: right"> 2023-03-21 12:18:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Programming Numerical Methods</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a large language model recently released by the OpenAI company. In
this technical report, we explore for the first time the capability of ChatGPT
for programming numerical algorithms. Specifically, we examine the capability
of GhatGPT for generating codes for numerical algorithms in different
programming languages, for debugging and improving written codes by users, for
completing missed parts of numerical codes, rewriting available codes in other
programming languages, and for parallelizing serial codes. Additionally, we
assess if ChatGPT can recognize if given codes are written by humans or
machines. To reach this goal, we consider a variety of mathematical problems
such as the Poisson equation, the diffusion equation, the incompressible
Navier-Stokes equations, compressible inviscid flow, eigenvalue problems,
solving linear systems of equations, storing sparse matrices, etc. Furthermore,
we exemplify scientific machine learning such as physics-informed neural
networks and convolutional neural networks with applications to computational
physics. Through these examples, we investigate the successes, failures, and
challenges of ChatGPT. Examples of failures are producing singular matrices,
operations on arrays with incompatible sizes, programming interruption for
relatively long codes, etc. Our outcomes suggest that ChatGPT can successfully
program numerical algorithms in different programming languages, but certain
limitations and challenges exist that require further improvement of this
machine learning model. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12093v3' target="_blank">http://arxiv.org/pdf/2303.12093v3</a><br> <br> <br> <font size='5'> 671 </font> <div style="text-align: right"> 2023-03-21 10:09:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has
made headlines everywhere because of its ability to analyze and create text,
images, and beyond. With such overwhelming media coverage, it is almost
impossible for us to miss the opportunity to glimpse AIGC from a certain angle.
In the era of AI transitioning from pure analysis to creation, it is worth
noting that ChatGPT, with its most recent language model GPT-4, is just a tool
out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many
people are wondering about its limits: can GPT-5 (or other future GPT variants)
help ChatGPT unify all AIGC tasks for diversified content creation? Toward
answering this question, a comprehensive review of existing AIGC tasks is
needed. As such, our work comes to fill this gap promptly by offering a first
look at AIGC, ranging from its techniques to applications. Modern generative AI
relies on various technical foundations, ranging from model architecture and
self-supervised pretraining to generative modeling methods (like GAN and
diffusion models). After introducing the fundamental techniques, this work
focuses on the technological development of various AIGC tasks based on their
output type, including text, images, videos, 3D content, etc., which depicts
the full potential of ChatGPT's future. Moreover, we summarize their
significant applications in some mainstream industries, such as education and
creativity content. Finally, we discuss the challenges currently faced and
present an outlook on how generative AI might evolve in the near future. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11717v1' target="_blank">http://arxiv.org/pdf/2303.11717v1</a><br> <br> <br> <font size='5'> 672 </font> <div style="text-align: right"> 2023-03-21 03:28:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large AI Models in Health Informatics: Applications, Challenges, and the Future</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large AI models, or foundation models, are models recently emerging with
massive scales both parameter-wise and data-wise, the magnitudes of which often
reach beyond billions. Once pretrained, large AI models demonstrate impressive
performance in various downstream tasks. A concrete example is the recent debut
of ChatGPT, whose capability has compelled people's imagination about the
far-reaching influence that large AI models can have and their potential to
transform different domains of our life. In health informatics, the advent of
large AI models has brought new paradigms for the design of methodologies. The
scale of multimodality data in the biomedical and health domain has been
ever-expanding especially since the community embraced the era of deep
learning, which provides the ground to develop, validate, and advance large AI
models for breakthroughs in health-related areas. This article presents an
up-to-date comprehensive review of large AI models, from background to their
applications. We identify seven key sectors that large AI models are applicable
and might have substantial influence, including 1) molecular biology and drug
discovery; 2) medical diagnosis and decision-making; 3) medical imaging and
vision; 4) medical informatics; 5) medical education; 6) public health; and 7)
medical robotics. We examine their challenges in health informatics, followed
by a critical discussion about potential future directions and pitfalls of
large AI models in transforming the field of health informatics. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11568v1' target="_blank">http://arxiv.org/pdf/2303.11568v1</a><br> <br> <br> <font size='5'> 673 </font> <div style="text-align: right"> 2023-03-20 18:31:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of
vision experts to achieve multimodal reasoning and action. In this paper, we
define and explore a comprehensive list of advanced vision tasks that are
intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, MM-REACT
introduces a textual prompt design that can represent text descriptions,
textualized spatial coordinates, and aligned file names for dense visual
signals such as images and videos. MM-REACT's prompt design allows language
models to accept, associate, and process multimodal information, thereby
facilitating the synergetic combination of ChatGPT and various vision experts.
Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the
specified capabilities of interests and its wide application in different
scenarios that require advanced visual understanding. Furthermore, we discuss
and compare MM-REACT's system paradigm with an alternative approach that
extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at
https://multimodal-react.github.io/ </font><br> Link: <a href='http://arxiv.org/pdf/2303.11381v1' target="_blank">http://arxiv.org/pdf/2303.11381v1</a><br> <br> <br> <font size='5'> 674 </font> <div style="text-align: right"> 2023-03-20 14:27:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In late 2022, OpenAI released a new version of ChatGPT, a sophisticated
natural language processing system capable of holding natural conversations
while preserving and responding to the context of the discussion. ChatGPT has
exceeded expectations in its abilities, leading to extensive considerations of
its potential applications and misuse. In this work, we evaluate the influence
of ChatGPT on university education, with a primary focus on computer
security-oriented specialization. We gather data regarding the effectiveness
and usability of this tool for completing exams, programming assignments, and
term papers. We evaluate multiple levels of tool misuse, ranging from utilizing
it as a consultant to simply copying its outputs. While we demonstrate how
easily ChatGPT can be used to cheat, we also discuss the potentially
significant benefits to the educational system. For instance, it might be used
as an aid (assistant) to discuss problems encountered while solving an
assignment or to speed up the learning process. Ultimately, we discuss how
computer science higher education should adapt to tools like ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11146v1' target="_blank">http://arxiv.org/pdf/2303.11146v1</a><br> <br> <br> <font size='5'> 675 </font> <div style="text-align: right"> 2023-03-20 11:34:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The digitization of healthcare has facilitated the sharing and re-using of
medical data but has also raised concerns about confidentiality and privacy.
HIPAA (Health Insurance Portability and Accountability Act) mandates removing
re-identifying information before the dissemination of medical records. Thus,
effective and efficient solutions for de-identifying medical data, especially
those in free-text forms, are highly needed. While various computer-assisted
de-identification methods, including both rule-based and learning-based, have
been developed and used in prior practice, such solutions still lack
generalizability or need to be fine-tuned according to different scenarios,
significantly imposing restrictions in wider use. The advancement of large
language models (LLM), such as ChatGPT and GPT-4, have shown great potential in
processing text data in the medical domain with zero-shot in-context learning,
especially in the task of privacy protection, as these models can identify
confidential information by their powerful named entity recognition (NER)
capability. In this work, we developed a novel GPT4-enabled de-identification
framework ("DeID-GPT") to automatically identify and remove the identifying
information. Compared to existing commonly used medical text data
de-identification methods, our developed DeID-GPT showed the highest accuracy
and remarkable reliability in masking private information from the unstructured
medical text while preserving the original structure and meaning of the text.
This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text
data processing and de-identification, which provides insights for further
research and solution development on the use of LLMs such as ChatGPT/GPT-4 in
healthcare. Codes and benchmarking data information are available at
https://github.com/yhydhx/ChatGPT-API. </font><br> Link: <a href='http://arxiv.org/pdf/2303.11032v1' target="_blank">http://arxiv.org/pdf/2303.11032v1</a><br> <br> <br> <font size='5'> 676 </font> <div style="text-align: right"> 2023-03-20 00:27:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI-assisted Protective Action: Study of ChatGPT as an Information Source for a Population Facing Climate Hazards</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has been emerging as a novel information source, and it is likely
that the public might seek information from ChatGPT while taking protective
actions when facing climate hazards such as floods and hurricanes. The
objective of this study is to evaluate the accuracy and completeness of
responses generated by ChatGPT when individuals seek information about aspects
of taking protective actions. The survey analysis results indicated that: (1)
the emergency managers considered the responses provided by ChatGPT as accurate
and complete to a great extent; (2) it was statistically verified in
evaluations that the generated information was accurate, but lacked
completeness, implying that the extent of information provided is accurate; and
(3) information generated for prompts related to hazard insurance received the
highest evaluation, whereas the information generated related to evacuation
received the lowest. This last result implies that, for complex,
context-specific protective actions (such as evacuation), the information was
rated as less complete compared with other protective actions. Also, the
results showed that the perception of respondents regarding the utility of AI-
assistive technologies (such as ChatGPT) for emergency preparedness and
response improved after taking the survey and evaluating the information
generated by ChatGPT. The findings from this study provide empirical evaluation
regarding the utility of AI-assistive technologies for improving public
decision-making and protective actions in disasters. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06124v2' target="_blank">http://arxiv.org/pdf/2304.06124v2</a><br> <br> <br> <font size='5'> 677 </font> <div style="text-align: right"> 2023-03-18 14:02:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,
have gained considerable attention due to their exceptional natural language
processing capabilities. However, despite the abundance of research on the
difference in capabilities between GPT series models and fine-tuned models,
there has been limited attention given to the evolution of GPT series models'
capabilities over time. To conduct a comprehensive analysis of the capabilities
of GPT series models, we select six representative models, comprising two GPT-3
series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series
models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and
gpt-3.5-turbo). We evaluate their performance on nine natural language
understanding (NLU) tasks using 21 datasets. In particular, we compare the
performance and robustness of different models for each task under zero-shot
and few-shot scenarios. Our extensive experiments reveal that the overall
ability of GPT series models on NLU tasks does not increase gradually as the
models evolve, especially with the introduction of the RLHF training strategy.
While this strategy enhances the models' ability to generate human-like
responses, it also compromises their ability to solve some tasks. Furthermore,
our findings indicate that there is still room for improvement in areas such as
model robustness. </font><br> Link: <a href='http://arxiv.org/pdf/2303.10420v1' target="_blank">http://arxiv.org/pdf/2303.10420v1</a><br> <br> <br> <font size='5'> 678 </font> <div style="text-align: right"> 2023-03-18 08:57:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large-scale pre-trained language models (PLMs) such as BERT have recently
achieved great success and become a milestone in natural language processing
(NLP). It is now the consensus of the NLP community to adopt PLMs as the
backbone for downstream tasks. In recent works on knowledge graph question
answering (KGQA), BERT or its variants have become necessary in their KGQA
models. However, there is still a lack of comprehensive research and comparison
of the performance of different PLMs in KGQA. To this end, we summarize two
basic KGQA frameworks based on PLMs without additional neural network modules
to compare the performance of nine PLMs in terms of accuracy and efficiency. In
addition, we present three benchmarks for larger-scale KGs based on the popular
SimpleQuestions benchmark to investigate the scalability of PLMs. We carefully
analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks
and two other popular datasets, WebQuestionSP and FreebaseQA, and find that
knowledge distillation techniques and knowledge enhancement methods in PLMs are
promising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal
of attention in the NLP community, demonstrating its impressive capabilities
and limitations in zero-shot KGQA. We have released the code and benchmarks to
promote the use of PLMs on KGQA. </font><br> Link: <a href='http://arxiv.org/pdf/2303.10368v1' target="_blank">http://arxiv.org/pdf/2303.10368v1</a><br> <br> <br> <font size='5'> 679 </font> <div style="text-align: right"> 2023-03-17 08:46:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Role of Large Language Models in the Recognition of Territorial Sovereignty: An Analysis of the Construction of Legitimacy</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We examine the potential impact of Large Language Models (LLM) on the
recognition of territorial sovereignty and its legitimization. We argue that
while technology tools, such as Google Maps and Large Language Models (LLM)
like OpenAI's ChatGPT, are often perceived as impartial and objective, this
perception is flawed, as AI algorithms reflect the biases of their designers or
the data they are built on. We also stress the importance of evaluating the
actions and decisions of AI and multinational companies that offer them, which
play a crucial role in aspects such as legitimizing and establishing ideas in
the collective imagination. Our paper highlights the case of three
controversial territories: Crimea, West Bank and Transnitria, by comparing the
responses of ChatGPT against Wikipedia information and United Nations
resolutions. We contend that the emergence of AI-based tools like LLMs is
leading to a new scenario in which emerging technology consolidates power and
influences our understanding of reality. Therefore, it is crucial to monitor
and analyze the role of AI in the construction of legitimacy and the
recognition of territorial sovereignty. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06030v2' target="_blank">http://arxiv.org/pdf/2304.06030v2</a><br> <br> <br> <font size='5'> 680 </font> <div style="text-align: right"> 2023-03-16 09:53:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Block-wise Bit-Compression of Transformer-based Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the popularity of the recent Transformer-based models represented by
BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range
of natural language processing tasks. However, the massive computations, huge
memory footprint, and thus high latency of Transformer-based models is an
inevitable challenge for the cloud with high real-time requirement. To tackle
the issue, we propose BBCT, a method of block-wise bit-compression for
transformer without retraining. Our method achieves more fine-grained
compression of the whole transformer, including embedding, matrix
multiplication, GELU, softmax, layer normalization, and all the intermediate
results. As a case, we compress an efficient BERT with the method of BBCT. Our
benchmark test results on General Language Understanding Evaluation (GLUE) show
that BBCT can achieve less than 1% accuracy drop in most tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09184v2' target="_blank">http://arxiv.org/pdf/2303.09184v2</a><br> <br> <br> <font size='5'> 681 </font> <div style="text-align: right"> 2023-03-16 09:28:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How well do Large Language Models perform in Arithmetic tasks?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have emerged abilities including chain-of-thought to
answer math word problems step by step. Solving math word problems not only
requires abilities to disassemble problems via chain-of-thought but also needs
to calculate arithmetic expressions correctly for each step. To the best of our
knowledge, there is no work to focus on evaluating the arithmetic ability of
large language models. In this work, we propose an arithmetic dataset MATH 401
to test the latest large language models including GPT-4, ChatGPT, InstrctGPT,
Galactica, and LLaMA with various arithmetic expressions and provide a detailed
analysis of the ability of large language models. MATH 401 and evaluation codes
are released at \url{https://github.com/GanjinZero/math401-llm}. </font><br> Link: <a href='http://arxiv.org/pdf/2304.02015v1' target="_blank">http://arxiv.org/pdf/2304.02015v1</a><br> <br> <br> <font size='5'> 682 </font> <div style="text-align: right"> 2023-03-16 02:21:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The large language model called ChatGPT has drawn extensively attention
because of its human-like expression and reasoning abilities. In this study, we
investigate the feasibility of using ChatGPT in experiments on using ChatGPT to
translate radiology reports into plain language for patients and healthcare
providers so that they are educated for improved healthcare. Radiology reports
from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI
metastases screening scans were collected in the first half of February for
this study. According to the evaluation by radiologists, ChatGPT can
successfully translate radiology reports into plain language with an average
score of 4.27 in the five-point system with 0.08 places of information missing
and 0.07 places of misinformation. In terms of the suggestions provided by
ChatGPT, they are general relevant such as keeping following-up with doctors
and closely monitoring any symptoms, and for about 37% of 138 cases in total
ChatGPT offers specific suggestions based on findings in the report. ChatGPT
also presents some randomness in its responses with occasionally
over-simplified or neglected information, which can be mitigated using a more
detailed prompt. Furthermore, ChatGPT results are compared with a newly
released large model GPT-4, showing that GPT-4 can significantly improve the
quality of translated reports. Our results show that it is feasible to utilize
large language models in clinical education, and further efforts are needed to
address limitations and maximize their potential. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09038v3' target="_blank">http://arxiv.org/pdf/2303.09038v3</a><br> <br> <br> <font size='5'> 683 </font> <div style="text-align: right"> 2023-03-15 19:31:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Large Language Models (LLMs) such as GPT-3 are capable of
generating highly fluent responses to a wide variety of user prompts. However,
LLMs are known to hallucinate facts and make non-factual statements which can
undermine trust in their output. Existing fact-checking approaches either
require access to the output probability distribution (which may not be
available for systems such as ChatGPT) or external databases that are
interfaced via separate, often complex, modules. In this work, we propose
"SelfCheckGPT", a simple sampling-based approach that can be used to fact-check
black-box models in a zero-resource fashion, i.e. without an external database.
SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given
concept, sampled responses are likely to be similar and contain consistent
facts. However, for hallucinated facts, stochastically sampled responses are
likely to diverge and contradict one another. We investigate this approach by
using GPT-3 to generate passages about individuals from the WikiBio dataset,
and manually annotate the factuality of the generated passages. We demonstrate
that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii)
rank passages in terms of factuality. We compare our approach to several
baselines and show that in sentence hallucination detection, our approach has
AUC-PR scores comparable to or better than grey-box methods, while SelfCheckGPT
is best at passage factuality assessment. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08896v2' target="_blank">http://arxiv.org/pdf/2303.08896v2</a><br> <br> <br> <font size='5'> 684 </font> <div style="text-align: right"> 2023-03-15 10:53:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) are popular for their impressive abilities, but
the need for model-specific fine-tuning or task-specific prompt engineering can
hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for
Improving zero-Shot Evaluation), which tunes a lightweight and versatile
retriever that automatically retrieves prompts for a given zero-shot task
input. Specifically, we demonstrate universality in a cross-task and
cross-model scenario: the retriever is tuned on a diverse set of tasks, but
tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for
tuning the retriever, but test the retriever on different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that
UPRISE mitigates the hallucination problem in our experiments with ChatGPT,
suggesting its potential to improve even the strongest LLMs. Our model and code
are available at https://github.com/microsoft/LMOps. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08518v2' target="_blank">http://arxiv.org/pdf/2303.08518v2</a><br> <br> <br> <font size='5'> 685 </font> <div style="text-align: right"> 2023-03-15 00:35:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a cutting-edge artificial intelligence language model developed by
OpenAI, which has attracted a lot of attention due to its surprisingly strong
ability in answering follow-up questions. In this report, we aim to evaluate
ChatGPT on the Grammatical Error Correction(GEC) task, and compare it with
commercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g.,
GECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT
performs not as well as those baselines in terms of the automatic evaluation
metrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the
outputs and find that ChatGPT goes beyond one-by-one corrections. Specifically,
it prefers to change the surface expression of certain phrases or sentence
structure while maintaining grammatical correctness. Human evaluation
quantitatively confirms this and suggests that ChatGPT produces less
under-correction or mis-correction issues but more over-corrections. These
results demonstrate that ChatGPT is severely under-estimated by the automatic
evaluation metrics and could be a promising tool for GEC. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13648v1' target="_blank">http://arxiv.org/pdf/2303.13648v1</a><br> <br> <br> <font size='5'> 686 </font> <div style="text-align: right"> 2023-03-14 20:59:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The Natural Language for Optimization (NL4Opt) Competition was created to
investigate methods of extracting the meaning and formulation of an
optimization problem based on its text description. Specifically, the goal of
the competition is to increase the accessibility and usability of optimization
solvers by allowing non-experts to interface with them using natural language.
We separate this challenging goal into two sub-tasks: (1) recognize and label
the semantic entities that correspond to the components of the optimization
problem; (2) generate a meaning representation (i.e., a logical form) of the
problem from its detected problem entities. The first task aims to reduce
ambiguity by detecting and tagging the entities of the optimization problems.
The second task creates an intermediate representation of the linear
programming (LP) problem that is converted into a format that can be used by
commercial solvers. In this report, we present the LP word problem dataset and
shared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and
compare the performance of the ChatGPT large language model against the winning
solutions. Through this competition, we hope to bring interest towards the
development of novel machine learning applications and datasets for
optimization modeling. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08233v2' target="_blank">http://arxiv.org/pdf/2303.08233v2</a><br> <br> <br> <font size='5'> 687 </font> <div style="text-align: right"> 2023-03-14 15:46:28+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a powerful large language model (LLM) that has made remarkable
progress in natural language understanding. Nevertheless, the performance and
limitations of the model still need to be extensively evaluated. As ChatGPT
covers resources such as Wikipedia and supports natural language question
answering, it has garnered attention as a potential replacement for traditional
knowledge based question answering (KBQA) models. Complex question answering is
a challenge task of KBQA, which comprehensively tests the ability of models in
semantic parsing and reasoning. To assess the performance of ChatGPT as a
question answering system (QAS) using its own knowledge, we present a framework
that evaluates its ability to answer complex questions. Our approach involves
categorizing the potential features of complex questions and describing each
test question with multiple labels to identify combinatorial reasoning.
Following the black-box testing specifications of CheckList proposed by Ribeiro
et.al, we develop an evaluation method to measure the functionality and
reliability of ChatGPT in reasoning for answering complex questions. We use the
proposed framework to evaluate the performance of ChatGPT in question answering
on 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual
datasets, with a total of approximately 190,000 test cases. We compare the
evaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common
long-term problems in LLMs. The dataset and code are available at
https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07992v1' target="_blank">http://arxiv.org/pdf/2303.07992v1</a><br> <br> <br> <font size='5'> 688 </font> <div style="text-align: right"> 2023-03-14 03:13:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As a natural language assistant, ChatGPT is capable of performing various
tasks, including but not limited to article generation, code completion, and
data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable
level of accuracy and reliability in terms of content evaluation, exhibiting
the capability of mimicking human preferences. To further explore ChatGPT's
potential in this regard, a study is conducted to assess its ability to rank
content. In order to do so, a test set consisting of prompts is created,
covering a wide range of use cases, and five models are utilized to generate
corresponding responses. ChatGPT is then instructed to rank the responses
generated by these models. The results on the test set show that ChatGPT's
ranking preferences are consistent with human to a certain extent. This
preliminary experimental finding implies that ChatGPT's zero-shot ranking
capability could be used to reduce annotation pressure in a number of ranking
tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07610v1' target="_blank">http://arxiv.org/pdf/2303.07610v1</a><br> <br> <br> <font size='5'> 689 </font> <div style="text-align: right"> 2023-03-13 23:33:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Thinking Upstream: Ethics and Policy Opportunities in AI Supply Chains</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: After children were pictured sewing its running shoes in the early 1990s,
Nike at first disavowed the "working conditions in its suppliers' factories",
before public pressure led them to take responsibility for ethics in their
upstream supply chain. In 2023, OpenAI responded to criticism that Kenyan
workers were paid less than $2 per hour to filter traumatic content from its
ChatGPT model by stating in part that it had outsourced the work to a
subcontractor, who managed workers' payment and mental health concerns. In this
position paper, we argue that policy interventions for AI Ethics must consider
AI as a supply chain problem, given how the political economy and intra-firm
relations structure AI production, in particular examining opportunities
upstream. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07529v1' target="_blank">http://arxiv.org/pdf/2303.07529v1</a><br> <br> <br> <font size='5'> 690 </font> <div style="text-align: right"> 2023-03-13 16:22:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has recently gathered attention from the general public and academia
as a tool that is able to generate plausible and human-sounding text answers to
various questions. One potential use, or abuse, of ChatGPT is in answering
various questions or even generating whole essays and research papers in an
academic or classroom setting. While recent works have explored the use of
ChatGPT in the context of humanities, business school, or medical school, this
work explores how ChatGPT performs in the context of an introductory computer
engineering course. This work assesses ChatGPT's aptitude in answering quizzes,
homework, exam, and laboratory questions in an introductory-level computer
engineering course. This work finds that ChatGPT can do well on questions
asking about generic concepts. However, predictably, as a text-only tool, it
cannot handle questions with diagrams or figures, nor can it generate diagrams
and figures. Further, also clearly, the tool cannot do hands-on lab
experiments, breadboard assembly, etc., but can generate plausible answers to
some laboratory manual questions. One of the key observations presented in this
work is that the ChatGPT tool could not be used to pass all components of the
course. Nevertheless, it does well on quizzes and short-answer questions. On
the other hand, plausible, human-sounding answers could confuse students when
generating incorrect but still plausible answers. </font><br> Link: <a href='http://arxiv.org/pdf/2304.06122v2' target="_blank">http://arxiv.org/pdf/2304.06122v2</a><br> <br> <br> <font size='5'> 691 </font> <div style="text-align: right"> 2023-03-13 03:28:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ODIN is an innovative approach that addresses the problem of dataset
constraints by integrating generative AI models. Traditional zero-shot learning
methods are constrained by the training dataset. To fundamentally overcome this
limitation, ODIN attempts to mitigate the dataset constraints by generating
on-demand datasets based on user requirements. ODIN consists of three main
modules: a prompt generator, a text-to-image generator, and an image
post-processor. To generate high-quality prompts and images, we adopted a large
language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g.,
Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms
of model accuracy and data diversity to demonstrate its potential, and
conducted post-experiments for further investigation. Overall, ODIN is a
feasible approach that enables Al to learn unseen knowledge beyond the training
dataset. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06832v2' target="_blank">http://arxiv.org/pdf/2303.06832v2</a><br> <br> <br> <font size='5'> 692 </font> <div style="text-align: right"> 2023-03-12 07:22:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Asking insightful questions is crucial for acquiring knowledge and expanding
our understanding of the world. However, the importance of questioning has been
largely overlooked in AI research, where models have been primarily developed
to answer questions. With the recent advancements of large language models
(LLMs) like ChatGPT, we discover their capability to ask high-quality questions
when provided with a suitable prompt. This discovery presents a new opportunity
to develop an automatic questioning system. In this paper, we introduce
ChatCaptioner, a novel automatic-questioning method deployed in image
captioning. Here, ChatGPT is prompted to ask a series of informative questions
about images to BLIP-2, a strong vision question-answering model. By keeping
acquiring new visual information from BLIP-2's answers, ChatCaptioner is able
to generate more enriched image descriptions. We conduct human-subject
evaluations on common image caption datasets such as COCO, Conceptual Caption,
and WikiArt, and compare ChatCaptioner with BLIP-2 as well as ground truth. Our
results demonstrate that ChatCaptioner's captions are significantly more
informative, receiving three times as many votes from human evaluators for
providing the most image information. Besides, ChatCaptioner identifies 53%
more objects within the image than BLIP-2 alone measured by WordNet synset
matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner </font><br> Link: <a href='http://arxiv.org/pdf/2303.06594v1' target="_blank">http://arxiv.org/pdf/2303.06594v1</a><br> <br> <br> <font size='5'> 693 </font> <div style="text-align: right"> 2023-03-12 04:22:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL
ability. Given the recent emergence of large-scale conversational language
model ChatGPT and its impressive capabilities in both conversational abilities
and code generation, we sought to evaluate its Text-to-SQL performance. We
conducted experiments on 12 benchmark datasets with different languages,
settings, or scenarios, and the results demonstrate that ChatGPT has strong
text-to-SQL abilities. Although there is still a gap from the current
state-of-the-art (SOTA) model performance, considering that the experiment was
conducted in a zero-shot scenario, ChatGPT's performance is still impressive.
Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms
the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%,
demonstrating its potential for use in practical applications. To support
further research in related fields, we have made the data generated by ChatGPT
publicly available at https://github.com/THU-BPM/chatgpt-sql. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13547v1' target="_blank">http://arxiv.org/pdf/2303.13547v1</a><br> <br> <br> <font size='5'> 694 </font> <div style="text-align: right"> 2023-03-11 23:15:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Context-based Ontology Modelling for Database: Enabling ChatGPT for Semantic Database Management</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research paper explores the use of ChatGPT in database management.
ChatGPT, an AI-powered chatbot, has limitations in performing tasks related to
database management due to the lack of standardized vocabulary and grammar for
representing database semantics. To address this limitation, the paper proposes
a solution that involves developing a set of syntaxes that can represent
database semantics in natural language. The syntax is used to convert database
schemas into natural language formats, providing a new application of ChatGPT
in database management. The proposed solution is demonstrated through a case
study where ChatGPT is used to perform two tasks, semantic integration, and
tables joining. Results demonstrate that the use of semantic database
representations produces more precise outcomes and avoids common mistakes
compared to cases with no semantic representation. The proposed method has the
potential to speed up the database management process, reduce the level of
understanding required for database domain knowledge, and enable automatic
database operations without accessing the actual data, thus illuminating
privacy protection concerns when using AI. This paper provides a promising new
direction for research in the field of AI-based database management. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07351v1' target="_blank">http://arxiv.org/pdf/2303.07351v1</a><br> <br> <br> <font size='5'> 695 </font> <div style="text-align: right"> 2023-03-11 14:43:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents prompt design techniques for software engineering, in the
form of patterns, to solve common problems when using large language models
(LLMs), such as ChatGPT to automate common software engineering activities,
such as ensuring code is decoupled from third-party libraries and simulating a
web application API before it is implemented. This paper provides two
contributions to research on using LLMs for software engineering. First, it
provides a catalog of patterns for software engineering that classifies
patterns according to the types of problems they solve. Second, it explores
several prompt patterns that have been applied to improve requirements
elicitation, rapid prototyping, code quality, refactoring, and system design. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07839v1' target="_blank">http://arxiv.org/pdf/2303.07839v1</a><br> <br> <br> <font size='5'> 696 </font> <div style="text-align: right"> 2023-03-11 13:54:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Art-ificial Intelligence: The Effect of AI Disclosure on Evaluations of Creative Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of generative AI technologies, such as OpenAI's ChatGPT
chatbot, has expanded the scope of tasks that AI tools can accomplish and
enabled AI-generated creative content. In this study, we explore how disclosure
regarding the use of AI in the creation of creative content affects human
evaluation of such content. In a series of pre-registered experimental studies,
we show that AI disclosure has no meaningful effect on evaluation either for
creative or descriptive short stories, but that AI disclosure has a negative
effect on evaluations for emotionally evocative poems written in the first
person. We interpret this result to suggest that reactions to AI-generated
content may be negative when the content is viewed as distinctly "human." We
discuss the implications of this work and outline planned pathways of research
to better understand whether and when AI disclosure may affect the evaluation
of creative content. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06217v1' target="_blank">http://arxiv.org/pdf/2303.06217v1</a><br> <br> <br> <font size='5'> 697 </font> <div style="text-align: right"> 2023-03-11 01:19:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Consistency Analysis of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, a question-and-answer dialogue system based on a large language
model, has gained huge popularity since its introduction. Its positive aspects
have been reported through many media platforms, and some analyses even showed
that ChatGPT achieved a decent grade in professional exams, including the law,
medical, and finance domains, adding extra support to the claim that AI now can
assist and, even, replace humans in industrial fields. Others, however, doubt
its reliability and trustworthiness. In this paper, we investigate ChatGPT's
trustworthiness regarding logically consistent behaviours. Our findings suggest
that, although ChatGPT seems to achieve an improved language understanding
ability, it still fails to generate logically correct predictions frequently.
Hence, while it is true that ChatGPT is an impressive and promising new
technique, we conclude that its usage in real-world applications without
thorough human inspection requires further consideration, especially for
risk-sensitive areas. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06273v1' target="_blank">http://arxiv.org/pdf/2303.06273v1</a><br> <br> <br> <font size='5'> 698 </font> <div style="text-align: right"> 2023-03-10 16:21:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT as the Transportation Equity Information Source for Scientific Writing</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Transportation equity is an interdisciplinary agenda that requires both
transportation and social inputs. Traditionally, transportation equity
information are sources from public libraries, conferences, televisions, social
media, among other. Artificial intelligence (AI) tools including advanced
language models such as ChatGPT are becoming favorite information sources.
However, their credibility has not been well explored. This study explored the
content and usefulness of ChatGPT-generated information related to
transportation equity. It utilized 152 papers retrieved through the Web of
Science (WoS) repository. The prompt was crafted for ChatGPT to provide an
abstract given the title of the paper. The ChatGPT-based abstracts were then
compared to human-written abstracts using statistical tools and unsupervised
text mining. The results indicate that a weak similarity between ChatGPT and
human-written abstracts. On average, the human-written abstracts and ChatGPT
generated abstracts were about 58% similar, with a maximum and minimum of 97%
and 1.4%, respectively. The keywords from the abstracts of papers with over the
mean similarity score were more likely to be similar whereas those from below
the average score were less likely to be similar. Themes with high similarity
scores include access, public transit, and policy, among others. Further, clear
differences in the key pattern of clusters for high and low similarity score
abstracts was observed. Contrarily, the findings from collocated keywords were
inconclusive. The study findings suggest that ChatGPT has the potential to be a
source of transportation equity information. However, currently, a great amount
of attention is needed before a user can utilize materials from ChatGPT </font><br> Link: <a href='http://arxiv.org/pdf/2303.11158v1' target="_blank">http://arxiv.org/pdf/2303.11158v1</a><br> <br> <br> <font size='5'> 699 </font> <div style="text-align: right"> 2023-03-10 15:35:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent success of ChatGPT and GPT-4 has drawn widespread attention to
multimodal dialogue systems. However, the academia community lacks a dataset
that can validate the multimodal generation capabilities of Visual Language
Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new
multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually
pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs
and outputs. Additionally, to enable the multimodal system to reject human
requests (i.e., demonstrate accountability), as in language-based ChatGPT
conversations, we develop and incorporate specific rules into the datasets as
supervisory signals. This allows the trained VLM to provide a yes or no answer
after visual and textual reasoning, accompanied by a language explanation as to
why the human instruction cannot be excuted. In our method, we propose a
two-state training procedure to train the image auto-encoder and
auto-regressive transformer from scratch. The first state involves a discrete
variational autoencoder (dVAE) to compress each image into short tokens, which
are then concatenated with text tokens as a single data stream to be fed into
the decoder-based transformer for generating visual re-creation and textual
feedback in the second state. We provide comprehensive analyses of experimental
results in terms of re-created image quality, answer accuracy, and the model
behavior when faced with uncertainty and imperfect user queries. We hope our
explorations and findings contribute valuable insights regarding the
accountability of textual-visual generative models. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05983v2' target="_blank">http://arxiv.org/pdf/2303.05983v2</a><br> <br> <br> <font size='5'> 700 </font> <div style="text-align: right"> 2023-03-10 10:47:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does ChatGPT resemble humans in language use?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have
shown remarkable capacities in comprehending and producing language. However,
their internal workings remain a black box in cognitive terms, and it is
unclear whether LLMs and chatbots can develop humanlike characteristics in
language use. Cognitive scientists have devised many experiments that probe,
and have made great progress in explaining, how people process language. We
subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000
runs per experiment. In 10 of them, ChatGPT replicated the human pattern of
language use. It associated unfamiliar words with different meanings depending
on their forms, continued to access recently encountered meanings of ambiguous
words, reused recent sentence structures, reinterpreted implausible sentences
that were likely to have been corrupted by noise, glossed over errors, drew
reasonable inferences, associated causality with different discourse entities
according to verb semantics, and accessed different meanings and retrieved
different words depending on the identity of its interlocutor. However, unlike
humans, it did not prefer using shorter words to convey less informative
content and it did not use context to disambiguate syntactic ambiguities. We
discuss how these convergences and divergences may occur in the transformer
architecture. Overall, these experiments demonstrate that LLM-driven chatbots
like ChatGPT are capable of mimicking human language processing to a great
extent, and that they have the potential to provide insights into how people
learn and use language. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08014v1' target="_blank">http://arxiv.org/pdf/2303.08014v1</a><br> <br> <br> <font size='5'> 701 </font> <div style="text-align: right"> 2023-03-09 17:52:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) are used to generate content for a wide range of
tasks, and are set to reach a growing audience in coming years due to
integration in product interfaces like ChatGPT or search engines like Bing.
This intensifies the need to ensure that models are aligned with human
preferences and do not produce unsafe, inaccurate or toxic outputs. While
alignment techniques like reinforcement learning with human feedback (RLHF) and
red-teaming can mitigate some safety concerns and improve model capabilities,
it is unlikely that an aggregate fine-tuning process can adequately represent
the full range of users' preferences and values. Different people may
legitimately disagree on their preferences for language and conversational
norms, as well as on values or ideologies which guide their communication.
Personalising LLMs through micro-level preference learning processes may result
in models that are better aligned with each user. However, there are several
normative challenges in defining the bounds of a societally-acceptable and safe
degree of personalisation. In this paper, we ask how, and in what ways, LLMs
should be personalised. First, we review literature on current paradigms for
aligning LLMs with human feedback, and identify issues including (i) a lack of
clarity regarding what alignment means; (ii) a tendency of technology providers
to prescribe definitions of inherently subjective preferences and values; and
(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in
who we are really aligning to. Second, we present a taxonomy of benefits and
risks associated with personalised LLMs, for individuals and society at large.
Finally, we propose a three-tiered policy framework that allows users to
experience the benefits of personalised alignment, while restraining unsafe and
undesirable LLM-behaviours within (supra-)national and organisational bounds. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05453v1' target="_blank">http://arxiv.org/pdf/2303.05453v1</a><br> <br> <br> <font size='5'> 702 </font> <div style="text-align: right"> 2023-03-09 16:42:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Following the hype around OpenAI's ChatGPT conversational agent, the last
straw in the recent development of Large Language Models (LLMs) that
demonstrate emergent unprecedented zero-shot capabilities, we audit the latest
OpenAI's GPT-3.5 model, `gpt-3.5-turbo', the first available ChatGPT model, in
the LexGLUE benchmark in a zero-shot fashion providing examples in a templated
instruction-following format. The results indicate that ChatGPT achieves an
average micro-F1 score of 47.6% across LexGLUE tasks, surpassing the baseline
guessing rates. Notably, the model performs exceptionally well in some
datasets, achieving micro-F1 scores of 62.8% and 70.2% in the ECtHR B and
LEDGAR datasets, respectively. The code base and model predictions are
available for review on https://github.com/coastalcph/zeroshot_lexglue. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12202v1' target="_blank">http://arxiv.org/pdf/2304.12202v1</a><br> <br> <br> <font size='5'> 703 </font> <div style="text-align: right"> 2023-03-09 15:46:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Advanced large language models like ChatGPT have gained considerable
attention recently, including among students. However, while the debate on
ChatGPT in academia is making waves, more understanding is needed among
lecturers and teachers on how students use and perceive ChatGPT. To address
this gap, we analyzed the content on ChatGPT available on TikTok in February
2023. TikTok is a rapidly growing social media platform popular among
individuals under 30. Specifically, we analyzed the content of the 100 most
popular videos in English tagged with #chatgpt, which collectively garnered
over 250 million views. Most of the videos we studied promoted the use of
ChatGPT for tasks like writing essays or code. In addition, many videos
discussed AI detectors, with a focus on how other tools can help to transform
ChatGPT output to fool these detectors. This also mirrors the discussion among
educators on how to treat ChatGPT as lecturers and teachers in teaching and
grading. What is, however, missing from the analyzed clips on TikTok are videos
that discuss ChatGPT producing content that is nonsensical or unfaithful to the
training data. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05349v1' target="_blank">http://arxiv.org/pdf/2303.05349v1</a><br> <br> <br> <font size='5'> 704 </font> <div style="text-align: right"> 2023-03-09 06:24:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated
remarkable results in various natural language processing (NLP) tasks with
in-context learning, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no investigation has been
conducted to assess the ability of LLMs to perform document information
extraction (DIE) using in-context learning. Applying LLMs to DIE poses two
challenges: the modality and task gap. To this end, we propose a simple but
effective in-context learning framework called ICL-D3IE, which enables LLMs to
perform DIE with different types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard training documents
as hard demonstrations for benefiting all test instances. We design
demonstrations describing relationships that enable LLMs to understand
positional relationships. We introduce formatting demonstrations for easy
answer extraction. Additionally, the framework improves diverse demonstrations
by updating them iteratively. Our experiments on three widely used benchmark
datasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to
achieve superior performance when compared to previous pre-trained methods
fine-tuned with full training in both the in-distribution (ID) setting and in
the out-of-distribution (OOD) setting. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05063v2' target="_blank">http://arxiv.org/pdf/2303.05063v2</a><br> <br> <br> <font size='5'> 705 </font> <div style="text-align: right"> 2023-03-08 21:44:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As AI systems proliferate, their greenhouse gas emissions are an increasingly
important concern for human societies. We analyze the emissions of several AI
systems (ChatGPT, BLOOM, DALL-E2, Midjourney) relative to those of humans
completing the same tasks. We find that an AI writing a page of text emits 130
to 1500 times less CO2e than a human doing so. Similarly, an AI creating an
image emits 310 to 2900 times less. Emissions analysis do not account for
social impacts such as professional displacement, legality, and rebound
effects. In addition, AI is not a substitute for all human tasks. Nevertheless,
at present, the use of AI holds the potential to carry out several major
activities at much lower emission levels than can humans. </font><br> Link: <a href='http://arxiv.org/pdf/2303.06219v1' target="_blank">http://arxiv.org/pdf/2303.06219v1</a><br> <br> <br> <font size='5'> 706 </font> <div style="text-align: right"> 2023-03-08 20:45:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: FaceChat: An Emotion-Aware Face-to-face Dialogue Framework</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: While current dialogue systems like ChatGPT have made significant
advancements in text-based interactions, they often overlook the potential of
other modalities in enhancing the overall user experience. We present FaceChat,
a web-based dialogue framework that enables emotionally-sensitive and
face-to-face conversations. By seamlessly integrating cutting-edge technologies
in natural language processing, computer vision, and speech processing,
FaceChat delivers a highly immersive and engaging user experience. FaceChat
framework has a wide range of potential applications, including counseling,
emotional support, and personalized customer service. The system is designed to
be simple and flexible as a platform for future researchers to advance the
field of multimodal dialogue systems. The code is publicly available at
https://github.com/qywu/FaceChat. </font><br> Link: <a href='http://arxiv.org/pdf/2303.07316v1' target="_blank">http://arxiv.org/pdf/2303.07316v1</a><br> <br> <br> <font size='5'> 707 </font> <div style="text-align: right"> 2023-03-08 15:50:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is attracting a cross-field interest as it provides a language
interface with remarkable conversational competency and reasoning capabilities
across many domains. However, since ChatGPT is trained with languages, it is
currently not capable of processing or generating images from the visual world.
At the same time, Visual Foundation Models, such as Visual Transformers or
Stable Diffusion, although showing great visual understanding and generation
capabilities, they are only experts on specific tasks with one-round fixed
inputs and outputs. To this end, We build a system called \textbf{Visual
ChatGPT}, incorporating different Visual Foundation Models, to enable the user
to interact with ChatGPT by 1) sending and receiving not only languages but
also images 2) providing complex visual questions or visual editing
instructions that require the collaboration of multiple AI models with
multi-steps. 3) providing feedback and asking for corrected results. We design
a series of prompts to inject the visual model information into ChatGPT,
considering models of multiple inputs/outputs and models that require visual
feedback. Experiments show that Visual ChatGPT opens the door to investigating
the visual roles of ChatGPT with the help of Visual Foundation Models. Our
system is publicly available at
\url{https://github.com/microsoft/visual-chatgpt}. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04671v1' target="_blank">http://arxiv.org/pdf/2303.04671v1</a><br> <br> <br> <font size='5'> 708 </font> <div style="text-align: right"> 2023-03-08 15:46:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Participates in a Computer Science Exam</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We asked ChatGPT to participate in an undergraduate computer science exam on
''Algorithms and Data Structures''. The program was evaluated on the entire
exam as posed to the students. We hand-copied its answers onto an exam sheet,
which was subsequently graded in a blind setup alongside those of 200
participating students. We find that ChatGPT narrowly passed the exam,
obtaining 20.5 out of 40 points. This impressive performance indicates that
ChatGPT can indeed succeed in challenging tasks like university exams. At the
same time, the questions in our exam are structurally similar to those of other
exams, solved homework problems, and teaching materials that can be found
online and might have been part of ChatGPT's training data. Therefore, it would
be inadequate to conclude from this experiment that ChatGPT has any
understanding of computer science. We also assess the improvements brought by
GPT-4. We find that GPT-4 would have obtained about 17\% more exam points than
GPT-3.5, reaching the performance of the average student. The transcripts of
our conversations with ChatGPT are available at
\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire
graded exam is in the appendix of this paper. </font><br> Link: <a href='http://arxiv.org/pdf/2303.09461v2' target="_blank">http://arxiv.org/pdf/2303.09461v2</a><br> <br> <br> <font size='5'> 709 </font> <div style="text-align: right"> 2023-03-08 03:56:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Does Synthetic Data Generation of LLMs Help Clinical Text Mining?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent advancements in large language models (LLMs) have led to the
development of highly potent models like OpenAI's ChatGPT. These models have
exhibited exceptional performance in a variety of tasks, such as question
answering, essay composition, and code generation. However, their effectiveness
in the healthcare sector remains uncertain. In this study, we seek to
investigate the potential of ChatGPT to aid in clinical text mining by
examining its ability to extract structured information from unstructured
healthcare texts, with a focus on biological named entity recognition and
relation extraction. However, our preliminary results indicate that employing
ChatGPT directly for these tasks resulted in poor performance and raised
privacy concerns associated with uploading patients' information to the ChatGPT
API. To overcome these limitations, we propose a new training paradigm that
involves generating a vast quantity of high-quality synthetic data with labels
utilizing ChatGPT and fine-tuning a local model for the downstream task. Our
method has resulted in significant improvements in the performance of
downstream tasks, improving the F1-score from 23.37% to 63.99% for the named
entity recognition task and from 75.86% to 83.59% for the relation extraction
task. Furthermore, generating data using ChatGPT can significantly reduce the
time and effort required for data collection and labeling, as well as mitigate
data privacy concerns. In summary, the proposed framework presents a promising
solution to enhance the applicability of LLM models to clinical text mining. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04360v2' target="_blank">http://arxiv.org/pdf/2303.04360v2</a><br> <br> <br> <font size='5'> 710 </font> <div style="text-align: right"> 2023-03-07 23:32:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Many bioinformatics programming tasks can be automated with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Computer programming is a fundamental tool for life scientists, allowing them
to carry out many essential research tasks. However, despite a variety of
educational efforts, learning to write code can be a challenging endeavor for
both researchers and students in life science disciplines. Recent advances in
artificial intelligence have made it possible to translate human-language
prompts to functional code, raising questions about whether these technologies
can aid (or replace) life scientists' efforts to write code. Using 184
programming exercises from an introductory-bioinformatics course, we evaluated
the extent to which one such model -- OpenAI's ChatGPT -- can successfully
complete basic- to moderate-level programming tasks. On its first attempt,
ChatGPT solved 139 (75.5%) of the exercises. For the remaining exercises, we
provided natural-language feedback to the model, prompting it to try different
approaches. Within 7 or fewer attempts, ChatGPT solved 179 (97.3%) of the
exercises. These findings have important implications for life-sciences
research and education. For many programming tasks, researchers no longer need
to write code from scratch. Instead, machine-learning models may produce usable
solutions. Instructors may need to adapt their pedagogical approaches and
assessment techniques to account for these new capabilities that are available
to the general public. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13528v1' target="_blank">http://arxiv.org/pdf/2303.13528v1</a><br> <br> <br> <font size='5'> 711 </font> <div style="text-align: right"> 2023-03-07 20:36:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant
attention from society. As a result, many individuals have become interested in
related resources and are seeking to uncover the background and secrets behind
its impressive performance. In fact, ChatGPT and other Generative AI (GAI)
techniques belong to the category of Artificial Intelligence Generated Content
(AIGC), which involves the creation of digital content, such as images, music,
and natural language, through AI models. The goal of AIGC is to make the
content creation process more efficient and accessible, allowing for the
production of high-quality content at a faster pace. AIGC is achieved by
extracting and understanding intent information from instructions provided by
human, and generating the content according to its knowledge and the intent
information. In recent years, large-scale models have become increasingly
important in AIGC as they provide better intent extraction and thus, improved
generation results. With the growth of data and the size of the models, the
distribution that the model can learn becomes more comprehensive and closer to
reality, leading to more realistic and high-quality content generation. This
survey provides a comprehensive review on the history of generative models, and
basic components, recent advances in AIGC from unimodal interaction and
multimodal interaction. From the perspective of unimodality, we introduce the
generation tasks and relative models of text and image. From the perspective of
multimodality, we introduce the cross-application between the modalities
mentioned above. Finally, we discuss the existing open problems and future
challenges in AIGC. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04226v1' target="_blank">http://arxiv.org/pdf/2303.04226v1</a><br> <br> <br> <font size='5'> 712 </font> <div style="text-align: right"> 2023-03-07 16:57:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a Good NLG Evaluator? A Preliminary Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, the emergence of ChatGPT has attracted wide attention from the
computational linguistics community. Many prior studies have shown that ChatGPT
achieves remarkable performance on various NLP tasks in terms of automatic
evaluation metrics. However, the ability of ChatGPT to serve as an evaluation
metric is still underexplored. Considering assessing the quality of natural
language generation (NLG) models is an arduous task and NLG metrics notoriously
show their poor correlation with human judgments, we wonder whether ChatGPT is
a good NLG evaluation metric. In this report, we provide a preliminary
meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,
we regard ChatGPT as a human evaluator and give task-specific (e.g.,
summarization) and aspect-specific (e.g., relevance) instruction to prompt
ChatGPT to evaluate the generated results of NLG models. We conduct experiments
on five NLG meta-evaluation datasets (including summarization, story generation
and data-to-text tasks). Experimental results show that compared with previous
automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation
with human judgments in most cases. In addition, we find that the effectiveness
of the ChatGPT evaluator might be influenced by the creation method of the
meta-evaluation datasets. For the meta-evaluation datasets which are created
greatly depending on the reference and thus are biased, the ChatGPT evaluator
might lose its effectiveness. We hope our preliminary study could prompt the
emergence of a general-purposed reliable NLG metric. </font><br> Link: <a href='http://arxiv.org/pdf/2303.04048v2' target="_blank">http://arxiv.org/pdf/2303.04048v2</a><br> <br> <br> <font size='5'> 713 </font> <div style="text-align: right"> 2023-03-07 16:44:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Making a Computational Attorney</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This "blue sky idea" paper outlines the opportunities and challenges in data
mining and machine learning involving making a computational attorney -- an
intelligent software agent capable of helping human lawyers with a wide range
of complex high-level legal tasks such as drafting legal briefs for the
prosecution or defense in court. In particular, we discuss what a ChatGPT-like
Large Legal Language Model (L$^3$M) can and cannot do today, which will inspire
researchers with promising short-term and long-term research objectives. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05383v1' target="_blank">http://arxiv.org/pdf/2303.05383v1</a><br> <br> <br> <font size='5'> 714 </font> <div style="text-align: right"> 2023-03-07 14:59:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has shown strong capabilities in natural language generation tasks,
which naturally leads researchers to explore where its abilities end. In this
paper, we examine whether ChatGPT can be used for zero-shot text
classification, more specifically, automatic genre identification. We compare
ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on
datasets, manually annotated with genres. The models are compared on test sets
in two languages: English and Slovenian. Results show that ChatGPT outperforms
the fine-tuned model when applied to the dataset which was not seen before by
either of the models. Even when applied on Slovenian language as an
under-resourced language, ChatGPT's performance is no worse than when applied
to English. However, if the model is fully prompted in Slovenian, the
performance drops significantly, showing the current limitations of ChatGPT
usage on smaller languages. The presented results lead us to questioning
whether this is the beginning of an end of laborious manual annotation
campaigns even for smaller languages, such as Slovenian. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03953v2' target="_blank">http://arxiv.org/pdf/2303.03953v2</a><br> <br> <br> <font size='5'> 715 </font> <div style="text-align: right"> 2023-03-07 12:03:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Feasibility of ChatGPT for Event Extraction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Event extraction is a fundamental task in natural language processing that
involves identifying and extracting information about events mentioned in text.
However, it is a challenging task due to the lack of annotated data, which is
expensive and time-consuming to obtain. The emergence of large language models
(LLMs) such as ChatGPT provides an opportunity to solve language tasks with
simple prompts without the need for task-specific datasets and fine-tuning.
While ChatGPT has demonstrated impressive results in tasks like machine
translation, text summarization, and question answering, it presents challenges
when used for complex tasks like event extraction. Unlike other tasks, event
extraction requires the model to be provided with a complex set of instructions
defining all event types and their schemas. To explore the feasibility of
ChatGPT for event extraction and the challenges it poses, we conducted a series
of experiments. Our results show that ChatGPT has, on average, only 51.04% of
the performance of a task-specific model such as EEQA in long-tail and complex
scenarios. Our usability testing experiments indicate that ChatGPT is not
robust enough, and continuous refinement of the prompt does not lead to stable
performance improvements, which can result in a poor user experience. Besides,
ChatGPT is highly sensitive to different prompt styles. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03836v2' target="_blank">http://arxiv.org/pdf/2303.03836v2</a><br> <br> <br> <font size='5'> 716 </font> <div style="text-align: right"> 2023-03-06 16:36:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, developed by OpenAI, is one of the milestone large language models
(LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive
language understanding capability of LLM, particularly in generating
conversational response. As LLMs start to gain more attention in various
research or engineering domains, it is time to envision how LLM may
revolutionize the way we approach intelligent transportation systems. This
paper explores the future applications of LLM in addressing key transportation
problems. By leveraging LLM with cross-modal encoder, an intelligent system can
also process traffic data from different modalities and execute transportation
operations through an LLM. We present and validate these potential
transportation applications equipped by LLM. To further demonstrate this
potential, we also provide a concrete smartphone-based crash report
auto-generation and analysis framework as a use case. Despite the potential
benefits, challenges related to data privacy, data quality, and model bias must
be considered. Overall, the use of LLM in intelligent transport systems holds
promise for more efficient, intelligent, and sustainable transportation systems
that further improve daily life around the world. </font><br> Link: <a href='http://arxiv.org/pdf/2303.05382v2' target="_blank">http://arxiv.org/pdf/2303.05382v2</a><br> <br> <br> <font size='5'> 717 </font> <div style="text-align: right"> 2023-03-06 06:47:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Systems that support users in the automatic creation of visualizations must
address several subtasks - understand the semantics of data, enumerate relevant
visualization goals and generate visualization specifications. In this work, we
pose visualization generation as a multi-stage generation problem and argue
that well-orchestrated pipelines based on large language models (LLMs) such as
ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing
these tasks. We present LIDA, a novel tool for generating grammar-agnostic
visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER
that converts data into a rich but compact natural language summary, a GOAL
EXPLORER that enumerates visualization goals given the data, a VISGENERATOR
that generates, refines, executes and filters visualization code and an
INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA
provides a python api, and a hybrid user interface (direct manipulation and
multilingual natural language) for interactive chart, infographics and data
story generation. Learn more about the project here -
https://microsoft.github.io/lida/ </font><br> Link: <a href='http://arxiv.org/pdf/2303.02927v3' target="_blank">http://arxiv.org/pdf/2303.02927v3</a><br> <br> <br> <font size='5'> 718 </font> <div style="text-align: right"> 2023-03-06 04:49:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Is it possible for machines to think like humans? And if it is, how should we
go about teaching them to do so? As early as 1950, Alan Turing stated that we
ought to teach machines in the way of teaching a child. Reinforcement learning
with human feedback (RLHF) has emerged as a strong candidate toward allowing
agents to learn from human feedback in a naturalistic manner. RLHF is distinct
from traditional reinforcement learning as it provides feedback from a human
teacher in addition to a reward signal. It has been catapulted into public view
by multiple high-profile AI applications, including OpenAI's ChatGPT,
DeepMind's Sparrow, and Anthropic's Claude. These highly capable chatbots are
already overturning our understanding of how AI interacts with humanity. The
wide applicability and burgeoning success of RLHF strongly motivate the need to
evaluate its social impacts. In light of recent developments, this paper
considers an important question: can RLHF be developed and used without
negatively affecting human societies? Our objectives are threefold: to provide
a systematic study of the social effects of RLHF; to identify key social and
ethical issues of RLHF; and to discuss social impacts for stakeholders.
Although text-based applications of RLHF have received much attention, it is
crucial to consider when evaluating its social implications the diverse range
of areas to which it may be deployed. We describe seven primary ways in which
RLHF-based technologies will affect society by positively transforming human
experiences with AI. This paper ultimately proposes that RLHF has potential to
net positively impact areas of misinformation, AI value-alignment, bias, AI
access, cross-cultural dialogue, industry, and workforce. As RLHF raises
concerns that echo those of existing AI technologies, it will be important for
all to be aware and intentional in the adoption of RLHF. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02891v1' target="_blank">http://arxiv.org/pdf/2303.02891v1</a><br> <br> <br> <font size='5'> 719 </font> <div style="text-align: right"> 2023-03-03 16:11:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has shown the potential of emerging general artificial intelligence
capabilities, as it has demonstrated competent performance across many natural
language processing tasks. In this work, we evaluate the capabilities of
ChatGPT to perform text classification on three affective computing problems,
namely, big-five personality prediction, sentiment analysis, and suicide
tendency detection. We utilise three baselines, a robust language model
(RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and
a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for
a specific downstream task generally has a superior performance. On the other
hand, ChatGPT provides decent results, and is relatively comparable to the
Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy
data, where Word2Vec models achieve worse results due to noise. Results
indicate that ChatGPT is a good generalist model that is capable of achieving
good results across various problems without any specialised training, however,
it is not as good as a specialised model for a downstream task. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03186v1' target="_blank">http://arxiv.org/pdf/2303.03186v1</a><br> <br> <br> <font size='5'> 720 </font> <div style="text-align: right"> 2023-03-03 04:27:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Although large conversational AI models such as OpenAI's ChatGPT have
demonstrated great potential, we question whether such models can guarantee
factual accuracy. Recently, technology companies such as Microsoft and Google
have announced new services which aim to combine search engines with
conversational AI. However, we have found numerous mistakes in the public
demonstrations that suggest we should not easily trust the factual claims of
the AI models. Rather than criticizing specific models or companies, we hope to
call on researchers and developers to improve AI models' transparency and
factual correctness. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11076v1' target="_blank">http://arxiv.org/pdf/2304.11076v1</a><br> <br> <br> <font size='5'> 721 </font> <div style="text-align: right"> 2023-03-03 04:26:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Ask and You Shall Receive (a Graph Drawing): Testing ChatGPT's Potential to Apply Graph Layout Algorithms</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have recently taken the world by storm. They can
generate coherent text, hold meaningful conversations, and be taught concepts
and basic sets of instructions - such as the steps of an algorithm. In this
context, we are interested in exploring the application of LLMs to graph
drawing algorithms by performing experiments on ChatGPT. These algorithms are
used to improve the readability of graph visualizations. The probabilistic
nature of LLMs presents challenges to implementing algorithms correctly, but we
believe that LLMs' ability to learn from vast amounts of data and apply complex
operations may lead to interesting graph drawing results. For example, we could
enable users with limited coding backgrounds to use simple natural language to
create effective graph visualizations. Natural language specification would
make data visualization more accessible and user-friendly for a wider range of
users. Exploring LLMs' capabilities for graph drawing can also help us better
understand how to formulate complex algorithms for LLMs; a type of knowledge
that could transfer to other areas of computer science. Overall, our goal is to
shed light on the exciting possibilities of using LLMs for graph drawing while
providing a balanced assessment of the challenges and opportunities they
present. A free copy of this paper with all supplemental materials required to
reproduce our results is available on
https://osf.io/n5rxd/?view_only=f09cbc2621f44074810b7d843f1e12f9 </font><br> Link: <a href='http://arxiv.org/pdf/2303.08819v1' target="_blank">http://arxiv.org/pdf/2303.08819v1</a><br> <br> <br> <font size='5'> 722 </font> <div style="text-align: right"> 2023-03-02 12:18:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9
"Multilingual Tweet Intimacy Analysis". We achieved second-best results in all
10 languages according to the official Pearson's correlation regression
evaluation measure. Our cross-lingual transfer learning approach explores the
benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates
only the regression head parameters and then also updates the pre-trained
transformer encoder parameters at a reduced learning rate. Additionally, we
study the impact of using a small set of automatically generated examples (in
our case, from ChatGPT) for low-resource settings where no human-labeled data
is available. Our study shows that HeFiT stabilizes training and consistently
improves results for pre-trained models that lack domain adaptation to tweets.
Our study also shows a noticeable performance increase in cross-lingual
learning when synthetic data is used, confirming the usefulness of current text
generation systems to improve zero-shot baseline results. Finally, we examine
how possible inconsistencies in the annotated data contribute to cross-lingual
interference issues. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01194v2' target="_blank">http://arxiv.org/pdf/2303.01194v2</a><br> <br> <br> <font size='5'> 723 </font> <div style="text-align: right"> 2023-03-02 11:04:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How will Language Modelers like ChatGPT Affect Occupations and Industries?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent dramatic increases in AI language modeling capabilities has led to
many questions about the effect of these technologies on the economy. In this
paper we present a methodology to systematically assess the extent to which
occupations, industries and geographies are exposed to advances in AI language
modeling capabilities. We find that the top occupations exposed to language
modeling include telemarketers and a variety of post-secondary teachers such as
English language and literature, foreign language and literature, and history
teachers. We find the top industries exposed to advances in language modeling
are legal services and securities, commodities, and investments. We also find a
positive correlation between wages and exposure to AI language modeling. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01157v2' target="_blank">http://arxiv.org/pdf/2303.01157v2</a><br> <br> <br> <font size='5'> 724 </font> <div style="text-align: right"> 2023-03-02 08:43:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI and the FCI: Can ChatGPT Project an Understanding of Introductory Physics?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a groundbreaking ``chatbot"--an AI interface built on a large
language model that was trained on an enormous corpus of human text to emulate
human conversation. Beyond its ability to converse in a plausible way, it has
attracted attention for its ability to competently answer questions from the
bar exam and from MBA coursework, and to provide useful assistance in writing
computer code. These apparent abilities have prompted discussion of ChatGPT as
both a threat to the integrity of higher education and conversely as a powerful
teaching tool. In this work we present a preliminary analysis of how two
versions of ChatGPT (ChatGPT3.5 and ChatGPT4) fare in the field of
first-semester university physics, using a modified version of the Force
Concept Inventory (FCI) to assess whether it can give correct responses to
conceptual physics questions about kinematics and Newtonian dynamics. We
demonstrate that, by some measures, ChatGPT3.5 can match or exceed the median
performance of a university student who has completed one semester of college
physics, though its performance is notably uneven and the results are nuanced.
By these same measures, we find that ChatGPT4's performance is approaching the
point of being indistinguishable from that of an expert physicist when it comes
to introductory mechanics topics. After the completion of our work we became
aware of Ref [1], which preceded us to publication and which completes an
extensive analysis of the abilities of ChatGPT3.5 in a physics class, including
a different modified version of the FCI. We view this work as confirming that
portion of their results, and extending the analysis to ChatGPT4, which shows
rapid and notable improvement in most, but not all respects. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01067v2' target="_blank">http://arxiv.org/pdf/2303.01067v2</a><br> <br> <br> <font size='5'> 725 </font> <div style="text-align: right"> 2023-03-01 12:11:23+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Succinct Representations for Concepts</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Foundation models like chatGPT have demonstrated remarkable performance on
various tasks. However, for many questions, they may produce false answers that
look accurate. How do we train the model to precisely understand the concepts?
In this paper, we introduce succinct representations of concepts based on
category theory. Such representation yields concept-wise invariance properties
under various tasks, resulting a new learning algorithm that can provably and
accurately learn complex concepts or fix misconceptions. Moreover, by
recursively expanding the succinct representations, one can generate a
hierarchical decomposition, and manually verify the concept by individually
examining each part inside the decomposition. </font><br> Link: <a href='http://arxiv.org/pdf/2303.00446v1' target="_blank">http://arxiv.org/pdf/2303.00446v1</a><br> <br> <br> <font size='5'> 726 </font> <div style="text-align: right"> 2023-03-01 06:16:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Assess Human Personalities? A General Evaluation Framework</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs) especially ChatGPT have produced impressive
results in various areas, but their potential human-like psychology is still
largely unexplored. Existing works study the virtual personalities of LLMs but
rarely explore the possibility of analyzing human personalities via LLMs. This
paper presents a generic evaluation framework for LLMs to assess human
personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,
we first devise unbiased prompts by randomly permuting options in MBTI
questions and adopt the average testing result to encourage more impartial
answer generation. Then, we propose to replace the subject in question
statements to enable flexible queries and assessments on different subjects
from LLMs. Finally, we re-formulate the question instructions in a manner of
correctness evaluation to facilitate LLMs to generate clearer responses. The
proposed framework enables LLMs to flexibly assess personalities of different
groups of people. We further propose three evaluation metrics to measure the
consistency, robustness, and fairness of assessment results from
state-of-the-art LLMs including ChatGPT and InstructGPT. Our experiments reveal
ChatGPT's ability to assess human personalities, and the average results
demonstrate that it can achieve more consistent and fairer assessments in spite
of lower robustness against prompt biases compared with InstructGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01248v2' target="_blank">http://arxiv.org/pdf/2303.01248v2</a><br> <br> <br> <font size='5'> 727 </font> <div style="text-align: right"> 2023-02-28 12:23:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Large Language Models Are State-of-the-Art Evaluators of Translation Quality</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We describe GEMBA, a GPT-based metric for assessment of translation quality,
which works both with a reference translation and without. In our evaluation,
we focus on zero-shot prompting, comparing four prompt variants in two modes,
based on the availability of the reference. We investigate nine versions of GPT
models, including ChatGPT and GPT-4. We show that our method for translation
quality assessment only works with GPT~3.5 and larger models. Comparing to
results from WMT22's Metrics shared task, our method achieves state-of-the-art
accuracy in both modes when compared to MQM-based human labels. Our results are
valid on the system level for all three WMT22 Metrics shared task language
pairs, namely English into German, English into Russian, and Chinese into
English. This provides a first glimpse into the usefulness of pre-trained,
generative large language models for quality assessment of translations. We
publicly release all our code and prompt templates used for the experiments
described in this work, as well as all corresponding scoring results, to allow
for external validation and reproducibility. </font><br> Link: <a href='http://arxiv.org/pdf/2302.14520v2' target="_blank">http://arxiv.org/pdf/2302.14520v2</a><br> <br> <br> <font size='5'> 728 </font> <div style="text-align: right"> 2023-02-28 01:27:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-Shot Cross-Lingual Summarization via Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has
attracted wide attention from the computational linguistics community. However,
it is not yet known the performance of LLMs on CLS. In this report, we
empirically use various prompts to guide LLMs to perform zero-shot CLS from
different paradigms (i.e., end-to-end and pipeline), and provide a preliminary
evaluation on the generated summaries. We find that ChatGPT and GPT-4
originally prefer to produce lengthy summaries with detailed information. These
two LLMs can further balance informativeness and conciseness with the help of
an interactive prompt, significantly improving their CLS performance.
Experimental results on three widely-used CLS datasets show that GPT-4 achieves
state-of-the-art zero-shot CLS performance, and performs competitively compared
with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and
bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited
zero-shot CLS ability. Due to the composite nature of CLS, which requires
models to perform summarization and translation simultaneously, accomplishing
this task in a zero-shot manner is even a challenge for LLMs. Therefore, we
sincerely hope and recommend future LLM research could use CLS as a testbed. </font><br> Link: <a href='http://arxiv.org/pdf/2302.14229v3' target="_blank">http://arxiv.org/pdf/2302.14229v3</a><br> <br> <br> <font size='5'> 729 </font> <div style="text-align: right"> 2023-02-27 14:26:29+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of an AI-powered chatbot that can generate human-like sentences
and write coherent essays has caught the world's attention. This paper
discusses the historical overview of chatbots and the technology behind Chat
Generative Pre-trained Transformer, better known as ChatGPT. Moreover,
potential applications of ChatGPT in various domains, including healthcare,
education, and research, are highlighted. Despite promising results, there are
several privacy and ethical concerns surrounding ChatGPT. In addition, we
highlight some of the important limitations of the current version of ChatGPT.
We also ask ChatGPT to provide its point of view and present its responses to
several questions we attempt to answer. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13817v3' target="_blank">http://arxiv.org/pdf/2302.13817v3</a><br> <br> <br> <font size='5'> 730 </font> <div style="text-align: right"> 2023-02-26 16:32:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Towards Human-Bot Collaborative Software Architecting with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Architecting software-intensive systems can be a complex process. It deals
with the daunting tasks of unifying stakeholders' perspectives, designers'
intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a
blueprint that guides software implementation and evaluation. Despite its
benefits, architecture-centric software engineering (ACSE) inherits a multitude
of challenges. ACSE challenges could stem from a lack of standardized
processes, socio-technical limitations, and scarcity of human expertise etc.
that can impede the development of existing and emergent classes of software
(e.g., IoTs, blockchain, quantum systems). Software Development Bots (DevBots)
trained on large language models can help synergise architects' knowledge with
artificially intelligent decision support to enable rapid architecting in a
human-bot collaborative ACSE. An emerging solution to enable this collaboration
is ChatGPT, a disruptive technology not primarily introduced for software
engineering, but is capable of articulating and refining architectural
artifacts based on natural language processing. We detail a case study that
involves collaboration between a novice software architect and ChatGPT for
architectural analysis, synthesis, and evaluation of a services-driven software
application. Preliminary results indicate that ChatGPT can mimic an architect's
role to support and often lead ACSE, however; it requires human oversight and
decision support for collaborative architecting. Future research focuses on
harnessing empirical evidence about architects' productivity and exploring
socio-technical aspects of architecting with ChatGPT to tackle emerging and
futuristic challenges of ACSE. </font><br> Link: <a href='http://arxiv.org/pdf/2302.14600v1' target="_blank">http://arxiv.org/pdf/2302.14600v1</a><br> <br> <br> <font size='5'> 731 </font> <div style="text-align: right"> 2023-02-26 02:42:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Fast Attention Requires Bounded Entries</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In modern machine learning, inner product attention computation is a
fundamental task for training large language models such as Transformer, GPT-1,
BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as
input three matrices $Q, K, V \in [-B,B]^{n \times d}$, and the goal is to
construct the matrix $\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A
V \in \mathbb{R}^{n \times d}$, where $A = \exp(QK^\top/d)$ is the `attention
matrix', and $\exp$ is applied entry-wise. Straightforward methods for this
problem explicitly compute the $n \times n$ attention matrix $A$, and hence
require time $\Omega(n^2)$ even when $d = n^{o(1)}$ is small.
  In this paper, we investigate whether faster algorithms are possible by
implicitly making use of the matrix $A$. We present two results, showing that
there is a sharp transition at $B = \Theta(\sqrt{\log n})$.
  $\bullet$ If $d = O(\log n)$ and $B = o(\sqrt{\log n})$, there is an
$n^{1+o(1)}$ time algorithm to approximate $\mathrm{Att}(Q,K,V)$ up to
$1/\mathrm{poly}(n)$ additive error.
  $\bullet$ If $d = O(\log n)$ and $B = \Theta (\sqrt{\log n})$, assuming the
Strong Exponential Time Hypothesis from fine-grained complexity theory, it is
impossible to approximate $\mathrm{Att}(Q,K,V)$ up to $1/\mathrm{poly}(n)$
additive error in truly subquadratic time $n^{2 - \Omega(1)}$.
  This gives a theoretical explanation for the phenomenon observed in practice
that attention computation is much more efficient when the input matrices have
smaller entries. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13214v2' target="_blank">http://arxiv.org/pdf/2302.13214v2</a><br> <br> <br> <font size='5'> 732 </font> <div style="text-align: right"> 2023-02-25 18:29:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Deployment of reinforcement learning algorithms for robotics applications in
the real world requires ensuring the safety of the robot and its environment.
Safe robot reinforcement learning (SRRL) is a crucial step towards achieving
human-robot coexistence. In this paper, we envision a human-centered SRRL
framework consisting of three stages: safe exploration, safety value alignment,
and safe collaboration. We examine the research gaps in these areas and propose
to leverage interactive behaviors for SRRL. Interactive behaviors enable
bi-directional information transfer between humans and robots, such as
conversational robot ChatGPT. We argue that interactive behaviors need further
attention from the SRRL community. We discuss four open challenges related to
the robustness, efficiency, transparency, and adaptability of SRRL with
interactive behaviors. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13137v2' target="_blank">http://arxiv.org/pdf/2302.13137v2</a><br> <br> <br> <font size='5'> 733 </font> <div style="text-align: right"> 2023-02-25 06:58:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AugGPT: Leveraging ChatGPT for Text Data Augmentation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Text data augmentation is an effective strategy for overcoming the challenge
of limited sample sizes in many natural language processing (NLP) tasks. This
challenge is especially prominent in the few-shot learning scenario, where the
data in the target domain is generally much scarcer and of lowered quality. A
natural and widely-used strategy to mitigate such challenges is to perform data
augmentation to better capture the data invariance and increase the sample
size. However, current text data augmentation methods either can't ensure the
correct labeling of the generated data (lacking faithfulness) or can't ensure
sufficient diversity in the generated data (lacking compactness), or both.
Inspired by the recent success of large language models, especially the
development of ChatGPT, which demonstrated improved language comprehension
abilities, in this work, we propose a text data augmentation approach based on
ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples
into multiple conceptually similar but semantically different samples. The
augmented samples can then be used in downstream model training. Experiment
results on few-shot learning text classification tasks show the superior
performance of the proposed AugGPT approach over state-of-the-art text data
augmentation methods in terms of testing accuracy and distribution of the
augmented samples. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13007v3' target="_blank">http://arxiv.org/pdf/2302.13007v3</a><br> <br> <br> <font size='5'> 734 </font> <div style="text-align: right"> 2023-02-24 18:48:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs), such as ChatGPT, are able to generate
human-like, fluent responses for many downstream tasks, e.g., task-oriented
dialog and question answering. However, applying LLMs to real-world,
mission-critical applications remains challenging mainly due to their tendency
to generate hallucinations and their inability to use external knowledge. This
paper proposes a LLM-Augmenter system, which augments a black-box LLM with a
set of plug-and-play modules. Our system makes the LLM generate responses
grounded in external knowledge, e.g., stored in task-specific databases. It
also iteratively revises LLM prompts to improve model responses using feedback
generated by utility functions, e.g., the factuality score of a LLM-generated
response. The effectiveness of LLM-Augmenter is empirically validated on two
types of scenarios, task-oriented dialog and open-domain question answering.
LLM-Augmenter significantly reduces ChatGPT's hallucinations without
sacrificing the fluency and informativeness of its responses. We make the
source code and models publicly available. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12813v3' target="_blank">http://arxiv.org/pdf/2302.12813v3</a><br> <br> <br> <font size='5'> 735 </font> <div style="text-align: right"> 2023-02-24 04:03:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: MUX-PLMs: Data Multiplexing for High-throughput Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The widespread adoption of large language models such as ChatGPT and Bard has
led to unprecedented demand for these technologies. The burgeoning cost of
inference for ever-increasing model sizes coupled with hardware shortages has
limited affordable access and poses a pressing need for efficiency approaches
geared towards high throughput and performance. Multi-input multi-output (MIMO)
algorithms such as data multiplexing, offer a promising solution with a
many-fold increase in throughput by performing inference for multiple inputs at
the cost of a single input. Yet these approaches are not currently performant
enough to be deployed in modern systems. We change that by developing MUX-PLMs,
a class of high throughput pre-trained language models (PLMs) trained with data
multiplexing, that can be fine-tuned for any downstream task to yield
high-throughput high-performance. Our novel multiplexing and demultiplexing
modules proficiently entangle and disentangle inputs, and enable
high-performance high throughput \muxplms{} that are competitive with vanilla
PLMs while achieving 2x/5x inference speedup with only a $1-4\%$ drop on a
broad suite of tasks. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12441v2' target="_blank">http://arxiv.org/pdf/2302.12441v2</a><br> <br> <br> <font size='5'> 736 </font> <div style="text-align: right"> 2023-02-23 22:14:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative pre-trained language models (GPLMs) like ChatGPT encode in the
model's parameters knowledge the models observe during the pre-training phase.
This knowledge is then used at inference to address the task specified by the
user in their prompt. For example, for the question-answering task, the GPLMs
leverage the knowledge and linguistic patterns learned at training to produce
an answer to a user question. Aside from the knowledge encoded in the model
itself, answers produced by GPLMs can also leverage knowledge provided in the
prompts. For example, a GPLM can be integrated into a retrieve-then-generate
paradigm where a search engine is used to retrieve documents relevant to the
question; the content of the documents is then transferred to the GPLM via the
prompt. In this paper we study the differences in answer correctness generated
by ChatGPT when leveraging the model's knowledge alone vs. in combination with
the prompt knowledge. We study this in the context of consumers seeking health
advice from the model. Aside from measuring the effectiveness of ChatGPT in
this context, we show that the knowledge passed in the prompt can overturn the
knowledge encoded in the model and this is, in our experiments, to the
detriment of answer correctness. This work has important implications for the
development of more robust and transparent question-answering systems based on
generative pre-trained language models. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13793v1' target="_blank">http://arxiv.org/pdf/2302.13793v1</a><br> <br> <br> <font size='5'> 737 </font> <div style="text-align: right"> 2023-02-23 17:35:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Talking Abortion (Mis)information with ChatGPT on TikTok</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this study, we tested users' perception of accuracy and engagement with
TikTok videos in which ChatGPT responded to prompts about "at-home" abortion
remedies. The chatbot's responses, though somewhat vague and confusing,
nonetheless recommended consulting with health professionals before attempting
an "at-home" abortion. We used ChatGPT to create two TikTok video variants -
one where users can see ChatGPT explicitly typing back a response, and one
where the text response is presented without any notion to the chatbot. We
randomly exposed 100 participants to each variant and found that the group of
participants unaware of ChatGPT's text synthetization was more inclined to
believe the responses were misinformation. Under the same impression, TikTok
itself attached misinformation warning labels ("Get the facts about abortion")
to all videos after we collected our initial results. We then decided to test
the videos again with another set of 50 participants and found that the labels
did not affect the perceptions of abortion misinformation except in the case
where ChatGPT explicitly responded to a prompt for a lyrical output. We also
found that more than 60% of the participants expressed negative or hesitant
opinions about chatbots as sources of credible health information. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13524v1' target="_blank">http://arxiv.org/pdf/2303.13524v1</a><br> <br> <br> <font size='5'> 738 </font> <div style="text-align: right"> 2023-02-23 16:06:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We study the performance of a commercially available large language model
(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.
To our knowledge, this is the first independent evaluation of ChatGPT. We found
that ChatGPT's performance changes dramatically based on the requirement to
show its work, failing 20% of the time when it provides work compared with 84%
when it does not. Further several factors about MWPs relating to the number of
unknowns and number of operations that lead to a higher probability of failure
when compared with the prior, specifically noting (across all experiments) that
the probability of failure increases linearly with the number of addition and
subtraction operations. We also have released the dataset of ChatGPT's
responses to the MWPs to support further work on the characterization of LLM
performance and present baseline machine learning models to predict if ChatGPT
can correctly answer an MWP. We have released a dataset comprised of ChatGPT's
responses to support further research in this area. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13814v2' target="_blank">http://arxiv.org/pdf/2302.13814v2</a><br> <br> <br> <font size='5'> 739 </font> <div style="text-align: right"> 2023-02-22 17:44:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Guiding Large Language Models via Directional Stimulus Prompting</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce a novel prompting framework called Directional Stimulus
Prompting for guiding black-box large language models (LLMs) toward desired
outputs. The framework introduces a new component called directional stimulus
into the prompt, providing more fine-grained guidance and control over LLMs.
The directional stimulus serves as hints or cues for each input query to guide
LLMs toward the desired output, such as keywords that the desired summary
should include for summarization. We utilize a small tunable model (e.g., T5)
to generate such directional stimulus for each query, allowing us to optimize
black-box LLMs by optimizing a small policy model. This policy model can be
trained through 1) supervised fine-tuning using labeled data and 2)
reinforcement learning from offline or online rewards to explore directional
stimulus that better aligns LLMs with desired behaviors. We evaluate our
framework on summarization and dialogue response generation tasks. Experimental
results show that our framework consistently improves ChatGPT's performance
over standard prompting with a small collection of training data, and
reinforcement learning further improves the performance. Notably, on the
MultWOZ dataset, our framework enables ChatGPT to achieve a remarkable 41.4%
improvement in its combined score with only 80 dialogues, matching or even
surpassing the performance of some fully trained state-of-the-art models. We
have made our code publicly available. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11520v3' target="_blank">http://arxiv.org/pdf/2302.11520v3</a><br> <br> <br> <font size='5'> 740 </font> <div style="text-align: right"> 2023-02-22 11:01:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is a recent chatbot service released by OpenAI and is receiving
increasing attention over the past few months. While evaluations of various
aspects of ChatGPT have been done, its robustness, i.e., the performance to
unexpected inputs, is still unclear to the public. Robustness is of particular
concern in responsible AI, especially for safety-critical applications. In this
paper, we conduct a thorough evaluation of the robustness of ChatGPT from the
adversarial and out-of-distribution (OOD) perspective. To do so, we employ the
AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart
review and DDXPlus medical diagnosis datasets for OOD evaluation. We select
several popular foundation models as baselines. Results show that ChatGPT shows
consistent advantages on most adversarial and OOD classification and
translation tasks. However, the absolute performance is far from perfection,
which suggests that adversarial and OOD robustness remains a significant threat
to foundation models. Moreover, ChatGPT shows astounding performance in
understanding dialogue-related texts and we find that it tends to provide
informal suggestions for medical tasks instead of definitive answers. Finally,
we present in-depth discussions of possible research directions. </font><br> Link: <a href='http://arxiv.org/pdf/2302.12095v4' target="_blank">http://arxiv.org/pdf/2302.12095v4</a><br> <br> <br> <font size='5'> 741 </font> <div style="text-align: right"> 2023-02-21 15:20:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: Jack of all trades, master of none</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and
revolutionized the approach in artificial intelligence to human-model
interaction. Several publications on ChatGPT evaluation test its effectiveness
on well-known natural language processing (NLP) tasks. However, the existing
studies are mostly non-automated and tested on a very limited scale. In this
work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,
most of them subjective even to humans, such as sentiment analysis, emotion
recognition, offensiveness, and stance detection. In contrast, the other tasks
require more objective reasoning like word sense disambiguation, linguistic
acceptability, and question answering. We also evaluated GPT-4 model on five
selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process
and analyzed more than 49k responses. Our comparison of its results with
available State-of-the-Art (SOTA) solutions showed that the average loss in
quality of the ChatGPT model was about 25% for zero-shot and few-shot
evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower
than for ChatGPT. We showed that the more difficult the task (lower SOTA
performance), the higher the ChatGPT loss. It especially refers to pragmatic
NLP problems like emotion recognition. We also tested the ability to
personalize ChatGPT responses for selected subjective tasks via Random
Contextual Few-Shot Personalization, and we obtained significantly better
user-based predictions. Additional qualitative analysis revealed a ChatGPT
bias, most likely due to the rules imposed on human trainers by OpenAI. Our
results provide the basis for a fundamental discussion of whether the high
quality of recent predictive NLP models can indicate a tool's usefulness to
society and how the learning and validation procedures for such systems should
be established. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10724v4' target="_blank">http://arxiv.org/pdf/2302.10724v4</a><br> <br> <br> <font size='5'> 742 </font> <div style="text-align: right"> 2023-02-21 12:42:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Prompt engineering is an increasingly important skill set needed to converse
effectively with large language models (LLMs), such as ChatGPT. Prompts are
instructions given to an LLM to enforce rules, automate processes, and ensure
specific qualities (and quantities) of generated output. Prompts are also a
form of programming that can customize the outputs and interactions with an
LLM. This paper describes a catalog of prompt engineering techniques presented
in pattern form that have been applied to solve common problems when conversing
with LLMs. Prompt patterns are a knowledge transfer method analogous to
software patterns since they provide reusable solutions to common problems
faced in a particular context, i.e., output generation and interaction when
working with LLMs. This paper provides the following contributions to research
on prompt engineering that apply LLMs to automate software development tasks.
First, it provides a framework for documenting patterns for structuring prompts
to solve a range of problems so that they can be adapted to different domains.
Second, it presents a catalog of patterns that have been applied successfully
to improve the outputs of LLM conversations. Third, it explains how prompts can
be built from multiple patterns and illustrates prompt patterns that benefit
from combination with other prompt patterns. </font><br> Link: <a href='http://arxiv.org/pdf/2302.11382v1' target="_blank">http://arxiv.org/pdf/2302.11382v1</a><br> <br> <br> <font size='5'> 743 </font> <div style="text-align: right"> 2023-02-21 08:59:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT has gained significant attention in research due to its
ability to interact with humans effectively. The core idea behind this model is
reinforcement learning (RL) fine-tuning, a new paradigm that allows language
models to align with human preferences, i.e., InstructGPT. In this study, we
propose BadGPT, the first backdoor attack against RL fine-tuning in language
models. By injecting a backdoor into the reward model, the language model can
be compromised during the fine-tuning stage. Our initial experiments on movie
reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated
text through BadGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12298v1' target="_blank">http://arxiv.org/pdf/2304.12298v1</a><br> <br> <br> <font size='5'> 744 </font> <div style="text-align: right"> 2023-02-20 15:43:22+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: A Meta-Analysis after 2.5 Months</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and
media attention since its release in November 2022. However, little hard
evidence is available regarding its perception in various sources. In this
paper, we analyze over 300,000 tweets and more than 150 scientific papers to
investigate how ChatGPT is perceived and discussed. Our findings show that
ChatGPT is generally viewed as of high quality, with positive sentiment and
emotions of joy dominating in social media. Its perception has slightly
decreased since its debut, however, with joy decreasing and (negative) surprise
on the rise, and it is perceived more negatively in languages other than
English. In recent scientific papers, ChatGPT is characterized as a great
opportunity across various fields including the medical domain, but also as a
threat concerning ethics and receives mixed assessments for education. Our
comprehensive meta-analysis of ChatGPT's current perception after 2.5 months
since its release can contribute to shaping the public debate and informing its
future development. We make our data available. </font><br> Link: <a href='http://arxiv.org/pdf/2302.13795v1' target="_blank">http://arxiv.org/pdf/2302.13795v1</a><br> <br> <br> <font size='5'> 745 </font> <div style="text-align: right"> 2023-02-20 12:57:12+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Zero-Shot Information Extraction via Chatting with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Zero-shot information extraction (IE) aims to build IE systems from the
unannotated text. It is challenging due to involving little human intervention.
Challenging but worthwhile, zero-shot IE reduces the time and effort that data
labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,
ChatGPT) show promising performance on zero-shot settings, thus inspiring us to
explore prompt-based methods. In this work, we ask whether strong IE models can
be constructed by directly prompting LLMs. Specifically, we transform the
zero-shot IE task into a multi-turn question-answering problem with a two-stage
framework (ChatIE). With the power of ChatGPT, we extensively evaluate our
framework on three IE tasks: entity-relation triple extract, named entity
recognition, and event extraction. Empirical results on six datasets across two
languages show that ChatIE achieves impressive performance and even surpasses
some full-shot models on several datasets (e.g., NYT11-HRL). We believe that
our work could shed light on building IE models with limited resources. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10205v1' target="_blank">http://arxiv.org/pdf/2302.10205v1</a><br> <br> <br> <font size='5'> 746 </font> <div style="text-align: right"> 2023-02-20 06:39:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT for Robotics: Design Principles and Model Abilities</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents an experimental study regarding the use of OpenAI's
ChatGPT for robotics applications. We outline a strategy that combines design
principles for prompt engineering and the creation of a high-level function
library which allows ChatGPT to adapt to different robotics tasks, simulators,
and form factors. We focus our evaluations on the effectiveness of different
prompt engineering techniques and dialog strategies towards the execution of
various types of robotics tasks. We explore ChatGPT's ability to use free-form
dialog, parse XML tags, and to synthesize code, in addition to the use of
task-specific prompting functions and closed-loop reasoning through dialogues.
Our study encompasses a range of tasks within the robotics domain, from basic
logical, geometrical, and mathematical reasoning all the way to complex domains
such as aerial navigation, manipulation, and embodied agents. We show that
ChatGPT can be effective at solving several of such tasks, while allowing users
to interact with it primarily via natural language instructions. In addition to
these studies, we introduce an open-sourced research tool called PromptCraft,
which contains a platform where researchers can collaboratively upload and vote
on examples of good prompting schemes for robotics applications, as well as a
sample robotics simulator with ChatGPT integration, making it easier for users
to get started with using ChatGPT for robotics. </font><br> Link: <a href='http://arxiv.org/pdf/2306.17582v1' target="_blank">http://arxiv.org/pdf/2306.17582v1</a><br> <br> <br> <font size='5'> 747 </font> <div style="text-align: right"> 2023-02-19 18:18:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Triple birthday matches in the Senate: Lies, damned lies and chatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Our question is ``What is the probability that at least three members of the
senate share the same birthday?'' Before the pandemic, I asked this question in
several popular math talks I gave at universities across the country. Inspired
by ChatGPT's abysmal failure to answer the question, I have recently come back
to this problem and now have a more satisfactory answer, thanks in no small
part to what I learned form a page of Wolfram's Math World, which I located by
a Google search. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09643v1' target="_blank">http://arxiv.org/pdf/2302.09643v1</a><br> <br> <br> <font size='5'> 748 </font> <div style="text-align: right"> 2023-02-19 12:29:33+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recently, ChatGPT has attracted great attention, as it can generate fluent
and high-quality responses to human inquiries. Several prior studies have shown
that ChatGPT attains remarkable generation ability compared with existing
models. However, the quantitative analysis of ChatGPT's understanding ability
has been given little attention. In this report, we explore the understanding
ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and
comparing it with 4 representative fine-tuned BERT-style models. We find that:
1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT
outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT
achieves comparable performance compared with BERT on sentiment analysis and
question-answering tasks. Additionally, by combining some advanced prompting
strategies, we show that the understanding ability of ChatGPT can be further
improved. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10198v2' target="_blank">http://arxiv.org/pdf/2302.10198v2</a><br> <br> <br> <font size='5'> 749 </font> <div style="text-align: right"> 2023-02-19 01:52:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT (Feb 13 Version) is a Chinese Room</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has gained both positive and negative publicity after reports
suggesting that it is able to pass various professional and licensing
examinations. This suggests that ChatGPT may pass Turing Test in the near
future. However, a computer program that passing Turing Test can either mean
that it is a Chinese Room or artificially conscious. Hence, the question of
whether the current state of ChatGPT is more of a Chinese Room or approaching
artificial consciousness remains. Here, I demonstrate that the current version
of ChatGPT (Feb 13 version) is a Chinese Room. Despite potential evidence of
cognitive connections, ChatGPT exhibits critical errors in causal reasoning. At
the same time, I demonstrate that ChatGPT can generate all possible categorical
responses to the same question and response with erroneous examples; thus,
questioning its utility as a learning tool. I also show that ChatGPT is capable
of artificial hallucination, which is defined as generating confidently wrong
replies. It is likely that errors in causal reasoning leads to hallucinations.
More critically, ChatGPT generates false references to mimic real publications.
Therefore, its utility is cautioned. </font><br> Link: <a href='http://arxiv.org/pdf/2304.12411v1' target="_blank">http://arxiv.org/pdf/2304.12411v1</a><br> <br> <br> <font size='5'> 750 </font> <div style="text-align: right"> 2023-02-18 20:51:09+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A PFM (e.g., BERT,
ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable
parameter initialization for a wide range of downstream applications. BERT
learns bidirectional encoder representations from Transformers, which are
trained on large datasets as contextual language models. Similarly, the
generative pretrained transformer (GPT) method employs Transformers as the
feature extractor and is trained using an autoregressive paradigm on large
datasets. Recently, ChatGPT shows promising success on large language models,
which applies an autoregressive language model with zero shot or few shot
prompting. The remarkable achievements of PFM have brought significant
breakthroughs to various fields of AI. Numerous studies have proposed different
methods, raising the demand for an updated survey. This study provides a
comprehensive review of recent research advancements, challenges, and
opportunities for PFMs in text, image, graph, as well as other data modalities.
The review covers the basic components and existing pretraining methods used in
natural language processing, computer vision, and graph learning. Additionally,
it explores advanced PFMs used for different data modalities and unified PFMs
that consider data quality and quantity. The review also discusses research
related to the fundamentals of PFMs, such as model efficiency and compression,
security, and privacy. Finally, the study provides key implications, future
research directions, challenges, and open problems in the field of PFMs.
Overall, this survey aims to shed light on the research of the PFMs on
scalability, security, logical reasoning ability, cross-domain learning
ability, and the user-friendly interactive ability for artificial general
intelligence. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09419v3' target="_blank">http://arxiv.org/pdf/2302.09419v3</a><br> <br> <br> <font size='5'> 751 </font> <div style="text-align: right"> 2023-02-18 02:11:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Pre-trained Transformer (GPT) models have shown remarkable
capabilities for natural language generation, but their performance for machine
translation has not been thoroughly investigated. In this paper, we present a
comprehensive evaluation of GPT models for machine translation, covering
various aspects such as quality of different GPT models in comparison with
state-of-the-art research and commercial systems, effect of prompting
strategies, robustness towards domain shifts and document-level translation. We
experiment with eighteen different translation directions involving high and
low resource languages, as well as non English-centric translations, and
evaluate the performance of three GPT models: ChatGPT, GPT3.5
(text-davinci-003), and text-davinci-002. Our results show that GPT models
achieve very competitive translation quality for high resource languages, while
having limited capabilities for low resource languages. We also show that
hybrid approaches, which combine GPT models with other translation systems, can
further enhance the translation quality. We perform comprehensive analysis and
human evaluation to further understand the characteristics of GPT translations.
We hope that our paper provides valuable insights for researchers and
practitioners in the field and helps to better understand the potential and
limitations of GPT models for translation. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09210v1' target="_blank">http://arxiv.org/pdf/2302.09210v1</a><br> <br> <br> <font size='5'> 752 </font> <div style="text-align: right"> 2023-02-17 23:25:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Prompting Large Language Models With the Socratic Method</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper presents a systematic approach to using the Socratic method in
developing prompt templates that effectively interact with large language
models, including GPT-3. Various methods are examined, and those that yield
precise answers and justifications while fostering creativity and imagination
to enhance creative writing are identified. Techniques such as {\em
definition}, {\em elenchus}, {\em dialectic}, {\em maieutics}, {\em
generalization}, and {\em counterfactual reasoning} are discussed for their
application in engineering prompt templates and their connections to inductive,
deductive, and abductive reasoning. Through examples, the effectiveness of
these dialogue and reasoning methods is demonstrated. An interesting
observation is made that when the task's goal and user intent are conveyed to
GPT-3 via ChatGPT before the start of a dialogue, the large language model
seems to connect to the external context expressed in the intent and perform
more effectively. </font><br> Link: <a href='http://arxiv.org/pdf/2303.08769v2' target="_blank">http://arxiv.org/pdf/2303.08769v2</a><br> <br> <br> <font size='5'> 753 </font> <div style="text-align: right"> 2023-02-17 18:31:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Complex QA and language models hybrid architectures, Survey</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper reviews the state-of-the-art of language models architectures and
strategies for "complex" question-answering (QA, CQA, CPS) with a focus on
hybridization. Large Language Models (LLM) are good at leveraging public data
on standard problems but once you want to tackle more specific complex
questions or problems (e.g. How does the concept of personal freedom vary
between different cultures ? What is the best mix of power generation methods
to reduce climate change ?) you may need specific architecture, knowledge,
skills, methods, sensitive data protection, explainability, human approval and
versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of LLM in complex QA. In this paper, we start by reviewing required
skills and evaluation techniques. We integrate findings from the robust
community edited research papers BIG, BLOOM and HELM which open source,
benchmark and analyze limits and challenges of LLM in terms of tasks complexity
and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as
a baseline. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form and
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, explainability and truthfulness, temporal reasoning. We
analyze current solutions and promising research trends, using elements such
as: hybrid LLM architectural patterns, training and prompting strategies,
active human reinforcement learning supervised with AI, neuro-symbolic and
structured knowledge grounding, program synthesis, iterated decomposition and
others. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09051v4' target="_blank">http://arxiv.org/pdf/2302.09051v4</a><br> <br> <br> <font size='5'> 754 </font> <div style="text-align: right"> 2023-02-17 17:39:41+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the span of a few months, generative Artificial Intelligence (AI) tools
that can generate realistic images or text have taken the Internet by storm,
making them one of the technologies with fastest adoption ever. Some of these
generative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide
public notoriety. Interestingly, these tools are possible because of the
massive amount of data (text and images) available on the Internet. The tools
are trained on massive data sets that are scraped from Internet sites. And now,
these generative AI tools are creating massive amounts of new data that are
being fed into the Internet. Therefore, future versions of generative AI tools
will be trained with Internet data that is a mix of original and AI-generated
data. As time goes on, a mixture of original data and data generated by
different versions of AI tools will populate the Internet. This raises a few
intriguing questions: how will future versions of generative AI tools behave
when trained on a mixture of real and AI generated data? Will they evolve with
the new data sets or degenerate? Will evolution introduce biases in subsequent
generations of generative AI tools? In this document, we explore these
questions and report some very initial simulation results using a simple
image-generation AI tool. These results suggest that the quality of the
generated images degrades as more AI-generated data is used for training thus
suggesting that generative AI may degenerate. Although these results are
preliminary and cannot be generalised without further study, they serve to
illustrate the potential issues of the interaction between generative AI and
the Internet. </font><br> Link: <a href='http://arxiv.org/pdf/2303.01255v1' target="_blank">http://arxiv.org/pdf/2303.01255v1</a><br> <br> <br> <font size='5'> 755 </font> <div style="text-align: right"> 2023-02-17 15:48:37+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Generative Artificial Intelligence (AI) models such as OpenAI's ChatGPT have
the potential to revolutionize Statistical Process Control (SPC) practice,
learning, and research. However, these tools are in the early stages of
development and can be easily misused or misunderstood. In this paper, we give
an overview of the development of Generative AI. Specifically, we explore
ChatGPT's ability to provide code, explain basic concepts, and create knowledge
related to SPC practice, learning, and research. By investigating responses to
structured prompts, we highlight the benefits and limitations of the results.
Our study indicates that the current version of ChatGPT performs well for
structured tasks, such as translating code from one language to another and
explaining well-known concepts but struggles with more nuanced tasks, such as
explaining less widely known terms and creating code from scratch. We find that
using new AI tools may help practitioners, educators, and researchers to be
more efficient and productive. However, in their current stages of development,
some results are misleading and wrong. Overall, the use of generative AI models
in SPC must be properly validated and used in conjunction with other methods to
ensure accurate results. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10916v1' target="_blank">http://arxiv.org/pdf/2302.10916v1</a><br> <br> <br> <font size='5'> 756 </font> <div style="text-align: right"> 2023-02-16 08:41:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI Usage Cards: Responsibly Reporting AI-generated Content</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Given AI systems like ChatGPT can generate content that is indistinguishable
from human-made work, the responsible use of this technology is a growing
concern. Although understanding the benefits and harms of using AI systems
requires more time, their rapid and indiscriminate adoption in practice is a
reality. Currently, we lack a common framework and language to define and
report the responsible use of AI for content generation. Prior work proposed
guidelines for using AI in specific scenarios (e.g., robotics or medicine)
which are not transferable to conducting and reporting scientific research. Our
work makes two contributions: First, we propose a three-dimensional model
consisting of transparency, integrity, and accountability to define the
responsible use of AI. Second, we introduce ``AI Usage Cards'', a standardized
way to report the use of AI in scientific research. Our model and cards allow
users to reflect on key principles of responsible AI usage. They also help the
research community trace, compare, and question various forms of AI usage and
support the development of accepted community norms. The proposed framework and
reporting system aims to promote the ethical and responsible use of AI in
scientific research and provide a standardized approach for reporting AI usage
across different research fields. We also provide a free service to easily
generate AI Usage Cards for scientific work via a questionnaire and export them
in various machine-readable formats for inclusion in different work products at
https://ai-cards.org. </font><br> Link: <a href='http://arxiv.org/pdf/2303.03886v2' target="_blank">http://arxiv.org/pdf/2303.03886v2</a><br> <br> <br> <font size='5'> 757 </font> <div style="text-align: right"> 2023-02-16 04:41:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Text summarization has been a crucial problem in natural language processing
(NLP) for several decades. It aims to condense lengthy documents into shorter
versions while retaining the most critical information. Various methods have
been proposed for text summarization, including extractive and abstractive
summarization. The emergence of large language models (LLMs) like GPT3 and
ChatGPT has recently created significant interest in using these models for
text summarization tasks. Recent studies \cite{goyal2022news,
zhang2023benchmarking} have shown that LLMs-generated news summaries are
already on par with humans. However, the performance of LLMs for more practical
applications like aspect or query-based summaries is underexplored. To fill
this gap, we conducted an evaluation of ChatGPT's performance on four widely
used benchmark datasets, encompassing diverse summaries from Reddit posts, news
articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's
performance is comparable to traditional fine-tuning methods in terms of Rouge
scores. Moreover, we highlight some unique differences between
ChatGPT-generated summaries and human references, providing valuable insights
into the superpower of ChatGPT for diverse text summarization tasks. Our
findings call for new directions in this area, and we plan to conduct further
research to systematically examine the characteristics of ChatGPT-generated
summaries through extensive human evaluation. </font><br> Link: <a href='http://arxiv.org/pdf/2302.08081v1' target="_blank">http://arxiv.org/pdf/2302.08081v1</a><br> <br> <br> <font size='5'> 758 </font> <div style="text-align: right"> 2023-02-15 18:31:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Translating natural language into Bash Commands is an emerging research field
that has gained attention in recent years. Most efforts have focused on
producing more accurate translation models. To the best of our knowledge, only
two datasets are available, with one based on the other. Both datasets involve
scraping through known data sources (through platforms like stack overflow,
crowdsourcing, etc.) and hiring experts to validate and correct either the
English text or Bash Commands. This paper provides two contributions to
research on synthesizing Bash Commands from scratch. First, we describe a
state-of-the-art translation model used to generate Bash Commands from the
corresponding English text. Second, we introduce a new NL2CMD dataset that is
automatically generated, involves minimal human intervention, and is over six
times larger than prior datasets. Since the generation pipeline does not rely
on existing Bash Commands, the distribution and types of commands can be custom
adjusted. We evaluate the performance of ChatGPT on this task and discuss the
potential of using it as a data generator. Our empirical results show how the
scale and diversity of our dataset can offer unique opportunities for semantic
parsing researchers. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07845v3' target="_blank">http://arxiv.org/pdf/2302.07845v3</a><br> <br> <br> <font size='5'> 759 </font> <div style="text-align: right"> 2023-02-15 05:04:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We conduct a pilot study selectively evaluating the cognitive abilities
(decision making and spatial reasoning) of two recently released generative
transformer models, ChatGPT and DALL-E 2. Input prompts were constructed
following neutral a priori guidelines, rather than adversarial intent. Post hoc
qualitative analysis of the outputs shows that DALL-E 2 is able to generate at
least one correct image for each spatial reasoning prompt, but most images
generated are incorrect (even though the model seems to have a clear
understanding of the objects mentioned in the prompt). Similarly, in evaluating
ChatGPT on the rationality axioms developed under the classical Von
Neumann-Morgenstern utility theorem, we find that, although it demonstrates
some level of rational decision-making, many of its decisions violate at least
one of the axioms even under reasonable constructions of preferences, bets, and
decision-making prompts. ChatGPT's outputs on such problems generally tended to
be unpredictable: even as it made irrational decisions (or employed an
incorrect reasoning process) for some simpler decision-making problems, it was
able to draw correct conclusions for more complex bet structures. We briefly
comment on the nuances and challenges involved in scaling up such a 'cognitive'
evaluation or conducting it with a closed set of answer keys ('ground truth'),
given that these models are inherently generative and open-ended in responding
to prompts. </font><br> Link: <a href='http://arxiv.org/pdf/2302.09068v1' target="_blank">http://arxiv.org/pdf/2302.09068v1</a><br> <br> <br> <font size='5'> 760 </font> <div style="text-align: right"> 2023-02-15 00:14:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Conversational AI-Powered Design: ChatGPT as Designer, User, and Product</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent advancements in Large Language Models (LLMs), particularly
conversational LLMs like ChatGPT, have prompted changes in a range of fields,
including design. This study aims to examine the capabilities of ChatGPT in a
human-centered design process. To this end, a hypothetical design project was
conducted, where ChatGPT was utilized to generate personas, simulate interviews
with fictional users, create new design ideas, simulate usage scenarios and
conversations between an imaginary prototype and fictional users, and lastly
evaluate user experience. The results show that ChatGPT effectively performed
the tasks assigned to it as a designer, user, or product, providing mostly
appropriate responses. The study does, however, highlight some drawbacks such
as forgotten information, partial responses, and a lack of output diversity.
The paper explains the potential benefits and limitations of using
conversational LLMs in design, discusses its implications, and suggests
directions for future research in this rapidly evolving area. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07406v1' target="_blank">http://arxiv.org/pdf/2302.07406v1</a><br> <br> <br> <font size='5'> 761 </font> <div style="text-align: right"> 2023-02-14 18:54:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have recently demonstrated their potential in
clinical applications, providing valuable medical knowledge and advice. For
example, a large dialog LLM like ChatGPT has successfully passed part of the US
medical licensing exam. However, LLMs currently have difficulty processing
images, making it challenging to interpret information from medical images,
which are rich in information that supports clinical decisions. On the other
hand, computer-aided diagnosis (CAD) networks for medical images have seen
significant success in the medical field by using advanced deep-learning
algorithms to support clinical decision-making. This paper presents a method
for integrating LLMs into medical-image CAD networks. The proposed framework
uses LLMs to enhance the output of multiple CAD networks, such as diagnosis
networks, lesion segmentation networks, and report generation networks, by
summarizing and reorganizing the information presented in natural language text
format. The goal is to merge the strengths of LLMs' medical domain knowledge
and logical reasoning with the vision understanding capability of existing
medical-image CAD models to create a more user-friendly and understandable
system for patients compared to conventional CAD systems. In the future, LLM's
medical knowledge can be also used to improve the performance of vision-based
medical-image CAD models. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07257v1' target="_blank">http://arxiv.org/pdf/2302.07257v1</a><br> <br> <br> <font size='5'> 762 </font> <div style="text-align: right"> 2023-02-14 07:20:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Learning gain differences between ChatGPT and human tutor generated algebra hints</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to
the frontiers of practical consumer use and leading industries to re-evaluate
how they allocate resources for content production. Authoring of open
educational resources and hint content within adaptive tutoring systems is
labor intensive. Should LLMs like ChatGPT produce educational content on par
with human-authored content, the implications would be significant for further
scaling of computer tutoring system approaches. In this paper, we conduct the
first learning gain evaluation of ChatGPT by comparing the efficacy of its
hints with hints authored by human tutors with 77 participants across two
algebra topic areas, Elementary Algebra and Intermediate Algebra. We find that
70% of hints produced by ChatGPT passed our manual quality checks and that both
human and ChatGPT conditions produced positive learning gains. However, gains
were only statistically significant for human tutor created hints. Learning
gains from human-created hints were substantially and statistically
significantly higher than ChatGPT hints in both topic areas, though ChatGPT
participants in the Intermediate Algebra experiment were near ceiling and not
even with the control at pre-test. We discuss the limitations of our study and
suggest several future directions for the field. Problem and hint content used
in the experiment is provided for replicability. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06871v1' target="_blank">http://arxiv.org/pdf/2302.06871v1</a><br> <br> <br> <font size='5'> 763 </font> <div style="text-align: right"> 2023-02-13 15:03:07+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Linguistic ambiguity analysis in ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Linguistic ambiguity is and has always been one of the main challenges in
Natural Language Processing (NLP) systems. Modern Transformer architectures
like BERT, T5 or more recently InstructGPT have achieved some impressive
improvements in many NLP fields, but there is still plenty of work to do.
Motivated by the uproar caused by ChatGPT, in this paper we provide an
introduction to linguistic ambiguity, its varieties and their relevance in
modern NLP, and perform an extensive empiric analysis. ChatGPT strengths and
weaknesses are revealed, as well as strategies to get the most of this model. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06426v2' target="_blank">http://arxiv.org/pdf/2302.06426v2</a><br> <br> <br> <font size='5'> 764 </font> <div style="text-align: right"> 2023-02-12 22:05:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The debate around the use of GPT 3.5 has been a popular topic among academics
since the release of ChatGPT. Whilst some have argued for the advantages of GPT
3.5 in enhancing academic writing, others have raised concerns such as
plagiarism, the spread of false information, and ecological issues. The need
for finding ways to use GPT 3.5 models transparently has been voiced, and
suggestions have been made on social media as to how to use GPT 3.5 models in a
smart way. Nevertheless, to date, there is a lack of literature which clearly
outlines how to use GPT 3.5 models in academic writing, how effective they are,
and how to use them transparently. To address this, I conducted a personal
experience experiment with GPT 3.5, specifically by using OpenAI text davinci
003 model, for writing this article. I identified five ways of using GPT 3.5:
Chunk Stylist, Bullet to Paragraph, Talk Textualizer, Research Buddy, and
Polisher. I reflected on their efficacy, and commented on their potential
impact on writing ethics. Additionally, I provided a comprehensive document
which shows the prompts I used, results I got from GPT 3.5, the final edits and
visually compares those by showing the differences in percentages. </font><br> Link: <a href='http://arxiv.org/pdf/2304.11079v1' target="_blank">http://arxiv.org/pdf/2304.11079v1</a><br> <br> <br> <font size='5'> 765 </font> <div style="text-align: right"> 2023-02-12 07:48:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Semantic Importance-Aware Communications Using Pre-trained Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This letter proposes a semantic importance-aware communication (SIAC) scheme
using pre-trained language models (e.g., ChatGPT, BERT, etc.). Specifically, we
propose a cross-layer design with a pre-trained language model embedded
in/connected by the cross-layer manager. The pre-trained language model is
utilized to quantify the semantic importance of data frames. Based on the
quantified semantic importance, we investigate semantic importance-aware power
allocation. Unlike existing deep joint source-channel coding (Deep-JSCC)-based
semantic communication schemes, SIAC can be directly embedded into current
communication systems by only introducing a cross-layer manager. Our
experimental results show that the proposed SIAC scheme can achieve lower
semantic loss than existing equal-priority communications. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07142v2' target="_blank">http://arxiv.org/pdf/2302.07142v2</a><br> <br> <br> <font size='5'> 766 </font> <div style="text-align: right"> 2023-02-12 01:26:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Transformer models: an introduction and catalog</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In the past few years we have seen the meteoric appearance of dozens of
foundation models of the Transformer family, all of which have memorable and
sometimes funny, but not self-explanatory, names. The goal of this paper is to
offer a somewhat comprehensive but simple catalog and classification of the
most popular Transformer models. The paper also includes an introduction to the
most important aspects and innovations in Transformer models. Our catalog will
include models that are trained using self-supervised learning (e.g., BERT or
GPT3) as well as those that are further trained using a human-in-the-loop (e.g.
the InstructGPT model used by ChatGPT). </font><br> Link: <a href='http://arxiv.org/pdf/2302.07730v3' target="_blank">http://arxiv.org/pdf/2302.07730v3</a><br> <br> <br> <font size='5'> 767 </font> <div style="text-align: right"> 2023-02-11 03:13:54+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent studies have alarmed that many online hate speeches are implicit. With
its subtle nature, the explainability of the detection of such hateful speech
has been a challenging problem. In this work, we examine whether ChatGPT can be
used for providing natural language explanations (NLEs) for implicit hateful
speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs
and conduct user studies to evaluate their qualities by comparison with
human-written NLEs. We discuss the potential and limitations of ChatGPT in the
context of implicit hateful speech research. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07736v2' target="_blank">http://arxiv.org/pdf/2302.07736v2</a><br> <br> <br> <font size='5'> 768 </font> <div style="text-align: right"> 2023-02-10 08:54:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The use of Artificial Intelligence (AI) to support cybersecurity operations
is now a consolidated practice, e.g., to detect malicious code or configure
traffic filtering policies. The recent surge of AI, generative techniques and
frameworks with efficient natural language processing capabilities dramatically
magnifies the number of possible applications aimed at increasing the security
of the Internet. Specifically, the ability of ChatGPT to produce textual
contents while mimicking realistic human interactions can be used to mitigate
the plague of emails containing scams. Therefore, this paper investigates the
use of AI to engage scammers in automatized and pointless communications, with
the goal of wasting both their time and resources. Preliminary results showcase
that ChatGPT is able to decoy scammers, thus confirming that AI is an effective
tool to counteract threats delivered via mail. In addition, we highlight the
multitude of implications and open research questions to be addressed in the
perspective of the ubiquitous adoption of AI. </font><br> Link: <a href='http://arxiv.org/pdf/2303.13521v1' target="_blank">http://arxiv.org/pdf/2303.13521v1</a><br> <br> <br> <font size='5'> 769 </font> <div style="text-align: right"> 2023-02-09 15:44:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) have taken the scientific world by storm,
changing the landscape of natural language processing and human-computer
interaction. These powerful tools can answer complex questions and,
surprisingly, perform challenging creative tasks (e.g., generate code and
applications to solve problems, write stories, pieces of music, etc.). In this
paper, we present a collaborative game design framework that combines
interactive evolution and large language models to simulate the typical human
design process. We use the former to exploit users' feedback for selecting the
most promising ideas and large language models for a very complex creative task
- the recombination and variation of ideas. In our framework, the process
starts with a brief and a set of candidate designs, either generated using a
language model or proposed by the users. Next, users collaborate on the design
process by providing feedback to an interactive genetic algorithm that selects,
recombines, and mutates the most promising designs. We evaluated our framework
on three game design tasks with human designers who collaborated remotely. </font><br> Link: <a href='http://arxiv.org/pdf/2303.02155v2' target="_blank">http://arxiv.org/pdf/2303.02155v2</a><br> <br> <br> <font size='5'> 770 </font> <div style="text-align: right"> 2023-02-08 20:59:18+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Will ChatGPT get you caught? Rethinking of Plagiarism Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rise of Artificial Intelligence (AI) technology and its impact on
education has been a topic of growing concern in recent years. The new
generation AI systems such as chatbots have become more accessible on the
Internet and stronger in terms of capabilities. The use of chatbots,
particularly ChatGPT, for generating academic essays at schools and colleges
has sparked fears among scholars. This study aims to explore the originality of
contents produced by one of the most popular AI chatbots, ChatGPT. To this end,
two popular plagiarism detection tools were used to evaluate the originality of
50 essays generated by ChatGPT on various topics. Our results manifest that
ChatGPT has a great potential to generate sophisticated text outputs without
being well caught by the plagiarism check software. In other words, ChatGPT can
create content on many topics with high originality as if they were written by
someone. These findings align with the recent concerns about students using
chatbots for an easy shortcut to success with minimal or no effort. Moreover,
ChatGPT was asked to verify if the essays were generated by itself, as an
additional measure of plagiarism check, and it showed superior performance
compared to the traditional plagiarism-detection tools. The paper discusses the
need for institutions to consider appropriate measures to mitigate potential
plagiarism issues and advise on the ongoing debate surrounding the impact of AI
technology on education. Further implications are discussed in the paper. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04335v1' target="_blank">http://arxiv.org/pdf/2302.04335v1</a><br> <br> <br> <font size='5'> 771 </font> <div style="text-align: right"> 2023-02-08 20:38:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Deep Machine Learning in Cosmology: Evolution or Revolution?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Could Machine Learning (ML) make fundamental discoveries and tackle unsolved
problems in Cosmology? Detailed observations of the present contents of the
universe are consistent with the Cosmological Constant Lambda and Cold Dark
Matter model, subject to some unresolved inconsistencies ('tensions') among
observations of the Hubble Constant and the clumpiness factor. To understand
these issues further, large surveys of billions of galaxies and other probes
require new statistical approaches. In recent years the power of ML, and in
particular 'Deep Learning', has been demonstrated for object classification,
photometric redshifts, anomaly detection, enhanced simulations, and inference
of cosmological parameters. It is argued that the more traditional 'shallow
learning' (i.e. with pre-processing feature extraction) is actually quite deep,
as it brings in human knowledge, while 'deep learning' might be perceived as a
black box, unless supplemented by explainability tools. The 'killer
applications' of ML for Cosmology are still to come. New ways to train the next
generation of scientists for the Data Intensive Science challenges ahead are
also discussed. Finally, the chatbot ChatGPT is challenged to address the
question posed in this article's title. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04324v1' target="_blank">http://arxiv.org/pdf/2302.04324v1</a><br> <br> <br> <font size='5'> 772 </font> <div style="text-align: right"> 2023-02-08 13:03:27+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversational AI and Question-Answering systems (QASs) for knowledge graphs
(KGs) are both emerging research areas: they empower users with natural
language interfaces for extracting information easily and effectively.
Conversational AI simulates conversations with humans; however, it is limited
by the data captured in the training datasets. In contrast, QASs retrieve the
most recent information from a KG by understanding and translating the natural
language question into a formal query supported by the database engine.
  In this paper, we present a comprehensive study of the characteristics of the
existing alternatives towards combining both worlds into novel KG chatbots. Our
framework compares two representative conversational models, ChatGPT and
Galactica, against KGQAN, the current state-of-the-art QAS. We conduct a
thorough evaluation using four real KGs across various application domains to
identify the current limitations of each category of systems. Based on our
findings, we propose open research opportunities to empower QASs with chatbot
capabilities for KGs. All benchmarks and all raw results are available1 for
further analysis. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06466v1' target="_blank">http://arxiv.org/pdf/2302.06466v1</a><br> <br> <br> <font size='5'> 773 </font> <div style="text-align: right"> 2023-02-08 12:35:34+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This paper proposes a framework for quantitatively evaluating interactive
LLMs such as ChatGPT using publicly available data sets. We carry out an
extensive technical evaluation of ChatGPT using 23 data sets covering 8
different common NLP application tasks. We evaluate the multitask, multilingual
and multi-modal aspects of ChatGPT based on these data sets and a newly
designed multimodal dataset. We find that ChatGPT outperforms LLMs with
zero-shot learning on most tasks and even outperforms fine-tuned models on some
tasks. We find that it is better at understanding non-Latin script languages
than generating them. It is able to generate multimodal content from textual
prompts, via an intermediate code generation step. Moreover, we find that
ChatGPT is 63.41% accurate on average in 10 different reasoning categories
under logical reasoning, non-textual reasoning, and commonsense reasoning,
hence making it an unreliable reasoner. It is, for example, better at deductive
than inductive reasoning. ChatGPT suffers from hallucination problems like
other LLMs and it generates more extrinsic hallucinations from its parametric
memory as it does not have access to an external knowledge base. Finally, the
interactive feature of ChatGPT enables human collaboration with the underlying
LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++
on machine translation, in a multi-turn "prompt engineering" fashion. We also
release codebase for evaluation set extraction. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04023v2' target="_blank">http://arxiv.org/pdf/2302.04023v2</a><br> <br> <br> <font size='5'> 774 </font> <div style="text-align: right"> 2023-02-08 09:44:51+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Spurred by advancements in scale, large language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,
the debut of ChatGPT has drawn a great deal of attention from the natural
language processing (NLP) community due to the fact that it can generate
high-quality responses to human input and self-correct previous mistakes based
on subsequent conversations. However, it is not yet known whether ChatGPT can
serve as a generalist model that can perform many NLP tasks zero-shot. In this
work, we empirically analyze the zero-shot learning ability of ChatGPT by
evaluating it on 20 popular NLP datasets covering 7 representative task
categories. With extensive empirical studies, we demonstrate both the
effectiveness and limitations of the current version of ChatGPT. We find that
ChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,
arithmetic reasoning) while it still faces challenges when solving specific
tasks such as sequence tagging. We additionally provide in-depth analysis
through qualitative case studies. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06476v2' target="_blank">http://arxiv.org/pdf/2302.06476v2</a><br> <br> <br> <font size='5'> 775 </font> <div style="text-align: right"> 2023-02-07 22:37:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Humans understand language by extracting information (meaning) from
sentences, combining it with existing commonsense knowledge, and then
performing reasoning to draw conclusions. While large language models (LLMs)
such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a
variety of NLP tasks, they fall short in problems that require reasoning. They
also cannot reliably explain the answers generated for a given question. In
order to emulate humans better, we propose STAR, a framework that combines LLMs
with Answer Set Programming (ASP). We show how LLMs can be used to effectively
extract knowledge -- represented as predicates -- from language. Goal-directed
ASP is then employed to reliably reason over this knowledge. We apply the STAR
framework to three different NLU tasks requiring reasoning: qualitative
reasoning, mathematical reasoning, and goal-directed conversation. Our
experiments reveal that STAR is able to bridge the gap of reasoning in NLU
tasks, leading to significant performance improvements, especially for smaller
LLMs, i.e., LLMs with a smaller number of parameters. NLU applications
developed using the STAR framework are also explainable: along with the
predicates generated, a justification in the form of a proof tree can be
produced for a given output. </font><br> Link: <a href='http://arxiv.org/pdf/2302.03780v2' target="_blank">http://arxiv.org/pdf/2302.03780v2</a><br> <br> <br> <font size='5'> 776 </font> <div style="text-align: right"> 2023-02-07 06:41:02+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT and Software Testing Education: Promises & Perils</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past decade, predictive language modeling for code has proven to be
a valuable tool for enabling new forms of automation for developers. More
recently, we have seen the advent of general purpose "large language models",
based on neural transformer architectures, that have been trained on massive
datasets of human written text spanning code and natural language. However,
despite the demonstrated representational power of such models, interacting
with them has historically been constrained to specific task settings, limiting
their general applicability. Many of these limitations were recently overcome
with the introduction of ChatGPT, a language model created by OpenAI and
trained to operate as a conversational agent, enabling it to answer questions
and respond to a wide variety of commands from end users. The introduction of
models, such as ChatGPT, has already spurred fervent discussion from educators,
ranging from fear that students could use these AI tools to circumvent
learning, to excitement about the new types of learning opportunities that they
might unlock. However, given the nascent nature of these tools, we currently
lack fundamental knowledge related to how well they perform in different
educational settings, and the potential promise (or danger) that they might
pose to traditional forms of instruction. As such, in this paper, we examine
how well ChatGPT performs when tasked with answering common questions in a
popular software testing curriculum. Our findings indicate that ChatGPT can
provide correct or partially correct answers in 55.6% of cases, provide correct
or partially correct explanations of answers in 53.0% of cases, and that
prompting the tool in a shared question context leads to a marginally higher
rate of correct responses. Based on these findings, we discuss the potential
promises and perils related to the use of ChatGPT by students and instructors. </font><br> Link: <a href='http://arxiv.org/pdf/2302.03287v3' target="_blank">http://arxiv.org/pdf/2302.03287v3</a><br> <br> <br> <font size='5'> 777 </font> <div style="text-align: right"> 2023-02-07 01:15:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific Literature</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This chapter presents a practical guide for conducting Sentiment Analysis
using Natural Language Processing (NLP) techniques in the domain of tick-borne
disease text. The aim is to demonstrate the process of how the presence of bias
in the discourse surrounding chronic manifestations of the disease can be
evaluated. The goal is to use a dataset of 5643 abstracts collected from
scientific journals on the topic of chronic Lyme disease to demonstrate using
Python, the steps for conducting sentiment analysis using pre-trained language
models and the process of validating the preliminary results using both
interpretable machine learning tools, as well as a novel methodology of using
emerging state-of-the-art large language models like ChatGPT. This serves as a
useful resource for researchers and practitioners interested in using NLP
techniques for sentiment analysis in the medical domain. </font><br> Link: <a href='http://arxiv.org/pdf/2302.06474v1' target="_blank">http://arxiv.org/pdf/2302.06474v1</a><br> <br> <br> <font size='5'> 778 </font> <div style="text-align: right"> 2023-02-06 04:21:59+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Categorical Archive of ChatGPT Failures</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots. </font><br> Link: <a href='http://arxiv.org/pdf/2302.03494v8' target="_blank">http://arxiv.org/pdf/2302.03494v8</a><br> <br> <br> <font size='5'> 779 </font> <div style="text-align: right"> 2023-02-05 08:56:45+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Regulating ChatGPT and other Large Generative AI Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable
Diffusion, are rapidly transforming the way we communicate, illustrate, and
create. However, AI regulation, in the EU and beyond, has primarily focused on
conventional AI models, not LGAIMs. This paper will situate these new
generative models in the current debate on trustworthy AI regulation, and ask
how the law can be tailored to their capabilities. After laying technical
foundations, the legal part of the paper proceeds in four steps, covering (1)
direct regulation, (2) data protection, (3) content moderation, and (4) policy
proposals. It suggests a novel terminology to capture the AI value chain in
LGAIM settings by differentiating between LGAIM developers, deployers,
professional and non-professional users, as well as recipients of LGAIM output.
We tailor regulatory duties to these different actors along the value chain and
suggest strategies to ensure that LGAIMs are trustworthy and deployed for the
benefit of society at large. Rules in the AI Act and other direct regulation
must match the specificities of pre-trained models. The paper argues for three
layers of obligations concerning LGAIMs (minimum standards for all LGAIMs;
high-risk obligations for high-risk use cases; collaborations along the AI
value chain). In general, regulation should focus on concrete high-risk
applications, and not the pre-trained model itself, and should include (i)
obligations regarding transparency and (ii) risk management. Non-discrimination
provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core
of the DSA content moderation rules should be expanded to cover LGAIMs. This
includes notice and action mechanisms, and trusted flaggers. In all areas,
regulators and lawmakers need to act fast to keep track with the dynamics of
ChatGPT et al. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02337v8' target="_blank">http://arxiv.org/pdf/2302.02337v8</a><br> <br> <br> <font size='5'> 780 </font> <div style="text-align: right"> 2023-02-04 05:19:31+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The field of data visualisation has long aimed to devise solutions for
generating visualisations directly from natural language text. Research in
Natural Language Interfaces (NLIs) has contributed towards the development of
such techniques. However, the implementation of workable NLIs has always been
challenging due to the inherent ambiguity of natural language, as well as in
consequence of unclear and poorly written user queries which pose problems for
existing language models in discerning user intent. Instead of pursuing the
usual path of developing new iterations of language models, this study uniquely
proposes leveraging the advancements in pre-trained large language models
(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly
into code for appropriate visualisations. This paper presents a novel system,
Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates
how, with effective prompt engineering, the complex problem of language
understanding can be solved more efficiently, resulting in simpler and more
accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs
together with the proposed prompts offer a reliable approach to rendering
visualisations from natural language queries, even when queries are highly
misspecified and underspecified. This solution also presents a significant
reduction in costs for the development of NLI systems, while attaining greater
visualisation inference abilities compared to traditional NLP approaches that
use hand-crafted grammar rules and tailored models. This study also presents
how LLM prompts can be constructed in a way that preserves data security and
privacy while being generalisable to different datasets. This work compares the
performance of GPT-3, Codex and ChatGPT across a number of case studies and
contrasts the performances with prior studies. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02094v2' target="_blank">http://arxiv.org/pdf/2302.02094v2</a><br> <br> <br> <font size='5'> 781 </font> <div style="text-align: right"> 2023-02-04 03:50:01+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Theory of Mind May Have Spontaneously Emerged in Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Theory of mind (ToM), or the ability to impute unobservable mental states to
others, is central to human social interactions, communication, empathy,
self-consciousness, and morality. We tested several language models using 40
classic false-belief tasks widely used to test ToM in humans. The models
published before 2020 showed virtually no ability to solve ToM tasks. Yet, the
first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40%
of false-belief tasks-performance comparable with 3.5-year-old children. Its
second version ("davinci-002"; January 2022) solved 70% of false-belief tasks,
performance comparable with six-year-olds. Its most recent version, GPT-3.5
("davinci-003"; November 2022), solved 90% of false-belief tasks, at the level
of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks
(95%). These findings suggest that ToM-like ability (thus far considered to be
uniquely human) may have spontaneously emerged as a byproduct of language
models' improving language skills. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02083v3' target="_blank">http://arxiv.org/pdf/2302.02083v3</a><br> <br> <br> <font size='5'> 782 </font> <div style="text-align: right"> 2023-02-03 19:25:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Exploring the Cognitive Dynamics of Artificial Intelligence in the Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The emergence of artificial intelligence has incited a paradigm shift across
the spectrum of human endeavors, with ChatGPT serving as a catalyst for the
transformation of various established domains, including but not limited to
education, journalism, security, and ethics. In the post-pandemic era, the
widespread adoption of remote work has prompted the educational sector to
reassess conventional pedagogical methods. This paper is to scrutinize the
underlying psychological principles of ChatGPT, delve into the factors that
captivate user attention, and implicate its ramifications on the future of
learning. The ultimate objective of this study is to instigate a scholarly
discourse on the interplay between technological advancements in education and
the evolution of human learning patterns, raising the question of whether
technology is driving human evolution or vice versa. </font><br> Link: <a href='http://arxiv.org/pdf/2302.04818v1' target="_blank">http://arxiv.org/pdf/2302.04818v1</a><br> <br> <br> <font size='5'> 783 </font> <div style="text-align: right"> 2023-02-03 01:28:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Systematic reviews are comprehensive reviews of the literature for a highly
focused research question. These reviews are often treated as the highest form
of evidence in evidence-based medicine, and are the key strategy to answer
research questions in the medical field. To create a high-quality systematic
review, complex Boolean queries are often constructed to retrieve studies for
the review topic. However, it often takes a long time for systematic review
researchers to construct a high quality systematic review Boolean query, and
often the resulting queries are far from effective. Poor queries may lead to
biased or invalid reviews, because they missed to retrieve key evidence, or to
extensive increase in review costs, because they retrieved too many irrelevant
studies. Recent advances in Transformer-based generative models have shown
great potential to effectively follow instructions from users and generate
answers based on the instructions being made. In this paper, we investigate the
effectiveness of the latest of such models, ChatGPT, in generating effective
Boolean queries for systematic review literature search. Through a number of
extensive experiments on standard test collections for the task, we find that
ChatGPT is capable of generating queries that lead to high search precision,
although trading-off this for recall. Overall, our study demonstrates the
potential of ChatGPT in generating effective Boolean queries for systematic
review literature search. The ability of ChatGPT to follow complex instructions
and generate queries with high precision makes it a valuable tool for
researchers conducting systematic reviews, particularly for rapid reviews where
time is a constraint and often trading-off higher precision for lower recall is
acceptable. </font><br> Link: <a href='http://arxiv.org/pdf/2302.03495v3' target="_blank">http://arxiv.org/pdf/2302.03495v3</a><br> <br> <br> <font size='5'> 784 </font> <div style="text-align: right"> 2023-02-01 10:59:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Netizens, Academicians, and Information Professionals' Opinions About AI With Special Reference To ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study aims to understand the perceptions and opinions of academicians
towards ChatGPT-3 by collecting and analyzing social media comments, and a
survey was conducted with library and information science professionals. The
research uses a content analysis method and finds that while ChatGPT-3 can be a
valuable tool for research and writing, it is not 100% accurate and should be
cross-checked. The study also finds that while some academicians may not accept
ChatGPT-3, most are starting to accept it. The study is beneficial for
academicians, content developers, and librarians. </font><br> Link: <a href='http://arxiv.org/pdf/2302.07136v1' target="_blank">http://arxiv.org/pdf/2302.07136v1</a><br> <br> <br> <font size='5'> 785 </font> <div style="text-align: right"> 2023-02-01 02:54:43+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Grading Conversational Responses Of Chatbots</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Chatbots have long been capable of answering basic questions and even
responding to obscure prompts, but recently their improvements have been far
more significant. Modern chatbots like Open AIs ChatGPT3 not only have the
ability to answer basic questions but can write code and movie scripts and
imitate well-known people. In this paper, we analyze ChatGPTs' responses to
various questions from a dataset of queries from the popular Quora forum. We
submitted sixty questions to ChatGPT and scored the answers based on three
industry-standard metrics for grading machine translation: BLEU, METEOR, and
ROUGE. These metrics allow us to compare the machine responses with the most
upvoted human answer to the same question to assess ChatGPT's ability to submit
a humanistic reply. The results showed that while the responses and translation
abilities of ChatGPT are remarkable, they still fall short of what a typical
human reaction would be. </font><br> Link: <a href='http://arxiv.org/pdf/2303.12038v1' target="_blank">http://arxiv.org/pdf/2303.12038v1</a><br> <br> <br> <font size='5'> 786 </font> <div style="text-align: right"> 2023-01-31 18:59:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Mathematical Capabilities of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We investigate the mathematical capabilities of ChatGPT by testing it on
publicly available datasets, as well as hand-crafted ones, and measuring its
performance against other models trained on a mathematical corpus, such as
Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily
professional activities of mathematicians (question answering, theorem
searching). In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover
elementary mathematics. We address this issue by introducing a new dataset:
GHOSTS. It is the first natural-language dataset made and curated by working
researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of
language models. We benchmark ChatGPT on GHOSTS and evaluate performance
against fine-grained criteria. We make this new dataset publicly available to
assist a community-driven comparison of ChatGPT with (future) large language
models in terms of advanced mathematical comprehension. We conclude that
contrary to many positive reports in the media (a potential case of selection
bias), ChatGPT's mathematical abilities are significantly below those of an
average mathematics graduate student. Our results show that ChatGPT often
understands the question but fails to provide correct solutions. Hence, if your
goal is to use it to pass a university exam, you would be better off copying
from your average peer! </font><br> Link: <a href='http://arxiv.org/pdf/2301.13867v1' target="_blank">http://arxiv.org/pdf/2301.13867v1</a><br> <br> <br> <font size='5'> 787 </font> <div style="text-align: right"> 2023-01-31 03:14:57+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique
testbeds for exploring the translation challenges of turning literacy into
numeracy. Previous publicly-available transformer models from eighteen months
prior and 1000 times smaller failed to provide basic arithmetic. The
statistical analysis of four complex datasets described here combines
arithmetic manipulations that cannot be memorized or encoded by simple rules.
The work examines whether next-token prediction succeeds from sentence
completion into the realm of actual numerical understanding. For example, the
work highlights cases for descriptive statistics on in-memory datasets that the
LLM initially loads from memory or generates randomly using python libraries.
The resulting exploratory data analysis showcases the model's capabilities to
group by or pivot categorical sums, infer feature importance, derive
correlations, and predict unseen test cases using linear regression. To extend
the model's testable range, the research deletes and appends random rows such
that recall alone cannot explain emergent numeracy. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13382v1' target="_blank">http://arxiv.org/pdf/2301.13382v1</a><br> <br> <br> <font size='5'> 788 </font> <div style="text-align: right"> 2023-01-30 19:22:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Conversational Automated Program Repair</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Automated Program Repair (APR) can help developers automatically generate
patches for bugs. Due to the impressive performance obtained using Large
Pre-Trained Language Models (LLMs) on many code related tasks, researchers have
started to directly use LLMs for APR. However, prior approaches simply
repeatedly sample the LLM given the same constructed input/prompt created from
the original buggy code, which not only leads to generating the same incorrect
patches repeatedly but also miss the critical information in testcases. To
address these limitations, we propose conversational APR, a new paradigm for
program repair that alternates between patch generation and validation in a
conversational manner. In conversational APR, we iteratively build the input to
the model by combining previously generated patches with validation feedback.
As such, we leverage the long-term context window of LLMs to not only avoid
generating previously incorrect patches but also incorporate validation
feedback to help the model understand the semantic meaning of the program under
test. We evaluate 10 different LLM including the newly developed ChatGPT model
to demonstrate the improvement of conversational APR over the prior LLM for APR
approach. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13246v1' target="_blank">http://arxiv.org/pdf/2301.13246v1</a><br> <br> <br> <font size='5'> 789 </font> <div style="text-align: right"> 2023-01-30 13:20:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language models (LLMs) have significantly impacted businesses such as report
summarization software and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is little systematic examination and user study of the
risks and harmful behaviors of current LLM usage. To further educate future
efforts on constructing ethical LLMs responsibly, we perform a qualitative
research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this
paper, ChatGPT refers to the version released on Dec 15th.} to better
understand the practical features of ethical dangers in recent LLMs. We analyze
ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)
\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on AI ethics
and harmal behaviors of ChatGPT, as well as future problems and practical
design considerations for responsible LLMs. We believe that our findings may
give light on future efforts to determine and mitigate the ethical hazards
posed by machines in LLM applications. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12867v4' target="_blank">http://arxiv.org/pdf/2301.12867v4</a><br> <br> <br> <font size='5'> 790 </font> <div style="text-align: right"> 2023-01-30 08:06:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has the ability to generate grammatically flawless and
seemingly-human replies to different types of questions from various domains.
The number of its users and of its applications is growing at an unprecedented
rate. Unfortunately, use and abuse come hand in hand. In this paper, we study
whether a machine learning model can be effectively trained to accurately
distinguish between original human and seemingly human (that is,
ChatGPT-generated) text, especially when this text is short. Furthermore, we
employ an explainable artificial intelligence framework to gain insight into
the reasoning behind the model trained to differentiate between
ChatGPT-generated and human-generated text. The goal is to analyze model's
decisions and determine if any specific patterns or characteristics can be
identified. Our study focuses on short online reviews, conducting two
experiments comparing human-generated and ChatGPT-generated text. The first
experiment involves ChatGPT text generated from custom queries, while the
second experiment involves text generated by rephrasing original
human-generated reviews. We fine-tune a Transformer-based model and use it to
make predictions, which are then explained using SHAP. We compare our model
with a perplexity score-based approach and find that disambiguation between
human and ChatGPT-generated reviews is more challenging for the ML model when
using rephrased text. However, our proposed approach still achieves an accuracy
of 79%. Using explainability, we observe that ChatGPT's writing is polite,
without specific details, using fancy and atypical vocabulary, impersonal, and
typically it does not express feelings. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13852v1' target="_blank">http://arxiv.org/pdf/2301.13852v1</a><br> <br> <br> <font size='5'> 791 </font> <div style="text-align: right"> 2023-01-28 11:47:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Navigating the diverse solution spaces of non-trivial software engineering
tasks requires a combination of technical knowledge, problem-solving skills,
and creativity. With multiple possible solutions available, each with its own
set of trade-offs, it is essential for programmers to evaluate the various
options and select the one that best suits the specific requirements and
constraints of a project. Whether it is choosing from a range of libraries,
weighing the pros and cons of different architecture and design solutions, or
finding unique ways to fulfill user requirements, the ability to think
creatively is crucial for making informed decisions that will result in
efficient and effective software. However, the interfaces of current chatbot
tools for programmers, such as OpenAI's ChatGPT or GitHub Copilot, are
optimized for presenting a single solution, even for complex queries. While
other solutions can be requested, they are not displayed by default and are not
intuitive to access. In this paper, we present our work-in-progress prototype
"GPTCompare", which allows programmers to visually compare multiple source code
solutions generated by GPT-n models for the same programming-related query by
highlighting their similarities and differences. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12169v1' target="_blank">http://arxiv.org/pdf/2301.12169v1</a><br> <br> <br> <font size='5'> 792 </font> <div style="text-align: right"> 2023-01-28 08:45:30+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Could an Artificial-Intelligence agent pass an introductory physics course?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Massive pre-trained language models have garnered attention and controversy
due to their ability to generate human-like responses: attention due to their
frequent indistinguishability from human-generated phraseology and narratives,
and controversy due to the fact that their convincingly presented arguments and
facts are frequently simply false. Just how human-like are these responses when
it comes to dialogues about physics, in particular about the standard content
of introductory physics courses? This study explores that question by having
ChatGTP, the pre-eminent language model in 2023, work through representative
assessment content of an actual calculus-based physics course and grading the
responses in the same way human responses would be graded. As it turns out,
ChatGPT would narrowly pass this course while exhibiting many of the
preconceptions and errors of a beginning learner. </font><br> Link: <a href='http://arxiv.org/pdf/2301.12127v2' target="_blank">http://arxiv.org/pdf/2301.12127v2</a><br> <br> <br> <font size='5'> 793 </font> <div style="text-align: right"> 2023-01-28 02:47:50+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Truth Machines: Synthesizing Veracity in AI Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As AI technologies are rolled out into healthcare, academia, human resources,
law, and a multitude of other domains, they become de-facto arbiters of truth.
But truth is highly contested, with many different definitions and approaches.
This article discusses the struggle for truth in AI systems and the general
responses to date. It then investigates the production of truth in InstructGPT,
a large language model, highlighting how data harvesting, model architectures,
and social feedback mechanisms weave together disparate understandings of
veracity. It conceptualizes this performance as an operationalization of truth,
where distinct, often conflicting claims are smoothly synthesized and
confidently presented into truth-statements. We argue that these same logics
and inconsistencies play out in Instruct's successor, ChatGPT, reiterating
truth as a non-trivial problem. We suggest that enriching sociality and
thickening "reality" are two promising vectors for enhancing the
truth-evaluating capacities of future language models. We conclude, however, by
stepping back to consider AI truth-telling as a social practice: what kind of
"truth" do we as listeners desire? </font><br> Link: <a href='http://arxiv.org/pdf/2301.12066v1' target="_blank">http://arxiv.org/pdf/2301.12066v1</a><br> <br> <br> <font size='5'> 794 </font> <div style="text-align: right"> 2023-01-27 12:05:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Investigating the use of ChatGPT for the scheduling of construction projects</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models such as ChatGPT have the potential to revolutionize the
construction industry by automating repetitive and time-consuming tasks. This
paper presents a study in which ChatGPT was used to generate a construction
schedule for a simple construction project. The output from ChatGPT was
evaluated by a pool of participants that provided feedback regarding their
overall interaction experience and the quality of the output. The results show
that ChatGPT can generate a coherent schedule that follows a logical approach
to fulfill the requirements of the scope indicated. The participants had an
overall positive interaction experience and indicated the great potential of
such a tool to automate many preliminary and time-consuming tasks. However, the
technology still has limitations, and further development is needed before it
can be widely adopted in the industry. Overall, this study highlights the
potential of using large language models in the construction industry and the
need for further research. </font><br> Link: <a href='http://arxiv.org/pdf/2302.02805v1' target="_blank">http://arxiv.org/pdf/2302.02805v1</a><br> <br> <br> <font size='5'> 795 </font> <div style="text-align: right"> 2023-01-27 08:45:53+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ThoughtSource: A central hub for large language model reasoning data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models (LLMs) such as GPT-3 and ChatGPT have recently
demonstrated impressive results across a wide range of tasks. LLMs are still
limited, however, in that they frequently fail at complex reasoning, their
reasoning processes are opaque, they are prone to 'hallucinate' facts, and
there are concerns about their underlying biases. Letting models verbalize
reasoning steps as natural language, a technique known as chain-of-thought
prompting, has recently been proposed as a way to address some of these issues.
Here we present the first release of ThoughtSource, a meta-dataset and software
library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to
improve future artificial intelligence systems by facilitating qualitative
understanding of CoTs, enabling empirical evaluations, and providing training
data. This first release of ThoughtSource integrates six scientific/medical,
three general-domain and five math word question answering datasets. </font><br> Link: <a href='http://arxiv.org/pdf/2301.11596v2' target="_blank">http://arxiv.org/pdf/2301.11596v2</a><br> <br> <br> <font size='5'> 796 </font> <div style="text-align: right"> 2023-01-27 03:00:46+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Theme-driven Keyphrase Extraction to Analyze Social Media Discourse</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Social media platforms are vital resources for sharing self-reported health
experiences, offering rich data on various health topics. Despite advancements
in Natural Language Processing (NLP) enabling large-scale social media data
analysis, a gap remains in applying keyphrase extraction to health-related
content. Keyphrase extraction is used to identify salient concepts in social
media discourse without being constrained by predefined entity classes. This
paper introduces a theme-driven keyphrase extraction framework tailored for
social media, a pioneering approach designed to capture clinically relevant
keyphrases from user-generated health texts. Themes are defined as broad
categories determined by the objectives of the extraction task. We formulate
this novel task of theme-driven keyphrase extraction and demonstrate its
potential for efficiently mining social media text for the use case of
treatment for opioid use disorder. This paper leverages qualitative and
quantitative analysis to demonstrate the feasibility of extracting actionable
insights from social media data and efficiently extracting keyphrases using
minimally supervised NLP models. Our contributions include the development of a
novel data collection and curation framework for theme-driven keyphrase
extraction and the creation of MOUD-Keyphrase, the first dataset of its kind
comprising human-annotated keyphrases from a Reddit community. We also identify
the scope of minimally supervised NLP models to extract keyphrases from social
media data efficiently. Lastly, we found that a large language model (ChatGPT)
outperforms unsupervised keyphrase extraction models, and we evaluate its
efficacy in this task. </font><br> Link: <a href='http://arxiv.org/pdf/2301.11508v2' target="_blank">http://arxiv.org/pdf/2301.11508v2</a><br> <br> <br> <font size='5'> 797 </font> <div style="text-align: right"> 2023-01-24 19:23:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery. </font><br> Link: <a href='http://arxiv.org/pdf/2301.13819v2' target="_blank">http://arxiv.org/pdf/2301.13819v2</a><br> <br> <br> <font size='5'> 798 </font> <div style="text-align: right"> 2023-01-24 14:24:44+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Putting ChatGPT's Medical Advice to the (Turing) Test</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Objective: Assess the feasibility of using ChatGPT or a similar AI-based
chatbot for patient-provider communication. Participants: A US representative
sample of 430 study participants aged 18 and above. 53.2% of respondents
analyzed were women; their average age was 47.1. Exposure: Ten representative
non-administrative patient-provider interactions were extracted from the EHR.
Patients' questions were placed in ChatGPT with a request for the chatbot to
respond using approximately the same word count as the human provider's
response. In the survey, each patient's question was followed by a provider- or
ChatGPT-generated response. Participants were informed that five responses were
provider-generated and five were chatbot-generated. Participants were asked,
and incentivized financially, to correctly identify the response source.
Participants were also asked about their trust in chatbots' functions in
patient-provider communication, using a Likert scale of 1-5. Results: The
correct classification of responses ranged between 49.0% to 85.7% for different
questions. On average, chatbot responses were correctly identified 65.5% of the
time, and provider responses were correctly distinguished 65.1% of the time. On
average, responses toward patients' trust in chatbots' functions were weakly
positive (mean Likert score: 3.4), with lower trust as the health-related
complexity of the task in questions increased. Conclusions: ChatGPT responses
to patient questions were weakly distinguishable from provider responses.
Laypeople appear to trust the use of chatbots to answer lower risk health
questions. </font><br> Link: <a href='http://arxiv.org/pdf/2301.10035v1' target="_blank">http://arxiv.org/pdf/2301.10035v1</a><br> <br> <br> <font size='5'> 799 </font> <div style="text-align: right"> 2023-01-20 16:01:47+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Analysis of the Automatic Bug Fixing Performance of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: To support software developers in finding and fixing software bugs, several
automated program repair techniques have been introduced. Given a test suite,
standard methods usually either synthesize a repair, or navigate a search space
of software edits to find test-suite passing variants. Recent program repair
methods are based on deep learning approaches. One of these novel methods,
which is not primarily intended for automated program repair, but is still
suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is
so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard
bug fixing benchmark set, QuixBugs, and compare the performance with the
results of several other approaches reported in the literature. We find that
ChatGPT's bug fixing performance is competitive to the common deep learning
approaches CoCoNut and Codex and notably better than the results reported for
the standard program repair approaches. In contrast to previous approaches,
ChatGPT offers a dialogue system through which further information, e.g., the
expected output for a certain input or an observed error message, can be
entered. By providing such hints to ChatGPT, its success rate can be further
increased, fixing 31 out of 40 bugs, outperforming state-of-the-art. </font><br> Link: <a href='http://arxiv.org/pdf/2301.08653v1' target="_blank">http://arxiv.org/pdf/2301.08653v1</a><br> <br> <br> <font size='5'> 800 </font> <div style="text-align: right"> 2023-01-20 08:51:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This report provides a preliminary evaluation of ChatGPT for machine
translation, including translation prompt, multilingual translation, and
translation robustness. We adopt the prompts advised by ChatGPT to trigger its
translation ability and find that the candidate prompts generally work well and
show minor performance differences. By evaluating on a number of benchmark test
sets, we find that ChatGPT performs competitively with commercial translation
products (e.g., Google Translate) on high-resource European languages but lags
behind significantly on low-resource or distant languages. For distant
languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$
that asks ChatGPT to translate the source sentence into a high-resource pivot
language before into the target language, which improves the translation
performance significantly. As for the translation robustness, ChatGPT does not
perform as well as the commercial systems on biomedical abstracts or Reddit
comments but exhibits good results on spoken language. With the launch of the
GPT-4 engine, the translation performance of ChatGPT is significantly boosted,
becoming comparable to commercial translation products, even for distant
languages. In other words,
$\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data:
https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator </font><br> Link: <a href='http://arxiv.org/pdf/2301.08745v3' target="_blank">http://arxiv.org/pdf/2301.08745v3</a><br> <br> <br> <font size='5'> 801 </font> <div style="text-align: right"> 2023-01-18 15:23:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The introduction of ChatGPT has garnered widespread attention in both
academic and industrial communities. ChatGPT is able to respond effectively to
a wide range of human questions, providing fluent and comprehensive answers
that significantly surpass previous public chatbots in terms of security and
usefulness. On one hand, people are curious about how ChatGPT is able to
achieve such strength and how far it is from human experts. On the other hand,
people are starting to worry about the potential negative impacts that large
language models (LLMs) like ChatGPT could have on society, such as fake news,
plagiarism, and social security issues. In this work, we collected tens of
thousands of comparison responses from both human experts and ChatGPT, with
questions ranging from open-domain, financial, medical, legal, and
psychological areas. We call the collected dataset the Human ChatGPT Comparison
Corpus (HC3). Based on the HC3 dataset, we study the characteristics of
ChatGPT's responses, the differences and gaps from human experts, and future
directions for LLMs. We conducted comprehensive human evaluations and
linguistic analyses of ChatGPT-generated content compared with that of humans,
where many interesting results are revealed. After that, we conduct extensive
experiments on how to effectively detect whether a certain text is generated by
ChatGPT or humans. We build three different detection systems, explore several
key factors that influence their effectiveness, and evaluate them in different
scenarios. The dataset, code, and models are all publicly available at
https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07597v1' target="_blank">http://arxiv.org/pdf/2301.07597v1</a><br> <br> <br> <font size='5'> 802 </font> <div style="text-align: right"> 2023-01-13 20:24:38+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The moral authority of ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: ChatGPT is not only fun to chat with, but it also searches information,
answers questions, and gives advice. With consistent moral advice, it might
improve the moral judgment and decisions of users, who often hold contradictory
moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral
advisor. Nonetheless, it influences users' moral judgment, we find in an
experiment, even if they know they are advised by a chatting bot, and they
underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt
rather than improves users' judgment. These findings raise the question of how
to ensure the responsible use of ChatGPT and similar AI. Transparency is often
touted but seems ineffective. We propose training to improve digital literacy. </font><br> Link: <a href='http://arxiv.org/pdf/2301.07098v1' target="_blank">http://arxiv.org/pdf/2301.07098v1</a><br> <br> <br> <font size='5'> 803 </font> <div style="text-align: right"> 2023-01-11 15:48:36+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT is not all you need. A State of the Art Review of large Generative AI models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: During the last two years there has been a plethora of large generative
models such as ChatGPT or Stable Diffusion that have been published.
Concretely, these models are able to perform tasks such as being a general
question and answering system or automatically creating artistic images that
are revolutionizing several sectors. Consequently, the implications that these
generative models have in the industry and society are enormous, as several job
positions may be transformed. For example, Generative AI is capable of
transforming effectively and creatively texts to images, like the DALLE-2
model; text to 3D images, like the Dreamfusion model; images to text, like the
Flamingo model; texts to video, like the Phenaki model; texts to audio, like
the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the
Codex model; texts to scientific texts, like the Galactica model or even create
algorithms like AlphaTensor. This work consists on an attempt to describe in a
concise way the main models are sectors that are affected by generative AI and
to provide a taxonomy of the main generative models published recently. </font><br> Link: <a href='http://arxiv.org/pdf/2301.04655v1' target="_blank">http://arxiv.org/pdf/2301.04655v1</a><br> <br> <br> <font size='5'> 804 </font> <div style="text-align: right"> 2023-01-10 16:57:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: In this case study, we explore the capabilities and limitations of ChatGPT, a
natural language processing model developed by OpenAI, in the field of string
theoretical swampland conjectures. We find that it is effective at paraphrasing
and explaining concepts in a variety of styles, but not at genuinely connecting
concepts. It will provide false information with full confidence and make up
statements when necessary. However, its ingenious use of language can be
fruitful for identifying analogies and describing visual representations of
abstract concepts. </font><br> Link: <a href='http://arxiv.org/pdf/2301.08155v1' target="_blank">http://arxiv.org/pdf/2301.08155v1</a><br> <br> <br> <font size='5'> 805 </font> <div style="text-align: right"> 2023-01-10 03:43:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots in a Honeypot World</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Question-and-answer agents like ChatGPT offer a novel tool for use as a
potential honeypot interface in cyber security. By imitating Linux, Mac, and
Windows terminal commands and providing an interface for TeamViewer, nmap, and
ping, it is possible to create a dynamic environment that can adapt to the
actions of attackers and provide insight into their tactics, techniques, and
procedures (TTPs). The paper illustrates ten diverse tasks that a
conversational agent or large language model might answer appropriately to the
effects of command-line attacker. The original result features feasibility
studies for ten model tasks meant for defensive teams to mimic expected
honeypot interfaces with minimal risks. Ultimately, the usefulness outside of
forensic activities stems from whether the dynamic honeypot can extend the
time-to-conquer or otherwise delay attacker timelines short of reaching key
network assets like databases or confidential information. While ongoing
maintenance and monitoring may be required, ChatGPT's ability to detect and
deflect malicious activity makes it a valuable option for organizations seeking
to enhance their cyber security posture. Future work will focus on
cybersecurity layers, including perimeter security, host virus detection, and
data security. </font><br> Link: <a href='http://arxiv.org/pdf/2301.03771v1' target="_blank">http://arxiv.org/pdf/2301.03771v1</a><br> <br> <br> <font size='5'> 806 </font> <div style="text-align: right"> 2023-01-05 18:49:21+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Can Large Language Models Change User Preference Adversarially?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Pretrained large language models (LLMs) are becoming increasingly powerful
and ubiquitous in mainstream applications such as being a personal assistant, a
dialogue model, etc. As these models become proficient in deducing user
preferences and offering tailored assistance, there is an increasing concern
about the ability of these models to influence, modify and in the extreme case
manipulate user preference adversarially. The issue of lack of interpretability
in these models in adversarial settings remains largely unsolved. This work
tries to study adversarial behavior in user preferences from the lens of
attention probing, red teaming and white-box analysis. Specifically, it
provides a bird's eye view of existing literature, offers red teaming samples
for dialogue models like ChatGPT and GODEL and probes the attention mechanism
in the latter for non-adversarial and adversarial settings. </font><br> Link: <a href='http://arxiv.org/pdf/2302.10291v1' target="_blank">http://arxiv.org/pdf/2302.10291v1</a><br> <br> <br> <font size='5'> 807 </font> <div style="text-align: right"> 2023-01-05 07:13:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Conversational artificial intelligence (AI) disrupts how humans interact with
technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue
model that can converse with its human counterparts with unprecedented
capabilities. ChatGPT has witnessed tremendous attention from the media,
academia, industry, and the general public, attracting more than a million
users within days of its release. However, its explosive adoption for
information search and as an automated decision aid underscores the importance
to understand its limitations and biases. This paper focuses on one of
democratic society's most important decision-making processes: political
elections. Prompting ChatGPT with 630 political statements from two leading
voting advice applications and the nation-agnostic political compass test in
three pre-registered experiments, we uncover ChatGPT's pro-environmental,
left-libertarian ideology. For example, ChatGPT would impose taxes on flights,
restrict rent increases, and legalize abortion. In the 2021 elections, it would
have voted most likely for the Greens both in Germany (B\"undnis 90/Die
Gr\"unen) and in the Netherlands (GroenLinks). Our findings are robust when
negating the prompts, reversing the order of the statements, varying prompt
formality, and across languages (English, German, Dutch, and Spanish). We
conclude by discussing the implications of politically biased conversational AI
on society. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01768v1' target="_blank">http://arxiv.org/pdf/2301.01768v1</a><br> <br> <br> <font size='5'> 808 </font> <div style="text-align: right"> 2023-01-03 23:11:03+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting. </font><br> Link: <a href='http://arxiv.org/pdf/2301.05517v2' target="_blank">http://arxiv.org/pdf/2301.05517v2</a><br> <br> <br> <font size='5'> 809 </font> <div style="text-align: right"> 2023-01-01 22:50:08+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Modeling Label Semantics Improves Activity Recognition</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Human activity recognition (HAR) aims to classify sensory time series into
different activities, with wide applications in activity tracking, healthcare,
human computer interaction, etc. Existing HAR works improve recognition
performance by designing more complicated feature extraction methods, but they
neglect the label semantics by simply treating labels as integer IDs. We find
that many activities in the current HAR datasets have shared label names, e.g.,
"open door" and "open fridge", "walk upstairs" and "walk downstairs". Through
some exploratory analysis, we find that such shared structure in activity names
also maps to similarity in the input features. To this end, we design a
sequence-to-sequence framework to decode the label name semantics rather than
classifying labels as integer IDs. Our proposed method decomposes learning
activities into learning shared tokens ("open", "walk"), which is easier than
learning the joint distribution ("open fridge", "walk upstairs") and helps
transfer learning to activities with insufficient data samples. For datasets
originally without shared tokens in label names, we also offer an automated
method, using OpenAI's ChatGPT, to generate shared actions and objects.
Extensive experiments on seven HAR benchmark datasets demonstrate the
state-of-the-art performance of our method. We also show better performance in
the long-tail activity distribution settings and few-shot settings. </font><br> Link: <a href='http://arxiv.org/pdf/2301.03462v1' target="_blank">http://arxiv.org/pdf/2301.03462v1</a><br> <br> <br> <font size='5'> 810 </font> <div style="text-align: right"> 2023-01-01 03:04:04+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: New chat AI applications like ChatGPT offer an advanced understanding of
question context and memory across multi-step tasks, such that experiments can
test its deductive reasoning. This paper proposes a multi-role and multi-step
challenge, where ChatGPT plays the classic twenty-questions game but
innovatively switches roles from the questioner to the answerer. The main
empirical result establishes that this generation of chat applications can
guess random object names in fewer than twenty questions (average, 12) and
correctly guess 94% of the time across sixteen different experimental setups.
The research introduces four novel cases where the chatbot fields the
questions, asks the questions, both question-answer roles, and finally tries to
guess appropriate contextual emotions. One task that humans typically fail but
trained chat applications complete involves playing bilingual games of twenty
questions (English answers to Spanish questions). Future variations address
direct problem-solving using a similar inquisitive format to arrive at novel
outcomes deductively, such as patentable inventions or combination thinking.
Featured applications of this dialogue format include complex protein designs,
neuroscience metadata, and child development educational materials. </font><br> Link: <a href='http://arxiv.org/pdf/2301.01743v1' target="_blank">http://arxiv.org/pdf/2301.01743v1</a><br> <br> <br> <font size='5'> 811 </font> <div style="text-align: right"> 2022-12-30 18:55:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The release of ChatGPT, a language model capable of generating text that
appears human-like and authentic, has gained significant attention beyond the
research community. We expect that the convincing performance of ChatGPT
incentivizes users to apply it to a variety of downstream tasks, including
prompting the model to simplify their own medical reports. To investigate this
phenomenon, we conducted an exploratory case study. In a questionnaire, we
asked 15 radiologists to assess the quality of radiology reports simplified by
ChatGPT. Most radiologists agreed that the simplified reports were factually
correct, complete, and not potentially harmful to the patient. Nevertheless,
instances of incorrect statements, missed key medical findings, and potentially
harmful passages were reported. While further studies are needed, the initial
insights of this study indicate a great potential in using large language
models like ChatGPT to improve patient-centered care in radiology and other
medical domains. </font><br> Link: <a href='http://arxiv.org/pdf/2212.14882v1' target="_blank">http://arxiv.org/pdf/2212.14882v1</a><br> <br> <br> <font size='5'> 812 </font> <div style="text-align: right"> 2022-12-30 05:03:15+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: How would Stance Detection Techniques Evolve after the Launch of ChatGPT?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection. </font><br> Link: <a href='http://arxiv.org/pdf/2212.14548v3' target="_blank">http://arxiv.org/pdf/2212.14548v3</a><br> <br> <br> <font size='5'> 813 </font> <div style="text-align: right"> 2022-12-22 13:05:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Death of the Short-Form Physics Essay in the Coming AI Revolution</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The latest AI language modules can produce original, high quality full
short-form ($300$-word) Physics essays within seconds. These technologies such
as ChatGPT and davinci-003 are freely available to anyone with an internet
connection. In this work, we present evidence of AI generated short-form essays
achieving first-class grades on an essay writing assessment from an accredited,
current university Physics module. The assessment requires students answer five
open-ended questions with a short, $300$-word essay each. Fifty AI answers were
generated to create ten submissions that were independently marked by five
separate markers. The AI generated submissions achieved an average mark of $71
\pm 2 \%$, in strong agreement with the current module average of $71 \pm 5 %$.
A typical AI submission would therefore most-likely be awarded a First Class,
the highest classification available at UK universities. Plagiarism detection
software returned a plagiarism score between $2 \pm 1$% (Grammarly) and $7 \pm
2$% (TurnitIn). We argue that these results indicate that current AI MLPs
represent a significant threat to the fidelity of short-form essays as an
assessment method in Physics courses. </font><br> Link: <a href='http://arxiv.org/pdf/2212.11661v1' target="_blank">http://arxiv.org/pdf/2212.11661v1</a><br> <br> <br> <font size='5'> 814 </font> <div style="text-align: right"> 2022-12-20 18:37:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We consider the end-to-end abstract-to-title generation problem, exploring
seven recent transformer based models (including ChatGPT) fine-tuned on more
than 30k abstract-title pairs from NLP and machine learning venues. As an
extension, we also consider the harder problem of generating humorous paper
titles. For the latter, we compile the first large-scale humor annotated
dataset for scientific papers in the NLP/ML domains, comprising almost 2.5k
titles. We evaluate all models using human and automatic metrics. Our human
evaluation suggests that our best end-to-end system performs similarly to human
authors (but arguably slightly worse). Generating funny titles is more
difficult, however, and our automatic systems clearly underperform relative to
humans and often learn dataset artefacts of humor. Finally, ChatGPT, without
any fine-tuning, performs on the level of our best fine-tuned system. </font><br> Link: <a href='http://arxiv.org/pdf/2212.10522v1' target="_blank">http://arxiv.org/pdf/2212.10522v1</a><br> <br> <br> <font size='5'> 815 </font> <div style="text-align: right"> 2022-12-20 17:49:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: State-of-the-art poetry generation systems are often complex. They either
consist of task-specific model pipelines, incorporate prior knowledge in the
form of manually created constraints, or both. In contrast, end-to-end models
would not suffer from the overhead of having to model prior knowledge and could
learn the nuances of poetry from data alone, reducing the degree of human
supervision required. In this work, we investigate end-to-end poetry generation
conditioned on styles such as rhyme, meter, and alliteration. We identify and
address lack of training data and mismatching tokenization algorithms as
possible limitations of past attempts. In particular, we successfully pre-train
ByGPT5, a new token-free decoder-only language model, and fine-tune it on a
large custom corpus of English and German quatrains annotated with our styles.
We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and
ChatGPT, while also being more parameter efficient and performing favorably
compared to humans. In addition, we analyze its runtime performance and
demonstrate that it is not prone to memorization. We make our code, models, and
datasets publicly available. </font><br> Link: <a href='http://arxiv.org/pdf/2212.10474v2' target="_blank">http://arxiv.org/pdf/2212.10474v2</a><br> <br> <br> <font size='5'> 816 </font> <div style="text-align: right"> 2022-12-20 04:33:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Are Deep Neural Networks SMARTer than Second Graders?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Recent times have witnessed an increasing number of applications of deep
neural networks towards solving tasks that require superior cognitive
abilities, e.g., playing Go, generating art, ChatGPT, etc. Such a dramatic
progress raises the question: how generalizable are neural networks in solving
problems that demand broad skills? To answer this question, we propose SMART: a
Simple Multimodal Algorithmic Reasoning Task and the associated SMART-101
dataset, for evaluating the abstraction, deduction, and generalization
abilities of neural networks in solving visuo-linguistic puzzles designed
specifically for children in the 6--8 age group. Our dataset consists of 101
unique puzzles; each puzzle comprises a picture and a question, and their
solution needs a mix of several elementary skills, including arithmetic,
algebra, and spatial reasoning, among others. To scale our dataset towards
training deep neural networks, we programmatically generate entirely new
instances for each puzzle, while retaining their solution algorithm. To
benchmark performances on SMART-101, we propose a vision and language
meta-learning model using varied state-of-the-art backbones. Our experiments
reveal that while powerful deep models offer reasonable performances on puzzles
in a supervised setting, they are not better than random accuracy when analyzed
for generalization. We also evaluate the recent ChatGPT and other large
language models on a subset of SMART-101 and find that while these models show
convincing reasoning abilities, the answers are often incorrect. </font><br> Link: <a href='http://arxiv.org/pdf/2212.09993v5' target="_blank">http://arxiv.org/pdf/2212.09993v5</a><br> <br> <br> <font size='5'> 817 </font> <div style="text-align: right"> 2022-12-19 16:29:26+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Unsupervised Summarization Re-ranking</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: With the rise of task-specific pre-training objectives, abstractive
summarization models like PEGASUS offer appealing zero-shot performance on
downstream summarization tasks. However, the performance of such unsupervised
models still lags significantly behind their supervised counterparts. Similarly
to the supervised setup, we notice a very high variance in quality among
summary candidates from these models while only one candidate is kept as the
summary output. In this paper, we propose to re-rank summary candidates in an
unsupervised manner, aiming to close the performance gap between unsupervised
and supervised models. Our approach improves the unsupervised PEGASUS by up to
7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted
summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%
from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on
a dataset, evaluating on another). </font><br> Link: <a href='http://arxiv.org/pdf/2212.09593v3' target="_blank">http://arxiv.org/pdf/2212.09593v3</a><br> <br> <br> <font size='5'> 818 </font> <div style="text-align: right"> 2022-12-19 12:40:13+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Million-scale Object Detection with Large Vision Model</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Over the past few years, there has been growing interest in developing a
broad, universal, and general-purpose computer vision system. Such a system
would have the potential to solve a wide range of vision tasks simultaneously,
without being restricted to a specific problem or data domain. This is crucial
for practical, real-world computer vision applications. In this study, we focus
on the million-scale multi-domain universal object detection problem, which
presents several challenges, including cross-dataset category label
duplication, label conflicts, and the need to handle hierarchical taxonomies.
Furthermore, there is an ongoing challenge in the field to find a
resource-efficient way to leverage large pre-trained vision models for
million-scale cross-dataset object detection. To address these challenges, we
introduce our approach to label handling, hierarchy-aware loss design, and
resource-efficient model training using a pre-trained large model. Our method
was ranked second in the object detection track of the Robust Vision Challenge
2022 (RVC 2022). We hope that our detailed study will serve as a useful
reference and alternative approach for similar problems in the computer vision
community. The code is available at https://github.com/linfeng93/Large-UniDet. </font><br> Link: <a href='http://arxiv.org/pdf/2212.09408v2' target="_blank">http://arxiv.org/pdf/2212.09408v2</a><br> <br> <br> <font size='5'> 819 </font> <div style="text-align: right"> 2022-12-19 08:15:16+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: ChatGPT: The End of Online Exam Integrity?</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This study evaluated the ability of ChatGPT, a recently developed artificial
intelligence (AI) agent, to perform high-level cognitive tasks and produce text
that is indistinguishable from human-generated text. This capacity raises
concerns about the potential use of ChatGPT as a tool for academic misconduct
in online exams. The study found that ChatGPT is capable of exhibiting critical
thinking skills and generating highly realistic text with minimal input, making
it a potential threat to the integrity of online exams, particularly in
tertiary education settings where such exams are becoming more prevalent.
Returning to invigilated and oral exams could form part of the solution, while
using advanced proctoring techniques and AI-text output detectors may be
effective in addressing this issue, they are not likely to be foolproof
solutions. Further research is needed to fully understand the implications of
large language models like ChatGPT and to devise strategies for combating the
risk of cheating using these tools. It is crucial for educators and
institutions to be aware of the possibility of ChatGPT being used for cheating
and to investigate measures to address it in order to maintain the fairness and
validity of online exams for all students. </font><br> Link: <a href='http://arxiv.org/pdf/2212.09292v1' target="_blank">http://arxiv.org/pdf/2212.09292v1</a><br> <br> <br> <font size='5'> 820 </font> <div style="text-align: right"> 2022-12-18 16:08:40+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Chatbots in a Botnet World</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Question-and-answer formats provide a novel experimental platform for
investigating cybersecurity questions. Unlike previous chatbots, the latest
ChatGPT model from OpenAI supports an advanced understanding of complex coding
questions. The research demonstrates thirteen coding tasks that generally
qualify as stages in the MITRE ATT&CK framework, ranging from credential access
to defense evasion. With varying success, the experimental prompts generate
examples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled
ransomware. The empirical results illustrate cases that support the broad gain
of functionality, including self-replication and self-modification, evasion,
and strategic understanding of complex cybersecurity goals. One surprising
feature of ChatGPT as a language-only model centers on its ability to spawn
coding approaches that yield images that obfuscate or embed executable
programming steps or links. </font><br> Link: <a href='http://arxiv.org/pdf/2212.11126v2' target="_blank">http://arxiv.org/pdf/2212.11126v2</a><br> <br> <br> <font size='5'> 821 </font> <div style="text-align: right"> 2022-12-13 23:06:20+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Paraphrase Identification with Deep Learning: A Review of Datasets and Methods</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The rapid advancement of AI technology has made text generation tools like
GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can
pose serious threat to the credibility of various forms of media if these
technologies are used for plagiarism, including scientific literature and news
sources. Despite the development of automated methods for paraphrase
identification, detecting this type of plagiarism remains a challenge due to
the disparate nature of the datasets on which these methods are trained. In
this study, we review traditional and current approaches to paraphrase
identification and propose a refined typology of paraphrases. We also
investigate how this typology is represented in popular datasets and how
under-representation of certain types of paraphrases impacts detection
capabilities. Finally, we outline new directions for future research and
datasets in the pursuit of more effective paraphrase detection using AI. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06933v1' target="_blank">http://arxiv.org/pdf/2212.06933v1</a><br> <br> <br> <font size='5'> 822 </font> <div style="text-align: right"> 2022-12-12 12:41:24+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: "I think this is the most disruptive technology": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users. </font><br> Link: <a href='http://arxiv.org/pdf/2212.05856v1' target="_blank">http://arxiv.org/pdf/2212.05856v1</a><br> <br> <br> <font size='5'> 823 </font> <div style="text-align: right"> 2022-12-09 16:32:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Turing Deception</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: This research revisits the classic Turing test and compares recent large
language models such as ChatGPT for their abilities to reproduce human-level
comprehension and compelling text generation. Two task challenges --
summarization, and question answering -- prompt ChatGPT to produce original
content (98-99%) from a single text entry and also sequential questions
originally posed by Turing in 1950. We score the original and generated content
against the OpenAI GPT-2 Output Detector from 2019, and establish multiple
cases where the generated content proves original and undetectable (98%). The
question of a machine fooling a human judge recedes in this work relative to
the question of "how would one prove it?" The original contribution of the work
presents a metric and simple grammatical set for understanding the writing
mechanics of chatbots in evaluating their readability and statistical clarity,
engagement, delivery, and overall quality. While Turing's original prose scores
at least 14% below the machine-generated output, the question of whether an
algorithm displays hints of Turing's truly original thoughts (the "Lovelace
2.0" test) remains unanswered and potentially unanswerable for now. </font><br> Link: <a href='http://arxiv.org/pdf/2212.06721v2' target="_blank">http://arxiv.org/pdf/2212.06721v2</a><br> <br> <br> <font size='5'> 824 </font> <div style="text-align: right"> 2022-12-08 23:23:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section. </font><br> Link: <a href='http://arxiv.org/pdf/2212.08104v1' target="_blank">http://arxiv.org/pdf/2212.08104v1</a><br> <br> <br> <font size='5'> 825 </font> <div style="text-align: right"> 2022-11-25 09:08:11+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI). </font><br> Link: <a href='http://arxiv.org/pdf/2211.13960v5' target="_blank">http://arxiv.org/pdf/2211.13960v5</a><br> <br> <br> <font size='5'> 826 </font> <div style="text-align: right"> 2022-11-21 09:41:25+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Self-supervised representation learning has proved to be a valuable component
for out-of-distribution (OoD) detection with only the texts of in-distribution
(ID) examples. These approaches either train a language model from scratch or
fine-tune a pre-trained language model using ID examples, and then take the
perplexity output by the language model as OoD scores. In this paper, we
analyze the complementary characteristics of both OoD detection methods and
propose a multi-level knowledge distillation approach that integrates their
strengths while mitigating their limitations. Specifically, we use a fine-tuned
model as the teacher to teach a randomly initialized student model on the ID
examples. Besides the prediction layer distillation, we present a
similarity-based intermediate layer distillation method to thoroughly explore
the representation space of the teacher model. In this way, the learned student
can better represent the ID data manifold while gaining a stronger ability to
map OoD examples outside the ID data manifold with the regularization inherited
from pre-training. Besides, the student model sees only ID examples during
parameter learning, further promoting more distinguishable features for OoD
detection. We conduct extensive experiments over multiple benchmark datasets,
i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the
proposed method yields new state-of-the-art performance. We also explore its
application as an AIGC detector to distinguish between answers generated by
ChatGPT and human experts. It is observed that our model exceeds human
evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus. </font><br> Link: <a href='http://arxiv.org/pdf/2211.11300v3' target="_blank">http://arxiv.org/pdf/2211.11300v3</a><br> <br> <br> <font size='5'> 827 </font> <div style="text-align: right"> 2022-11-13 10:16:39+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: What would Harry say? Building Dialogue Agents for Characters in a Story</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We have a Christmas gift for Harry Potter fans all over the world. In this
paper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry
Potter-like dialogue agents. Such a task is typically viewed as a variant of
personalized dialogue agents, but they differ significantly in three respects:
1) Harry lived in a virtual world of wizards, thus, real-world commonsense may
not apply to Harry's conversations; 2) Harry's behavior is strongly linked to
background information in conversations: the scene, its attributes and its
relationship to other speakers; and 3) Such backgrounds are dynamically altered
as the storyline goes on. The HPD dataset, as the first dataset to facilitate
the study of dialogue agent construction for characters within a story,
provides rich contextual information about each dialogue session such as
scenes, character attributes, and relations. More importantly, all the
background information will change over the course of the story. In addition,
HPD could support both dialogue generation and retrieval tasks. We evaluate
baselines such as Dialog-GPT and BOB to determine the extent to which they can
generate Harry Potter-like responses. The experimental results disappoint us in
that although the generated responses are fluent, they still seem out of
character for Harry. Besides, we validate the current most robust dialogue
agent, ChatGPT, which also can't generate plausible Harry-Potter-like responses
in some cases, either. Our results suggest that there is much scope for future
research. </font><br> Link: <a href='http://arxiv.org/pdf/2211.06869v3' target="_blank">http://arxiv.org/pdf/2211.06869v3</a><br> <br> <br> <font size='5'> 828 </font> <div style="text-align: right"> 2022-10-24 14:58:58+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial. In this
work, we examine the open question of social intelligence and Theory of Mind in
modern NLP systems from an empirical and theory-based perspective. We show that
one of today's largest language models (GPT-3; Brown et al., 2020) lacks this
kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et
al., 2019), which measures models' ability to understand intents and reactions
of participants of social interactions, and ToMi (Le et al., 2019), which
measures whether models can infer mental states and realities of participants
of situations. Our results show that models struggle substantially at these
Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on
SocialIQa and ToMi, respectively. To conclude, we draw on theories from
pragmatics to contextualize this shortcoming of large language models, by
examining the limitations stemming from their data, neural architecture, and
training paradigms. Challenging the prevalent narrative that only scale is
needed, we posit that person-centric NLP approaches might be more effective
towards neural Theory of Mind.
  In our updated version, we also analyze newer instruction tuned and RLFH
models for neural ToM. We find that even ChatGPT and GPT-4 do not display
emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on
the ToMi questions related to mental states and realities. </font><br> Link: <a href='http://arxiv.org/pdf/2210.13312v2' target="_blank">http://arxiv.org/pdf/2210.13312v2</a><br> <br> <br> <font size='5'> 829 </font> <div style="text-align: right"> 2022-10-13 19:46:14+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Machine generated text is increasingly difficult to distinguish from human
authored text. Powerful open-source models are freely available, and
user-friendly tools that democratize access to generative models are
proliferating. ChatGPT, which was released shortly after the first edition of
this survey, epitomizes these trends. The great potential of state-of-the-art
natural language generation (NLG) systems is tempered by the multitude of
avenues for abuse. Detection of machine generated text is a key countermeasure
for reducing abuse of NLG models, with significant technical challenges and
numerous open problems. We provide a survey that includes both 1) an extensive
analysis of threat models posed by contemporary NLG systems, and 2) the most
complete review of machine generated text detection methods to date. This
survey places machine generated text within its cybersecurity and social
context, and provides strong guidance for future work addressing the most
critical threat models, and ensuring detection systems themselves demonstrate
trustworthiness through fairness, robustness, and accountability. </font><br> Link: <a href='http://arxiv.org/pdf/2210.07321v4' target="_blank">http://arxiv.org/pdf/2210.07321v4</a><br> <br> <br> <font size='5'> 830 </font> <div style="text-align: right"> 2022-09-29 08:41:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Open-domain dialogue systems aim to interact with humans through natural
language texts in an open-ended fashion. Despite the recent success of super
large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue
systems remains the common practice as they are more lightweight and
accessible; however, generating diverse dialogue responses is challenging,
especially with smaller models. In this work, we propose an Equal-size Hard
Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model
for diverse dialogue generation. Our algorithm assigns a sample to a decoder in
a hard manner and additionally imposes an equal-assignment constraint to ensure
that all decoders are well-trained. We provide detailed theoretical analysis to
justify our approach. Further, experiments on two large-scale open-domain
dialogue datasets verify that our EqHard-EM algorithm generates high-quality
diverse responses. </font><br> Link: <a href='http://arxiv.org/pdf/2209.14627v2' target="_blank">http://arxiv.org/pdf/2209.14627v2</a><br> <br> <br> <font size='5'> 831 </font> <div style="text-align: right"> 2022-08-31 00:57:17+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: A Prescriptive Learning Analytics Framework: Beyond Predictive Modelling and onto Explainable AI with Prescriptive Analytics and ChatGPT</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A significant body of recent research in the field of Learning Analytics has
focused on leveraging machine learning approaches for predicting at-risk
students in order to initiate timely interventions and thereby elevate
retention and completion rates. The overarching feature of the majority of
these research studies has been on the science of prediction only. The
component of predictive analytics concerned with interpreting the internals of
the models and explaining their predictions for individual cases to
stakeholders has largely been neglected. Additionally, works that attempt to
employ data-driven prescriptive analytics to automatically generate
evidence-based remedial advice for at-risk learners are in their infancy.
  eXplainable AI is a field that has recently emerged providing cutting-edge
tools which support transparent predictive analytics and techniques for
generating tailored advice for at-risk students. This study proposes a novel
framework that unifies both transparent machine learning as well as techniques
for enabling prescriptive analytics, while integrating the latest advances in
large language models. This work practically demonstrates the proposed
framework using predictive models for identifying at-risk learners of programme
non-completion. The study then further demonstrates how predictive modelling
can be augmented with prescriptive analytics on two case studies in order to
generate human-readable prescriptive feedback for those who are at risk using
ChatGPT. </font><br> Link: <a href='http://arxiv.org/pdf/2208.14582v2' target="_blank">http://arxiv.org/pdf/2208.14582v2</a><br> <br> <br> <font size='5'> 832 </font> <div style="text-align: right"> 2022-08-18 17:54:49+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We introduce a new type of test, called a Turing Experiment (TE), for
evaluating to what extent a given language model, such as GPT models, can
simulate different aspects of human behavior. A TE can also reveal consistent
distortions in a language model's simulation of a specific human behavior.
Unlike the Turing Test, which involves simulating a single arbitrary
individual, a TE requires simulating a representative sample of participants in
human subject research. We carry out TEs that attempt to replicate
well-established findings from prior studies. We design a methodology for
simulating TEs and illustrate its use to compare how well different language
models are able to reproduce classic economic, psycholinguistic, and social
psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock
Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings
were replicated using recent models, while the last TE reveals a
"hyper-accuracy distortion" present in some language models (including ChatGPT
and GPT-4), which could affect downstream applications in education and the
arts. </font><br> Link: <a href='http://arxiv.org/pdf/2208.10264v5' target="_blank">http://arxiv.org/pdf/2208.10264v5</a><br> <br> <br> <font size='5'> 833 </font> <div style="text-align: right"> 2022-07-14 12:52:32+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Open Tracing Tools: Overview and Critical Comparison</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Background. Coping with the rapid growing complexity in contemporary software
architecture, tracing has become an increasingly critical practice and been
adopted widely by software engineers. By adopting tracing tools, practitioners
are able to monitor, debug, and optimize distributed software architectures
easily. However, with excessive number of valid candidates, researchers and
practitioners have a hard time finding and selecting the suitable tracing tools
by systematically considering their features and advantages.Objective. To such
a purpose, this paper aims to provide an overview of popular Open tracing tools
via comparison. Method. Herein, we first identified \ra{30} tools in an
objective, systematic, and reproducible manner adopting the Systematic
Multivocal Literature Review protocol. Then, we characterized each tool looking
at the 1) measured features, 2) popularity both in peer-reviewed literature and
online media, and 3) benefits and issues. We used topic modeling and sentiment
analysis to extract and summarize the benefits and issues. Specially, we
adopted ChatGPT to support the topic interpretation. Results. As a result, this
paper presents a systematic comparison amongst the selected tracing tools in
terms of their features, popularity, benefits and issues. Conclusion. The
result mainly shows that each tracing tool provides a unique combination of
features with also different pros and cons. The contribution of this paper is
to provide the practitioners better understanding of the tracing tools
facilitating their adoption. </font><br> Link: <a href='http://arxiv.org/pdf/2207.06875v2' target="_blank">http://arxiv.org/pdf/2207.06875v2</a><br> <br> <br> <font size='5'> 834 </font> <div style="text-align: right"> 2022-06-11 06:38:06+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: A final exam in machine learning at a top institution such as MIT, Harvard,
or Cornell typically takes faculty days to write, and students hours to solve.
We demonstrate that large language models pass machine learning finals at a
human level, on finals available online after the models were trained, and
automatically generate new human-quality final exam questions in seconds.
Previous work has developed program synthesis and few-shot learning methods to
solve university-level problem set questions in mathematics and STEM courses.
In this work, we develop and compare methods that solve final exams, which
differ from problem sets in several ways: the questions are longer, have
multiple parts, are more complicated, and span a broader set of topics. We
curate a dataset and benchmark of questions from machine learning final exams
available online and code for answering these questions and generating new
questions. We show how to generate new questions from other questions and
course notes. For reproducibility and future research on this final exam
benchmark, we use automatic checkers for multiple-choice, numeric, and
questions with expression answers. We perform ablation studies comparing
zero-shot learning with few-shot learning and chain-of-thought prompting using
GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that
few-shot learning methods perform best. We highlight the transformative
potential of language models to streamline the writing and solution of
large-scale assessments, significantly reducing the workload from human days to
mere machine seconds. Our results suggest that rather than banning large
language models such as ChatGPT in class, instructors should teach students to
harness them by asking students meta-questions about correctness, completeness,
and originality of the responses generated, encouraging critical thinking in
academic studies. </font><br> Link: <a href='http://arxiv.org/pdf/2206.05442v7' target="_blank">http://arxiv.org/pdf/2206.05442v7</a><br> <br> <br> <font size='5'> 835 </font> <div style="text-align: right"> 2022-05-25 15:26:48+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: The propensity of abstractive summarization models to make factual errors has
been studied extensively, including design of metrics to detect factual errors
and annotation of errors in current systems' outputs. However, the
ever-evolving nature of summarization systems, metrics, and annotated
benchmarks makes factuality evaluation a moving target, and drawing clear
comparisons among metrics has become increasingly difficult. In this work, we
aggregate factuality error annotations from nine existing datasets and stratify
them according to the underlying summarization model. We compare performance of
state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on
this stratified benchmark and show that their performance varies significantly
across different types of summarization models. Critically, our analysis shows
that much of the recent improvement in the factuality detection space has been
on summaries from older (pre-Transformer) models instead of more relevant
recent summarization models. We further perform a finer-grained analysis per
error-type and find similar performance variance across error types for
different factuality metrics. Our results show that no one metric is superior
in all settings or for all error types, and we provide recommendations for best
practices given these insights. </font><br> Link: <a href='http://arxiv.org/pdf/2205.12854v2' target="_blank">http://arxiv.org/pdf/2205.12854v2</a><br> <br> <br> <font size='5'> 836 </font> <div style="text-align: right"> 2021-12-07 18:01:35+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: EmTract: Extracting Emotions from Social Media</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: We develop an open-source tool (EmTract) that extracts emotions from social
media text tailed for financial context. To do so, we annotate ten thousand
short messages from a financial social media platform (StockTwits) and combine
it with open-source emotion data. We then use a pre-tuned NLP model,
DistilBERT, augment its embedding space by including 4,861 tokens (emojis and
emoticons), and then fit it first on the open-source emotion data, then
transfer it to our annotated financial social media data. Our model outperforms
competing open-source state-of-the-art emotion classifiers, such as Emotion
English DistilRoBERTa-base on both human and chatGPT annotated data. Compared
to dictionary based methods, our methodology has three main advantages for
research in finance. First, our model is tailored to financial social media
text; second, it incorporates key aspects of social media data, such as
non-standard phrases, emojis, and emoticons; and third, it operates by
sequentially learning a latent representation that includes features such as
word order, word usage, and local context. Using EmTract, we explore the
relationship between investor emotions expressed on social media and asset
prices. We show that firm-specific investor emotions are predictive of daily
price movements. Our findings show that emotions and market dynamics are
closely related, and we provide a tool to help study the role emotions play in
financial markets. </font><br> Link: <a href='http://arxiv.org/pdf/2112.03868v3' target="_blank">http://arxiv.org/pdf/2112.03868v3</a><br> <br> <br> <font size='5'> 837 </font> <div style="text-align: right"> 2021-09-16 16:10:05+00:00 </div><hr style='border-style: dotted;' /> <b> <font size='5'> Title: Context-NER : Contextual Phrase Generation at Scale</b> </font><hr style='border-style: dotted;' /> <br> <font size='3'> Summary: Named Entity Recognition (NER) has seen significant progress in recent years,
with numerous state-of-the-art (SOTA) models achieving high performance.
However, very few studies have focused on the generation of entities' context.
In this paper, we introduce CONTEXT-NER, a task that aims to generate the
relevant context for entities in a sentence, where the context is a phrase
describing the entity but not necessarily present in the sentence. To
facilitate research in this task, we also present the EDGAR10-Q dataset, which
consists of annual and quarterly reports from the top 1500 publicly traded
companies. The dataset is the largest of its kind, containing 1M sentences,
2.8M entities, and an average of 35 tokens per sentence, making it a
challenging dataset. We propose a baseline approach that combines a phrase
generation algorithm with inferencing using a 220M language model, achieving a
ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot
inference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the
difficulty of the dataset. We also evaluate models such as T5 and BART, which
achieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We
also find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results
on downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming
vanilla version by 10.81 points. To our surprise, this 66x smaller
pre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by
15 points. We hope that our dataset and generated artifacts will encourage
further research in this direction, leading to the development of more
sophisticated language models for financial text analysis </font><br> Link: <a href='http://arxiv.org/pdf/2109.08079v4' target="_blank">http://arxiv.org/pdf/2109.08079v4</a>
    </body>
    </html>